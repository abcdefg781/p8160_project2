---
title: "newton_method"
author: "Jungang Zou"
date: "3/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(glmnet)
library(matlib)
library(glmnet)
```

# Loading the Loglike, gradient, hess
```{r}
# this function is to use the function in .R file
try(source("./loglike_grad_hess_func.R"), silent = TRUE)
```



# Newton Method

input:
x: predictors without intercept
y: response variables
beta: if not specified, 0 will be set to all coefficients
tol: the threshold to end up the function if the difference between loglike function at 2 adjacent steps below this value.
lambda_init: the initial lambda to control the number of each step and lambda will change in halving process.
decay_rate: the ratio of decayed lambda to lambda at last step in havling process.

output:
beta: a vector of coeffients

```{r}
newton_optimize = function(x, y, beta = NULL, tol = 0.001, lambda_init = 1, decay_rate = 0.5){
  
  # add the intercept
  x = cbind(rep(1, nrow(x)), x)
  
  # if beta is not specified, set all initial coefficients to 0
  if (is.null(beta))
    beta = matrix(rep(0, ncol(x)))
  
  # calculate the initial gradient, Hessian matrix and negative loglike funtion
  optimization = func(x, y, beta)
  step = 1
  previous_loglik = -optimization$loglik

  # start the interations to optimize the beta
  while (TRUE) {
    print(paste("step:", step, "  negative loglike loss:", -optimization$loglik))
   
    # set initial lambda at this step equals to the parameters, this variable will change in havling step
    lambda = lambda_init
    
    # since there maybe some issues when calculate new beta, so we use try-catch sentence. If some errors ocurr, the beta will be kept as the beta at last step.
    beta_new <- tryCatch({
        beta - lambda * inv(optimization$Hess) %*% optimization$grad # calculate new beta, if no errors, the result will be given to variable "beta_new" 
      }, error = function(err) {return(beta)})

    
    # calculate gradient, Hessian and loglike   
    optimization = func(x, y, beta_new)
   
    
    # havling steps start only when it optimizes at opposite direction.
    # if it optimizes at opposite direction, lambda will be havled to make the step smaller. 
    while (previous_loglik <= -optimization$loglik) {
      lambda = lambda * decay_rate # lambda decay
      
      # same reason to use try-catch
      # but if errors occur, although beta keeps, the lambda will be havled at next step, makes the result different.
      beta_new <- tryCatch({
        beta - lambda * inv(optimization$Hess) %*% optimization$grad
      }, error = function(err) {return(beta)})
      
      # optimize by decayed lambda
      optimization = func(x, y, beta_new)
      
      # if the optimized differences are too small, end up the function and return beta. 
      if ((previous_loglik - -optimization$loglik) <= tol)
        return(beta)
    }
    
    # if the differences calculated from normal calculation or havling steps are too small, end up the function and return beta. 
    if (abs(previous_loglik - -optimization$loglik) <= tol)
      return(beta)
    
    # save the negative loglike value at this step and will be used as previous loglike value at next step.
    previous_loglik = -optimization$loglik
    
    # if the function is not ended up, then the new beta is valid. save it.
    beta = beta_new 
    
    step = step + 1
  }
  
  # so the loop will be ended up by 2 conditions.
  # 1. the differences calculated by havling steps are too small.
  # 2. the differences calculated by normal optimization are too small.
  return(beta)
}


```

# Prediction

```{r}
predict = function(beta, x){
  x = cbind(rep(1, nrow(x)), x)
  predict_y = 1 / (1 + exp(-x %*% beta))
  predict_y[predict_y < 0.5] = 0 
  predict_y[predict_y >= 0.5] = 1
  return(predict_y)
}
  

```

# Loading the data and run function

We will add the intercept in function "newton_optimize" to keep the original data.

```{r}
# read data
cancer_data = read.csv("./breast-cancer.csv")

# make the predictors, and the intercept will be added in function "newton_optimize" instead of here.
x = cancer_data %>% select(-id, -diagnosis) %>% as.matrix()

# make the response variables
y = cancer_data %>% 
  select(diagnosis) %>% 
  mutate(diagnosis = as.integer(diagnosis) - 1) %>% 
  as.matrix()


# calculate beta_hat by newton method 
beta = newton_optimize(x, y, tol = 0.01)

# predict
predict(beta, x)

# check the data in glm
model = glm(y ~ x, family = "binomial")
model$coefficients
```

# Training and Tests with CV

```{r}
cv2 = function(data, k = 10) {
  # Create folds for N-fold cv
  flds = createFolds(data$diagnosis, k = k, list = T)
  
  # Initialize RMSE
  rmse = rep(NA, k - 1)
  
  # Loop through each fold
  for (fold in 1:(length(flds) - 1)) {
    # Split training and test
    test = data[flds[[fold]],]
    train = data[-flds[[fold]],]
    
    # Prepare data for logistic regression
    x_train = train %>% select(-id, -diagnosis) %>% as.matrix()
    y_train = train %>% 
      select(diagnosis) %>% 
      mutate(diagnosis = as.integer(as.factor(diagnosis)) - 1) %>% 
      as.matrix()
    x_test = test %>% select(-id, -diagnosis) %>% as.matrix()
    y_test = test %>% 
      select(diagnosis) %>% 
      mutate(diagnosis = as.integer(as.factor(diagnosis)) - 1) %>% 
      as.matrix()
    
    # Train model
    train_model = newton_optimize(x_train, y_train, tol = 0.01)
    predict_result = predict(train_model, x_test)
    
    # Calculate RMSE
    error = predict_result[[1]] - y_test
    rmse[fold] = sqrt(mean(error^2))
  }
  return(rmse)
}

set.seed(12345)
rmse = mean(cv2(data, k = 10))
# RMSE is 0.7918
```

