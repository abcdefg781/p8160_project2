<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Adeline Shin" />


<title>test</title>

<script>/*! jQuery v1.11.3 | (c) 2005, 2015 jQuery Foundation, Inc. | jquery.org/license */
!function(a,b){"object"==typeof module&&"object"==typeof module.exports?module.exports=a.document?b(a,!0):function(a){if(!a.document)throw new Error("jQuery requires a window with a document");return b(a)}:b(a)}("undefined"!=typeof window?window:this,function(a,b){var c=[],d=c.slice,e=c.concat,f=c.push,g=c.indexOf,h={},i=h.toString,j=h.hasOwnProperty,k={},l="1.11.3",m=function(a,b){return new m.fn.init(a,b)},n=/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,o=/^-ms-/,p=/-([\da-z])/gi,q=function(a,b){return b.toUpperCase()};m.fn=m.prototype={jquery:l,constructor:m,selector:"",length:0,toArray:function(){return d.call(this)},get:function(a){return null!=a?0>a?this[a+this.length]:this[a]:d.call(this)},pushStack:function(a){var b=m.merge(this.constructor(),a);return b.prevObject=this,b.context=this.context,b},each:function(a,b){return m.each(this,a,b)},map:function(a){return this.pushStack(m.map(this,function(b,c){return a.call(b,c,b)}))},slice:function(){return this.pushStack(d.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},eq:function(a){var b=this.length,c=+a+(0>a?b:0);return this.pushStack(c>=0&&b>c?[this[c]]:[])},end:function(){return this.prevObject||this.constructor(null)},push:f,sort:c.sort,splice:c.splice},m.extend=m.fn.extend=function(){var a,b,c,d,e,f,g=arguments[0]||{},h=1,i=arguments.length,j=!1;for("boolean"==typeof g&&(j=g,g=arguments[h]||{},h++),"object"==typeof g||m.isFunction(g)||(g={}),h===i&&(g=this,h--);i>h;h++)if(null!=(e=arguments[h]))for(d in e)a=g[d],c=e[d],g!==c&&(j&&c&&(m.isPlainObject(c)||(b=m.isArray(c)))?(b?(b=!1,f=a&&m.isArray(a)?a:[]):f=a&&m.isPlainObject(a)?a:{},g[d]=m.extend(j,f,c)):void 0!==c&&(g[d]=c));return g},m.extend({expando:"jQuery"+(l+Math.random()).replace(/\D/g,""),isReady:!0,error:function(a){throw new Error(a)},noop:function(){},isFunction:function(a){return"function"===m.type(a)},isArray:Array.isArray||function(a){return"array"===m.type(a)},isWindow:function(a){return null!=a&&a==a.window},isNumeric:function(a){return!m.isArray(a)&&a-parseFloat(a)+1>=0},isEmptyObject:function(a){var b;for(b in a)return!1;return!0},isPlainObject:function(a){var b;if(!a||"object"!==m.type(a)||a.nodeType||m.isWindow(a))return!1;try{if(a.constructor&&!j.call(a,"constructor")&&!j.call(a.constructor.prototype,"isPrototypeOf"))return!1}catch(c){return!1}if(k.ownLast)for(b in a)return j.call(a,b);for(b in a);return void 0===b||j.call(a,b)},type:function(a){return null==a?a+"":"object"==typeof a||"function"==typeof a?h[i.call(a)]||"object":typeof a},globalEval:function(b){b&&m.trim(b)&&(a.execScript||function(b){a.eval.call(a,b)})(b)},camelCase:function(a){return a.replace(o,"ms-").replace(p,q)},nodeName:function(a,b){return a.nodeName&&a.nodeName.toLowerCase()===b.toLowerCase()},each:function(a,b,c){var d,e=0,f=a.length,g=r(a);if(c){if(g){for(;f>e;e++)if(d=b.apply(a[e],c),d===!1)break}else for(e in a)if(d=b.apply(a[e],c),d===!1)break}else if(g){for(;f>e;e++)if(d=b.call(a[e],e,a[e]),d===!1)break}else for(e in a)if(d=b.call(a[e],e,a[e]),d===!1)break;return a},trim:function(a){return null==a?"":(a+"").replace(n,"")},makeArray:function(a,b){var c=b||[];return null!=a&&(r(Object(a))?m.merge(c,"string"==typeof a?[a]:a):f.call(c,a)),c},inArray:function(a,b,c){var d;if(b){if(g)return g.call(b,a,c);for(d=b.length,c=c?0>c?Math.max(0,d+c):c:0;d>c;c++)if(c in b&&b[c]===a)return c}return-1},merge:function(a,b){var c=+b.length,d=0,e=a.length;while(c>d)a[e++]=b[d++];if(c!==c)while(void 0!==b[d])a[e++]=b[d++];return a.length=e,a},grep:function(a,b,c){for(var d,e=[],f=0,g=a.length,h=!c;g>f;f++)d=!b(a[f],f),d!==h&&e.push(a[f]);return e},map:function(a,b,c){var d,f=0,g=a.length,h=r(a),i=[];if(h)for(;g>f;f++)d=b(a[f],f,c),null!=d&&i.push(d);else for(f in a)d=b(a[f],f,c),null!=d&&i.push(d);return e.apply([],i)},guid:1,proxy:function(a,b){var c,e,f;return"string"==typeof b&&(f=a[b],b=a,a=f),m.isFunction(a)?(c=d.call(arguments,2),e=function(){return a.apply(b||this,c.concat(d.call(arguments)))},e.guid=a.guid=a.guid||m.guid++,e):void 0},now:function(){return+new Date},support:k}),m.each("Boolean Number String Function Array Date RegExp Object Error".split(" "),function(a,b){h["[object "+b+"]"]=b.toLowerCase()});function r(a){var b="length"in a&&a.length,c=m.type(a);return"function"===c||m.isWindow(a)?!1:1===a.nodeType&&b?!0:"array"===c||0===b||"number"==typeof b&&b>0&&b-1 in a}var s=function(a){var b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u="sizzle"+1*new Date,v=a.document,w=0,x=0,y=ha(),z=ha(),A=ha(),B=function(a,b){return a===b&&(l=!0),0},C=1<<31,D={}.hasOwnProperty,E=[],F=E.pop,G=E.push,H=E.push,I=E.slice,J=function(a,b){for(var c=0,d=a.length;d>c;c++)if(a[c]===b)return c;return-1},K="checked|selected|async|autofocus|autoplay|controls|defer|disabled|hidden|ismap|loop|multiple|open|readonly|required|scoped",L="[\\x20\\t\\r\\n\\f]",M="(?:\\\\.|[\\w-]|[^\\x00-\\xa0])+",N=M.replace("w","w#"),O="\\["+L+"*("+M+")(?:"+L+"*([*^$|!~]?=)"+L+"*(?:'((?:\\\\.|[^\\\\'])*)'|\"((?:\\\\.|[^\\\\\"])*)\"|("+N+"))|)"+L+"*\\]",P=":("+M+")(?:\\((('((?:\\\\.|[^\\\\'])*)'|\"((?:\\\\.|[^\\\\\"])*)\")|((?:\\\\.|[^\\\\()[\\]]|"+O+")*)|.*)\\)|)",Q=new RegExp(L+"+","g"),R=new RegExp("^"+L+"+|((?:^|[^\\\\])(?:\\\\.)*)"+L+"+$","g"),S=new RegExp("^"+L+"*,"+L+"*"),T=new RegExp("^"+L+"*([>+~]|"+L+")"+L+"*"),U=new RegExp("="+L+"*([^\\]'\"]*?)"+L+"*\\]","g"),V=new RegExp(P),W=new RegExp("^"+N+"$"),X={ID:new RegExp("^#("+M+")"),CLASS:new RegExp("^\\.("+M+")"),TAG:new RegExp("^("+M.replace("w","w*")+")"),ATTR:new RegExp("^"+O),PSEUDO:new RegExp("^"+P),CHILD:new RegExp("^:(only|first|last|nth|nth-last)-(child|of-type)(?:\\("+L+"*(even|odd|(([+-]|)(\\d*)n|)"+L+"*(?:([+-]|)"+L+"*(\\d+)|))"+L+"*\\)|)","i"),bool:new RegExp("^(?:"+K+")$","i"),needsContext:new RegExp("^"+L+"*[>+~]|:(even|odd|eq|gt|lt|nth|first|last)(?:\\("+L+"*((?:-\\d)?\\d*)"+L+"*\\)|)(?=[^-]|$)","i")},Y=/^(?:input|select|textarea|button)$/i,Z=/^h\d$/i,$=/^[^{]+\{\s*\[native \w/,_=/^(?:#([\w-]+)|(\w+)|\.([\w-]+))$/,aa=/[+~]/,ba=/'|\\/g,ca=new RegExp("\\\\([\\da-f]{1,6}"+L+"?|("+L+")|.)","ig"),da=function(a,b,c){var d="0x"+b-65536;return d!==d||c?b:0>d?String.fromCharCode(d+65536):String.fromCharCode(d>>10|55296,1023&d|56320)},ea=function(){m()};try{H.apply(E=I.call(v.childNodes),v.childNodes),E[v.childNodes.length].nodeType}catch(fa){H={apply:E.length?function(a,b){G.apply(a,I.call(b))}:function(a,b){var c=a.length,d=0;while(a[c++]=b[d++]);a.length=c-1}}}function ga(a,b,d,e){var f,h,j,k,l,o,r,s,w,x;if((b?b.ownerDocument||b:v)!==n&&m(b),b=b||n,d=d||[],k=b.nodeType,"string"!=typeof a||!a||1!==k&&9!==k&&11!==k)return d;if(!e&&p){if(11!==k&&(f=_.exec(a)))if(j=f[1]){if(9===k){if(h=b.getElementById(j),!h||!h.parentNode)return d;if(h.id===j)return d.push(h),d}else if(b.ownerDocument&&(h=b.ownerDocument.getElementById(j))&&t(b,h)&&h.id===j)return d.push(h),d}else{if(f[2])return H.apply(d,b.getElementsByTagName(a)),d;if((j=f[3])&&c.getElementsByClassName)return H.apply(d,b.getElementsByClassName(j)),d}if(c.qsa&&(!q||!q.test(a))){if(s=r=u,w=b,x=1!==k&&a,1===k&&"object"!==b.nodeName.toLowerCase()){o=g(a),(r=b.getAttribute("id"))?s=r.replace(ba,"\\$&"):b.setAttribute("id",s),s="[id='"+s+"'] ",l=o.length;while(l--)o[l]=s+ra(o[l]);w=aa.test(a)&&pa(b.parentNode)||b,x=o.join(",")}if(x)try{return H.apply(d,w.querySelectorAll(x)),d}catch(y){}finally{r||b.removeAttribute("id")}}}return i(a.replace(R,"$1"),b,d,e)}function ha(){var a=[];function b(c,e){return a.push(c+" ")>d.cacheLength&&delete b[a.shift()],b[c+" "]=e}return b}function ia(a){return a[u]=!0,a}function ja(a){var b=n.createElement("div");try{return!!a(b)}catch(c){return!1}finally{b.parentNode&&b.parentNode.removeChild(b),b=null}}function ka(a,b){var c=a.split("|"),e=a.length;while(e--)d.attrHandle[c[e]]=b}function la(a,b){var c=b&&a,d=c&&1===a.nodeType&&1===b.nodeType&&(~b.sourceIndex||C)-(~a.sourceIndex||C);if(d)return d;if(c)while(c=c.nextSibling)if(c===b)return-1;return a?1:-1}function ma(a){return function(b){var c=b.nodeName.toLowerCase();return"input"===c&&b.type===a}}function na(a){return function(b){var c=b.nodeName.toLowerCase();return("input"===c||"button"===c)&&b.type===a}}function oa(a){return ia(function(b){return b=+b,ia(function(c,d){var e,f=a([],c.length,b),g=f.length;while(g--)c[e=f[g]]&&(c[e]=!(d[e]=c[e]))})})}function pa(a){return a&&"undefined"!=typeof a.getElementsByTagName&&a}c=ga.support={},f=ga.isXML=function(a){var b=a&&(a.ownerDocument||a).documentElement;return b?"HTML"!==b.nodeName:!1},m=ga.setDocument=function(a){var b,e,g=a?a.ownerDocument||a:v;return g!==n&&9===g.nodeType&&g.documentElement?(n=g,o=g.documentElement,e=g.defaultView,e&&e!==e.top&&(e.addEventListener?e.addEventListener("unload",ea,!1):e.attachEvent&&e.attachEvent("onunload",ea)),p=!f(g),c.attributes=ja(function(a){return a.className="i",!a.getAttribute("className")}),c.getElementsByTagName=ja(function(a){return a.appendChild(g.createComment("")),!a.getElementsByTagName("*").length}),c.getElementsByClassName=$.test(g.getElementsByClassName),c.getById=ja(function(a){return o.appendChild(a).id=u,!g.getElementsByName||!g.getElementsByName(u).length}),c.getById?(d.find.ID=function(a,b){if("undefined"!=typeof b.getElementById&&p){var c=b.getElementById(a);return c&&c.parentNode?[c]:[]}},d.filter.ID=function(a){var b=a.replace(ca,da);return function(a){return a.getAttribute("id")===b}}):(delete d.find.ID,d.filter.ID=function(a){var b=a.replace(ca,da);return function(a){var c="undefined"!=typeof a.getAttributeNode&&a.getAttributeNode("id");return c&&c.value===b}}),d.find.TAG=c.getElementsByTagName?function(a,b){return"undefined"!=typeof b.getElementsByTagName?b.getElementsByTagName(a):c.qsa?b.querySelectorAll(a):void 0}:function(a,b){var c,d=[],e=0,f=b.getElementsByTagName(a);if("*"===a){while(c=f[e++])1===c.nodeType&&d.push(c);return d}return f},d.find.CLASS=c.getElementsByClassName&&function(a,b){return p?b.getElementsByClassName(a):void 0},r=[],q=[],(c.qsa=$.test(g.querySelectorAll))&&(ja(function(a){o.appendChild(a).innerHTML="<a id='"+u+"'></a><select id='"+u+"-\f]' msallowcapture=''><option selected=''></option></select>",a.querySelectorAll("[msallowcapture^='']").length&&q.push("[*^$]="+L+"*(?:''|\"\")"),a.querySelectorAll("[selected]").length||q.push("\\["+L+"*(?:value|"+K+")"),a.querySelectorAll("[id~="+u+"-]").length||q.push("~="),a.querySelectorAll(":checked").length||q.push(":checked"),a.querySelectorAll("a#"+u+"+*").length||q.push(".#.+[+~]")}),ja(function(a){var b=g.createElement("input");b.setAttribute("type","hidden"),a.appendChild(b).setAttribute("name","D"),a.querySelectorAll("[name=d]").length&&q.push("name"+L+"*[*^$|!~]?="),a.querySelectorAll(":enabled").length||q.push(":enabled",":disabled"),a.querySelectorAll("*,:x"),q.push(",.*:")})),(c.matchesSelector=$.test(s=o.matches||o.webkitMatchesSelector||o.mozMatchesSelector||o.oMatchesSelector||o.msMatchesSelector))&&ja(function(a){c.disconnectedMatch=s.call(a,"div"),s.call(a,"[s!='']:x"),r.push("!=",P)}),q=q.length&&new RegExp(q.join("|")),r=r.length&&new RegExp(r.join("|")),b=$.test(o.compareDocumentPosition),t=b||$.test(o.contains)?function(a,b){var c=9===a.nodeType?a.documentElement:a,d=b&&b.parentNode;return a===d||!(!d||1!==d.nodeType||!(c.contains?c.contains(d):a.compareDocumentPosition&&16&a.compareDocumentPosition(d)))}:function(a,b){if(b)while(b=b.parentNode)if(b===a)return!0;return!1},B=b?function(a,b){if(a===b)return l=!0,0;var d=!a.compareDocumentPosition-!b.compareDocumentPosition;return d?d:(d=(a.ownerDocument||a)===(b.ownerDocument||b)?a.compareDocumentPosition(b):1,1&d||!c.sortDetached&&b.compareDocumentPosition(a)===d?a===g||a.ownerDocument===v&&t(v,a)?-1:b===g||b.ownerDocument===v&&t(v,b)?1:k?J(k,a)-J(k,b):0:4&d?-1:1)}:function(a,b){if(a===b)return l=!0,0;var c,d=0,e=a.parentNode,f=b.parentNode,h=[a],i=[b];if(!e||!f)return a===g?-1:b===g?1:e?-1:f?1:k?J(k,a)-J(k,b):0;if(e===f)return la(a,b);c=a;while(c=c.parentNode)h.unshift(c);c=b;while(c=c.parentNode)i.unshift(c);while(h[d]===i[d])d++;return d?la(h[d],i[d]):h[d]===v?-1:i[d]===v?1:0},g):n},ga.matches=function(a,b){return ga(a,null,null,b)},ga.matchesSelector=function(a,b){if((a.ownerDocument||a)!==n&&m(a),b=b.replace(U,"='$1']"),!(!c.matchesSelector||!p||r&&r.test(b)||q&&q.test(b)))try{var d=s.call(a,b);if(d||c.disconnectedMatch||a.document&&11!==a.document.nodeType)return d}catch(e){}return ga(b,n,null,[a]).length>0},ga.contains=function(a,b){return(a.ownerDocument||a)!==n&&m(a),t(a,b)},ga.attr=function(a,b){(a.ownerDocument||a)!==n&&m(a);var e=d.attrHandle[b.toLowerCase()],f=e&&D.call(d.attrHandle,b.toLowerCase())?e(a,b,!p):void 0;return void 0!==f?f:c.attributes||!p?a.getAttribute(b):(f=a.getAttributeNode(b))&&f.specified?f.value:null},ga.error=function(a){throw new Error("Syntax error, unrecognized expression: "+a)},ga.uniqueSort=function(a){var b,d=[],e=0,f=0;if(l=!c.detectDuplicates,k=!c.sortStable&&a.slice(0),a.sort(B),l){while(b=a[f++])b===a[f]&&(e=d.push(f));while(e--)a.splice(d[e],1)}return k=null,a},e=ga.getText=function(a){var b,c="",d=0,f=a.nodeType;if(f){if(1===f||9===f||11===f){if("string"==typeof a.textContent)return a.textContent;for(a=a.firstChild;a;a=a.nextSibling)c+=e(a)}else if(3===f||4===f)return a.nodeValue}else while(b=a[d++])c+=e(b);return c},d=ga.selectors={cacheLength:50,createPseudo:ia,match:X,attrHandle:{},find:{},relative:{">":{dir:"parentNode",first:!0}," ":{dir:"parentNode"},"+":{dir:"previousSibling",first:!0},"~":{dir:"previousSibling"}},preFilter:{ATTR:function(a){return a[1]=a[1].replace(ca,da),a[3]=(a[3]||a[4]||a[5]||"").replace(ca,da),"~="===a[2]&&(a[3]=" "+a[3]+" "),a.slice(0,4)},CHILD:function(a){return a[1]=a[1].toLowerCase(),"nth"===a[1].slice(0,3)?(a[3]||ga.error(a[0]),a[4]=+(a[4]?a[5]+(a[6]||1):2*("even"===a[3]||"odd"===a[3])),a[5]=+(a[7]+a[8]||"odd"===a[3])):a[3]&&ga.error(a[0]),a},PSEUDO:function(a){var b,c=!a[6]&&a[2];return X.CHILD.test(a[0])?null:(a[3]?a[2]=a[4]||a[5]||"":c&&V.test(c)&&(b=g(c,!0))&&(b=c.indexOf(")",c.length-b)-c.length)&&(a[0]=a[0].slice(0,b),a[2]=c.slice(0,b)),a.slice(0,3))}},filter:{TAG:function(a){var b=a.replace(ca,da).toLowerCase();return"*"===a?function(){return!0}:function(a){return a.nodeName&&a.nodeName.toLowerCase()===b}},CLASS:function(a){var b=y[a+" "];return b||(b=new RegExp("(^|"+L+")"+a+"("+L+"|$)"))&&y(a,function(a){return b.test("string"==typeof a.className&&a.className||"undefined"!=typeof a.getAttribute&&a.getAttribute("class")||"")})},ATTR:function(a,b,c){return function(d){var e=ga.attr(d,a);return null==e?"!="===b:b?(e+="","="===b?e===c:"!="===b?e!==c:"^="===b?c&&0===e.indexOf(c):"*="===b?c&&e.indexOf(c)>-1:"$="===b?c&&e.slice(-c.length)===c:"~="===b?(" "+e.replace(Q," ")+" ").indexOf(c)>-1:"|="===b?e===c||e.slice(0,c.length+1)===c+"-":!1):!0}},CHILD:function(a,b,c,d,e){var f="nth"!==a.slice(0,3),g="last"!==a.slice(-4),h="of-type"===b;return 1===d&&0===e?function(a){return!!a.parentNode}:function(b,c,i){var j,k,l,m,n,o,p=f!==g?"nextSibling":"previousSibling",q=b.parentNode,r=h&&b.nodeName.toLowerCase(),s=!i&&!h;if(q){if(f){while(p){l=b;while(l=l[p])if(h?l.nodeName.toLowerCase()===r:1===l.nodeType)return!1;o=p="only"===a&&!o&&"nextSibling"}return!0}if(o=[g?q.firstChild:q.lastChild],g&&s){k=q[u]||(q[u]={}),j=k[a]||[],n=j[0]===w&&j[1],m=j[0]===w&&j[2],l=n&&q.childNodes[n];while(l=++n&&l&&l[p]||(m=n=0)||o.pop())if(1===l.nodeType&&++m&&l===b){k[a]=[w,n,m];break}}else if(s&&(j=(b[u]||(b[u]={}))[a])&&j[0]===w)m=j[1];else while(l=++n&&l&&l[p]||(m=n=0)||o.pop())if((h?l.nodeName.toLowerCase()===r:1===l.nodeType)&&++m&&(s&&((l[u]||(l[u]={}))[a]=[w,m]),l===b))break;return m-=e,m===d||m%d===0&&m/d>=0}}},PSEUDO:function(a,b){var c,e=d.pseudos[a]||d.setFilters[a.toLowerCase()]||ga.error("unsupported pseudo: "+a);return e[u]?e(b):e.length>1?(c=[a,a,"",b],d.setFilters.hasOwnProperty(a.toLowerCase())?ia(function(a,c){var d,f=e(a,b),g=f.length;while(g--)d=J(a,f[g]),a[d]=!(c[d]=f[g])}):function(a){return e(a,0,c)}):e}},pseudos:{not:ia(function(a){var b=[],c=[],d=h(a.replace(R,"$1"));return d[u]?ia(function(a,b,c,e){var f,g=d(a,null,e,[]),h=a.length;while(h--)(f=g[h])&&(a[h]=!(b[h]=f))}):function(a,e,f){return b[0]=a,d(b,null,f,c),b[0]=null,!c.pop()}}),has:ia(function(a){return function(b){return ga(a,b).length>0}}),contains:ia(function(a){return a=a.replace(ca,da),function(b){return(b.textContent||b.innerText||e(b)).indexOf(a)>-1}}),lang:ia(function(a){return W.test(a||"")||ga.error("unsupported lang: "+a),a=a.replace(ca,da).toLowerCase(),function(b){var c;do if(c=p?b.lang:b.getAttribute("xml:lang")||b.getAttribute("lang"))return c=c.toLowerCase(),c===a||0===c.indexOf(a+"-");while((b=b.parentNode)&&1===b.nodeType);return!1}}),target:function(b){var c=a.location&&a.location.hash;return c&&c.slice(1)===b.id},root:function(a){return a===o},focus:function(a){return a===n.activeElement&&(!n.hasFocus||n.hasFocus())&&!!(a.type||a.href||~a.tabIndex)},enabled:function(a){return a.disabled===!1},disabled:function(a){return a.disabled===!0},checked:function(a){var b=a.nodeName.toLowerCase();return"input"===b&&!!a.checked||"option"===b&&!!a.selected},selected:function(a){return a.parentNode&&a.parentNode.selectedIndex,a.selected===!0},empty:function(a){for(a=a.firstChild;a;a=a.nextSibling)if(a.nodeType<6)return!1;return!0},parent:function(a){return!d.pseudos.empty(a)},header:function(a){return Z.test(a.nodeName)},input:function(a){return Y.test(a.nodeName)},button:function(a){var b=a.nodeName.toLowerCase();return"input"===b&&"button"===a.type||"button"===b},text:function(a){var b;return"input"===a.nodeName.toLowerCase()&&"text"===a.type&&(null==(b=a.getAttribute("type"))||"text"===b.toLowerCase())},first:oa(function(){return[0]}),last:oa(function(a,b){return[b-1]}),eq:oa(function(a,b,c){return[0>c?c+b:c]}),even:oa(function(a,b){for(var c=0;b>c;c+=2)a.push(c);return a}),odd:oa(function(a,b){for(var c=1;b>c;c+=2)a.push(c);return a}),lt:oa(function(a,b,c){for(var d=0>c?c+b:c;--d>=0;)a.push(d);return a}),gt:oa(function(a,b,c){for(var d=0>c?c+b:c;++d<b;)a.push(d);return a})}},d.pseudos.nth=d.pseudos.eq;for(b in{radio:!0,checkbox:!0,file:!0,password:!0,image:!0})d.pseudos[b]=ma(b);for(b in{submit:!0,reset:!0})d.pseudos[b]=na(b);function qa(){}qa.prototype=d.filters=d.pseudos,d.setFilters=new qa,g=ga.tokenize=function(a,b){var c,e,f,g,h,i,j,k=z[a+" "];if(k)return b?0:k.slice(0);h=a,i=[],j=d.preFilter;while(h){(!c||(e=S.exec(h)))&&(e&&(h=h.slice(e[0].length)||h),i.push(f=[])),c=!1,(e=T.exec(h))&&(c=e.shift(),f.push({value:c,type:e[0].replace(R," ")}),h=h.slice(c.length));for(g in d.filter)!(e=X[g].exec(h))||j[g]&&!(e=j[g](e))||(c=e.shift(),f.push({value:c,type:g,matches:e}),h=h.slice(c.length));if(!c)break}return b?h.length:h?ga.error(a):z(a,i).slice(0)};function ra(a){for(var b=0,c=a.length,d="";c>b;b++)d+=a[b].value;return d}function sa(a,b,c){var d=b.dir,e=c&&"parentNode"===d,f=x++;return b.first?function(b,c,f){while(b=b[d])if(1===b.nodeType||e)return a(b,c,f)}:function(b,c,g){var h,i,j=[w,f];if(g){while(b=b[d])if((1===b.nodeType||e)&&a(b,c,g))return!0}else while(b=b[d])if(1===b.nodeType||e){if(i=b[u]||(b[u]={}),(h=i[d])&&h[0]===w&&h[1]===f)return j[2]=h[2];if(i[d]=j,j[2]=a(b,c,g))return!0}}}function ta(a){return a.length>1?function(b,c,d){var e=a.length;while(e--)if(!a[e](b,c,d))return!1;return!0}:a[0]}function ua(a,b,c){for(var d=0,e=b.length;e>d;d++)ga(a,b[d],c);return c}function va(a,b,c,d,e){for(var f,g=[],h=0,i=a.length,j=null!=b;i>h;h++)(f=a[h])&&(!c||c(f,d,e))&&(g.push(f),j&&b.push(h));return g}function wa(a,b,c,d,e,f){return d&&!d[u]&&(d=wa(d)),e&&!e[u]&&(e=wa(e,f)),ia(function(f,g,h,i){var j,k,l,m=[],n=[],o=g.length,p=f||ua(b||"*",h.nodeType?[h]:h,[]),q=!a||!f&&b?p:va(p,m,a,h,i),r=c?e||(f?a:o||d)?[]:g:q;if(c&&c(q,r,h,i),d){j=va(r,n),d(j,[],h,i),k=j.length;while(k--)(l=j[k])&&(r[n[k]]=!(q[n[k]]=l))}if(f){if(e||a){if(e){j=[],k=r.length;while(k--)(l=r[k])&&j.push(q[k]=l);e(null,r=[],j,i)}k=r.length;while(k--)(l=r[k])&&(j=e?J(f,l):m[k])>-1&&(f[j]=!(g[j]=l))}}else r=va(r===g?r.splice(o,r.length):r),e?e(null,g,r,i):H.apply(g,r)})}function xa(a){for(var b,c,e,f=a.length,g=d.relative[a[0].type],h=g||d.relative[" "],i=g?1:0,k=sa(function(a){return a===b},h,!0),l=sa(function(a){return J(b,a)>-1},h,!0),m=[function(a,c,d){var e=!g&&(d||c!==j)||((b=c).nodeType?k(a,c,d):l(a,c,d));return b=null,e}];f>i;i++)if(c=d.relative[a[i].type])m=[sa(ta(m),c)];else{if(c=d.filter[a[i].type].apply(null,a[i].matches),c[u]){for(e=++i;f>e;e++)if(d.relative[a[e].type])break;return wa(i>1&&ta(m),i>1&&ra(a.slice(0,i-1).concat({value:" "===a[i-2].type?"*":""})).replace(R,"$1"),c,e>i&&xa(a.slice(i,e)),f>e&&xa(a=a.slice(e)),f>e&&ra(a))}m.push(c)}return ta(m)}function ya(a,b){var c=b.length>0,e=a.length>0,f=function(f,g,h,i,k){var l,m,o,p=0,q="0",r=f&&[],s=[],t=j,u=f||e&&d.find.TAG("*",k),v=w+=null==t?1:Math.random()||.1,x=u.length;for(k&&(j=g!==n&&g);q!==x&&null!=(l=u[q]);q++){if(e&&l){m=0;while(o=a[m++])if(o(l,g,h)){i.push(l);break}k&&(w=v)}c&&((l=!o&&l)&&p--,f&&r.push(l))}if(p+=q,c&&q!==p){m=0;while(o=b[m++])o(r,s,g,h);if(f){if(p>0)while(q--)r[q]||s[q]||(s[q]=F.call(i));s=va(s)}H.apply(i,s),k&&!f&&s.length>0&&p+b.length>1&&ga.uniqueSort(i)}return k&&(w=v,j=t),r};return c?ia(f):f}return h=ga.compile=function(a,b){var c,d=[],e=[],f=A[a+" "];if(!f){b||(b=g(a)),c=b.length;while(c--)f=xa(b[c]),f[u]?d.push(f):e.push(f);f=A(a,ya(e,d)),f.selector=a}return f},i=ga.select=function(a,b,e,f){var i,j,k,l,m,n="function"==typeof a&&a,o=!f&&g(a=n.selector||a);if(e=e||[],1===o.length){if(j=o[0]=o[0].slice(0),j.length>2&&"ID"===(k=j[0]).type&&c.getById&&9===b.nodeType&&p&&d.relative[j[1].type]){if(b=(d.find.ID(k.matches[0].replace(ca,da),b)||[])[0],!b)return e;n&&(b=b.parentNode),a=a.slice(j.shift().value.length)}i=X.needsContext.test(a)?0:j.length;while(i--){if(k=j[i],d.relative[l=k.type])break;if((m=d.find[l])&&(f=m(k.matches[0].replace(ca,da),aa.test(j[0].type)&&pa(b.parentNode)||b))){if(j.splice(i,1),a=f.length&&ra(j),!a)return H.apply(e,f),e;break}}}return(n||h(a,o))(f,b,!p,e,aa.test(a)&&pa(b.parentNode)||b),e},c.sortStable=u.split("").sort(B).join("")===u,c.detectDuplicates=!!l,m(),c.sortDetached=ja(function(a){return 1&a.compareDocumentPosition(n.createElement("div"))}),ja(function(a){return a.innerHTML="<a href='#'></a>","#"===a.firstChild.getAttribute("href")})||ka("type|href|height|width",function(a,b,c){return c?void 0:a.getAttribute(b,"type"===b.toLowerCase()?1:2)}),c.attributes&&ja(function(a){return a.innerHTML="<input/>",a.firstChild.setAttribute("value",""),""===a.firstChild.getAttribute("value")})||ka("value",function(a,b,c){return c||"input"!==a.nodeName.toLowerCase()?void 0:a.defaultValue}),ja(function(a){return null==a.getAttribute("disabled")})||ka(K,function(a,b,c){var d;return c?void 0:a[b]===!0?b.toLowerCase():(d=a.getAttributeNode(b))&&d.specified?d.value:null}),ga}(a);m.find=s,m.expr=s.selectors,m.expr[":"]=m.expr.pseudos,m.unique=s.uniqueSort,m.text=s.getText,m.isXMLDoc=s.isXML,m.contains=s.contains;var t=m.expr.match.needsContext,u=/^<(\w+)\s*\/?>(?:<\/\1>|)$/,v=/^.[^:#\[\.,]*$/;function w(a,b,c){if(m.isFunction(b))return m.grep(a,function(a,d){return!!b.call(a,d,a)!==c});if(b.nodeType)return m.grep(a,function(a){return a===b!==c});if("string"==typeof b){if(v.test(b))return m.filter(b,a,c);b=m.filter(b,a)}return m.grep(a,function(a){return m.inArray(a,b)>=0!==c})}m.filter=function(a,b,c){var d=b[0];return c&&(a=":not("+a+")"),1===b.length&&1===d.nodeType?m.find.matchesSelector(d,a)?[d]:[]:m.find.matches(a,m.grep(b,function(a){return 1===a.nodeType}))},m.fn.extend({find:function(a){var b,c=[],d=this,e=d.length;if("string"!=typeof a)return this.pushStack(m(a).filter(function(){for(b=0;e>b;b++)if(m.contains(d[b],this))return!0}));for(b=0;e>b;b++)m.find(a,d[b],c);return c=this.pushStack(e>1?m.unique(c):c),c.selector=this.selector?this.selector+" "+a:a,c},filter:function(a){return this.pushStack(w(this,a||[],!1))},not:function(a){return this.pushStack(w(this,a||[],!0))},is:function(a){return!!w(this,"string"==typeof a&&t.test(a)?m(a):a||[],!1).length}});var x,y=a.document,z=/^(?:\s*(<[\w\W]+>)[^>]*|#([\w-]*))$/,A=m.fn.init=function(a,b){var c,d;if(!a)return this;if("string"==typeof a){if(c="<"===a.charAt(0)&&">"===a.charAt(a.length-1)&&a.length>=3?[null,a,null]:z.exec(a),!c||!c[1]&&b)return!b||b.jquery?(b||x).find(a):this.constructor(b).find(a);if(c[1]){if(b=b instanceof m?b[0]:b,m.merge(this,m.parseHTML(c[1],b&&b.nodeType?b.ownerDocument||b:y,!0)),u.test(c[1])&&m.isPlainObject(b))for(c in b)m.isFunction(this[c])?this[c](b[c]):this.attr(c,b[c]);return this}if(d=y.getElementById(c[2]),d&&d.parentNode){if(d.id!==c[2])return x.find(a);this.length=1,this[0]=d}return this.context=y,this.selector=a,this}return a.nodeType?(this.context=this[0]=a,this.length=1,this):m.isFunction(a)?"undefined"!=typeof x.ready?x.ready(a):a(m):(void 0!==a.selector&&(this.selector=a.selector,this.context=a.context),m.makeArray(a,this))};A.prototype=m.fn,x=m(y);var B=/^(?:parents|prev(?:Until|All))/,C={children:!0,contents:!0,next:!0,prev:!0};m.extend({dir:function(a,b,c){var d=[],e=a[b];while(e&&9!==e.nodeType&&(void 0===c||1!==e.nodeType||!m(e).is(c)))1===e.nodeType&&d.push(e),e=e[b];return d},sibling:function(a,b){for(var c=[];a;a=a.nextSibling)1===a.nodeType&&a!==b&&c.push(a);return c}}),m.fn.extend({has:function(a){var b,c=m(a,this),d=c.length;return this.filter(function(){for(b=0;d>b;b++)if(m.contains(this,c[b]))return!0})},closest:function(a,b){for(var c,d=0,e=this.length,f=[],g=t.test(a)||"string"!=typeof a?m(a,b||this.context):0;e>d;d++)for(c=this[d];c&&c!==b;c=c.parentNode)if(c.nodeType<11&&(g?g.index(c)>-1:1===c.nodeType&&m.find.matchesSelector(c,a))){f.push(c);break}return this.pushStack(f.length>1?m.unique(f):f)},index:function(a){return a?"string"==typeof a?m.inArray(this[0],m(a)):m.inArray(a.jquery?a[0]:a,this):this[0]&&this[0].parentNode?this.first().prevAll().length:-1},add:function(a,b){return this.pushStack(m.unique(m.merge(this.get(),m(a,b))))},addBack:function(a){return this.add(null==a?this.prevObject:this.prevObject.filter(a))}});function D(a,b){do a=a[b];while(a&&1!==a.nodeType);return a}m.each({parent:function(a){var b=a.parentNode;return b&&11!==b.nodeType?b:null},parents:function(a){return m.dir(a,"parentNode")},parentsUntil:function(a,b,c){return m.dir(a,"parentNode",c)},next:function(a){return D(a,"nextSibling")},prev:function(a){return D(a,"previousSibling")},nextAll:function(a){return m.dir(a,"nextSibling")},prevAll:function(a){return m.dir(a,"previousSibling")},nextUntil:function(a,b,c){return m.dir(a,"nextSibling",c)},prevUntil:function(a,b,c){return m.dir(a,"previousSibling",c)},siblings:function(a){return m.sibling((a.parentNode||{}).firstChild,a)},children:function(a){return m.sibling(a.firstChild)},contents:function(a){return m.nodeName(a,"iframe")?a.contentDocument||a.contentWindow.document:m.merge([],a.childNodes)}},function(a,b){m.fn[a]=function(c,d){var e=m.map(this,b,c);return"Until"!==a.slice(-5)&&(d=c),d&&"string"==typeof d&&(e=m.filter(d,e)),this.length>1&&(C[a]||(e=m.unique(e)),B.test(a)&&(e=e.reverse())),this.pushStack(e)}});var E=/\S+/g,F={};function G(a){var b=F[a]={};return m.each(a.match(E)||[],function(a,c){b[c]=!0}),b}m.Callbacks=function(a){a="string"==typeof a?F[a]||G(a):m.extend({},a);var b,c,d,e,f,g,h=[],i=!a.once&&[],j=function(l){for(c=a.memory&&l,d=!0,f=g||0,g=0,e=h.length,b=!0;h&&e>f;f++)if(h[f].apply(l[0],l[1])===!1&&a.stopOnFalse){c=!1;break}b=!1,h&&(i?i.length&&j(i.shift()):c?h=[]:k.disable())},k={add:function(){if(h){var d=h.length;!function f(b){m.each(b,function(b,c){var d=m.type(c);"function"===d?a.unique&&k.has(c)||h.push(c):c&&c.length&&"string"!==d&&f(c)})}(arguments),b?e=h.length:c&&(g=d,j(c))}return this},remove:function(){return h&&m.each(arguments,function(a,c){var d;while((d=m.inArray(c,h,d))>-1)h.splice(d,1),b&&(e>=d&&e--,f>=d&&f--)}),this},has:function(a){return a?m.inArray(a,h)>-1:!(!h||!h.length)},empty:function(){return h=[],e=0,this},disable:function(){return h=i=c=void 0,this},disabled:function(){return!h},lock:function(){return i=void 0,c||k.disable(),this},locked:function(){return!i},fireWith:function(a,c){return!h||d&&!i||(c=c||[],c=[a,c.slice?c.slice():c],b?i.push(c):j(c)),this},fire:function(){return k.fireWith(this,arguments),this},fired:function(){return!!d}};return k},m.extend({Deferred:function(a){var b=[["resolve","done",m.Callbacks("once memory"),"resolved"],["reject","fail",m.Callbacks("once memory"),"rejected"],["notify","progress",m.Callbacks("memory")]],c="pending",d={state:function(){return c},always:function(){return e.done(arguments).fail(arguments),this},then:function(){var a=arguments;return m.Deferred(function(c){m.each(b,function(b,f){var g=m.isFunction(a[b])&&a[b];e[f[1]](function(){var a=g&&g.apply(this,arguments);a&&m.isFunction(a.promise)?a.promise().done(c.resolve).fail(c.reject).progress(c.notify):c[f[0]+"With"](this===d?c.promise():this,g?[a]:arguments)})}),a=null}).promise()},promise:function(a){return null!=a?m.extend(a,d):d}},e={};return d.pipe=d.then,m.each(b,function(a,f){var g=f[2],h=f[3];d[f[1]]=g.add,h&&g.add(function(){c=h},b[1^a][2].disable,b[2][2].lock),e[f[0]]=function(){return e[f[0]+"With"](this===e?d:this,arguments),this},e[f[0]+"With"]=g.fireWith}),d.promise(e),a&&a.call(e,e),e},when:function(a){var b=0,c=d.call(arguments),e=c.length,f=1!==e||a&&m.isFunction(a.promise)?e:0,g=1===f?a:m.Deferred(),h=function(a,b,c){return function(e){b[a]=this,c[a]=arguments.length>1?d.call(arguments):e,c===i?g.notifyWith(b,c):--f||g.resolveWith(b,c)}},i,j,k;if(e>1)for(i=new Array(e),j=new Array(e),k=new Array(e);e>b;b++)c[b]&&m.isFunction(c[b].promise)?c[b].promise().done(h(b,k,c)).fail(g.reject).progress(h(b,j,i)):--f;return f||g.resolveWith(k,c),g.promise()}});var H;m.fn.ready=function(a){return m.ready.promise().done(a),this},m.extend({isReady:!1,readyWait:1,holdReady:function(a){a?m.readyWait++:m.ready(!0)},ready:function(a){if(a===!0?!--m.readyWait:!m.isReady){if(!y.body)return setTimeout(m.ready);m.isReady=!0,a!==!0&&--m.readyWait>0||(H.resolveWith(y,[m]),m.fn.triggerHandler&&(m(y).triggerHandler("ready"),m(y).off("ready")))}}});function I(){y.addEventListener?(y.removeEventListener("DOMContentLoaded",J,!1),a.removeEventListener("load",J,!1)):(y.detachEvent("onreadystatechange",J),a.detachEvent("onload",J))}function J(){(y.addEventListener||"load"===event.type||"complete"===y.readyState)&&(I(),m.ready())}m.ready.promise=function(b){if(!H)if(H=m.Deferred(),"complete"===y.readyState)setTimeout(m.ready);else if(y.addEventListener)y.addEventListener("DOMContentLoaded",J,!1),a.addEventListener("load",J,!1);else{y.attachEvent("onreadystatechange",J),a.attachEvent("onload",J);var c=!1;try{c=null==a.frameElement&&y.documentElement}catch(d){}c&&c.doScroll&&!function e(){if(!m.isReady){try{c.doScroll("left")}catch(a){return setTimeout(e,50)}I(),m.ready()}}()}return H.promise(b)};var K="undefined",L;for(L in m(k))break;k.ownLast="0"!==L,k.inlineBlockNeedsLayout=!1,m(function(){var a,b,c,d;c=y.getElementsByTagName("body")[0],c&&c.style&&(b=y.createElement("div"),d=y.createElement("div"),d.style.cssText="position:absolute;border:0;width:0;height:0;top:0;left:-9999px",c.appendChild(d).appendChild(b),typeof b.style.zoom!==K&&(b.style.cssText="display:inline;margin:0;border:0;padding:1px;width:1px;zoom:1",k.inlineBlockNeedsLayout=a=3===b.offsetWidth,a&&(c.style.zoom=1)),c.removeChild(d))}),function(){var a=y.createElement("div");if(null==k.deleteExpando){k.deleteExpando=!0;try{delete a.test}catch(b){k.deleteExpando=!1}}a=null}(),m.acceptData=function(a){var b=m.noData[(a.nodeName+" ").toLowerCase()],c=+a.nodeType||1;return 1!==c&&9!==c?!1:!b||b!==!0&&a.getAttribute("classid")===b};var M=/^(?:\{[\w\W]*\}|\[[\w\W]*\])$/,N=/([A-Z])/g;function O(a,b,c){if(void 0===c&&1===a.nodeType){var d="data-"+b.replace(N,"-$1").toLowerCase();if(c=a.getAttribute(d),"string"==typeof c){try{c="true"===c?!0:"false"===c?!1:"null"===c?null:+c+""===c?+c:M.test(c)?m.parseJSON(c):c}catch(e){}m.data(a,b,c)}else c=void 0}return c}function P(a){var b;for(b in a)if(("data"!==b||!m.isEmptyObject(a[b]))&&"toJSON"!==b)return!1;

return!0}function Q(a,b,d,e){if(m.acceptData(a)){var f,g,h=m.expando,i=a.nodeType,j=i?m.cache:a,k=i?a[h]:a[h]&&h;if(k&&j[k]&&(e||j[k].data)||void 0!==d||"string"!=typeof b)return k||(k=i?a[h]=c.pop()||m.guid++:h),j[k]||(j[k]=i?{}:{toJSON:m.noop}),("object"==typeof b||"function"==typeof b)&&(e?j[k]=m.extend(j[k],b):j[k].data=m.extend(j[k].data,b)),g=j[k],e||(g.data||(g.data={}),g=g.data),void 0!==d&&(g[m.camelCase(b)]=d),"string"==typeof b?(f=g[b],null==f&&(f=g[m.camelCase(b)])):f=g,f}}function R(a,b,c){if(m.acceptData(a)){var d,e,f=a.nodeType,g=f?m.cache:a,h=f?a[m.expando]:m.expando;if(g[h]){if(b&&(d=c?g[h]:g[h].data)){m.isArray(b)?b=b.concat(m.map(b,m.camelCase)):b in d?b=[b]:(b=m.camelCase(b),b=b in d?[b]:b.split(" ")),e=b.length;while(e--)delete d[b[e]];if(c?!P(d):!m.isEmptyObject(d))return}(c||(delete g[h].data,P(g[h])))&&(f?m.cleanData([a],!0):k.deleteExpando||g!=g.window?delete g[h]:g[h]=null)}}}m.extend({cache:{},noData:{"applet ":!0,"embed ":!0,"object ":"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000"},hasData:function(a){return a=a.nodeType?m.cache[a[m.expando]]:a[m.expando],!!a&&!P(a)},data:function(a,b,c){return Q(a,b,c)},removeData:function(a,b){return R(a,b)},_data:function(a,b,c){return Q(a,b,c,!0)},_removeData:function(a,b){return R(a,b,!0)}}),m.fn.extend({data:function(a,b){var c,d,e,f=this[0],g=f&&f.attributes;if(void 0===a){if(this.length&&(e=m.data(f),1===f.nodeType&&!m._data(f,"parsedAttrs"))){c=g.length;while(c--)g[c]&&(d=g[c].name,0===d.indexOf("data-")&&(d=m.camelCase(d.slice(5)),O(f,d,e[d])));m._data(f,"parsedAttrs",!0)}return e}return"object"==typeof a?this.each(function(){m.data(this,a)}):arguments.length>1?this.each(function(){m.data(this,a,b)}):f?O(f,a,m.data(f,a)):void 0},removeData:function(a){return this.each(function(){m.removeData(this,a)})}}),m.extend({queue:function(a,b,c){var d;return a?(b=(b||"fx")+"queue",d=m._data(a,b),c&&(!d||m.isArray(c)?d=m._data(a,b,m.makeArray(c)):d.push(c)),d||[]):void 0},dequeue:function(a,b){b=b||"fx";var c=m.queue(a,b),d=c.length,e=c.shift(),f=m._queueHooks(a,b),g=function(){m.dequeue(a,b)};"inprogress"===e&&(e=c.shift(),d--),e&&("fx"===b&&c.unshift("inprogress"),delete f.stop,e.call(a,g,f)),!d&&f&&f.empty.fire()},_queueHooks:function(a,b){var c=b+"queueHooks";return m._data(a,c)||m._data(a,c,{empty:m.Callbacks("once memory").add(function(){m._removeData(a,b+"queue"),m._removeData(a,c)})})}}),m.fn.extend({queue:function(a,b){var c=2;return"string"!=typeof a&&(b=a,a="fx",c--),arguments.length<c?m.queue(this[0],a):void 0===b?this:this.each(function(){var c=m.queue(this,a,b);m._queueHooks(this,a),"fx"===a&&"inprogress"!==c[0]&&m.dequeue(this,a)})},dequeue:function(a){return this.each(function(){m.dequeue(this,a)})},clearQueue:function(a){return this.queue(a||"fx",[])},promise:function(a,b){var c,d=1,e=m.Deferred(),f=this,g=this.length,h=function(){--d||e.resolveWith(f,[f])};"string"!=typeof a&&(b=a,a=void 0),a=a||"fx";while(g--)c=m._data(f[g],a+"queueHooks"),c&&c.empty&&(d++,c.empty.add(h));return h(),e.promise(b)}});var S=/[+-]?(?:\d*\.|)\d+(?:[eE][+-]?\d+|)/.source,T=["Top","Right","Bottom","Left"],U=function(a,b){return a=b||a,"none"===m.css(a,"display")||!m.contains(a.ownerDocument,a)},V=m.access=function(a,b,c,d,e,f,g){var h=0,i=a.length,j=null==c;if("object"===m.type(c)){e=!0;for(h in c)m.access(a,b,h,c[h],!0,f,g)}else if(void 0!==d&&(e=!0,m.isFunction(d)||(g=!0),j&&(g?(b.call(a,d),b=null):(j=b,b=function(a,b,c){return j.call(m(a),c)})),b))for(;i>h;h++)b(a[h],c,g?d:d.call(a[h],h,b(a[h],c)));return e?a:j?b.call(a):i?b(a[0],c):f},W=/^(?:checkbox|radio)$/i;!function(){var a=y.createElement("input"),b=y.createElement("div"),c=y.createDocumentFragment();if(b.innerHTML="  <link/><table></table><a href='/a'>a</a><input type='checkbox'/>",k.leadingWhitespace=3===b.firstChild.nodeType,k.tbody=!b.getElementsByTagName("tbody").length,k.htmlSerialize=!!b.getElementsByTagName("link").length,k.html5Clone="<:nav></:nav>"!==y.createElement("nav").cloneNode(!0).outerHTML,a.type="checkbox",a.checked=!0,c.appendChild(a),k.appendChecked=a.checked,b.innerHTML="<textarea>x</textarea>",k.noCloneChecked=!!b.cloneNode(!0).lastChild.defaultValue,c.appendChild(b),b.innerHTML="<input type='radio' checked='checked' name='t'/>",k.checkClone=b.cloneNode(!0).cloneNode(!0).lastChild.checked,k.noCloneEvent=!0,b.attachEvent&&(b.attachEvent("onclick",function(){k.noCloneEvent=!1}),b.cloneNode(!0).click()),null==k.deleteExpando){k.deleteExpando=!0;try{delete b.test}catch(d){k.deleteExpando=!1}}}(),function(){var b,c,d=y.createElement("div");for(b in{submit:!0,change:!0,focusin:!0})c="on"+b,(k[b+"Bubbles"]=c in a)||(d.setAttribute(c,"t"),k[b+"Bubbles"]=d.attributes[c].expando===!1);d=null}();var X=/^(?:input|select|textarea)$/i,Y=/^key/,Z=/^(?:mouse|pointer|contextmenu)|click/,$=/^(?:focusinfocus|focusoutblur)$/,_=/^([^.]*)(?:\.(.+)|)$/;function aa(){return!0}function ba(){return!1}function ca(){try{return y.activeElement}catch(a){}}m.event={global:{},add:function(a,b,c,d,e){var f,g,h,i,j,k,l,n,o,p,q,r=m._data(a);if(r){c.handler&&(i=c,c=i.handler,e=i.selector),c.guid||(c.guid=m.guid++),(g=r.events)||(g=r.events={}),(k=r.handle)||(k=r.handle=function(a){return typeof m===K||a&&m.event.triggered===a.type?void 0:m.event.dispatch.apply(k.elem,arguments)},k.elem=a),b=(b||"").match(E)||[""],h=b.length;while(h--)f=_.exec(b[h])||[],o=q=f[1],p=(f[2]||"").split(".").sort(),o&&(j=m.event.special[o]||{},o=(e?j.delegateType:j.bindType)||o,j=m.event.special[o]||{},l=m.extend({type:o,origType:q,data:d,handler:c,guid:c.guid,selector:e,needsContext:e&&m.expr.match.needsContext.test(e),namespace:p.join(".")},i),(n=g[o])||(n=g[o]=[],n.delegateCount=0,j.setup&&j.setup.call(a,d,p,k)!==!1||(a.addEventListener?a.addEventListener(o,k,!1):a.attachEvent&&a.attachEvent("on"+o,k))),j.add&&(j.add.call(a,l),l.handler.guid||(l.handler.guid=c.guid)),e?n.splice(n.delegateCount++,0,l):n.push(l),m.event.global[o]=!0);a=null}},remove:function(a,b,c,d,e){var f,g,h,i,j,k,l,n,o,p,q,r=m.hasData(a)&&m._data(a);if(r&&(k=r.events)){b=(b||"").match(E)||[""],j=b.length;while(j--)if(h=_.exec(b[j])||[],o=q=h[1],p=(h[2]||"").split(".").sort(),o){l=m.event.special[o]||{},o=(d?l.delegateType:l.bindType)||o,n=k[o]||[],h=h[2]&&new RegExp("(^|\\.)"+p.join("\\.(?:.*\\.|)")+"(\\.|$)"),i=f=n.length;while(f--)g=n[f],!e&&q!==g.origType||c&&c.guid!==g.guid||h&&!h.test(g.namespace)||d&&d!==g.selector&&("**"!==d||!g.selector)||(n.splice(f,1),g.selector&&n.delegateCount--,l.remove&&l.remove.call(a,g));i&&!n.length&&(l.teardown&&l.teardown.call(a,p,r.handle)!==!1||m.removeEvent(a,o,r.handle),delete k[o])}else for(o in k)m.event.remove(a,o+b[j],c,d,!0);m.isEmptyObject(k)&&(delete r.handle,m._removeData(a,"events"))}},trigger:function(b,c,d,e){var f,g,h,i,k,l,n,o=[d||y],p=j.call(b,"type")?b.type:b,q=j.call(b,"namespace")?b.namespace.split("."):[];if(h=l=d=d||y,3!==d.nodeType&&8!==d.nodeType&&!$.test(p+m.event.triggered)&&(p.indexOf(".")>=0&&(q=p.split("."),p=q.shift(),q.sort()),g=p.indexOf(":")<0&&"on"+p,b=b[m.expando]?b:new m.Event(p,"object"==typeof b&&b),b.isTrigger=e?2:3,b.namespace=q.join("."),b.namespace_re=b.namespace?new RegExp("(^|\\.)"+q.join("\\.(?:.*\\.|)")+"(\\.|$)"):null,b.result=void 0,b.target||(b.target=d),c=null==c?[b]:m.makeArray(c,[b]),k=m.event.special[p]||{},e||!k.trigger||k.trigger.apply(d,c)!==!1)){if(!e&&!k.noBubble&&!m.isWindow(d)){for(i=k.delegateType||p,$.test(i+p)||(h=h.parentNode);h;h=h.parentNode)o.push(h),l=h;l===(d.ownerDocument||y)&&o.push(l.defaultView||l.parentWindow||a)}n=0;while((h=o[n++])&&!b.isPropagationStopped())b.type=n>1?i:k.bindType||p,f=(m._data(h,"events")||{})[b.type]&&m._data(h,"handle"),f&&f.apply(h,c),f=g&&h[g],f&&f.apply&&m.acceptData(h)&&(b.result=f.apply(h,c),b.result===!1&&b.preventDefault());if(b.type=p,!e&&!b.isDefaultPrevented()&&(!k._default||k._default.apply(o.pop(),c)===!1)&&m.acceptData(d)&&g&&d[p]&&!m.isWindow(d)){l=d[g],l&&(d[g]=null),m.event.triggered=p;try{d[p]()}catch(r){}m.event.triggered=void 0,l&&(d[g]=l)}return b.result}},dispatch:function(a){a=m.event.fix(a);var b,c,e,f,g,h=[],i=d.call(arguments),j=(m._data(this,"events")||{})[a.type]||[],k=m.event.special[a.type]||{};if(i[0]=a,a.delegateTarget=this,!k.preDispatch||k.preDispatch.call(this,a)!==!1){h=m.event.handlers.call(this,a,j),b=0;while((f=h[b++])&&!a.isPropagationStopped()){a.currentTarget=f.elem,g=0;while((e=f.handlers[g++])&&!a.isImmediatePropagationStopped())(!a.namespace_re||a.namespace_re.test(e.namespace))&&(a.handleObj=e,a.data=e.data,c=((m.event.special[e.origType]||{}).handle||e.handler).apply(f.elem,i),void 0!==c&&(a.result=c)===!1&&(a.preventDefault(),a.stopPropagation()))}return k.postDispatch&&k.postDispatch.call(this,a),a.result}},handlers:function(a,b){var c,d,e,f,g=[],h=b.delegateCount,i=a.target;if(h&&i.nodeType&&(!a.button||"click"!==a.type))for(;i!=this;i=i.parentNode||this)if(1===i.nodeType&&(i.disabled!==!0||"click"!==a.type)){for(e=[],f=0;h>f;f++)d=b[f],c=d.selector+" ",void 0===e[c]&&(e[c]=d.needsContext?m(c,this).index(i)>=0:m.find(c,this,null,[i]).length),e[c]&&e.push(d);e.length&&g.push({elem:i,handlers:e})}return h<b.length&&g.push({elem:this,handlers:b.slice(h)}),g},fix:function(a){if(a[m.expando])return a;var b,c,d,e=a.type,f=a,g=this.fixHooks[e];g||(this.fixHooks[e]=g=Z.test(e)?this.mouseHooks:Y.test(e)?this.keyHooks:{}),d=g.props?this.props.concat(g.props):this.props,a=new m.Event(f),b=d.length;while(b--)c=d[b],a[c]=f[c];return a.target||(a.target=f.srcElement||y),3===a.target.nodeType&&(a.target=a.target.parentNode),a.metaKey=!!a.metaKey,g.filter?g.filter(a,f):a},props:"altKey bubbles cancelable ctrlKey currentTarget eventPhase metaKey relatedTarget shiftKey target timeStamp view which".split(" "),fixHooks:{},keyHooks:{props:"char charCode key keyCode".split(" "),filter:function(a,b){return null==a.which&&(a.which=null!=b.charCode?b.charCode:b.keyCode),a}},mouseHooks:{props:"button buttons clientX clientY fromElement offsetX offsetY pageX pageY screenX screenY toElement".split(" "),filter:function(a,b){var c,d,e,f=b.button,g=b.fromElement;return null==a.pageX&&null!=b.clientX&&(d=a.target.ownerDocument||y,e=d.documentElement,c=d.body,a.pageX=b.clientX+(e&&e.scrollLeft||c&&c.scrollLeft||0)-(e&&e.clientLeft||c&&c.clientLeft||0),a.pageY=b.clientY+(e&&e.scrollTop||c&&c.scrollTop||0)-(e&&e.clientTop||c&&c.clientTop||0)),!a.relatedTarget&&g&&(a.relatedTarget=g===a.target?b.toElement:g),a.which||void 0===f||(a.which=1&f?1:2&f?3:4&f?2:0),a}},special:{load:{noBubble:!0},focus:{trigger:function(){if(this!==ca()&&this.focus)try{return this.focus(),!1}catch(a){}},delegateType:"focusin"},blur:{trigger:function(){return this===ca()&&this.blur?(this.blur(),!1):void 0},delegateType:"focusout"},click:{trigger:function(){return m.nodeName(this,"input")&&"checkbox"===this.type&&this.click?(this.click(),!1):void 0},_default:function(a){return m.nodeName(a.target,"a")}},beforeunload:{postDispatch:function(a){void 0!==a.result&&a.originalEvent&&(a.originalEvent.returnValue=a.result)}}},simulate:function(a,b,c,d){var e=m.extend(new m.Event,c,{type:a,isSimulated:!0,originalEvent:{}});d?m.event.trigger(e,null,b):m.event.dispatch.call(b,e),e.isDefaultPrevented()&&c.preventDefault()}},m.removeEvent=y.removeEventListener?function(a,b,c){a.removeEventListener&&a.removeEventListener(b,c,!1)}:function(a,b,c){var d="on"+b;a.detachEvent&&(typeof a[d]===K&&(a[d]=null),a.detachEvent(d,c))},m.Event=function(a,b){return this instanceof m.Event?(a&&a.type?(this.originalEvent=a,this.type=a.type,this.isDefaultPrevented=a.defaultPrevented||void 0===a.defaultPrevented&&a.returnValue===!1?aa:ba):this.type=a,b&&m.extend(this,b),this.timeStamp=a&&a.timeStamp||m.now(),void(this[m.expando]=!0)):new m.Event(a,b)},m.Event.prototype={isDefaultPrevented:ba,isPropagationStopped:ba,isImmediatePropagationStopped:ba,preventDefault:function(){var a=this.originalEvent;this.isDefaultPrevented=aa,a&&(a.preventDefault?a.preventDefault():a.returnValue=!1)},stopPropagation:function(){var a=this.originalEvent;this.isPropagationStopped=aa,a&&(a.stopPropagation&&a.stopPropagation(),a.cancelBubble=!0)},stopImmediatePropagation:function(){var a=this.originalEvent;this.isImmediatePropagationStopped=aa,a&&a.stopImmediatePropagation&&a.stopImmediatePropagation(),this.stopPropagation()}},m.each({mouseenter:"mouseover",mouseleave:"mouseout",pointerenter:"pointerover",pointerleave:"pointerout"},function(a,b){m.event.special[a]={delegateType:b,bindType:b,handle:function(a){var c,d=this,e=a.relatedTarget,f=a.handleObj;return(!e||e!==d&&!m.contains(d,e))&&(a.type=f.origType,c=f.handler.apply(this,arguments),a.type=b),c}}}),k.submitBubbles||(m.event.special.submit={setup:function(){return m.nodeName(this,"form")?!1:void m.event.add(this,"click._submit keypress._submit",function(a){var b=a.target,c=m.nodeName(b,"input")||m.nodeName(b,"button")?b.form:void 0;c&&!m._data(c,"submitBubbles")&&(m.event.add(c,"submit._submit",function(a){a._submit_bubble=!0}),m._data(c,"submitBubbles",!0))})},postDispatch:function(a){a._submit_bubble&&(delete a._submit_bubble,this.parentNode&&!a.isTrigger&&m.event.simulate("submit",this.parentNode,a,!0))},teardown:function(){return m.nodeName(this,"form")?!1:void m.event.remove(this,"._submit")}}),k.changeBubbles||(m.event.special.change={setup:function(){return X.test(this.nodeName)?(("checkbox"===this.type||"radio"===this.type)&&(m.event.add(this,"propertychange._change",function(a){"checked"===a.originalEvent.propertyName&&(this._just_changed=!0)}),m.event.add(this,"click._change",function(a){this._just_changed&&!a.isTrigger&&(this._just_changed=!1),m.event.simulate("change",this,a,!0)})),!1):void m.event.add(this,"beforeactivate._change",function(a){var b=a.target;X.test(b.nodeName)&&!m._data(b,"changeBubbles")&&(m.event.add(b,"change._change",function(a){!this.parentNode||a.isSimulated||a.isTrigger||m.event.simulate("change",this.parentNode,a,!0)}),m._data(b,"changeBubbles",!0))})},handle:function(a){var b=a.target;return this!==b||a.isSimulated||a.isTrigger||"radio"!==b.type&&"checkbox"!==b.type?a.handleObj.handler.apply(this,arguments):void 0},teardown:function(){return m.event.remove(this,"._change"),!X.test(this.nodeName)}}),k.focusinBubbles||m.each({focus:"focusin",blur:"focusout"},function(a,b){var c=function(a){m.event.simulate(b,a.target,m.event.fix(a),!0)};m.event.special[b]={setup:function(){var d=this.ownerDocument||this,e=m._data(d,b);e||d.addEventListener(a,c,!0),m._data(d,b,(e||0)+1)},teardown:function(){var d=this.ownerDocument||this,e=m._data(d,b)-1;e?m._data(d,b,e):(d.removeEventListener(a,c,!0),m._removeData(d,b))}}}),m.fn.extend({on:function(a,b,c,d,e){var f,g;if("object"==typeof a){"string"!=typeof b&&(c=c||b,b=void 0);for(f in a)this.on(f,b,c,a[f],e);return this}if(null==c&&null==d?(d=b,c=b=void 0):null==d&&("string"==typeof b?(d=c,c=void 0):(d=c,c=b,b=void 0)),d===!1)d=ba;else if(!d)return this;return 1===e&&(g=d,d=function(a){return m().off(a),g.apply(this,arguments)},d.guid=g.guid||(g.guid=m.guid++)),this.each(function(){m.event.add(this,a,d,c,b)})},one:function(a,b,c,d){return this.on(a,b,c,d,1)},off:function(a,b,c){var d,e;if(a&&a.preventDefault&&a.handleObj)return d=a.handleObj,m(a.delegateTarget).off(d.namespace?d.origType+"."+d.namespace:d.origType,d.selector,d.handler),this;if("object"==typeof a){for(e in a)this.off(e,b,a[e]);return this}return(b===!1||"function"==typeof b)&&(c=b,b=void 0),c===!1&&(c=ba),this.each(function(){m.event.remove(this,a,c,b)})},trigger:function(a,b){return this.each(function(){m.event.trigger(a,b,this)})},triggerHandler:function(a,b){var c=this[0];return c?m.event.trigger(a,b,c,!0):void 0}});function da(a){var b=ea.split("|"),c=a.createDocumentFragment();if(c.createElement)while(b.length)c.createElement(b.pop());return c}var ea="abbr|article|aside|audio|bdi|canvas|data|datalist|details|figcaption|figure|footer|header|hgroup|mark|meter|nav|output|progress|section|summary|time|video",fa=/ jQuery\d+="(?:null|\d+)"/g,ga=new RegExp("<(?:"+ea+")[\\s/>]","i"),ha=/^\s+/,ia=/<(?!area|br|col|embed|hr|img|input|link|meta|param)(([\w:]+)[^>]*)\/>/gi,ja=/<([\w:]+)/,ka=/<tbody/i,la=/<|&#?\w+;/,ma=/<(?:script|style|link)/i,na=/checked\s*(?:[^=]|=\s*.checked.)/i,oa=/^$|\/(?:java|ecma)script/i,pa=/^true\/(.*)/,qa=/^\s*<!(?:\[CDATA\[|--)|(?:\]\]|--)>\s*$/g,ra={option:[1,"<select multiple='multiple'>","</select>"],legend:[1,"<fieldset>","</fieldset>"],area:[1,"<map>","</map>"],param:[1,"<object>","</object>"],thead:[1,"<table>","</table>"],tr:[2,"<table><tbody>","</tbody></table>"],col:[2,"<table><tbody></tbody><colgroup>","</colgroup></table>"],td:[3,"<table><tbody><tr>","</tr></tbody></table>"],_default:k.htmlSerialize?[0,"",""]:[1,"X<div>","</div>"]},sa=da(y),ta=sa.appendChild(y.createElement("div"));ra.optgroup=ra.option,ra.tbody=ra.tfoot=ra.colgroup=ra.caption=ra.thead,ra.th=ra.td;function ua(a,b){var c,d,e=0,f=typeof a.getElementsByTagName!==K?a.getElementsByTagName(b||"*"):typeof a.querySelectorAll!==K?a.querySelectorAll(b||"*"):void 0;if(!f)for(f=[],c=a.childNodes||a;null!=(d=c[e]);e++)!b||m.nodeName(d,b)?f.push(d):m.merge(f,ua(d,b));return void 0===b||b&&m.nodeName(a,b)?m.merge([a],f):f}function va(a){W.test(a.type)&&(a.defaultChecked=a.checked)}function wa(a,b){return m.nodeName(a,"table")&&m.nodeName(11!==b.nodeType?b:b.firstChild,"tr")?a.getElementsByTagName("tbody")[0]||a.appendChild(a.ownerDocument.createElement("tbody")):a}function xa(a){return a.type=(null!==m.find.attr(a,"type"))+"/"+a.type,a}function ya(a){var b=pa.exec(a.type);return b?a.type=b[1]:a.removeAttribute("type"),a}function za(a,b){for(var c,d=0;null!=(c=a[d]);d++)m._data(c,"globalEval",!b||m._data(b[d],"globalEval"))}function Aa(a,b){if(1===b.nodeType&&m.hasData(a)){var c,d,e,f=m._data(a),g=m._data(b,f),h=f.events;if(h){delete g.handle,g.events={};for(c in h)for(d=0,e=h[c].length;e>d;d++)m.event.add(b,c,h[c][d])}g.data&&(g.data=m.extend({},g.data))}}function Ba(a,b){var c,d,e;if(1===b.nodeType){if(c=b.nodeName.toLowerCase(),!k.noCloneEvent&&b[m.expando]){e=m._data(b);for(d in e.events)m.removeEvent(b,d,e.handle);b.removeAttribute(m.expando)}"script"===c&&b.text!==a.text?(xa(b).text=a.text,ya(b)):"object"===c?(b.parentNode&&(b.outerHTML=a.outerHTML),k.html5Clone&&a.innerHTML&&!m.trim(b.innerHTML)&&(b.innerHTML=a.innerHTML)):"input"===c&&W.test(a.type)?(b.defaultChecked=b.checked=a.checked,b.value!==a.value&&(b.value=a.value)):"option"===c?b.defaultSelected=b.selected=a.defaultSelected:("input"===c||"textarea"===c)&&(b.defaultValue=a.defaultValue)}}m.extend({clone:function(a,b,c){var d,e,f,g,h,i=m.contains(a.ownerDocument,a);if(k.html5Clone||m.isXMLDoc(a)||!ga.test("<"+a.nodeName+">")?f=a.cloneNode(!0):(ta.innerHTML=a.outerHTML,ta.removeChild(f=ta.firstChild)),!(k.noCloneEvent&&k.noCloneChecked||1!==a.nodeType&&11!==a.nodeType||m.isXMLDoc(a)))for(d=ua(f),h=ua(a),g=0;null!=(e=h[g]);++g)d[g]&&Ba(e,d[g]);if(b)if(c)for(h=h||ua(a),d=d||ua(f),g=0;null!=(e=h[g]);g++)Aa(e,d[g]);else Aa(a,f);return d=ua(f,"script"),d.length>0&&za(d,!i&&ua(a,"script")),d=h=e=null,f},buildFragment:function(a,b,c,d){for(var e,f,g,h,i,j,l,n=a.length,o=da(b),p=[],q=0;n>q;q++)if(f=a[q],f||0===f)if("object"===m.type(f))m.merge(p,f.nodeType?[f]:f);else if(la.test(f)){h=h||o.appendChild(b.createElement("div")),i=(ja.exec(f)||["",""])[1].toLowerCase(),l=ra[i]||ra._default,h.innerHTML=l[1]+f.replace(ia,"<$1></$2>")+l[2],e=l[0];while(e--)h=h.lastChild;if(!k.leadingWhitespace&&ha.test(f)&&p.push(b.createTextNode(ha.exec(f)[0])),!k.tbody){f="table"!==i||ka.test(f)?"<table>"!==l[1]||ka.test(f)?0:h:h.firstChild,e=f&&f.childNodes.length;while(e--)m.nodeName(j=f.childNodes[e],"tbody")&&!j.childNodes.length&&f.removeChild(j)}m.merge(p,h.childNodes),h.textContent="";while(h.firstChild)h.removeChild(h.firstChild);h=o.lastChild}else p.push(b.createTextNode(f));h&&o.removeChild(h),k.appendChecked||m.grep(ua(p,"input"),va),q=0;while(f=p[q++])if((!d||-1===m.inArray(f,d))&&(g=m.contains(f.ownerDocument,f),h=ua(o.appendChild(f),"script"),g&&za(h),c)){e=0;while(f=h[e++])oa.test(f.type||"")&&c.push(f)}return h=null,o},cleanData:function(a,b){for(var d,e,f,g,h=0,i=m.expando,j=m.cache,l=k.deleteExpando,n=m.event.special;null!=(d=a[h]);h++)if((b||m.acceptData(d))&&(f=d[i],g=f&&j[f])){if(g.events)for(e in g.events)n[e]?m.event.remove(d,e):m.removeEvent(d,e,g.handle);j[f]&&(delete j[f],l?delete d[i]:typeof d.removeAttribute!==K?d.removeAttribute(i):d[i]=null,c.push(f))}}}),m.fn.extend({text:function(a){return V(this,function(a){return void 0===a?m.text(this):this.empty().append((this[0]&&this[0].ownerDocument||y).createTextNode(a))},null,a,arguments.length)},append:function(){return this.domManip(arguments,function(a){if(1===this.nodeType||11===this.nodeType||9===this.nodeType){var b=wa(this,a);b.appendChild(a)}})},prepend:function(){return this.domManip(arguments,function(a){if(1===this.nodeType||11===this.nodeType||9===this.nodeType){var b=wa(this,a);b.insertBefore(a,b.firstChild)}})},before:function(){return this.domManip(arguments,function(a){this.parentNode&&this.parentNode.insertBefore(a,this)})},after:function(){return this.domManip(arguments,function(a){this.parentNode&&this.parentNode.insertBefore(a,this.nextSibling)})},remove:function(a,b){for(var c,d=a?m.filter(a,this):this,e=0;null!=(c=d[e]);e++)b||1!==c.nodeType||m.cleanData(ua(c)),c.parentNode&&(b&&m.contains(c.ownerDocument,c)&&za(ua(c,"script")),c.parentNode.removeChild(c));return this},empty:function(){for(var a,b=0;null!=(a=this[b]);b++){1===a.nodeType&&m.cleanData(ua(a,!1));while(a.firstChild)a.removeChild(a.firstChild);a.options&&m.nodeName(a,"select")&&(a.options.length=0)}return this},clone:function(a,b){return a=null==a?!1:a,b=null==b?a:b,this.map(function(){return m.clone(this,a,b)})},html:function(a){return V(this,function(a){var b=this[0]||{},c=0,d=this.length;if(void 0===a)return 1===b.nodeType?b.innerHTML.replace(fa,""):void 0;if(!("string"!=typeof a||ma.test(a)||!k.htmlSerialize&&ga.test(a)||!k.leadingWhitespace&&ha.test(a)||ra[(ja.exec(a)||["",""])[1].toLowerCase()])){a=a.replace(ia,"<$1></$2>");try{for(;d>c;c++)b=this[c]||{},1===b.nodeType&&(m.cleanData(ua(b,!1)),b.innerHTML=a);b=0}catch(e){}}b&&this.empty().append(a)},null,a,arguments.length)},replaceWith:function(){var a=arguments[0];return this.domManip(arguments,function(b){a=this.parentNode,m.cleanData(ua(this)),a&&a.replaceChild(b,this)}),a&&(a.length||a.nodeType)?this:this.remove()},detach:function(a){return this.remove(a,!0)},domManip:function(a,b){a=e.apply([],a);var c,d,f,g,h,i,j=0,l=this.length,n=this,o=l-1,p=a[0],q=m.isFunction(p);if(q||l>1&&"string"==typeof p&&!k.checkClone&&na.test(p))return this.each(function(c){var d=n.eq(c);q&&(a[0]=p.call(this,c,d.html())),d.domManip(a,b)});if(l&&(i=m.buildFragment(a,this[0].ownerDocument,!1,this),c=i.firstChild,1===i.childNodes.length&&(i=c),c)){for(g=m.map(ua(i,"script"),xa),f=g.length;l>j;j++)d=i,j!==o&&(d=m.clone(d,!0,!0),f&&m.merge(g,ua(d,"script"))),b.call(this[j],d,j);if(f)for(h=g[g.length-1].ownerDocument,m.map(g,ya),j=0;f>j;j++)d=g[j],oa.test(d.type||"")&&!m._data(d,"globalEval")&&m.contains(h,d)&&(d.src?m._evalUrl&&m._evalUrl(d.src):m.globalEval((d.text||d.textContent||d.innerHTML||"").replace(qa,"")));i=c=null}return this}}),m.each({appendTo:"append",prependTo:"prepend",insertBefore:"before",insertAfter:"after",replaceAll:"replaceWith"},function(a,b){m.fn[a]=function(a){for(var c,d=0,e=[],g=m(a),h=g.length-1;h>=d;d++)c=d===h?this:this.clone(!0),m(g[d])[b](c),f.apply(e,c.get());return this.pushStack(e)}});var Ca,Da={};function Ea(b,c){var d,e=m(c.createElement(b)).appendTo(c.body),f=a.getDefaultComputedStyle&&(d=a.getDefaultComputedStyle(e[0]))?d.display:m.css(e[0],"display");return e.detach(),f}function Fa(a){var b=y,c=Da[a];return c||(c=Ea(a,b),"none"!==c&&c||(Ca=(Ca||m("<iframe frameborder='0' width='0' height='0'/>")).appendTo(b.documentElement),b=(Ca[0].contentWindow||Ca[0].contentDocument).document,b.write(),b.close(),c=Ea(a,b),Ca.detach()),Da[a]=c),c}!function(){var a;k.shrinkWrapBlocks=function(){if(null!=a)return a;a=!1;var b,c,d;return c=y.getElementsByTagName("body")[0],c&&c.style?(b=y.createElement("div"),d=y.createElement("div"),d.style.cssText="position:absolute;border:0;width:0;height:0;top:0;left:-9999px",c.appendChild(d).appendChild(b),typeof b.style.zoom!==K&&(b.style.cssText="-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box;display:block;margin:0;border:0;padding:1px;width:1px;zoom:1",b.appendChild(y.createElement("div")).style.width="5px",a=3!==b.offsetWidth),c.removeChild(d),a):void 0}}();var Ga=/^margin/,Ha=new RegExp("^("+S+")(?!px)[a-z%]+$","i"),Ia,Ja,Ka=/^(top|right|bottom|left)$/;a.getComputedStyle?(Ia=function(b){return b.ownerDocument.defaultView.opener?b.ownerDocument.defaultView.getComputedStyle(b,null):a.getComputedStyle(b,null)},Ja=function(a,b,c){var d,e,f,g,h=a.style;return c=c||Ia(a),g=c?c.getPropertyValue(b)||c[b]:void 0,c&&(""!==g||m.contains(a.ownerDocument,a)||(g=m.style(a,b)),Ha.test(g)&&Ga.test(b)&&(d=h.width,e=h.minWidth,f=h.maxWidth,h.minWidth=h.maxWidth=h.width=g,g=c.width,h.width=d,h.minWidth=e,h.maxWidth=f)),void 0===g?g:g+""}):y.documentElement.currentStyle&&(Ia=function(a){return a.currentStyle},Ja=function(a,b,c){var d,e,f,g,h=a.style;return c=c||Ia(a),g=c?c[b]:void 0,null==g&&h&&h[b]&&(g=h[b]),Ha.test(g)&&!Ka.test(b)&&(d=h.left,e=a.runtimeStyle,f=e&&e.left,f&&(e.left=a.currentStyle.left),h.left="fontSize"===b?"1em":g,g=h.pixelLeft+"px",h.left=d,f&&(e.left=f)),void 0===g?g:g+""||"auto"});function La(a,b){return{get:function(){var c=a();if(null!=c)return c?void delete this.get:(this.get=b).apply(this,arguments)}}}!function(){var b,c,d,e,f,g,h;if(b=y.createElement("div"),b.innerHTML="  <link/><table></table><a href='/a'>a</a><input type='checkbox'/>",d=b.getElementsByTagName("a")[0],c=d&&d.style){c.cssText="float:left;opacity:.5",k.opacity="0.5"===c.opacity,k.cssFloat=!!c.cssFloat,b.style.backgroundClip="content-box",b.cloneNode(!0).style.backgroundClip="",k.clearCloneStyle="content-box"===b.style.backgroundClip,k.boxSizing=""===c.boxSizing||""===c.MozBoxSizing||""===c.WebkitBoxSizing,m.extend(k,{reliableHiddenOffsets:function(){return null==g&&i(),g},boxSizingReliable:function(){return null==f&&i(),f},pixelPosition:function(){return null==e&&i(),e},reliableMarginRight:function(){return null==h&&i(),h}});function i(){var b,c,d,i;c=y.getElementsByTagName("body")[0],c&&c.style&&(b=y.createElement("div"),d=y.createElement("div"),d.style.cssText="position:absolute;border:0;width:0;height:0;top:0;left:-9999px",c.appendChild(d).appendChild(b),b.style.cssText="-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;display:block;margin-top:1%;top:1%;border:1px;padding:1px;width:4px;position:absolute",e=f=!1,h=!0,a.getComputedStyle&&(e="1%"!==(a.getComputedStyle(b,null)||{}).top,f="4px"===(a.getComputedStyle(b,null)||{width:"4px"}).width,i=b.appendChild(y.createElement("div")),i.style.cssText=b.style.cssText="-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box;display:block;margin:0;border:0;padding:0",i.style.marginRight=i.style.width="0",b.style.width="1px",h=!parseFloat((a.getComputedStyle(i,null)||{}).marginRight),b.removeChild(i)),b.innerHTML="<table><tr><td></td><td>t</td></tr></table>",i=b.getElementsByTagName("td"),i[0].style.cssText="margin:0;border:0;padding:0;display:none",g=0===i[0].offsetHeight,g&&(i[0].style.display="",i[1].style.display="none",g=0===i[0].offsetHeight),c.removeChild(d))}}}(),m.swap=function(a,b,c,d){var e,f,g={};for(f in b)g[f]=a.style[f],a.style[f]=b[f];e=c.apply(a,d||[]);for(f in b)a.style[f]=g[f];return e};var Ma=/alpha\([^)]*\)/i,Na=/opacity\s*=\s*([^)]*)/,Oa=/^(none|table(?!-c[ea]).+)/,Pa=new RegExp("^("+S+")(.*)$","i"),Qa=new RegExp("^([+-])=("+S+")","i"),Ra={position:"absolute",visibility:"hidden",display:"block"},Sa={letterSpacing:"0",fontWeight:"400"},Ta=["Webkit","O","Moz","ms"];function Ua(a,b){if(b in a)return b;var c=b.charAt(0).toUpperCase()+b.slice(1),d=b,e=Ta.length;while(e--)if(b=Ta[e]+c,b in a)return b;return d}function Va(a,b){for(var c,d,e,f=[],g=0,h=a.length;h>g;g++)d=a[g],d.style&&(f[g]=m._data(d,"olddisplay"),c=d.style.display,b?(f[g]||"none"!==c||(d.style.display=""),""===d.style.display&&U(d)&&(f[g]=m._data(d,"olddisplay",Fa(d.nodeName)))):(e=U(d),(c&&"none"!==c||!e)&&m._data(d,"olddisplay",e?c:m.css(d,"display"))));for(g=0;h>g;g++)d=a[g],d.style&&(b&&"none"!==d.style.display&&""!==d.style.display||(d.style.display=b?f[g]||"":"none"));return a}function Wa(a,b,c){var d=Pa.exec(b);return d?Math.max(0,d[1]-(c||0))+(d[2]||"px"):b}function Xa(a,b,c,d,e){for(var f=c===(d?"border":"content")?4:"width"===b?1:0,g=0;4>f;f+=2)"margin"===c&&(g+=m.css(a,c+T[f],!0,e)),d?("content"===c&&(g-=m.css(a,"padding"+T[f],!0,e)),"margin"!==c&&(g-=m.css(a,"border"+T[f]+"Width",!0,e))):(g+=m.css(a,"padding"+T[f],!0,e),"padding"!==c&&(g+=m.css(a,"border"+T[f]+"Width",!0,e)));return g}function Ya(a,b,c){var d=!0,e="width"===b?a.offsetWidth:a.offsetHeight,f=Ia(a),g=k.boxSizing&&"border-box"===m.css(a,"boxSizing",!1,f);if(0>=e||null==e){if(e=Ja(a,b,f),(0>e||null==e)&&(e=a.style[b]),Ha.test(e))return e;d=g&&(k.boxSizingReliable()||e===a.style[b]),e=parseFloat(e)||0}return e+Xa(a,b,c||(g?"border":"content"),d,f)+"px"}m.extend({cssHooks:{opacity:{get:function(a,b){if(b){var c=Ja(a,"opacity");return""===c?"1":c}}}},cssNumber:{columnCount:!0,fillOpacity:!0,flexGrow:!0,flexShrink:!0,fontWeight:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,widows:!0,zIndex:!0,zoom:!0},cssProps:{"float":k.cssFloat?"cssFloat":"styleFloat"},style:function(a,b,c,d){if(a&&3!==a.nodeType&&8!==a.nodeType&&a.style){var e,f,g,h=m.camelCase(b),i=a.style;if(b=m.cssProps[h]||(m.cssProps[h]=Ua(i,h)),g=m.cssHooks[b]||m.cssHooks[h],void 0===c)return g&&"get"in g&&void 0!==(e=g.get(a,!1,d))?e:i[b];if(f=typeof c,"string"===f&&(e=Qa.exec(c))&&(c=(e[1]+1)*e[2]+parseFloat(m.css(a,b)),f="number"),null!=c&&c===c&&("number"!==f||m.cssNumber[h]||(c+="px"),k.clearCloneStyle||""!==c||0!==b.indexOf("background")||(i[b]="inherit"),!(g&&"set"in g&&void 0===(c=g.set(a,c,d)))))try{i[b]=c}catch(j){}}},css:function(a,b,c,d){var e,f,g,h=m.camelCase(b);return b=m.cssProps[h]||(m.cssProps[h]=Ua(a.style,h)),g=m.cssHooks[b]||m.cssHooks[h],g&&"get"in g&&(f=g.get(a,!0,c)),void 0===f&&(f=Ja(a,b,d)),"normal"===f&&b in Sa&&(f=Sa[b]),""===c||c?(e=parseFloat(f),c===!0||m.isNumeric(e)?e||0:f):f}}),m.each(["height","width"],function(a,b){m.cssHooks[b]={get:function(a,c,d){return c?Oa.test(m.css(a,"display"))&&0===a.offsetWidth?m.swap(a,Ra,function(){return Ya(a,b,d)}):Ya(a,b,d):void 0},set:function(a,c,d){var e=d&&Ia(a);return Wa(a,c,d?Xa(a,b,d,k.boxSizing&&"border-box"===m.css(a,"boxSizing",!1,e),e):0)}}}),k.opacity||(m.cssHooks.opacity={get:function(a,b){return Na.test((b&&a.currentStyle?a.currentStyle.filter:a.style.filter)||"")?.01*parseFloat(RegExp.$1)+"":b?"1":""},set:function(a,b){var c=a.style,d=a.currentStyle,e=m.isNumeric(b)?"alpha(opacity="+100*b+")":"",f=d&&d.filter||c.filter||"";c.zoom=1,(b>=1||""===b)&&""===m.trim(f.replace(Ma,""))&&c.removeAttribute&&(c.removeAttribute("filter"),""===b||d&&!d.filter)||(c.filter=Ma.test(f)?f.replace(Ma,e):f+" "+e)}}),m.cssHooks.marginRight=La(k.reliableMarginRight,function(a,b){return b?m.swap(a,{display:"inline-block"},Ja,[a,"marginRight"]):void 0}),m.each({margin:"",padding:"",border:"Width"},function(a,b){m.cssHooks[a+b]={expand:function(c){for(var d=0,e={},f="string"==typeof c?c.split(" "):[c];4>d;d++)e[a+T[d]+b]=f[d]||f[d-2]||f[0];return e}},Ga.test(a)||(m.cssHooks[a+b].set=Wa)}),m.fn.extend({css:function(a,b){return V(this,function(a,b,c){var d,e,f={},g=0;if(m.isArray(b)){for(d=Ia(a),e=b.length;e>g;g++)f[b[g]]=m.css(a,b[g],!1,d);return f}return void 0!==c?m.style(a,b,c):m.css(a,b)},a,b,arguments.length>1)},show:function(){return Va(this,!0)},hide:function(){return Va(this)},toggle:function(a){return"boolean"==typeof a?a?this.show():this.hide():this.each(function(){U(this)?m(this).show():m(this).hide()})}});function Za(a,b,c,d,e){
return new Za.prototype.init(a,b,c,d,e)}m.Tween=Za,Za.prototype={constructor:Za,init:function(a,b,c,d,e,f){this.elem=a,this.prop=c,this.easing=e||"swing",this.options=b,this.start=this.now=this.cur(),this.end=d,this.unit=f||(m.cssNumber[c]?"":"px")},cur:function(){var a=Za.propHooks[this.prop];return a&&a.get?a.get(this):Za.propHooks._default.get(this)},run:function(a){var b,c=Za.propHooks[this.prop];return this.options.duration?this.pos=b=m.easing[this.easing](a,this.options.duration*a,0,1,this.options.duration):this.pos=b=a,this.now=(this.end-this.start)*b+this.start,this.options.step&&this.options.step.call(this.elem,this.now,this),c&&c.set?c.set(this):Za.propHooks._default.set(this),this}},Za.prototype.init.prototype=Za.prototype,Za.propHooks={_default:{get:function(a){var b;return null==a.elem[a.prop]||a.elem.style&&null!=a.elem.style[a.prop]?(b=m.css(a.elem,a.prop,""),b&&"auto"!==b?b:0):a.elem[a.prop]},set:function(a){m.fx.step[a.prop]?m.fx.step[a.prop](a):a.elem.style&&(null!=a.elem.style[m.cssProps[a.prop]]||m.cssHooks[a.prop])?m.style(a.elem,a.prop,a.now+a.unit):a.elem[a.prop]=a.now}}},Za.propHooks.scrollTop=Za.propHooks.scrollLeft={set:function(a){a.elem.nodeType&&a.elem.parentNode&&(a.elem[a.prop]=a.now)}},m.easing={linear:function(a){return a},swing:function(a){return.5-Math.cos(a*Math.PI)/2}},m.fx=Za.prototype.init,m.fx.step={};var $a,_a,ab=/^(?:toggle|show|hide)$/,bb=new RegExp("^(?:([+-])=|)("+S+")([a-z%]*)$","i"),cb=/queueHooks$/,db=[ib],eb={"*":[function(a,b){var c=this.createTween(a,b),d=c.cur(),e=bb.exec(b),f=e&&e[3]||(m.cssNumber[a]?"":"px"),g=(m.cssNumber[a]||"px"!==f&&+d)&&bb.exec(m.css(c.elem,a)),h=1,i=20;if(g&&g[3]!==f){f=f||g[3],e=e||[],g=+d||1;do h=h||".5",g/=h,m.style(c.elem,a,g+f);while(h!==(h=c.cur()/d)&&1!==h&&--i)}return e&&(g=c.start=+g||+d||0,c.unit=f,c.end=e[1]?g+(e[1]+1)*e[2]:+e[2]),c}]};function fb(){return setTimeout(function(){$a=void 0}),$a=m.now()}function gb(a,b){var c,d={height:a},e=0;for(b=b?1:0;4>e;e+=2-b)c=T[e],d["margin"+c]=d["padding"+c]=a;return b&&(d.opacity=d.width=a),d}function hb(a,b,c){for(var d,e=(eb[b]||[]).concat(eb["*"]),f=0,g=e.length;g>f;f++)if(d=e[f].call(c,b,a))return d}function ib(a,b,c){var d,e,f,g,h,i,j,l,n=this,o={},p=a.style,q=a.nodeType&&U(a),r=m._data(a,"fxshow");c.queue||(h=m._queueHooks(a,"fx"),null==h.unqueued&&(h.unqueued=0,i=h.empty.fire,h.empty.fire=function(){h.unqueued||i()}),h.unqueued++,n.always(function(){n.always(function(){h.unqueued--,m.queue(a,"fx").length||h.empty.fire()})})),1===a.nodeType&&("height"in b||"width"in b)&&(c.overflow=[p.overflow,p.overflowX,p.overflowY],j=m.css(a,"display"),l="none"===j?m._data(a,"olddisplay")||Fa(a.nodeName):j,"inline"===l&&"none"===m.css(a,"float")&&(k.inlineBlockNeedsLayout&&"inline"!==Fa(a.nodeName)?p.zoom=1:p.display="inline-block")),c.overflow&&(p.overflow="hidden",k.shrinkWrapBlocks()||n.always(function(){p.overflow=c.overflow[0],p.overflowX=c.overflow[1],p.overflowY=c.overflow[2]}));for(d in b)if(e=b[d],ab.exec(e)){if(delete b[d],f=f||"toggle"===e,e===(q?"hide":"show")){if("show"!==e||!r||void 0===r[d])continue;q=!0}o[d]=r&&r[d]||m.style(a,d)}else j=void 0;if(m.isEmptyObject(o))"inline"===("none"===j?Fa(a.nodeName):j)&&(p.display=j);else{r?"hidden"in r&&(q=r.hidden):r=m._data(a,"fxshow",{}),f&&(r.hidden=!q),q?m(a).show():n.done(function(){m(a).hide()}),n.done(function(){var b;m._removeData(a,"fxshow");for(b in o)m.style(a,b,o[b])});for(d in o)g=hb(q?r[d]:0,d,n),d in r||(r[d]=g.start,q&&(g.end=g.start,g.start="width"===d||"height"===d?1:0))}}function jb(a,b){var c,d,e,f,g;for(c in a)if(d=m.camelCase(c),e=b[d],f=a[c],m.isArray(f)&&(e=f[1],f=a[c]=f[0]),c!==d&&(a[d]=f,delete a[c]),g=m.cssHooks[d],g&&"expand"in g){f=g.expand(f),delete a[d];for(c in f)c in a||(a[c]=f[c],b[c]=e)}else b[d]=e}function kb(a,b,c){var d,e,f=0,g=db.length,h=m.Deferred().always(function(){delete i.elem}),i=function(){if(e)return!1;for(var b=$a||fb(),c=Math.max(0,j.startTime+j.duration-b),d=c/j.duration||0,f=1-d,g=0,i=j.tweens.length;i>g;g++)j.tweens[g].run(f);return h.notifyWith(a,[j,f,c]),1>f&&i?c:(h.resolveWith(a,[j]),!1)},j=h.promise({elem:a,props:m.extend({},b),opts:m.extend(!0,{specialEasing:{}},c),originalProperties:b,originalOptions:c,startTime:$a||fb(),duration:c.duration,tweens:[],createTween:function(b,c){var d=m.Tween(a,j.opts,b,c,j.opts.specialEasing[b]||j.opts.easing);return j.tweens.push(d),d},stop:function(b){var c=0,d=b?j.tweens.length:0;if(e)return this;for(e=!0;d>c;c++)j.tweens[c].run(1);return b?h.resolveWith(a,[j,b]):h.rejectWith(a,[j,b]),this}}),k=j.props;for(jb(k,j.opts.specialEasing);g>f;f++)if(d=db[f].call(j,a,k,j.opts))return d;return m.map(k,hb,j),m.isFunction(j.opts.start)&&j.opts.start.call(a,j),m.fx.timer(m.extend(i,{elem:a,anim:j,queue:j.opts.queue})),j.progress(j.opts.progress).done(j.opts.done,j.opts.complete).fail(j.opts.fail).always(j.opts.always)}m.Animation=m.extend(kb,{tweener:function(a,b){m.isFunction(a)?(b=a,a=["*"]):a=a.split(" ");for(var c,d=0,e=a.length;e>d;d++)c=a[d],eb[c]=eb[c]||[],eb[c].unshift(b)},prefilter:function(a,b){b?db.unshift(a):db.push(a)}}),m.speed=function(a,b,c){var d=a&&"object"==typeof a?m.extend({},a):{complete:c||!c&&b||m.isFunction(a)&&a,duration:a,easing:c&&b||b&&!m.isFunction(b)&&b};return d.duration=m.fx.off?0:"number"==typeof d.duration?d.duration:d.duration in m.fx.speeds?m.fx.speeds[d.duration]:m.fx.speeds._default,(null==d.queue||d.queue===!0)&&(d.queue="fx"),d.old=d.complete,d.complete=function(){m.isFunction(d.old)&&d.old.call(this),d.queue&&m.dequeue(this,d.queue)},d},m.fn.extend({fadeTo:function(a,b,c,d){return this.filter(U).css("opacity",0).show().end().animate({opacity:b},a,c,d)},animate:function(a,b,c,d){var e=m.isEmptyObject(a),f=m.speed(b,c,d),g=function(){var b=kb(this,m.extend({},a),f);(e||m._data(this,"finish"))&&b.stop(!0)};return g.finish=g,e||f.queue===!1?this.each(g):this.queue(f.queue,g)},stop:function(a,b,c){var d=function(a){var b=a.stop;delete a.stop,b(c)};return"string"!=typeof a&&(c=b,b=a,a=void 0),b&&a!==!1&&this.queue(a||"fx",[]),this.each(function(){var b=!0,e=null!=a&&a+"queueHooks",f=m.timers,g=m._data(this);if(e)g[e]&&g[e].stop&&d(g[e]);else for(e in g)g[e]&&g[e].stop&&cb.test(e)&&d(g[e]);for(e=f.length;e--;)f[e].elem!==this||null!=a&&f[e].queue!==a||(f[e].anim.stop(c),b=!1,f.splice(e,1));(b||!c)&&m.dequeue(this,a)})},finish:function(a){return a!==!1&&(a=a||"fx"),this.each(function(){var b,c=m._data(this),d=c[a+"queue"],e=c[a+"queueHooks"],f=m.timers,g=d?d.length:0;for(c.finish=!0,m.queue(this,a,[]),e&&e.stop&&e.stop.call(this,!0),b=f.length;b--;)f[b].elem===this&&f[b].queue===a&&(f[b].anim.stop(!0),f.splice(b,1));for(b=0;g>b;b++)d[b]&&d[b].finish&&d[b].finish.call(this);delete c.finish})}}),m.each(["toggle","show","hide"],function(a,b){var c=m.fn[b];m.fn[b]=function(a,d,e){return null==a||"boolean"==typeof a?c.apply(this,arguments):this.animate(gb(b,!0),a,d,e)}}),m.each({slideDown:gb("show"),slideUp:gb("hide"),slideToggle:gb("toggle"),fadeIn:{opacity:"show"},fadeOut:{opacity:"hide"},fadeToggle:{opacity:"toggle"}},function(a,b){m.fn[a]=function(a,c,d){return this.animate(b,a,c,d)}}),m.timers=[],m.fx.tick=function(){var a,b=m.timers,c=0;for($a=m.now();c<b.length;c++)a=b[c],a()||b[c]!==a||b.splice(c--,1);b.length||m.fx.stop(),$a=void 0},m.fx.timer=function(a){m.timers.push(a),a()?m.fx.start():m.timers.pop()},m.fx.interval=13,m.fx.start=function(){_a||(_a=setInterval(m.fx.tick,m.fx.interval))},m.fx.stop=function(){clearInterval(_a),_a=null},m.fx.speeds={slow:600,fast:200,_default:400},m.fn.delay=function(a,b){return a=m.fx?m.fx.speeds[a]||a:a,b=b||"fx",this.queue(b,function(b,c){var d=setTimeout(b,a);c.stop=function(){clearTimeout(d)}})},function(){var a,b,c,d,e;b=y.createElement("div"),b.setAttribute("className","t"),b.innerHTML="  <link/><table></table><a href='/a'>a</a><input type='checkbox'/>",d=b.getElementsByTagName("a")[0],c=y.createElement("select"),e=c.appendChild(y.createElement("option")),a=b.getElementsByTagName("input")[0],d.style.cssText="top:1px",k.getSetAttribute="t"!==b.className,k.style=/top/.test(d.getAttribute("style")),k.hrefNormalized="/a"===d.getAttribute("href"),k.checkOn=!!a.value,k.optSelected=e.selected,k.enctype=!!y.createElement("form").enctype,c.disabled=!0,k.optDisabled=!e.disabled,a=y.createElement("input"),a.setAttribute("value",""),k.input=""===a.getAttribute("value"),a.value="t",a.setAttribute("type","radio"),k.radioValue="t"===a.value}();var lb=/\r/g;m.fn.extend({val:function(a){var b,c,d,e=this[0];{if(arguments.length)return d=m.isFunction(a),this.each(function(c){var e;1===this.nodeType&&(e=d?a.call(this,c,m(this).val()):a,null==e?e="":"number"==typeof e?e+="":m.isArray(e)&&(e=m.map(e,function(a){return null==a?"":a+""})),b=m.valHooks[this.type]||m.valHooks[this.nodeName.toLowerCase()],b&&"set"in b&&void 0!==b.set(this,e,"value")||(this.value=e))});if(e)return b=m.valHooks[e.type]||m.valHooks[e.nodeName.toLowerCase()],b&&"get"in b&&void 0!==(c=b.get(e,"value"))?c:(c=e.value,"string"==typeof c?c.replace(lb,""):null==c?"":c)}}}),m.extend({valHooks:{option:{get:function(a){var b=m.find.attr(a,"value");return null!=b?b:m.trim(m.text(a))}},select:{get:function(a){for(var b,c,d=a.options,e=a.selectedIndex,f="select-one"===a.type||0>e,g=f?null:[],h=f?e+1:d.length,i=0>e?h:f?e:0;h>i;i++)if(c=d[i],!(!c.selected&&i!==e||(k.optDisabled?c.disabled:null!==c.getAttribute("disabled"))||c.parentNode.disabled&&m.nodeName(c.parentNode,"optgroup"))){if(b=m(c).val(),f)return b;g.push(b)}return g},set:function(a,b){var c,d,e=a.options,f=m.makeArray(b),g=e.length;while(g--)if(d=e[g],m.inArray(m.valHooks.option.get(d),f)>=0)try{d.selected=c=!0}catch(h){d.scrollHeight}else d.selected=!1;return c||(a.selectedIndex=-1),e}}}}),m.each(["radio","checkbox"],function(){m.valHooks[this]={set:function(a,b){return m.isArray(b)?a.checked=m.inArray(m(a).val(),b)>=0:void 0}},k.checkOn||(m.valHooks[this].get=function(a){return null===a.getAttribute("value")?"on":a.value})});var mb,nb,ob=m.expr.attrHandle,pb=/^(?:checked|selected)$/i,qb=k.getSetAttribute,rb=k.input;m.fn.extend({attr:function(a,b){return V(this,m.attr,a,b,arguments.length>1)},removeAttr:function(a){return this.each(function(){m.removeAttr(this,a)})}}),m.extend({attr:function(a,b,c){var d,e,f=a.nodeType;if(a&&3!==f&&8!==f&&2!==f)return typeof a.getAttribute===K?m.prop(a,b,c):(1===f&&m.isXMLDoc(a)||(b=b.toLowerCase(),d=m.attrHooks[b]||(m.expr.match.bool.test(b)?nb:mb)),void 0===c?d&&"get"in d&&null!==(e=d.get(a,b))?e:(e=m.find.attr(a,b),null==e?void 0:e):null!==c?d&&"set"in d&&void 0!==(e=d.set(a,c,b))?e:(a.setAttribute(b,c+""),c):void m.removeAttr(a,b))},removeAttr:function(a,b){var c,d,e=0,f=b&&b.match(E);if(f&&1===a.nodeType)while(c=f[e++])d=m.propFix[c]||c,m.expr.match.bool.test(c)?rb&&qb||!pb.test(c)?a[d]=!1:a[m.camelCase("default-"+c)]=a[d]=!1:m.attr(a,c,""),a.removeAttribute(qb?c:d)},attrHooks:{type:{set:function(a,b){if(!k.radioValue&&"radio"===b&&m.nodeName(a,"input")){var c=a.value;return a.setAttribute("type",b),c&&(a.value=c),b}}}}}),nb={set:function(a,b,c){return b===!1?m.removeAttr(a,c):rb&&qb||!pb.test(c)?a.setAttribute(!qb&&m.propFix[c]||c,c):a[m.camelCase("default-"+c)]=a[c]=!0,c}},m.each(m.expr.match.bool.source.match(/\w+/g),function(a,b){var c=ob[b]||m.find.attr;ob[b]=rb&&qb||!pb.test(b)?function(a,b,d){var e,f;return d||(f=ob[b],ob[b]=e,e=null!=c(a,b,d)?b.toLowerCase():null,ob[b]=f),e}:function(a,b,c){return c?void 0:a[m.camelCase("default-"+b)]?b.toLowerCase():null}}),rb&&qb||(m.attrHooks.value={set:function(a,b,c){return m.nodeName(a,"input")?void(a.defaultValue=b):mb&&mb.set(a,b,c)}}),qb||(mb={set:function(a,b,c){var d=a.getAttributeNode(c);return d||a.setAttributeNode(d=a.ownerDocument.createAttribute(c)),d.value=b+="","value"===c||b===a.getAttribute(c)?b:void 0}},ob.id=ob.name=ob.coords=function(a,b,c){var d;return c?void 0:(d=a.getAttributeNode(b))&&""!==d.value?d.value:null},m.valHooks.button={get:function(a,b){var c=a.getAttributeNode(b);return c&&c.specified?c.value:void 0},set:mb.set},m.attrHooks.contenteditable={set:function(a,b,c){mb.set(a,""===b?!1:b,c)}},m.each(["width","height"],function(a,b){m.attrHooks[b]={set:function(a,c){return""===c?(a.setAttribute(b,"auto"),c):void 0}}})),k.style||(m.attrHooks.style={get:function(a){return a.style.cssText||void 0},set:function(a,b){return a.style.cssText=b+""}});var sb=/^(?:input|select|textarea|button|object)$/i,tb=/^(?:a|area)$/i;m.fn.extend({prop:function(a,b){return V(this,m.prop,a,b,arguments.length>1)},removeProp:function(a){return a=m.propFix[a]||a,this.each(function(){try{this[a]=void 0,delete this[a]}catch(b){}})}}),m.extend({propFix:{"for":"htmlFor","class":"className"},prop:function(a,b,c){var d,e,f,g=a.nodeType;if(a&&3!==g&&8!==g&&2!==g)return f=1!==g||!m.isXMLDoc(a),f&&(b=m.propFix[b]||b,e=m.propHooks[b]),void 0!==c?e&&"set"in e&&void 0!==(d=e.set(a,c,b))?d:a[b]=c:e&&"get"in e&&null!==(d=e.get(a,b))?d:a[b]},propHooks:{tabIndex:{get:function(a){var b=m.find.attr(a,"tabindex");return b?parseInt(b,10):sb.test(a.nodeName)||tb.test(a.nodeName)&&a.href?0:-1}}}}),k.hrefNormalized||m.each(["href","src"],function(a,b){m.propHooks[b]={get:function(a){return a.getAttribute(b,4)}}}),k.optSelected||(m.propHooks.selected={get:function(a){var b=a.parentNode;return b&&(b.selectedIndex,b.parentNode&&b.parentNode.selectedIndex),null}}),m.each(["tabIndex","readOnly","maxLength","cellSpacing","cellPadding","rowSpan","colSpan","useMap","frameBorder","contentEditable"],function(){m.propFix[this.toLowerCase()]=this}),k.enctype||(m.propFix.enctype="encoding");var ub=/[\t\r\n\f]/g;m.fn.extend({addClass:function(a){var b,c,d,e,f,g,h=0,i=this.length,j="string"==typeof a&&a;if(m.isFunction(a))return this.each(function(b){m(this).addClass(a.call(this,b,this.className))});if(j)for(b=(a||"").match(E)||[];i>h;h++)if(c=this[h],d=1===c.nodeType&&(c.className?(" "+c.className+" ").replace(ub," "):" ")){f=0;while(e=b[f++])d.indexOf(" "+e+" ")<0&&(d+=e+" ");g=m.trim(d),c.className!==g&&(c.className=g)}return this},removeClass:function(a){var b,c,d,e,f,g,h=0,i=this.length,j=0===arguments.length||"string"==typeof a&&a;if(m.isFunction(a))return this.each(function(b){m(this).removeClass(a.call(this,b,this.className))});if(j)for(b=(a||"").match(E)||[];i>h;h++)if(c=this[h],d=1===c.nodeType&&(c.className?(" "+c.className+" ").replace(ub," "):"")){f=0;while(e=b[f++])while(d.indexOf(" "+e+" ")>=0)d=d.replace(" "+e+" "," ");g=a?m.trim(d):"",c.className!==g&&(c.className=g)}return this},toggleClass:function(a,b){var c=typeof a;return"boolean"==typeof b&&"string"===c?b?this.addClass(a):this.removeClass(a):this.each(m.isFunction(a)?function(c){m(this).toggleClass(a.call(this,c,this.className,b),b)}:function(){if("string"===c){var b,d=0,e=m(this),f=a.match(E)||[];while(b=f[d++])e.hasClass(b)?e.removeClass(b):e.addClass(b)}else(c===K||"boolean"===c)&&(this.className&&m._data(this,"__className__",this.className),this.className=this.className||a===!1?"":m._data(this,"__className__")||"")})},hasClass:function(a){for(var b=" "+a+" ",c=0,d=this.length;d>c;c++)if(1===this[c].nodeType&&(" "+this[c].className+" ").replace(ub," ").indexOf(b)>=0)return!0;return!1}}),m.each("blur focus focusin focusout load resize scroll unload click dblclick mousedown mouseup mousemove mouseover mouseout mouseenter mouseleave change select submit keydown keypress keyup error contextmenu".split(" "),function(a,b){m.fn[b]=function(a,c){return arguments.length>0?this.on(b,null,a,c):this.trigger(b)}}),m.fn.extend({hover:function(a,b){return this.mouseenter(a).mouseleave(b||a)},bind:function(a,b,c){return this.on(a,null,b,c)},unbind:function(a,b){return this.off(a,null,b)},delegate:function(a,b,c,d){return this.on(b,a,c,d)},undelegate:function(a,b,c){return 1===arguments.length?this.off(a,"**"):this.off(b,a||"**",c)}});var vb=m.now(),wb=/\?/,xb=/(,)|(\[|{)|(}|])|"(?:[^"\\\r\n]|\\["\\\/bfnrt]|\\u[\da-fA-F]{4})*"\s*:?|true|false|null|-?(?!0\d)\d+(?:\.\d+|)(?:[eE][+-]?\d+|)/g;m.parseJSON=function(b){if(a.JSON&&a.JSON.parse)return a.JSON.parse(b+"");var c,d=null,e=m.trim(b+"");return e&&!m.trim(e.replace(xb,function(a,b,e,f){return c&&b&&(d=0),0===d?a:(c=e||b,d+=!f-!e,"")}))?Function("return "+e)():m.error("Invalid JSON: "+b)},m.parseXML=function(b){var c,d;if(!b||"string"!=typeof b)return null;try{a.DOMParser?(d=new DOMParser,c=d.parseFromString(b,"text/xml")):(c=new ActiveXObject("Microsoft.XMLDOM"),c.async="false",c.loadXML(b))}catch(e){c=void 0}return c&&c.documentElement&&!c.getElementsByTagName("parsererror").length||m.error("Invalid XML: "+b),c};var yb,zb,Ab=/#.*$/,Bb=/([?&])_=[^&]*/,Cb=/^(.*?):[ \t]*([^\r\n]*)\r?$/gm,Db=/^(?:about|app|app-storage|.+-extension|file|res|widget):$/,Eb=/^(?:GET|HEAD)$/,Fb=/^\/\//,Gb=/^([\w.+-]+:)(?:\/\/(?:[^\/?#]*@|)([^\/?#:]*)(?::(\d+)|)|)/,Hb={},Ib={},Jb="*/".concat("*");try{zb=location.href}catch(Kb){zb=y.createElement("a"),zb.href="",zb=zb.href}yb=Gb.exec(zb.toLowerCase())||[];function Lb(a){return function(b,c){"string"!=typeof b&&(c=b,b="*");var d,e=0,f=b.toLowerCase().match(E)||[];if(m.isFunction(c))while(d=f[e++])"+"===d.charAt(0)?(d=d.slice(1)||"*",(a[d]=a[d]||[]).unshift(c)):(a[d]=a[d]||[]).push(c)}}function Mb(a,b,c,d){var e={},f=a===Ib;function g(h){var i;return e[h]=!0,m.each(a[h]||[],function(a,h){var j=h(b,c,d);return"string"!=typeof j||f||e[j]?f?!(i=j):void 0:(b.dataTypes.unshift(j),g(j),!1)}),i}return g(b.dataTypes[0])||!e["*"]&&g("*")}function Nb(a,b){var c,d,e=m.ajaxSettings.flatOptions||{};for(d in b)void 0!==b[d]&&((e[d]?a:c||(c={}))[d]=b[d]);return c&&m.extend(!0,a,c),a}function Ob(a,b,c){var d,e,f,g,h=a.contents,i=a.dataTypes;while("*"===i[0])i.shift(),void 0===e&&(e=a.mimeType||b.getResponseHeader("Content-Type"));if(e)for(g in h)if(h[g]&&h[g].test(e)){i.unshift(g);break}if(i[0]in c)f=i[0];else{for(g in c){if(!i[0]||a.converters[g+" "+i[0]]){f=g;break}d||(d=g)}f=f||d}return f?(f!==i[0]&&i.unshift(f),c[f]):void 0}function Pb(a,b,c,d){var e,f,g,h,i,j={},k=a.dataTypes.slice();if(k[1])for(g in a.converters)j[g.toLowerCase()]=a.converters[g];f=k.shift();while(f)if(a.responseFields[f]&&(c[a.responseFields[f]]=b),!i&&d&&a.dataFilter&&(b=a.dataFilter(b,a.dataType)),i=f,f=k.shift())if("*"===f)f=i;else if("*"!==i&&i!==f){if(g=j[i+" "+f]||j["* "+f],!g)for(e in j)if(h=e.split(" "),h[1]===f&&(g=j[i+" "+h[0]]||j["* "+h[0]])){g===!0?g=j[e]:j[e]!==!0&&(f=h[0],k.unshift(h[1]));break}if(g!==!0)if(g&&a["throws"])b=g(b);else try{b=g(b)}catch(l){return{state:"parsererror",error:g?l:"No conversion from "+i+" to "+f}}}return{state:"success",data:b}}m.extend({active:0,lastModified:{},etag:{},ajaxSettings:{url:zb,type:"GET",isLocal:Db.test(yb[1]),global:!0,processData:!0,async:!0,contentType:"application/x-www-form-urlencoded; charset=UTF-8",accepts:{"*":Jb,text:"text/plain",html:"text/html",xml:"application/xml, text/xml",json:"application/json, text/javascript"},contents:{xml:/xml/,html:/html/,json:/json/},responseFields:{xml:"responseXML",text:"responseText",json:"responseJSON"},converters:{"* text":String,"text html":!0,"text json":m.parseJSON,"text xml":m.parseXML},flatOptions:{url:!0,context:!0}},ajaxSetup:function(a,b){return b?Nb(Nb(a,m.ajaxSettings),b):Nb(m.ajaxSettings,a)},ajaxPrefilter:Lb(Hb),ajaxTransport:Lb(Ib),ajax:function(a,b){"object"==typeof a&&(b=a,a=void 0),b=b||{};var c,d,e,f,g,h,i,j,k=m.ajaxSetup({},b),l=k.context||k,n=k.context&&(l.nodeType||l.jquery)?m(l):m.event,o=m.Deferred(),p=m.Callbacks("once memory"),q=k.statusCode||{},r={},s={},t=0,u="canceled",v={readyState:0,getResponseHeader:function(a){var b;if(2===t){if(!j){j={};while(b=Cb.exec(f))j[b[1].toLowerCase()]=b[2]}b=j[a.toLowerCase()]}return null==b?null:b},getAllResponseHeaders:function(){return 2===t?f:null},setRequestHeader:function(a,b){var c=a.toLowerCase();return t||(a=s[c]=s[c]||a,r[a]=b),this},overrideMimeType:function(a){return t||(k.mimeType=a),this},statusCode:function(a){var b;if(a)if(2>t)for(b in a)q[b]=[q[b],a[b]];else v.always(a[v.status]);return this},abort:function(a){var b=a||u;return i&&i.abort(b),x(0,b),this}};if(o.promise(v).complete=p.add,v.success=v.done,v.error=v.fail,k.url=((a||k.url||zb)+"").replace(Ab,"").replace(Fb,yb[1]+"//"),k.type=b.method||b.type||k.method||k.type,k.dataTypes=m.trim(k.dataType||"*").toLowerCase().match(E)||[""],null==k.crossDomain&&(c=Gb.exec(k.url.toLowerCase()),k.crossDomain=!(!c||c[1]===yb[1]&&c[2]===yb[2]&&(c[3]||("http:"===c[1]?"80":"443"))===(yb[3]||("http:"===yb[1]?"80":"443")))),k.data&&k.processData&&"string"!=typeof k.data&&(k.data=m.param(k.data,k.traditional)),Mb(Hb,k,b,v),2===t)return v;h=m.event&&k.global,h&&0===m.active++&&m.event.trigger("ajaxStart"),k.type=k.type.toUpperCase(),k.hasContent=!Eb.test(k.type),e=k.url,k.hasContent||(k.data&&(e=k.url+=(wb.test(e)?"&":"?")+k.data,delete k.data),k.cache===!1&&(k.url=Bb.test(e)?e.replace(Bb,"$1_="+vb++):e+(wb.test(e)?"&":"?")+"_="+vb++)),k.ifModified&&(m.lastModified[e]&&v.setRequestHeader("If-Modified-Since",m.lastModified[e]),m.etag[e]&&v.setRequestHeader("If-None-Match",m.etag[e])),(k.data&&k.hasContent&&k.contentType!==!1||b.contentType)&&v.setRequestHeader("Content-Type",k.contentType),v.setRequestHeader("Accept",k.dataTypes[0]&&k.accepts[k.dataTypes[0]]?k.accepts[k.dataTypes[0]]+("*"!==k.dataTypes[0]?", "+Jb+"; q=0.01":""):k.accepts["*"]);for(d in k.headers)v.setRequestHeader(d,k.headers[d]);if(k.beforeSend&&(k.beforeSend.call(l,v,k)===!1||2===t))return v.abort();u="abort";for(d in{success:1,error:1,complete:1})v[d](k[d]);if(i=Mb(Ib,k,b,v)){v.readyState=1,h&&n.trigger("ajaxSend",[v,k]),k.async&&k.timeout>0&&(g=setTimeout(function(){v.abort("timeout")},k.timeout));try{t=1,i.send(r,x)}catch(w){if(!(2>t))throw w;x(-1,w)}}else x(-1,"No Transport");function x(a,b,c,d){var j,r,s,u,w,x=b;2!==t&&(t=2,g&&clearTimeout(g),i=void 0,f=d||"",v.readyState=a>0?4:0,j=a>=200&&300>a||304===a,c&&(u=Ob(k,v,c)),u=Pb(k,u,v,j),j?(k.ifModified&&(w=v.getResponseHeader("Last-Modified"),w&&(m.lastModified[e]=w),w=v.getResponseHeader("etag"),w&&(m.etag[e]=w)),204===a||"HEAD"===k.type?x="nocontent":304===a?x="notmodified":(x=u.state,r=u.data,s=u.error,j=!s)):(s=x,(a||!x)&&(x="error",0>a&&(a=0))),v.status=a,v.statusText=(b||x)+"",j?o.resolveWith(l,[r,x,v]):o.rejectWith(l,[v,x,s]),v.statusCode(q),q=void 0,h&&n.trigger(j?"ajaxSuccess":"ajaxError",[v,k,j?r:s]),p.fireWith(l,[v,x]),h&&(n.trigger("ajaxComplete",[v,k]),--m.active||m.event.trigger("ajaxStop")))}return v},getJSON:function(a,b,c){return m.get(a,b,c,"json")},getScript:function(a,b){return m.get(a,void 0,b,"script")}}),m.each(["get","post"],function(a,b){m[b]=function(a,c,d,e){return m.isFunction(c)&&(e=e||d,d=c,c=void 0),m.ajax({url:a,type:b,dataType:e,data:c,success:d})}}),m._evalUrl=function(a){return m.ajax({url:a,type:"GET",dataType:"script",async:!1,global:!1,"throws":!0})},m.fn.extend({wrapAll:function(a){if(m.isFunction(a))return this.each(function(b){m(this).wrapAll(a.call(this,b))});if(this[0]){var b=m(a,this[0].ownerDocument).eq(0).clone(!0);this[0].parentNode&&b.insertBefore(this[0]),b.map(function(){var a=this;while(a.firstChild&&1===a.firstChild.nodeType)a=a.firstChild;return a}).append(this)}return this},wrapInner:function(a){return this.each(m.isFunction(a)?function(b){m(this).wrapInner(a.call(this,b))}:function(){var b=m(this),c=b.contents();c.length?c.wrapAll(a):b.append(a)})},wrap:function(a){var b=m.isFunction(a);return this.each(function(c){m(this).wrapAll(b?a.call(this,c):a)})},unwrap:function(){return this.parent().each(function(){m.nodeName(this,"body")||m(this).replaceWith(this.childNodes)}).end()}}),m.expr.filters.hidden=function(a){return a.offsetWidth<=0&&a.offsetHeight<=0||!k.reliableHiddenOffsets()&&"none"===(a.style&&a.style.display||m.css(a,"display"))},m.expr.filters.visible=function(a){return!m.expr.filters.hidden(a)};var Qb=/%20/g,Rb=/\[\]$/,Sb=/\r?\n/g,Tb=/^(?:submit|button|image|reset|file)$/i,Ub=/^(?:input|select|textarea|keygen)/i;function Vb(a,b,c,d){var e;if(m.isArray(b))m.each(b,function(b,e){c||Rb.test(a)?d(a,e):Vb(a+"["+("object"==typeof e?b:"")+"]",e,c,d)});else if(c||"object"!==m.type(b))d(a,b);else for(e in b)Vb(a+"["+e+"]",b[e],c,d)}m.param=function(a,b){var c,d=[],e=function(a,b){b=m.isFunction(b)?b():null==b?"":b,d[d.length]=encodeURIComponent(a)+"="+encodeURIComponent(b)};if(void 0===b&&(b=m.ajaxSettings&&m.ajaxSettings.traditional),m.isArray(a)||a.jquery&&!m.isPlainObject(a))m.each(a,function(){e(this.name,this.value)});else for(c in a)Vb(c,a[c],b,e);return d.join("&").replace(Qb,"+")},m.fn.extend({serialize:function(){return m.param(this.serializeArray())},serializeArray:function(){return this.map(function(){var a=m.prop(this,"elements");return a?m.makeArray(a):this}).filter(function(){var a=this.type;return this.name&&!m(this).is(":disabled")&&Ub.test(this.nodeName)&&!Tb.test(a)&&(this.checked||!W.test(a))}).map(function(a,b){var c=m(this).val();return null==c?null:m.isArray(c)?m.map(c,function(a){return{name:b.name,value:a.replace(Sb,"\r\n")}}):{name:b.name,value:c.replace(Sb,"\r\n")}}).get()}}),m.ajaxSettings.xhr=void 0!==a.ActiveXObject?function(){return!this.isLocal&&/^(get|post|head|put|delete|options)$/i.test(this.type)&&Zb()||$b()}:Zb;var Wb=0,Xb={},Yb=m.ajaxSettings.xhr();a.attachEvent&&a.attachEvent("onunload",function(){for(var a in Xb)Xb[a](void 0,!0)}),k.cors=!!Yb&&"withCredentials"in Yb,Yb=k.ajax=!!Yb,Yb&&m.ajaxTransport(function(a){if(!a.crossDomain||k.cors){var b;return{send:function(c,d){var e,f=a.xhr(),g=++Wb;if(f.open(a.type,a.url,a.async,a.username,a.password),a.xhrFields)for(e in a.xhrFields)f[e]=a.xhrFields[e];a.mimeType&&f.overrideMimeType&&f.overrideMimeType(a.mimeType),a.crossDomain||c["X-Requested-With"]||(c["X-Requested-With"]="XMLHttpRequest");for(e in c)void 0!==c[e]&&f.setRequestHeader(e,c[e]+"");f.send(a.hasContent&&a.data||null),b=function(c,e){var h,i,j;if(b&&(e||4===f.readyState))if(delete Xb[g],b=void 0,f.onreadystatechange=m.noop,e)4!==f.readyState&&f.abort();else{j={},h=f.status,"string"==typeof f.responseText&&(j.text=f.responseText);try{i=f.statusText}catch(k){i=""}h||!a.isLocal||a.crossDomain?1223===h&&(h=204):h=j.text?200:404}j&&d(h,i,j,f.getAllResponseHeaders())},a.async?4===f.readyState?setTimeout(b):f.onreadystatechange=Xb[g]=b:b()},abort:function(){b&&b(void 0,!0)}}}});function Zb(){try{return new a.XMLHttpRequest}catch(b){}}function $b(){try{return new a.ActiveXObject("Microsoft.XMLHTTP")}catch(b){}}m.ajaxSetup({accepts:{script:"text/javascript, application/javascript, application/ecmascript, application/x-ecmascript"},contents:{script:/(?:java|ecma)script/},converters:{"text script":function(a){return m.globalEval(a),a}}}),m.ajaxPrefilter("script",function(a){void 0===a.cache&&(a.cache=!1),a.crossDomain&&(a.type="GET",a.global=!1)}),m.ajaxTransport("script",function(a){if(a.crossDomain){var b,c=y.head||m("head")[0]||y.documentElement;return{send:function(d,e){b=y.createElement("script"),b.async=!0,a.scriptCharset&&(b.charset=a.scriptCharset),b.src=a.url,b.onload=b.onreadystatechange=function(a,c){(c||!b.readyState||/loaded|complete/.test(b.readyState))&&(b.onload=b.onreadystatechange=null,b.parentNode&&b.parentNode.removeChild(b),b=null,c||e(200,"success"))},c.insertBefore(b,c.firstChild)},abort:function(){b&&b.onload(void 0,!0)}}}});var _b=[],ac=/(=)\?(?=&|$)|\?\?/;m.ajaxSetup({jsonp:"callback",jsonpCallback:function(){var a=_b.pop()||m.expando+"_"+vb++;return this[a]=!0,a}}),m.ajaxPrefilter("json jsonp",function(b,c,d){var e,f,g,h=b.jsonp!==!1&&(ac.test(b.url)?"url":"string"==typeof b.data&&!(b.contentType||"").indexOf("application/x-www-form-urlencoded")&&ac.test(b.data)&&"data");return h||"jsonp"===b.dataTypes[0]?(e=b.jsonpCallback=m.isFunction(b.jsonpCallback)?b.jsonpCallback():b.jsonpCallback,h?b[h]=b[h].replace(ac,"$1"+e):b.jsonp!==!1&&(b.url+=(wb.test(b.url)?"&":"?")+b.jsonp+"="+e),b.converters["script json"]=function(){return g||m.error(e+" was not called"),g[0]},b.dataTypes[0]="json",f=a[e],a[e]=function(){g=arguments},d.always(function(){a[e]=f,b[e]&&(b.jsonpCallback=c.jsonpCallback,_b.push(e)),g&&m.isFunction(f)&&f(g[0]),g=f=void 0}),"script"):void 0}),m.parseHTML=function(a,b,c){if(!a||"string"!=typeof a)return null;"boolean"==typeof b&&(c=b,b=!1),b=b||y;var d=u.exec(a),e=!c&&[];return d?[b.createElement(d[1])]:(d=m.buildFragment([a],b,e),e&&e.length&&m(e).remove(),m.merge([],d.childNodes))};var bc=m.fn.load;m.fn.load=function(a,b,c){if("string"!=typeof a&&bc)return bc.apply(this,arguments);var d,e,f,g=this,h=a.indexOf(" ");return h>=0&&(d=m.trim(a.slice(h,a.length)),a=a.slice(0,h)),m.isFunction(b)?(c=b,b=void 0):b&&"object"==typeof b&&(f="POST"),g.length>0&&m.ajax({url:a,type:f,dataType:"html",data:b}).done(function(a){e=arguments,g.html(d?m("<div>").append(m.parseHTML(a)).find(d):a)}).complete(c&&function(a,b){g.each(c,e||[a.responseText,b,a])}),this},m.each(["ajaxStart","ajaxStop","ajaxComplete","ajaxError","ajaxSuccess","ajaxSend"],function(a,b){m.fn[b]=function(a){return this.on(b,a)}}),m.expr.filters.animated=function(a){return m.grep(m.timers,function(b){return a===b.elem}).length};var cc=a.document.documentElement;function dc(a){return m.isWindow(a)?a:9===a.nodeType?a.defaultView||a.parentWindow:!1}m.offset={setOffset:function(a,b,c){var d,e,f,g,h,i,j,k=m.css(a,"position"),l=m(a),n={};"static"===k&&(a.style.position="relative"),h=l.offset(),f=m.css(a,"top"),i=m.css(a,"left"),j=("absolute"===k||"fixed"===k)&&m.inArray("auto",[f,i])>-1,j?(d=l.position(),g=d.top,e=d.left):(g=parseFloat(f)||0,e=parseFloat(i)||0),m.isFunction(b)&&(b=b.call(a,c,h)),null!=b.top&&(n.top=b.top-h.top+g),null!=b.left&&(n.left=b.left-h.left+e),"using"in b?b.using.call(a,n):l.css(n)}},m.fn.extend({offset:function(a){if(arguments.length)return void 0===a?this:this.each(function(b){m.offset.setOffset(this,a,b)});var b,c,d={top:0,left:0},e=this[0],f=e&&e.ownerDocument;if(f)return b=f.documentElement,m.contains(b,e)?(typeof e.getBoundingClientRect!==K&&(d=e.getBoundingClientRect()),c=dc(f),{top:d.top+(c.pageYOffset||b.scrollTop)-(b.clientTop||0),left:d.left+(c.pageXOffset||b.scrollLeft)-(b.clientLeft||0)}):d},position:function(){if(this[0]){var a,b,c={top:0,left:0},d=this[0];return"fixed"===m.css(d,"position")?b=d.getBoundingClientRect():(a=this.offsetParent(),b=this.offset(),m.nodeName(a[0],"html")||(c=a.offset()),c.top+=m.css(a[0],"borderTopWidth",!0),c.left+=m.css(a[0],"borderLeftWidth",!0)),{top:b.top-c.top-m.css(d,"marginTop",!0),left:b.left-c.left-m.css(d,"marginLeft",!0)}}},offsetParent:function(){return this.map(function(){var a=this.offsetParent||cc;while(a&&!m.nodeName(a,"html")&&"static"===m.css(a,"position"))a=a.offsetParent;return a||cc})}}),m.each({scrollLeft:"pageXOffset",scrollTop:"pageYOffset"},function(a,b){var c=/Y/.test(b);m.fn[a]=function(d){return V(this,function(a,d,e){var f=dc(a);return void 0===e?f?b in f?f[b]:f.document.documentElement[d]:a[d]:void(f?f.scrollTo(c?m(f).scrollLeft():e,c?e:m(f).scrollTop()):a[d]=e)},a,d,arguments.length,null)}}),m.each(["top","left"],function(a,b){m.cssHooks[b]=La(k.pixelPosition,function(a,c){return c?(c=Ja(a,b),Ha.test(c)?m(a).position()[b]+"px":c):void 0})}),m.each({Height:"height",Width:"width"},function(a,b){m.each({padding:"inner"+a,content:b,"":"outer"+a},function(c,d){m.fn[d]=function(d,e){var f=arguments.length&&(c||"boolean"!=typeof d),g=c||(d===!0||e===!0?"margin":"border");return V(this,function(b,c,d){var e;return m.isWindow(b)?b.document.documentElement["client"+a]:9===b.nodeType?(e=b.documentElement,Math.max(b.body["scroll"+a],e["scroll"+a],b.body["offset"+a],e["offset"+a],e["client"+a])):void 0===d?m.css(b,c,g):m.style(b,c,d,g)},b,f?d:void 0,f,null)}})}),m.fn.size=function(){return this.length},m.fn.andSelf=m.fn.addBack,"function"==typeof define&&define.amd&&define("jquery",[],function(){return m});var ec=a.jQuery,fc=a.$;return m.noConflict=function(b){return a.$===m&&(a.$=fc),b&&a.jQuery===m&&(a.jQuery=ec),m},typeof b===K&&(a.jQuery=a.$=m),m});
</script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<style type="text/css">html{font-family:sans-serif;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,hgroup,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:700}dfn{font-style:italic}h1{margin:.67em 0;font-size:2em}mark{color:#000;background:#ff0}small{font-size:80%}sub,sup{position:relative;font-size:75%;line-height:0;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{height:0;-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;font:inherit;color:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type=checkbox],input[type=radio]{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;padding:0}input[type=number]::-webkit-inner-spin-button,input[type=number]::-webkit-outer-spin-button{height:auto}input[type=search]{-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box;-webkit-appearance:textfield}input[type=search]::-webkit-search-cancel-button,input[type=search]::-webkit-search-decoration{-webkit-appearance:none}fieldset{padding:.35em .625em .75em;margin:0 2px;border:1px solid silver}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:700}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}@media print{*,:after,:before{color:#000!important;text-shadow:none!important;background:0 0!important;-webkit-box-shadow:none!important;box-shadow:none!important}a,a:visited{text-decoration:underline}a[href]:after{content:" (" attr(href) ")"}abbr[title]:after{content:" (" attr(title) ")"}a[href^="javascript:"]:after,a[href^="#"]:after{content:""}blockquote,pre{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}img,tr{page-break-inside:avoid}img{max-width:100%!important}h2,h3,p{orphans:3;widows:3}h2,h3{page-break-after:avoid}.navbar{display:none}.btn>.caret,.dropup>.btn>.caret{border-top-color:#000!important}.label{border:1px solid #000}.table{border-collapse:collapse!important}.table td,.table th{background-color:#fff!important}.table-bordered td,.table-bordered th{border:1px solid #ddd!important}}@font-face{font-family:'Glyphicons Halflings';src:url(data:application/vnd.ms-fontobject;base64,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);src:url(data:application/vnd.ms-fontobject;base64,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) format('embedded-opentype'),url(data:application/font-woff;base64,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) format('woff'),url(data:application/x-font-truetype;base64,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) format('truetype'),url(data:image/svg+xml;base64,<?xml version="1.0" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd" >
<svg xmlns="http://www.w3.org/2000/svg">
<metadata></metadata>
<defs>
<font id="glyphicons_halflingsregular" horiz-adv-x="1200" >
<font-face units-per-em="1200" ascent="960" descent="-240" />
<missing-glyph horiz-adv-x="500" />
<glyph horiz-adv-x="0" />
<glyph horiz-adv-x="400" />
<glyph unicode=" " />
<glyph unicode="*" d="M600 1100q15 0 34 -1.5t30 -3.5l11 -1q10 -2 17.5 -10.5t7.5 -18.5v-224l158 158q7 7 18 8t19 -6l106 -106q7 -8 6 -19t-8 -18l-158 -158h224q10 0 18.5 -7.5t10.5 -17.5q6 -41 6 -75q0 -15 -1.5 -34t-3.5 -30l-1 -11q-2 -10 -10.5 -17.5t-18.5 -7.5h-224l158 -158 q7 -7 8 -18t-6 -19l-106 -106q-8 -7 -19 -6t-18 8l-158 158v-224q0 -10 -7.5 -18.5t-17.5 -10.5q-41 -6 -75 -6q-15 0 -34 1.5t-30 3.5l-11 1q-10 2 -17.5 10.5t-7.5 18.5v224l-158 -158q-7 -7 -18 -8t-19 6l-106 106q-7 8 -6 19t8 18l158 158h-224q-10 0 -18.5 7.5 t-10.5 17.5q-6 41 -6 75q0 15 1.5 34t3.5 30l1 11q2 10 10.5 17.5t18.5 7.5h224l-158 158q-7 7 -8 18t6 19l106 106q8 7 19 6t18 -8l158 -158v224q0 10 7.5 18.5t17.5 10.5q41 6 75 6z" />
<glyph unicode="+" d="M450 1100h200q21 0 35.5 -14.5t14.5 -35.5v-350h350q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-350v-350q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v350h-350q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5 h350v350q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xa0;" />
<glyph unicode="&#xa5;" d="M825 1100h250q10 0 12.5 -5t-5.5 -13l-364 -364q-6 -6 -11 -18h268q10 0 13 -6t-3 -14l-120 -160q-6 -8 -18 -14t-22 -6h-125v-100h275q10 0 13 -6t-3 -14l-120 -160q-6 -8 -18 -14t-22 -6h-125v-174q0 -11 -7.5 -18.5t-18.5 -7.5h-148q-11 0 -18.5 7.5t-7.5 18.5v174 h-275q-10 0 -13 6t3 14l120 160q6 8 18 14t22 6h125v100h-275q-10 0 -13 6t3 14l120 160q6 8 18 14t22 6h118q-5 12 -11 18l-364 364q-8 8 -5.5 13t12.5 5h250q25 0 43 -18l164 -164q8 -8 18 -8t18 8l164 164q18 18 43 18z" />
<glyph unicode="&#x2000;" horiz-adv-x="650" />
<glyph unicode="&#x2001;" horiz-adv-x="1300" />
<glyph unicode="&#x2002;" horiz-adv-x="650" />
<glyph unicode="&#x2003;" horiz-adv-x="1300" />
<glyph unicode="&#x2004;" horiz-adv-x="433" />
<glyph unicode="&#x2005;" horiz-adv-x="325" />
<glyph unicode="&#x2006;" horiz-adv-x="216" />
<glyph unicode="&#x2007;" horiz-adv-x="216" />
<glyph unicode="&#x2008;" horiz-adv-x="162" />
<glyph unicode="&#x2009;" horiz-adv-x="260" />
<glyph unicode="&#x200a;" horiz-adv-x="72" />
<glyph unicode="&#x202f;" horiz-adv-x="260" />
<glyph unicode="&#x205f;" horiz-adv-x="325" />
<glyph unicode="&#x20ac;" d="M744 1198q242 0 354 -189q60 -104 66 -209h-181q0 45 -17.5 82.5t-43.5 61.5t-58 40.5t-60.5 24t-51.5 7.5q-19 0 -40.5 -5.5t-49.5 -20.5t-53 -38t-49 -62.5t-39 -89.5h379l-100 -100h-300q-6 -50 -6 -100h406l-100 -100h-300q9 -74 33 -132t52.5 -91t61.5 -54.5t59 -29 t47 -7.5q22 0 50.5 7.5t60.5 24.5t58 41t43.5 61t17.5 80h174q-30 -171 -128 -278q-107 -117 -274 -117q-206 0 -324 158q-36 48 -69 133t-45 204h-217l100 100h112q1 47 6 100h-218l100 100h134q20 87 51 153.5t62 103.5q117 141 297 141z" />
<glyph unicode="&#x20bd;" d="M428 1200h350q67 0 120 -13t86 -31t57 -49.5t35 -56.5t17 -64.5t6.5 -60.5t0.5 -57v-16.5v-16.5q0 -36 -0.5 -57t-6.5 -61t-17 -65t-35 -57t-57 -50.5t-86 -31.5t-120 -13h-178l-2 -100h288q10 0 13 -6t-3 -14l-120 -160q-6 -8 -18 -14t-22 -6h-138v-175q0 -11 -5.5 -18 t-15.5 -7h-149q-10 0 -17.5 7.5t-7.5 17.5v175h-267q-10 0 -13 6t3 14l120 160q6 8 18 14t22 6h117v100h-267q-10 0 -13 6t3 14l120 160q6 8 18 14t22 6h117v475q0 10 7.5 17.5t17.5 7.5zM600 1000v-300h203q64 0 86.5 33t22.5 119q0 84 -22.5 116t-86.5 32h-203z" />
<glyph unicode="&#x2212;" d="M250 700h800q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#x231b;" d="M1000 1200v-150q0 -21 -14.5 -35.5t-35.5 -14.5h-50v-100q0 -91 -49.5 -165.5t-130.5 -109.5q81 -35 130.5 -109.5t49.5 -165.5v-150h50q21 0 35.5 -14.5t14.5 -35.5v-150h-800v150q0 21 14.5 35.5t35.5 14.5h50v150q0 91 49.5 165.5t130.5 109.5q-81 35 -130.5 109.5 t-49.5 165.5v100h-50q-21 0 -35.5 14.5t-14.5 35.5v150h800zM400 1000v-100q0 -60 32.5 -109.5t87.5 -73.5q28 -12 44 -37t16 -55t-16 -55t-44 -37q-55 -24 -87.5 -73.5t-32.5 -109.5v-150h400v150q0 60 -32.5 109.5t-87.5 73.5q-28 12 -44 37t-16 55t16 55t44 37 q55 24 87.5 73.5t32.5 109.5v100h-400z" />
<glyph unicode="&#x25fc;" horiz-adv-x="500" d="M0 0z" />
<glyph unicode="&#x2601;" d="M503 1089q110 0 200.5 -59.5t134.5 -156.5q44 14 90 14q120 0 205 -86.5t85 -206.5q0 -121 -85 -207.5t-205 -86.5h-750q-79 0 -135.5 57t-56.5 137q0 69 42.5 122.5t108.5 67.5q-2 12 -2 37q0 153 108 260.5t260 107.5z" />
<glyph unicode="&#x26fa;" d="M774 1193.5q16 -9.5 20.5 -27t-5.5 -33.5l-136 -187l467 -746h30q20 0 35 -18.5t15 -39.5v-42h-1200v42q0 21 15 39.5t35 18.5h30l468 746l-135 183q-10 16 -5.5 34t20.5 28t34 5.5t28 -20.5l111 -148l112 150q9 16 27 20.5t34 -5zM600 200h377l-182 112l-195 534v-646z " />
<glyph unicode="&#x2709;" d="M25 1100h1150q10 0 12.5 -5t-5.5 -13l-564 -567q-8 -8 -18 -8t-18 8l-564 567q-8 8 -5.5 13t12.5 5zM18 882l264 -264q8 -8 8 -18t-8 -18l-264 -264q-8 -8 -13 -5.5t-5 12.5v550q0 10 5 12.5t13 -5.5zM918 618l264 264q8 8 13 5.5t5 -12.5v-550q0 -10 -5 -12.5t-13 5.5 l-264 264q-8 8 -8 18t8 18zM818 482l364 -364q8 -8 5.5 -13t-12.5 -5h-1150q-10 0 -12.5 5t5.5 13l364 364q8 8 18 8t18 -8l164 -164q8 -8 18 -8t18 8l164 164q8 8 18 8t18 -8z" />
<glyph unicode="&#x270f;" d="M1011 1210q19 0 33 -13l153 -153q13 -14 13 -33t-13 -33l-99 -92l-214 214l95 96q13 14 32 14zM1013 800l-615 -614l-214 214l614 614zM317 96l-333 -112l110 335z" />
<glyph unicode="&#xe001;" d="M700 650v-550h250q21 0 35.5 -14.5t14.5 -35.5v-50h-800v50q0 21 14.5 35.5t35.5 14.5h250v550l-500 550h1200z" />
<glyph unicode="&#xe002;" d="M368 1017l645 163q39 15 63 0t24 -49v-831q0 -55 -41.5 -95.5t-111.5 -63.5q-79 -25 -147 -4.5t-86 75t25.5 111.5t122.5 82q72 24 138 8v521l-600 -155v-606q0 -42 -44 -90t-109 -69q-79 -26 -147 -5.5t-86 75.5t25.5 111.5t122.5 82.5q72 24 138 7v639q0 38 14.5 59 t53.5 34z" />
<glyph unicode="&#xe003;" d="M500 1191q100 0 191 -39t156.5 -104.5t104.5 -156.5t39 -191l-1 -2l1 -5q0 -141 -78 -262l275 -274q23 -26 22.5 -44.5t-22.5 -42.5l-59 -58q-26 -20 -46.5 -20t-39.5 20l-275 274q-119 -77 -261 -77l-5 1l-2 -1q-100 0 -191 39t-156.5 104.5t-104.5 156.5t-39 191 t39 191t104.5 156.5t156.5 104.5t191 39zM500 1022q-88 0 -162 -43t-117 -117t-43 -162t43 -162t117 -117t162 -43t162 43t117 117t43 162t-43 162t-117 117t-162 43z" />
<glyph unicode="&#xe005;" d="M649 949q48 68 109.5 104t121.5 38.5t118.5 -20t102.5 -64t71 -100.5t27 -123q0 -57 -33.5 -117.5t-94 -124.5t-126.5 -127.5t-150 -152.5t-146 -174q-62 85 -145.5 174t-150 152.5t-126.5 127.5t-93.5 124.5t-33.5 117.5q0 64 28 123t73 100.5t104 64t119 20 t120.5 -38.5t104.5 -104z" />
<glyph unicode="&#xe006;" d="M407 800l131 353q7 19 17.5 19t17.5 -19l129 -353h421q21 0 24 -8.5t-14 -20.5l-342 -249l130 -401q7 -20 -0.5 -25.5t-24.5 6.5l-343 246l-342 -247q-17 -12 -24.5 -6.5t-0.5 25.5l130 400l-347 251q-17 12 -14 20.5t23 8.5h429z" />
<glyph unicode="&#xe007;" d="M407 800l131 353q7 19 17.5 19t17.5 -19l129 -353h421q21 0 24 -8.5t-14 -20.5l-342 -249l130 -401q7 -20 -0.5 -25.5t-24.5 6.5l-343 246l-342 -247q-17 -12 -24.5 -6.5t-0.5 25.5l130 400l-347 251q-17 12 -14 20.5t23 8.5h429zM477 700h-240l197 -142l-74 -226 l193 139l195 -140l-74 229l192 140h-234l-78 211z" />
<glyph unicode="&#xe008;" d="M600 1200q124 0 212 -88t88 -212v-250q0 -46 -31 -98t-69 -52v-75q0 -10 6 -21.5t15 -17.5l358 -230q9 -5 15 -16.5t6 -21.5v-93q0 -10 -7.5 -17.5t-17.5 -7.5h-1150q-10 0 -17.5 7.5t-7.5 17.5v93q0 10 6 21.5t15 16.5l358 230q9 6 15 17.5t6 21.5v75q-38 0 -69 52 t-31 98v250q0 124 88 212t212 88z" />
<glyph unicode="&#xe009;" d="M25 1100h1150q10 0 17.5 -7.5t7.5 -17.5v-1050q0 -10 -7.5 -17.5t-17.5 -7.5h-1150q-10 0 -17.5 7.5t-7.5 17.5v1050q0 10 7.5 17.5t17.5 7.5zM100 1000v-100h100v100h-100zM875 1000h-550q-10 0 -17.5 -7.5t-7.5 -17.5v-350q0 -10 7.5 -17.5t17.5 -7.5h550 q10 0 17.5 7.5t7.5 17.5v350q0 10 -7.5 17.5t-17.5 7.5zM1000 1000v-100h100v100h-100zM100 800v-100h100v100h-100zM1000 800v-100h100v100h-100zM100 600v-100h100v100h-100zM1000 600v-100h100v100h-100zM875 500h-550q-10 0 -17.5 -7.5t-7.5 -17.5v-350q0 -10 7.5 -17.5 t17.5 -7.5h550q10 0 17.5 7.5t7.5 17.5v350q0 10 -7.5 17.5t-17.5 7.5zM100 400v-100h100v100h-100zM1000 400v-100h100v100h-100zM100 200v-100h100v100h-100zM1000 200v-100h100v100h-100z" />
<glyph unicode="&#xe010;" d="M50 1100h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5zM650 1100h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v400 q0 21 14.5 35.5t35.5 14.5zM50 500h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5zM650 500h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400 q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe011;" d="M50 1100h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM450 1100h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200 q0 21 14.5 35.5t35.5 14.5zM850 1100h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM50 700h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200 q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM450 700h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM850 700h200q21 0 35.5 -14.5t14.5 -35.5v-200 q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM50 300h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM450 300h200 q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM850 300h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5 t35.5 14.5z" />
<glyph unicode="&#xe012;" d="M50 1100h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM450 1100h700q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-700q-21 0 -35.5 14.5t-14.5 35.5v200 q0 21 14.5 35.5t35.5 14.5zM50 700h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM450 700h700q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-700 q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM50 300h200q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5zM450 300h700q21 0 35.5 -14.5t14.5 -35.5v-200 q0 -21 -14.5 -35.5t-35.5 -14.5h-700q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe013;" d="M465 477l571 571q8 8 18 8t17 -8l177 -177q8 -7 8 -17t-8 -18l-783 -784q-7 -8 -17.5 -8t-17.5 8l-384 384q-8 8 -8 18t8 17l177 177q7 8 17 8t18 -8l171 -171q7 -7 18 -7t18 7z" />
<glyph unicode="&#xe014;" d="M904 1083l178 -179q8 -8 8 -18.5t-8 -17.5l-267 -268l267 -268q8 -7 8 -17.5t-8 -18.5l-178 -178q-8 -8 -18.5 -8t-17.5 8l-268 267l-268 -267q-7 -8 -17.5 -8t-18.5 8l-178 178q-8 8 -8 18.5t8 17.5l267 268l-267 268q-8 7 -8 17.5t8 18.5l178 178q8 8 18.5 8t17.5 -8 l268 -267l268 268q7 7 17.5 7t18.5 -7z" />
<glyph unicode="&#xe015;" d="M507 1177q98 0 187.5 -38.5t154.5 -103.5t103.5 -154.5t38.5 -187.5q0 -141 -78 -262l300 -299q8 -8 8 -18.5t-8 -18.5l-109 -108q-7 -8 -17.5 -8t-18.5 8l-300 299q-119 -77 -261 -77q-98 0 -188 38.5t-154.5 103t-103 154.5t-38.5 188t38.5 187.5t103 154.5 t154.5 103.5t188 38.5zM506.5 1023q-89.5 0 -165.5 -44t-120 -120.5t-44 -166t44 -165.5t120 -120t165.5 -44t166 44t120.5 120t44 165.5t-44 166t-120.5 120.5t-166 44zM425 900h150q10 0 17.5 -7.5t7.5 -17.5v-75h75q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5 t-17.5 -7.5h-75v-75q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v75h-75q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5h75v75q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe016;" d="M507 1177q98 0 187.5 -38.5t154.5 -103.5t103.5 -154.5t38.5 -187.5q0 -141 -78 -262l300 -299q8 -8 8 -18.5t-8 -18.5l-109 -108q-7 -8 -17.5 -8t-18.5 8l-300 299q-119 -77 -261 -77q-98 0 -188 38.5t-154.5 103t-103 154.5t-38.5 188t38.5 187.5t103 154.5 t154.5 103.5t188 38.5zM506.5 1023q-89.5 0 -165.5 -44t-120 -120.5t-44 -166t44 -165.5t120 -120t165.5 -44t166 44t120.5 120t44 165.5t-44 166t-120.5 120.5t-166 44zM325 800h350q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-350q-10 0 -17.5 7.5 t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe017;" d="M550 1200h100q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5zM800 975v166q167 -62 272 -209.5t105 -331.5q0 -117 -45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5 t-184.5 123t-123 184.5t-45.5 224q0 184 105 331.5t272 209.5v-166q-103 -55 -165 -155t-62 -220q0 -116 57 -214.5t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5q0 120 -62 220t-165 155z" />
<glyph unicode="&#xe018;" d="M1025 1200h150q10 0 17.5 -7.5t7.5 -17.5v-1150q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v1150q0 10 7.5 17.5t17.5 7.5zM725 800h150q10 0 17.5 -7.5t7.5 -17.5v-750q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v750 q0 10 7.5 17.5t17.5 7.5zM425 500h150q10 0 17.5 -7.5t7.5 -17.5v-450q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v450q0 10 7.5 17.5t17.5 7.5zM125 300h150q10 0 17.5 -7.5t7.5 -17.5v-250q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5 v250q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe019;" d="M600 1174q33 0 74 -5l38 -152l5 -1q49 -14 94 -39l5 -2l134 80q61 -48 104 -105l-80 -134l3 -5q25 -44 39 -93l1 -6l152 -38q5 -43 5 -73q0 -34 -5 -74l-152 -38l-1 -6q-15 -49 -39 -93l-3 -5l80 -134q-48 -61 -104 -105l-134 81l-5 -3q-44 -25 -94 -39l-5 -2l-38 -151 q-43 -5 -74 -5q-33 0 -74 5l-38 151l-5 2q-49 14 -94 39l-5 3l-134 -81q-60 48 -104 105l80 134l-3 5q-25 45 -38 93l-2 6l-151 38q-6 42 -6 74q0 33 6 73l151 38l2 6q13 48 38 93l3 5l-80 134q47 61 105 105l133 -80l5 2q45 25 94 39l5 1l38 152q43 5 74 5zM600 815 q-89 0 -152 -63t-63 -151.5t63 -151.5t152 -63t152 63t63 151.5t-63 151.5t-152 63z" />
<glyph unicode="&#xe020;" d="M500 1300h300q41 0 70.5 -29.5t29.5 -70.5v-100h275q10 0 17.5 -7.5t7.5 -17.5v-75h-1100v75q0 10 7.5 17.5t17.5 7.5h275v100q0 41 29.5 70.5t70.5 29.5zM500 1200v-100h300v100h-300zM1100 900v-800q0 -41 -29.5 -70.5t-70.5 -29.5h-700q-41 0 -70.5 29.5t-29.5 70.5 v800h900zM300 800v-700h100v700h-100zM500 800v-700h100v700h-100zM700 800v-700h100v700h-100zM900 800v-700h100v700h-100z" />
<glyph unicode="&#xe021;" d="M18 618l620 608q8 7 18.5 7t17.5 -7l608 -608q8 -8 5.5 -13t-12.5 -5h-175v-575q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v375h-300v-375q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v575h-175q-10 0 -12.5 5t5.5 13z" />
<glyph unicode="&#xe022;" d="M600 1200v-400q0 -41 29.5 -70.5t70.5 -29.5h300v-650q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5v1100q0 21 14.5 35.5t35.5 14.5h450zM1000 800h-250q-21 0 -35.5 14.5t-14.5 35.5v250z" />
<glyph unicode="&#xe023;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 1027q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5 t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5t-57 214.5t-155.5 155.5t-214.5 57zM525 900h50q10 0 17.5 -7.5t7.5 -17.5v-275h175q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v350q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe024;" d="M1300 0h-538l-41 400h-242l-41 -400h-538l431 1200h209l-21 -300h162l-20 300h208zM515 800l-27 -300h224l-27 300h-170z" />
<glyph unicode="&#xe025;" d="M550 1200h200q21 0 35.5 -14.5t14.5 -35.5v-450h191q20 0 25.5 -11.5t-7.5 -27.5l-327 -400q-13 -16 -32 -16t-32 16l-327 400q-13 16 -7.5 27.5t25.5 11.5h191v450q0 21 14.5 35.5t35.5 14.5zM1125 400h50q10 0 17.5 -7.5t7.5 -17.5v-350q0 -10 -7.5 -17.5t-17.5 -7.5 h-1050q-10 0 -17.5 7.5t-7.5 17.5v350q0 10 7.5 17.5t17.5 7.5h50q10 0 17.5 -7.5t7.5 -17.5v-175h900v175q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe026;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 1027q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5 t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5t-57 214.5t-155.5 155.5t-214.5 57zM525 900h150q10 0 17.5 -7.5t7.5 -17.5v-275h137q21 0 26 -11.5t-8 -27.5l-223 -275q-13 -16 -32 -16t-32 16l-223 275q-13 16 -8 27.5t26 11.5h137v275q0 10 7.5 17.5t17.5 7.5z " />
<glyph unicode="&#xe027;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 1027q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5 t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5t-57 214.5t-155.5 155.5t-214.5 57zM632 914l223 -275q13 -16 8 -27.5t-26 -11.5h-137v-275q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v275h-137q-21 0 -26 11.5t8 27.5l223 275q13 16 32 16 t32 -16z" />
<glyph unicode="&#xe028;" d="M225 1200h750q10 0 19.5 -7t12.5 -17l186 -652q7 -24 7 -49v-425q0 -12 -4 -27t-9 -17q-12 -6 -37 -6h-1100q-12 0 -27 4t-17 8q-6 13 -6 38l1 425q0 25 7 49l185 652q3 10 12.5 17t19.5 7zM878 1000h-556q-10 0 -19 -7t-11 -18l-87 -450q-2 -11 4 -18t16 -7h150 q10 0 19.5 -7t11.5 -17l38 -152q2 -10 11.5 -17t19.5 -7h250q10 0 19.5 7t11.5 17l38 152q2 10 11.5 17t19.5 7h150q10 0 16 7t4 18l-87 450q-2 11 -11 18t-19 7z" />
<glyph unicode="&#xe029;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 1027q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5 t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5t-57 214.5t-155.5 155.5t-214.5 57zM540 820l253 -190q17 -12 17 -30t-17 -30l-253 -190q-16 -12 -28 -6.5t-12 26.5v400q0 21 12 26.5t28 -6.5z" />
<glyph unicode="&#xe030;" d="M947 1060l135 135q7 7 12.5 5t5.5 -13v-362q0 -10 -7.5 -17.5t-17.5 -7.5h-362q-11 0 -13 5.5t5 12.5l133 133q-109 76 -238 76q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5h150q0 -117 -45.5 -224 t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5q192 0 347 -117z" />
<glyph unicode="&#xe031;" d="M947 1060l135 135q7 7 12.5 5t5.5 -13v-361q0 -11 -7.5 -18.5t-18.5 -7.5h-361q-11 0 -13 5.5t5 12.5l134 134q-110 75 -239 75q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5h-150q0 117 45.5 224t123 184.5t184.5 123t224 45.5q192 0 347 -117zM1027 600h150 q0 -117 -45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5q-192 0 -348 118l-134 -134q-7 -8 -12.5 -5.5t-5.5 12.5v360q0 11 7.5 18.5t18.5 7.5h360q10 0 12.5 -5.5t-5.5 -12.5l-133 -133q110 -76 240 -76q116 0 214.5 57t155.5 155.5t57 214.5z" />
<glyph unicode="&#xe032;" d="M125 1200h1050q10 0 17.5 -7.5t7.5 -17.5v-1150q0 -10 -7.5 -17.5t-17.5 -7.5h-1050q-10 0 -17.5 7.5t-7.5 17.5v1150q0 10 7.5 17.5t17.5 7.5zM1075 1000h-850q-10 0 -17.5 -7.5t-7.5 -17.5v-850q0 -10 7.5 -17.5t17.5 -7.5h850q10 0 17.5 7.5t7.5 17.5v850 q0 10 -7.5 17.5t-17.5 7.5zM325 900h50q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-50q-10 0 -17.5 7.5t-7.5 17.5v50q0 10 7.5 17.5t17.5 7.5zM525 900h450q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-450q-10 0 -17.5 7.5t-7.5 17.5v50 q0 10 7.5 17.5t17.5 7.5zM325 700h50q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-50q-10 0 -17.5 7.5t-7.5 17.5v50q0 10 7.5 17.5t17.5 7.5zM525 700h450q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-450q-10 0 -17.5 7.5t-7.5 17.5v50 q0 10 7.5 17.5t17.5 7.5zM325 500h50q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-50q-10 0 -17.5 7.5t-7.5 17.5v50q0 10 7.5 17.5t17.5 7.5zM525 500h450q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-450q-10 0 -17.5 7.5t-7.5 17.5v50 q0 10 7.5 17.5t17.5 7.5zM325 300h50q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-50q-10 0 -17.5 7.5t-7.5 17.5v50q0 10 7.5 17.5t17.5 7.5zM525 300h450q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-450q-10 0 -17.5 7.5t-7.5 17.5v50 q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe033;" d="M900 800v200q0 83 -58.5 141.5t-141.5 58.5h-300q-82 0 -141 -59t-59 -141v-200h-100q-41 0 -70.5 -29.5t-29.5 -70.5v-600q0 -41 29.5 -70.5t70.5 -29.5h900q41 0 70.5 29.5t29.5 70.5v600q0 41 -29.5 70.5t-70.5 29.5h-100zM400 800v150q0 21 15 35.5t35 14.5h200 q20 0 35 -14.5t15 -35.5v-150h-300z" />
<glyph unicode="&#xe034;" d="M125 1100h50q10 0 17.5 -7.5t7.5 -17.5v-1075h-100v1075q0 10 7.5 17.5t17.5 7.5zM1075 1052q4 0 9 -2q16 -6 16 -23v-421q0 -6 -3 -12q-33 -59 -66.5 -99t-65.5 -58t-56.5 -24.5t-52.5 -6.5q-26 0 -57.5 6.5t-52.5 13.5t-60 21q-41 15 -63 22.5t-57.5 15t-65.5 7.5 q-85 0 -160 -57q-7 -5 -15 -5q-6 0 -11 3q-14 7 -14 22v438q22 55 82 98.5t119 46.5q23 2 43 0.5t43 -7t32.5 -8.5t38 -13t32.5 -11q41 -14 63.5 -21t57 -14t63.5 -7q103 0 183 87q7 8 18 8z" />
<glyph unicode="&#xe035;" d="M600 1175q116 0 227 -49.5t192.5 -131t131 -192.5t49.5 -227v-300q0 -10 -7.5 -17.5t-17.5 -7.5h-50q-10 0 -17.5 7.5t-7.5 17.5v300q0 127 -70.5 231.5t-184.5 161.5t-245 57t-245 -57t-184.5 -161.5t-70.5 -231.5v-300q0 -10 -7.5 -17.5t-17.5 -7.5h-50 q-10 0 -17.5 7.5t-7.5 17.5v300q0 116 49.5 227t131 192.5t192.5 131t227 49.5zM220 500h160q8 0 14 -6t6 -14v-460q0 -8 -6 -14t-14 -6h-160q-8 0 -14 6t-6 14v460q0 8 6 14t14 6zM820 500h160q8 0 14 -6t6 -14v-460q0 -8 -6 -14t-14 -6h-160q-8 0 -14 6t-6 14v460 q0 8 6 14t14 6z" />
<glyph unicode="&#xe036;" d="M321 814l258 172q9 6 15 2.5t6 -13.5v-750q0 -10 -6 -13.5t-15 2.5l-258 172q-21 14 -46 14h-250q-10 0 -17.5 7.5t-7.5 17.5v350q0 10 7.5 17.5t17.5 7.5h250q25 0 46 14zM900 668l120 120q7 7 17 7t17 -7l34 -34q7 -7 7 -17t-7 -17l-120 -120l120 -120q7 -7 7 -17 t-7 -17l-34 -34q-7 -7 -17 -7t-17 7l-120 119l-120 -119q-7 -7 -17 -7t-17 7l-34 34q-7 7 -7 17t7 17l119 120l-119 120q-7 7 -7 17t7 17l34 34q7 8 17 8t17 -8z" />
<glyph unicode="&#xe037;" d="M321 814l258 172q9 6 15 2.5t6 -13.5v-750q0 -10 -6 -13.5t-15 2.5l-258 172q-21 14 -46 14h-250q-10 0 -17.5 7.5t-7.5 17.5v350q0 10 7.5 17.5t17.5 7.5h250q25 0 46 14zM766 900h4q10 -1 16 -10q96 -129 96 -290q0 -154 -90 -281q-6 -9 -17 -10l-3 -1q-9 0 -16 6 l-29 23q-7 7 -8.5 16.5t4.5 17.5q72 103 72 229q0 132 -78 238q-6 8 -4.5 18t9.5 17l29 22q7 5 15 5z" />
<glyph unicode="&#xe038;" d="M967 1004h3q11 -1 17 -10q135 -179 135 -396q0 -105 -34 -206.5t-98 -185.5q-7 -9 -17 -10h-3q-9 0 -16 6l-42 34q-8 6 -9 16t5 18q111 150 111 328q0 90 -29.5 176t-84.5 157q-6 9 -5 19t10 16l42 33q7 5 15 5zM321 814l258 172q9 6 15 2.5t6 -13.5v-750q0 -10 -6 -13.5 t-15 2.5l-258 172q-21 14 -46 14h-250q-10 0 -17.5 7.5t-7.5 17.5v350q0 10 7.5 17.5t17.5 7.5h250q25 0 46 14zM766 900h4q10 -1 16 -10q96 -129 96 -290q0 -154 -90 -281q-6 -9 -17 -10l-3 -1q-9 0 -16 6l-29 23q-7 7 -8.5 16.5t4.5 17.5q72 103 72 229q0 132 -78 238 q-6 8 -4.5 18.5t9.5 16.5l29 22q7 5 15 5z" />
<glyph unicode="&#xe039;" d="M500 900h100v-100h-100v-100h-400v-100h-100v600h500v-300zM1200 700h-200v-100h200v-200h-300v300h-200v300h-100v200h600v-500zM100 1100v-300h300v300h-300zM800 1100v-300h300v300h-300zM300 900h-100v100h100v-100zM1000 900h-100v100h100v-100zM300 500h200v-500 h-500v500h200v100h100v-100zM800 300h200v-100h-100v-100h-200v100h-100v100h100v200h-200v100h300v-300zM100 400v-300h300v300h-300zM300 200h-100v100h100v-100zM1200 200h-100v100h100v-100zM700 0h-100v100h100v-100zM1200 0h-300v100h300v-100z" />
<glyph unicode="&#xe040;" d="M100 200h-100v1000h100v-1000zM300 200h-100v1000h100v-1000zM700 200h-200v1000h200v-1000zM900 200h-100v1000h100v-1000zM1200 200h-200v1000h200v-1000zM400 0h-300v100h300v-100zM600 0h-100v91h100v-91zM800 0h-100v91h100v-91zM1100 0h-200v91h200v-91z" />
<glyph unicode="&#xe041;" d="M500 1200l682 -682q8 -8 8 -18t-8 -18l-464 -464q-8 -8 -18 -8t-18 8l-682 682l1 475q0 10 7.5 17.5t17.5 7.5h474zM319.5 1024.5q-29.5 29.5 -71 29.5t-71 -29.5t-29.5 -71.5t29.5 -71.5t71 -29.5t71 29.5t29.5 71.5t-29.5 71.5z" />
<glyph unicode="&#xe042;" d="M500 1200l682 -682q8 -8 8 -18t-8 -18l-464 -464q-8 -8 -18 -8t-18 8l-682 682l1 475q0 10 7.5 17.5t17.5 7.5h474zM800 1200l682 -682q8 -8 8 -18t-8 -18l-464 -464q-8 -8 -18 -8t-18 8l-56 56l424 426l-700 700h150zM319.5 1024.5q-29.5 29.5 -71 29.5t-71 -29.5 t-29.5 -71.5t29.5 -71.5t71 -29.5t71 29.5t29.5 71.5t-29.5 71.5z" />
<glyph unicode="&#xe043;" d="M300 1200h825q75 0 75 -75v-900q0 -25 -18 -43l-64 -64q-8 -8 -13 -5.5t-5 12.5v950q0 10 -7.5 17.5t-17.5 7.5h-700q-25 0 -43 -18l-64 -64q-8 -8 -5.5 -13t12.5 -5h700q10 0 17.5 -7.5t7.5 -17.5v-950q0 -10 -7.5 -17.5t-17.5 -7.5h-850q-10 0 -17.5 7.5t-7.5 17.5v975 q0 25 18 43l139 139q18 18 43 18z" />
<glyph unicode="&#xe044;" d="M250 1200h800q21 0 35.5 -14.5t14.5 -35.5v-1150l-450 444l-450 -445v1151q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe045;" d="M822 1200h-444q-11 0 -19 -7.5t-9 -17.5l-78 -301q-7 -24 7 -45l57 -108q6 -9 17.5 -15t21.5 -6h450q10 0 21.5 6t17.5 15l62 108q14 21 7 45l-83 301q-1 10 -9 17.5t-19 7.5zM1175 800h-150q-10 0 -21 -6.5t-15 -15.5l-78 -156q-4 -9 -15 -15.5t-21 -6.5h-550 q-10 0 -21 6.5t-15 15.5l-78 156q-4 9 -15 15.5t-21 6.5h-150q-10 0 -17.5 -7.5t-7.5 -17.5v-650q0 -10 7.5 -17.5t17.5 -7.5h150q10 0 17.5 7.5t7.5 17.5v150q0 10 7.5 17.5t17.5 7.5h750q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 7.5 -17.5t17.5 -7.5h150q10 0 17.5 7.5 t7.5 17.5v650q0 10 -7.5 17.5t-17.5 7.5zM850 200h-500q-10 0 -19.5 -7t-11.5 -17l-38 -152q-2 -10 3.5 -17t15.5 -7h600q10 0 15.5 7t3.5 17l-38 152q-2 10 -11.5 17t-19.5 7z" />
<glyph unicode="&#xe046;" d="M500 1100h200q56 0 102.5 -20.5t72.5 -50t44 -59t25 -50.5l6 -20h150q41 0 70.5 -29.5t29.5 -70.5v-600q0 -41 -29.5 -70.5t-70.5 -29.5h-1000q-41 0 -70.5 29.5t-29.5 70.5v600q0 41 29.5 70.5t70.5 29.5h150q2 8 6.5 21.5t24 48t45 61t72 48t102.5 21.5zM900 800v-100 h100v100h-100zM600 730q-95 0 -162.5 -67.5t-67.5 -162.5t67.5 -162.5t162.5 -67.5t162.5 67.5t67.5 162.5t-67.5 162.5t-162.5 67.5zM600 603q43 0 73 -30t30 -73t-30 -73t-73 -30t-73 30t-30 73t30 73t73 30z" />
<glyph unicode="&#xe047;" d="M681 1199l385 -998q20 -50 60 -92q18 -19 36.5 -29.5t27.5 -11.5l10 -2v-66h-417v66q53 0 75 43.5t5 88.5l-82 222h-391q-58 -145 -92 -234q-11 -34 -6.5 -57t25.5 -37t46 -20t55 -6v-66h-365v66q56 24 84 52q12 12 25 30.5t20 31.5l7 13l399 1006h93zM416 521h340 l-162 457z" />
<glyph unicode="&#xe048;" d="M753 641q5 -1 14.5 -4.5t36 -15.5t50.5 -26.5t53.5 -40t50.5 -54.5t35.5 -70t14.5 -87q0 -67 -27.5 -125.5t-71.5 -97.5t-98.5 -66.5t-108.5 -40.5t-102 -13h-500v89q41 7 70.5 32.5t29.5 65.5v827q0 24 -0.5 34t-3.5 24t-8.5 19.5t-17 13.5t-28 12.5t-42.5 11.5v71 l471 -1q57 0 115.5 -20.5t108 -57t80.5 -94t31 -124.5q0 -51 -15.5 -96.5t-38 -74.5t-45 -50.5t-38.5 -30.5zM400 700h139q78 0 130.5 48.5t52.5 122.5q0 41 -8.5 70.5t-29.5 55.5t-62.5 39.5t-103.5 13.5h-118v-350zM400 200h216q80 0 121 50.5t41 130.5q0 90 -62.5 154.5 t-156.5 64.5h-159v-400z" />
<glyph unicode="&#xe049;" d="M877 1200l2 -57q-83 -19 -116 -45.5t-40 -66.5l-132 -839q-9 -49 13 -69t96 -26v-97h-500v97q186 16 200 98l173 832q3 17 3 30t-1.5 22.5t-9 17.5t-13.5 12.5t-21.5 10t-26 8.5t-33.5 10q-13 3 -19 5v57h425z" />
<glyph unicode="&#xe050;" d="M1300 900h-50q0 21 -4 37t-9.5 26.5t-18 17.5t-22 11t-28.5 5.5t-31 2t-37 0.5h-200v-850q0 -22 25 -34.5t50 -13.5l25 -2v-100h-400v100q4 0 11 0.5t24 3t30 7t24 15t11 24.5v850h-200q-25 0 -37 -0.5t-31 -2t-28.5 -5.5t-22 -11t-18 -17.5t-9.5 -26.5t-4 -37h-50v300 h1000v-300zM175 1000h-75v-800h75l-125 -167l-125 167h75v800h-75l125 167z" />
<glyph unicode="&#xe051;" d="M1100 900h-50q0 21 -4 37t-9.5 26.5t-18 17.5t-22 11t-28.5 5.5t-31 2t-37 0.5h-200v-650q0 -22 25 -34.5t50 -13.5l25 -2v-100h-400v100q4 0 11 0.5t24 3t30 7t24 15t11 24.5v650h-200q-25 0 -37 -0.5t-31 -2t-28.5 -5.5t-22 -11t-18 -17.5t-9.5 -26.5t-4 -37h-50v300 h1000v-300zM1167 50l-167 -125v75h-800v-75l-167 125l167 125v-75h800v75z" />
<glyph unicode="&#xe052;" d="M50 1100h600q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-600q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 800h1000q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1000q-21 0 -35.5 14.5t-14.5 35.5v100 q0 21 14.5 35.5t35.5 14.5zM50 500h800q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 200h1100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100 q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe053;" d="M250 1100h700q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-700q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 800h1100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5v100 q0 21 14.5 35.5t35.5 14.5zM250 500h700q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-700q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 200h1100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100 q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe054;" d="M500 950v100q0 21 14.5 35.5t35.5 14.5h600q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-600q-21 0 -35.5 14.5t-14.5 35.5zM100 650v100q0 21 14.5 35.5t35.5 14.5h1000q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1000 q-21 0 -35.5 14.5t-14.5 35.5zM300 350v100q0 21 14.5 35.5t35.5 14.5h800q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5zM0 50v100q0 21 14.5 35.5t35.5 14.5h1100q21 0 35.5 -14.5t14.5 -35.5v-100 q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5z" />
<glyph unicode="&#xe055;" d="M50 1100h1100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 800h1100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5v100 q0 21 14.5 35.5t35.5 14.5zM50 500h1100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 200h1100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100 q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe056;" d="M50 1100h100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM350 1100h800q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5v100 q0 21 14.5 35.5t35.5 14.5zM50 800h100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM350 800h800q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-800 q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 500h100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM350 500h800q21 0 35.5 -14.5t14.5 -35.5v-100 q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 200h100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM350 200h800 q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe057;" d="M400 0h-100v1100h100v-1100zM550 1100h100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM550 800h500q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-500 q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM267 550l-167 -125v75h-200v100h200v75zM550 500h300q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-300q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM550 200h600 q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-600q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe058;" d="M50 1100h100q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM900 0h-100v1100h100v-1100zM50 800h500q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-500 q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM1100 600h200v-100h-200v-75l-167 125l167 125v-75zM50 500h300q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-300q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5zM50 200h600 q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-600q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe059;" d="M75 1000h750q31 0 53 -22t22 -53v-650q0 -31 -22 -53t-53 -22h-750q-31 0 -53 22t-22 53v650q0 31 22 53t53 22zM1200 300l-300 300l300 300v-600z" />
<glyph unicode="&#xe060;" d="M44 1100h1112q18 0 31 -13t13 -31v-1012q0 -18 -13 -31t-31 -13h-1112q-18 0 -31 13t-13 31v1012q0 18 13 31t31 13zM100 1000v-737l247 182l298 -131l-74 156l293 318l236 -288v500h-1000zM342 884q56 0 95 -39t39 -94.5t-39 -95t-95 -39.5t-95 39.5t-39 95t39 94.5 t95 39z" />
<glyph unicode="&#xe062;" d="M648 1169q117 0 216 -60t156.5 -161t57.5 -218q0 -115 -70 -258q-69 -109 -158 -225.5t-143 -179.5l-54 -62q-9 8 -25.5 24.5t-63.5 67.5t-91 103t-98.5 128t-95.5 148q-60 132 -60 249q0 88 34 169.5t91.5 142t137 96.5t166.5 36zM652.5 974q-91.5 0 -156.5 -65 t-65 -157t65 -156.5t156.5 -64.5t156.5 64.5t65 156.5t-65 157t-156.5 65z" />
<glyph unicode="&#xe063;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 173v854q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5 t155.5 -155.5t214.5 -57z" />
<glyph unicode="&#xe064;" d="M554 1295q21 -72 57.5 -143.5t76 -130t83 -118t82.5 -117t70 -116t49.5 -126t18.5 -136.5q0 -71 -25.5 -135t-68.5 -111t-99 -82t-118.5 -54t-125.5 -23q-84 5 -161.5 34t-139.5 78.5t-99 125t-37 164.5q0 69 18 136.5t49.5 126.5t69.5 116.5t81.5 117.5t83.5 119 t76.5 131t58.5 143zM344 710q-23 -33 -43.5 -70.5t-40.5 -102.5t-17 -123q1 -37 14.5 -69.5t30 -52t41 -37t38.5 -24.5t33 -15q21 -7 32 -1t13 22l6 34q2 10 -2.5 22t-13.5 19q-5 4 -14 12t-29.5 40.5t-32.5 73.5q-26 89 6 271q2 11 -6 11q-8 1 -15 -10z" />
<glyph unicode="&#xe065;" d="M1000 1013l108 115q2 1 5 2t13 2t20.5 -1t25 -9.5t28.5 -21.5q22 -22 27 -43t0 -32l-6 -10l-108 -115zM350 1100h400q50 0 105 -13l-187 -187h-368q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5v182l200 200v-332 q0 -165 -93.5 -257.5t-256.5 -92.5h-400q-165 0 -257.5 92.5t-92.5 257.5v400q0 165 92.5 257.5t257.5 92.5zM1009 803l-362 -362l-161 -50l55 170l355 355z" />
<glyph unicode="&#xe066;" d="M350 1100h361q-164 -146 -216 -200h-195q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5l200 153v-103q0 -165 -92.5 -257.5t-257.5 -92.5h-400q-165 0 -257.5 92.5t-92.5 257.5v400q0 165 92.5 257.5t257.5 92.5z M824 1073l339 -301q8 -7 8 -17.5t-8 -17.5l-340 -306q-7 -6 -12.5 -4t-6.5 11v203q-26 1 -54.5 0t-78.5 -7.5t-92 -17.5t-86 -35t-70 -57q10 59 33 108t51.5 81.5t65 58.5t68.5 40.5t67 24.5t56 13.5t40 4.5v210q1 10 6.5 12.5t13.5 -4.5z" />
<glyph unicode="&#xe067;" d="M350 1100h350q60 0 127 -23l-178 -177h-349q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5v69l200 200v-219q0 -165 -92.5 -257.5t-257.5 -92.5h-400q-165 0 -257.5 92.5t-92.5 257.5v400q0 165 92.5 257.5t257.5 92.5z M643 639l395 395q7 7 17.5 7t17.5 -7l101 -101q7 -7 7 -17.5t-7 -17.5l-531 -532q-7 -7 -17.5 -7t-17.5 7l-248 248q-7 7 -7 17.5t7 17.5l101 101q7 7 17.5 7t17.5 -7l111 -111q8 -7 18 -7t18 7z" />
<glyph unicode="&#xe068;" d="M318 918l264 264q8 8 18 8t18 -8l260 -264q7 -8 4.5 -13t-12.5 -5h-170v-200h200v173q0 10 5 12t13 -5l264 -260q8 -7 8 -17.5t-8 -17.5l-264 -265q-8 -7 -13 -5t-5 12v173h-200v-200h170q10 0 12.5 -5t-4.5 -13l-260 -264q-8 -8 -18 -8t-18 8l-264 264q-8 8 -5.5 13 t12.5 5h175v200h-200v-173q0 -10 -5 -12t-13 5l-264 265q-8 7 -8 17.5t8 17.5l264 260q8 7 13 5t5 -12v-173h200v200h-175q-10 0 -12.5 5t5.5 13z" />
<glyph unicode="&#xe069;" d="M250 1100h100q21 0 35.5 -14.5t14.5 -35.5v-438l464 453q15 14 25.5 10t10.5 -25v-1000q0 -21 -10.5 -25t-25.5 10l-464 453v-438q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v1000q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe070;" d="M50 1100h100q21 0 35.5 -14.5t14.5 -35.5v-438l464 453q15 14 25.5 10t10.5 -25v-438l464 453q15 14 25.5 10t10.5 -25v-1000q0 -21 -10.5 -25t-25.5 10l-464 453v-438q0 -21 -10.5 -25t-25.5 10l-464 453v-438q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5 t-14.5 35.5v1000q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe071;" d="M1200 1050v-1000q0 -21 -10.5 -25t-25.5 10l-464 453v-438q0 -21 -10.5 -25t-25.5 10l-492 480q-15 14 -15 35t15 35l492 480q15 14 25.5 10t10.5 -25v-438l464 453q15 14 25.5 10t10.5 -25z" />
<glyph unicode="&#xe072;" d="M243 1074l814 -498q18 -11 18 -26t-18 -26l-814 -498q-18 -11 -30.5 -4t-12.5 28v1000q0 21 12.5 28t30.5 -4z" />
<glyph unicode="&#xe073;" d="M250 1000h200q21 0 35.5 -14.5t14.5 -35.5v-800q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v800q0 21 14.5 35.5t35.5 14.5zM650 1000h200q21 0 35.5 -14.5t14.5 -35.5v-800q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v800 q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe074;" d="M1100 950v-800q0 -21 -14.5 -35.5t-35.5 -14.5h-800q-21 0 -35.5 14.5t-14.5 35.5v800q0 21 14.5 35.5t35.5 14.5h800q21 0 35.5 -14.5t14.5 -35.5z" />
<glyph unicode="&#xe075;" d="M500 612v438q0 21 10.5 25t25.5 -10l492 -480q15 -14 15 -35t-15 -35l-492 -480q-15 -14 -25.5 -10t-10.5 25v438l-464 -453q-15 -14 -25.5 -10t-10.5 25v1000q0 21 10.5 25t25.5 -10z" />
<glyph unicode="&#xe076;" d="M1048 1102l100 1q20 0 35 -14.5t15 -35.5l5 -1000q0 -21 -14.5 -35.5t-35.5 -14.5l-100 -1q-21 0 -35.5 14.5t-14.5 35.5l-2 437l-463 -454q-14 -15 -24.5 -10.5t-10.5 25.5l-2 437l-462 -455q-15 -14 -25.5 -9.5t-10.5 24.5l-5 1000q0 21 10.5 25.5t25.5 -10.5l466 -450 l-2 438q0 20 10.5 24.5t25.5 -9.5l466 -451l-2 438q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe077;" d="M850 1100h100q21 0 35.5 -14.5t14.5 -35.5v-1000q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v438l-464 -453q-15 -14 -25.5 -10t-10.5 25v1000q0 21 10.5 25t25.5 -10l464 -453v438q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe078;" d="M686 1081l501 -540q15 -15 10.5 -26t-26.5 -11h-1042q-22 0 -26.5 11t10.5 26l501 540q15 15 36 15t36 -15zM150 400h1000q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1000q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe079;" d="M885 900l-352 -353l352 -353l-197 -198l-552 552l552 550z" />
<glyph unicode="&#xe080;" d="M1064 547l-551 -551l-198 198l353 353l-353 353l198 198z" />
<glyph unicode="&#xe081;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM650 900h-100q-21 0 -35.5 -14.5t-14.5 -35.5v-150h-150 q-21 0 -35.5 -14.5t-14.5 -35.5v-100q0 -21 14.5 -35.5t35.5 -14.5h150v-150q0 -21 14.5 -35.5t35.5 -14.5h100q21 0 35.5 14.5t14.5 35.5v150h150q21 0 35.5 14.5t14.5 35.5v100q0 21 -14.5 35.5t-35.5 14.5h-150v150q0 21 -14.5 35.5t-35.5 14.5z" />
<glyph unicode="&#xe082;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM850 700h-500q-21 0 -35.5 -14.5t-14.5 -35.5v-100q0 -21 14.5 -35.5 t35.5 -14.5h500q21 0 35.5 14.5t14.5 35.5v100q0 21 -14.5 35.5t-35.5 14.5z" />
<glyph unicode="&#xe083;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM741.5 913q-12.5 0 -21.5 -9l-120 -120l-120 120q-9 9 -21.5 9 t-21.5 -9l-141 -141q-9 -9 -9 -21.5t9 -21.5l120 -120l-120 -120q-9 -9 -9 -21.5t9 -21.5l141 -141q9 -9 21.5 -9t21.5 9l120 120l120 -120q9 -9 21.5 -9t21.5 9l141 141q9 9 9 21.5t-9 21.5l-120 120l120 120q9 9 9 21.5t-9 21.5l-141 141q-9 9 -21.5 9z" />
<glyph unicode="&#xe084;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM546 623l-84 85q-7 7 -17.5 7t-18.5 -7l-139 -139q-7 -8 -7 -18t7 -18 l242 -241q7 -8 17.5 -8t17.5 8l375 375q7 7 7 17.5t-7 18.5l-139 139q-7 7 -17.5 7t-17.5 -7z" />
<glyph unicode="&#xe085;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM588 941q-29 0 -59 -5.5t-63 -20.5t-58 -38.5t-41.5 -63t-16.5 -89.5 q0 -25 20 -25h131q30 -5 35 11q6 20 20.5 28t45.5 8q20 0 31.5 -10.5t11.5 -28.5q0 -23 -7 -34t-26 -18q-1 0 -13.5 -4t-19.5 -7.5t-20 -10.5t-22 -17t-18.5 -24t-15.5 -35t-8 -46q-1 -8 5.5 -16.5t20.5 -8.5h173q7 0 22 8t35 28t37.5 48t29.5 74t12 100q0 47 -17 83 t-42.5 57t-59.5 34.5t-64 18t-59 4.5zM675 400h-150q-10 0 -17.5 -7.5t-7.5 -17.5v-150q0 -10 7.5 -17.5t17.5 -7.5h150q10 0 17.5 7.5t7.5 17.5v150q0 10 -7.5 17.5t-17.5 7.5z" />
<glyph unicode="&#xe086;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM675 1000h-150q-10 0 -17.5 -7.5t-7.5 -17.5v-150q0 -10 7.5 -17.5 t17.5 -7.5h150q10 0 17.5 7.5t7.5 17.5v150q0 10 -7.5 17.5t-17.5 7.5zM675 700h-250q-10 0 -17.5 -7.5t-7.5 -17.5v-50q0 -10 7.5 -17.5t17.5 -7.5h75v-200h-75q-10 0 -17.5 -7.5t-7.5 -17.5v-50q0 -10 7.5 -17.5t17.5 -7.5h350q10 0 17.5 7.5t7.5 17.5v50q0 10 -7.5 17.5 t-17.5 7.5h-75v275q0 10 -7.5 17.5t-17.5 7.5z" />
<glyph unicode="&#xe087;" d="M525 1200h150q10 0 17.5 -7.5t7.5 -17.5v-194q103 -27 178.5 -102.5t102.5 -178.5h194q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-194q-27 -103 -102.5 -178.5t-178.5 -102.5v-194q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v194 q-103 27 -178.5 102.5t-102.5 178.5h-194q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5h194q27 103 102.5 178.5t178.5 102.5v194q0 10 7.5 17.5t17.5 7.5zM700 893v-168q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v168q-68 -23 -119 -74 t-74 -119h168q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-168q23 -68 74 -119t119 -74v168q0 10 7.5 17.5t17.5 7.5h150q10 0 17.5 -7.5t7.5 -17.5v-168q68 23 119 74t74 119h-168q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5h168 q-23 68 -74 119t-119 74z" />
<glyph unicode="&#xe088;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 1027q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5 t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5t-57 214.5t-155.5 155.5t-214.5 57zM759 823l64 -64q7 -7 7 -17.5t-7 -17.5l-124 -124l124 -124q7 -7 7 -17.5t-7 -17.5l-64 -64q-7 -7 -17.5 -7t-17.5 7l-124 124l-124 -124q-7 -7 -17.5 -7t-17.5 7l-64 64 q-7 7 -7 17.5t7 17.5l124 124l-124 124q-7 7 -7 17.5t7 17.5l64 64q7 7 17.5 7t17.5 -7l124 -124l124 124q7 7 17.5 7t17.5 -7z" />
<glyph unicode="&#xe089;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 1027q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5t57 -214.5 t155.5 -155.5t214.5 -57t214.5 57t155.5 155.5t57 214.5t-57 214.5t-155.5 155.5t-214.5 57zM782 788l106 -106q7 -7 7 -17.5t-7 -17.5l-320 -321q-8 -7 -18 -7t-18 7l-202 203q-8 7 -8 17.5t8 17.5l106 106q7 8 17.5 8t17.5 -8l79 -79l197 197q7 7 17.5 7t17.5 -7z" />
<glyph unicode="&#xe090;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM600 1027q-116 0 -214.5 -57t-155.5 -155.5t-57 -214.5q0 -120 65 -225 l587 587q-105 65 -225 65zM965 819l-584 -584q104 -62 219 -62q116 0 214.5 57t155.5 155.5t57 214.5q0 115 -62 219z" />
<glyph unicode="&#xe091;" d="M39 582l522 427q16 13 27.5 8t11.5 -26v-291h550q21 0 35.5 -14.5t14.5 -35.5v-200q0 -21 -14.5 -35.5t-35.5 -14.5h-550v-291q0 -21 -11.5 -26t-27.5 8l-522 427q-16 13 -16 32t16 32z" />
<glyph unicode="&#xe092;" d="M639 1009l522 -427q16 -13 16 -32t-16 -32l-522 -427q-16 -13 -27.5 -8t-11.5 26v291h-550q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5h550v291q0 21 11.5 26t27.5 -8z" />
<glyph unicode="&#xe093;" d="M682 1161l427 -522q13 -16 8 -27.5t-26 -11.5h-291v-550q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v550h-291q-21 0 -26 11.5t8 27.5l427 522q13 16 32 16t32 -16z" />
<glyph unicode="&#xe094;" d="M550 1200h200q21 0 35.5 -14.5t14.5 -35.5v-550h291q21 0 26 -11.5t-8 -27.5l-427 -522q-13 -16 -32 -16t-32 16l-427 522q-13 16 -8 27.5t26 11.5h291v550q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe095;" d="M639 1109l522 -427q16 -13 16 -32t-16 -32l-522 -427q-16 -13 -27.5 -8t-11.5 26v291q-94 -2 -182 -20t-170.5 -52t-147 -92.5t-100.5 -135.5q5 105 27 193.5t67.5 167t113 135t167 91.5t225.5 42v262q0 21 11.5 26t27.5 -8z" />
<glyph unicode="&#xe096;" d="M850 1200h300q21 0 35.5 -14.5t14.5 -35.5v-300q0 -21 -10.5 -25t-24.5 10l-94 94l-249 -249q-8 -7 -18 -7t-18 7l-106 106q-7 8 -7 18t7 18l249 249l-94 94q-14 14 -10 24.5t25 10.5zM350 0h-300q-21 0 -35.5 14.5t-14.5 35.5v300q0 21 10.5 25t24.5 -10l94 -94l249 249 q8 7 18 7t18 -7l106 -106q7 -8 7 -18t-7 -18l-249 -249l94 -94q14 -14 10 -24.5t-25 -10.5z" />
<glyph unicode="&#xe097;" d="M1014 1120l106 -106q7 -8 7 -18t-7 -18l-249 -249l94 -94q14 -14 10 -24.5t-25 -10.5h-300q-21 0 -35.5 14.5t-14.5 35.5v300q0 21 10.5 25t24.5 -10l94 -94l249 249q8 7 18 7t18 -7zM250 600h300q21 0 35.5 -14.5t14.5 -35.5v-300q0 -21 -10.5 -25t-24.5 10l-94 94 l-249 -249q-8 -7 -18 -7t-18 7l-106 106q-7 8 -7 18t7 18l249 249l-94 94q-14 14 -10 24.5t25 10.5z" />
<glyph unicode="&#xe101;" d="M600 1177q117 0 224 -45.5t184.5 -123t123 -184.5t45.5 -224t-45.5 -224t-123 -184.5t-184.5 -123t-224 -45.5t-224 45.5t-184.5 123t-123 184.5t-45.5 224t45.5 224t123 184.5t184.5 123t224 45.5zM704 900h-208q-20 0 -32 -14.5t-8 -34.5l58 -302q4 -20 21.5 -34.5 t37.5 -14.5h54q20 0 37.5 14.5t21.5 34.5l58 302q4 20 -8 34.5t-32 14.5zM675 400h-150q-10 0 -17.5 -7.5t-7.5 -17.5v-150q0 -10 7.5 -17.5t17.5 -7.5h150q10 0 17.5 7.5t7.5 17.5v150q0 10 -7.5 17.5t-17.5 7.5z" />
<glyph unicode="&#xe102;" d="M260 1200q9 0 19 -2t15 -4l5 -2q22 -10 44 -23l196 -118q21 -13 36 -24q29 -21 37 -12q11 13 49 35l196 118q22 13 45 23q17 7 38 7q23 0 47 -16.5t37 -33.5l13 -16q14 -21 18 -45l25 -123l8 -44q1 -9 8.5 -14.5t17.5 -5.5h61q10 0 17.5 -7.5t7.5 -17.5v-50 q0 -10 -7.5 -17.5t-17.5 -7.5h-50q-10 0 -17.5 -7.5t-7.5 -17.5v-175h-400v300h-200v-300h-400v175q0 10 -7.5 17.5t-17.5 7.5h-50q-10 0 -17.5 7.5t-7.5 17.5v50q0 10 7.5 17.5t17.5 7.5h61q11 0 18 3t7 8q0 4 9 52l25 128q5 25 19 45q2 3 5 7t13.5 15t21.5 19.5t26.5 15.5 t29.5 7zM915 1079l-166 -162q-7 -7 -5 -12t12 -5h219q10 0 15 7t2 17l-51 149q-3 10 -11 12t-15 -6zM463 917l-177 157q-8 7 -16 5t-11 -12l-51 -143q-3 -10 2 -17t15 -7h231q11 0 12.5 5t-5.5 12zM500 0h-375q-10 0 -17.5 7.5t-7.5 17.5v375h400v-400zM1100 400v-375 q0 -10 -7.5 -17.5t-17.5 -7.5h-375v400h400z" />
<glyph unicode="&#xe103;" d="M1165 1190q8 3 21 -6.5t13 -17.5q-2 -178 -24.5 -323.5t-55.5 -245.5t-87 -174.5t-102.5 -118.5t-118 -68.5t-118.5 -33t-120 -4.5t-105 9.5t-90 16.5q-61 12 -78 11q-4 1 -12.5 0t-34 -14.5t-52.5 -40.5l-153 -153q-26 -24 -37 -14.5t-11 43.5q0 64 42 102q8 8 50.5 45 t66.5 58q19 17 35 47t13 61q-9 55 -10 102.5t7 111t37 130t78 129.5q39 51 80 88t89.5 63.5t94.5 45t113.5 36t129 31t157.5 37t182 47.5zM1116 1098q-8 9 -22.5 -3t-45.5 -50q-38 -47 -119 -103.5t-142 -89.5l-62 -33q-56 -30 -102 -57t-104 -68t-102.5 -80.5t-85.5 -91 t-64 -104.5q-24 -56 -31 -86t2 -32t31.5 17.5t55.5 59.5q25 30 94 75.5t125.5 77.5t147.5 81q70 37 118.5 69t102 79.5t99 111t86.5 148.5q22 50 24 60t-6 19z" />
<glyph unicode="&#xe104;" d="M653 1231q-39 -67 -54.5 -131t-10.5 -114.5t24.5 -96.5t47.5 -80t63.5 -62.5t68.5 -46.5t65 -30q-4 7 -17.5 35t-18.5 39.5t-17 39.5t-17 43t-13 42t-9.5 44.5t-2 42t4 43t13.5 39t23 38.5q96 -42 165 -107.5t105 -138t52 -156t13 -159t-19 -149.5q-13 -55 -44 -106.5 t-68 -87t-78.5 -64.5t-72.5 -45t-53 -22q-72 -22 -127 -11q-31 6 -13 19q6 3 17 7q13 5 32.5 21t41 44t38.5 63.5t21.5 81.5t-6.5 94.5t-50 107t-104 115.5q10 -104 -0.5 -189t-37 -140.5t-65 -93t-84 -52t-93.5 -11t-95 24.5q-80 36 -131.5 114t-53.5 171q-2 23 0 49.5 t4.5 52.5t13.5 56t27.5 60t46 64.5t69.5 68.5q-8 -53 -5 -102.5t17.5 -90t34 -68.5t44.5 -39t49 -2q31 13 38.5 36t-4.5 55t-29 64.5t-36 75t-26 75.5q-15 85 2 161.5t53.5 128.5t85.5 92.5t93.5 61t81.5 25.5z" />
<glyph unicode="&#xe105;" d="M600 1094q82 0 160.5 -22.5t140 -59t116.5 -82.5t94.5 -95t68 -95t42.5 -82.5t14 -57.5t-14 -57.5t-43 -82.5t-68.5 -95t-94.5 -95t-116.5 -82.5t-140 -59t-159.5 -22.5t-159.5 22.5t-140 59t-116.5 82.5t-94.5 95t-68.5 95t-43 82.5t-14 57.5t14 57.5t42.5 82.5t68 95 t94.5 95t116.5 82.5t140 59t160.5 22.5zM888 829q-15 15 -18 12t5 -22q25 -57 25 -119q0 -124 -88 -212t-212 -88t-212 88t-88 212q0 59 23 114q8 19 4.5 22t-17.5 -12q-70 -69 -160 -184q-13 -16 -15 -40.5t9 -42.5q22 -36 47 -71t70 -82t92.5 -81t113 -58.5t133.5 -24.5 t133.5 24t113 58.5t92.5 81.5t70 81.5t47 70.5q11 18 9 42.5t-14 41.5q-90 117 -163 189zM448 727l-35 -36q-15 -15 -19.5 -38.5t4.5 -41.5q37 -68 93 -116q16 -13 38.5 -11t36.5 17l35 34q14 15 12.5 33.5t-16.5 33.5q-44 44 -89 117q-11 18 -28 20t-32 -12z" />
<glyph unicode="&#xe106;" d="M592 0h-148l31 120q-91 20 -175.5 68.5t-143.5 106.5t-103.5 119t-66.5 110t-22 76q0 21 14 57.5t42.5 82.5t68 95t94.5 95t116.5 82.5t140 59t160.5 22.5q61 0 126 -15l32 121h148zM944 770l47 181q108 -85 176.5 -192t68.5 -159q0 -26 -19.5 -71t-59.5 -102t-93 -112 t-129 -104.5t-158 -75.5l46 173q77 49 136 117t97 131q11 18 9 42.5t-14 41.5q-54 70 -107 130zM310 824q-70 -69 -160 -184q-13 -16 -15 -40.5t9 -42.5q18 -30 39 -60t57 -70.5t74 -73t90 -61t105 -41.5l41 154q-107 18 -178.5 101.5t-71.5 193.5q0 59 23 114q8 19 4.5 22 t-17.5 -12zM448 727l-35 -36q-15 -15 -19.5 -38.5t4.5 -41.5q37 -68 93 -116q16 -13 38.5 -11t36.5 17l12 11l22 86l-3 4q-44 44 -89 117q-11 18 -28 20t-32 -12z" />
<glyph unicode="&#xe107;" d="M-90 100l642 1066q20 31 48 28.5t48 -35.5l642 -1056q21 -32 7.5 -67.5t-50.5 -35.5h-1294q-37 0 -50.5 34t7.5 66zM155 200h345v75q0 10 7.5 17.5t17.5 7.5h150q10 0 17.5 -7.5t7.5 -17.5v-75h345l-445 723zM496 700h208q20 0 32 -14.5t8 -34.5l-58 -252 q-4 -20 -21.5 -34.5t-37.5 -14.5h-54q-20 0 -37.5 14.5t-21.5 34.5l-58 252q-4 20 8 34.5t32 14.5z" />
<glyph unicode="&#xe108;" d="M650 1200q62 0 106 -44t44 -106v-339l363 -325q15 -14 26 -38.5t11 -44.5v-41q0 -20 -12 -26.5t-29 5.5l-359 249v-263q100 -93 100 -113v-64q0 -21 -13 -29t-32 1l-205 128l-205 -128q-19 -9 -32 -1t-13 29v64q0 20 100 113v263l-359 -249q-17 -12 -29 -5.5t-12 26.5v41 q0 20 11 44.5t26 38.5l363 325v339q0 62 44 106t106 44z" />
<glyph unicode="&#xe109;" d="M850 1200h100q21 0 35.5 -14.5t14.5 -35.5v-50h50q21 0 35.5 -14.5t14.5 -35.5v-150h-1100v150q0 21 14.5 35.5t35.5 14.5h50v50q0 21 14.5 35.5t35.5 14.5h100q21 0 35.5 -14.5t14.5 -35.5v-50h500v50q0 21 14.5 35.5t35.5 14.5zM1100 800v-750q0 -21 -14.5 -35.5 t-35.5 -14.5h-1000q-21 0 -35.5 14.5t-14.5 35.5v750h1100zM100 600v-100h100v100h-100zM300 600v-100h100v100h-100zM500 600v-100h100v100h-100zM700 600v-100h100v100h-100zM900 600v-100h100v100h-100zM100 400v-100h100v100h-100zM300 400v-100h100v100h-100zM500 400 v-100h100v100h-100zM700 400v-100h100v100h-100zM900 400v-100h100v100h-100zM100 200v-100h100v100h-100zM300 200v-100h100v100h-100zM500 200v-100h100v100h-100zM700 200v-100h100v100h-100zM900 200v-100h100v100h-100z" />
<glyph unicode="&#xe110;" d="M1135 1165l249 -230q15 -14 15 -35t-15 -35l-249 -230q-14 -14 -24.5 -10t-10.5 25v150h-159l-600 -600h-291q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5h209l600 600h241v150q0 21 10.5 25t24.5 -10zM522 819l-141 -141l-122 122h-209q-21 0 -35.5 14.5 t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5h291zM1135 565l249 -230q15 -14 15 -35t-15 -35l-249 -230q-14 -14 -24.5 -10t-10.5 25v150h-241l-181 181l141 141l122 -122h159v150q0 21 10.5 25t24.5 -10z" />
<glyph unicode="&#xe111;" d="M100 1100h1000q41 0 70.5 -29.5t29.5 -70.5v-600q0 -41 -29.5 -70.5t-70.5 -29.5h-596l-304 -300v300h-100q-41 0 -70.5 29.5t-29.5 70.5v600q0 41 29.5 70.5t70.5 29.5z" />
<glyph unicode="&#xe112;" d="M150 1200h200q21 0 35.5 -14.5t14.5 -35.5v-250h-300v250q0 21 14.5 35.5t35.5 14.5zM850 1200h200q21 0 35.5 -14.5t14.5 -35.5v-250h-300v250q0 21 14.5 35.5t35.5 14.5zM1100 800v-300q0 -41 -3 -77.5t-15 -89.5t-32 -96t-58 -89t-89 -77t-129 -51t-174 -20t-174 20 t-129 51t-89 77t-58 89t-32 96t-15 89.5t-3 77.5v300h300v-250v-27v-42.5t1.5 -41t5 -38t10 -35t16.5 -30t25.5 -24.5t35 -19t46.5 -12t60 -4t60 4.5t46.5 12.5t35 19.5t25 25.5t17 30.5t10 35t5 38t2 40.5t-0.5 42v25v250h300z" />
<glyph unicode="&#xe113;" d="M1100 411l-198 -199l-353 353l-353 -353l-197 199l551 551z" />
<glyph unicode="&#xe114;" d="M1101 789l-550 -551l-551 551l198 199l353 -353l353 353z" />
<glyph unicode="&#xe115;" d="M404 1000h746q21 0 35.5 -14.5t14.5 -35.5v-551h150q21 0 25 -10.5t-10 -24.5l-230 -249q-14 -15 -35 -15t-35 15l-230 249q-14 14 -10 24.5t25 10.5h150v401h-381zM135 984l230 -249q14 -14 10 -24.5t-25 -10.5h-150v-400h385l215 -200h-750q-21 0 -35.5 14.5 t-14.5 35.5v550h-150q-21 0 -25 10.5t10 24.5l230 249q14 15 35 15t35 -15z" />
<glyph unicode="&#xe116;" d="M56 1200h94q17 0 31 -11t18 -27l38 -162h896q24 0 39 -18.5t10 -42.5l-100 -475q-5 -21 -27 -42.5t-55 -21.5h-633l48 -200h535q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-50v-50q0 -21 -14.5 -35.5t-35.5 -14.5t-35.5 14.5t-14.5 35.5v50h-300v-50 q0 -21 -14.5 -35.5t-35.5 -14.5t-35.5 14.5t-14.5 35.5v50h-31q-18 0 -32.5 10t-20.5 19l-5 10l-201 961h-54q-20 0 -35 14.5t-15 35.5t15 35.5t35 14.5z" />
<glyph unicode="&#xe117;" d="M1200 1000v-100h-1200v100h200q0 41 29.5 70.5t70.5 29.5h300q41 0 70.5 -29.5t29.5 -70.5h500zM0 800h1200v-800h-1200v800z" />
<glyph unicode="&#xe118;" d="M200 800l-200 -400v600h200q0 41 29.5 70.5t70.5 29.5h300q42 0 71 -29.5t29 -70.5h500v-200h-1000zM1500 700l-300 -700h-1200l300 700h1200z" />
<glyph unicode="&#xe119;" d="M635 1184l230 -249q14 -14 10 -24.5t-25 -10.5h-150v-601h150q21 0 25 -10.5t-10 -24.5l-230 -249q-14 -15 -35 -15t-35 15l-230 249q-14 14 -10 24.5t25 10.5h150v601h-150q-21 0 -25 10.5t10 24.5l230 249q14 15 35 15t35 -15z" />
<glyph unicode="&#xe120;" d="M936 864l249 -229q14 -15 14 -35.5t-14 -35.5l-249 -229q-15 -15 -25.5 -10.5t-10.5 24.5v151h-600v-151q0 -20 -10.5 -24.5t-25.5 10.5l-249 229q-14 15 -14 35.5t14 35.5l249 229q15 15 25.5 10.5t10.5 -25.5v-149h600v149q0 21 10.5 25.5t25.5 -10.5z" />
<glyph unicode="&#xe121;" d="M1169 400l-172 732q-5 23 -23 45.5t-38 22.5h-672q-20 0 -38 -20t-23 -41l-172 -739h1138zM1100 300h-1000q-41 0 -70.5 -29.5t-29.5 -70.5v-100q0 -41 29.5 -70.5t70.5 -29.5h1000q41 0 70.5 29.5t29.5 70.5v100q0 41 -29.5 70.5t-70.5 29.5zM800 100v100h100v-100h-100 zM1000 100v100h100v-100h-100z" />
<glyph unicode="&#xe122;" d="M1150 1100q21 0 35.5 -14.5t14.5 -35.5v-850q0 -21 -14.5 -35.5t-35.5 -14.5t-35.5 14.5t-14.5 35.5v850q0 21 14.5 35.5t35.5 14.5zM1000 200l-675 200h-38l47 -276q3 -16 -5.5 -20t-29.5 -4h-7h-84q-20 0 -34.5 14t-18.5 35q-55 337 -55 351v250v6q0 16 1 23.5t6.5 14 t17.5 6.5h200l675 250v-850zM0 750v-250q-4 0 -11 0.5t-24 6t-30 15t-24 30t-11 48.5v50q0 26 10.5 46t25 30t29 16t25.5 7z" />
<glyph unicode="&#xe123;" d="M553 1200h94q20 0 29 -10.5t3 -29.5l-18 -37q83 -19 144 -82.5t76 -140.5l63 -327l118 -173h17q19 0 33 -14.5t14 -35t-13 -40.5t-31 -27q-8 -4 -23 -9.5t-65 -19.5t-103 -25t-132.5 -20t-158.5 -9q-57 0 -115 5t-104 12t-88.5 15.5t-73.5 17.5t-54.5 16t-35.5 12l-11 4 q-18 8 -31 28t-13 40.5t14 35t33 14.5h17l118 173l63 327q15 77 76 140t144 83l-18 32q-6 19 3.5 32t28.5 13zM498 110q50 -6 102 -6q53 0 102 6q-12 -49 -39.5 -79.5t-62.5 -30.5t-63 30.5t-39 79.5z" />
<glyph unicode="&#xe124;" d="M800 946l224 78l-78 -224l234 -45l-180 -155l180 -155l-234 -45l78 -224l-224 78l-45 -234l-155 180l-155 -180l-45 234l-224 -78l78 224l-234 45l180 155l-180 155l234 45l-78 224l224 -78l45 234l155 -180l155 180z" />
<glyph unicode="&#xe125;" d="M650 1200h50q40 0 70 -40.5t30 -84.5v-150l-28 -125h328q40 0 70 -40.5t30 -84.5v-100q0 -45 -29 -74l-238 -344q-16 -24 -38 -40.5t-45 -16.5h-250q-7 0 -42 25t-66 50l-31 25h-61q-45 0 -72.5 18t-27.5 57v400q0 36 20 63l145 196l96 198q13 28 37.5 48t51.5 20z M650 1100l-100 -212l-150 -213v-375h100l136 -100h214l250 375v125h-450l50 225v175h-50zM50 800h100q21 0 35.5 -14.5t14.5 -35.5v-500q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v500q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe126;" d="M600 1100h250q23 0 45 -16.5t38 -40.5l238 -344q29 -29 29 -74v-100q0 -44 -30 -84.5t-70 -40.5h-328q28 -118 28 -125v-150q0 -44 -30 -84.5t-70 -40.5h-50q-27 0 -51.5 20t-37.5 48l-96 198l-145 196q-20 27 -20 63v400q0 39 27.5 57t72.5 18h61q124 100 139 100z M50 1000h100q21 0 35.5 -14.5t14.5 -35.5v-500q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v500q0 21 14.5 35.5t35.5 14.5zM636 1000l-136 -100h-100v-375l150 -213l100 -212h50v175l-50 225h450v125l-250 375h-214z" />
<glyph unicode="&#xe127;" d="M356 873l363 230q31 16 53 -6l110 -112q13 -13 13.5 -32t-11.5 -34l-84 -121h302q84 0 138 -38t54 -110t-55 -111t-139 -39h-106l-131 -339q-6 -21 -19.5 -41t-28.5 -20h-342q-7 0 -90 81t-83 94v525q0 17 14 35.5t28 28.5zM400 792v-503l100 -89h293l131 339 q6 21 19.5 41t28.5 20h203q21 0 30.5 25t0.5 50t-31 25h-456h-7h-6h-5.5t-6 0.5t-5 1.5t-5 2t-4 2.5t-4 4t-2.5 4.5q-12 25 5 47l146 183l-86 83zM50 800h100q21 0 35.5 -14.5t14.5 -35.5v-500q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v500 q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe128;" d="M475 1103l366 -230q2 -1 6 -3.5t14 -10.5t18 -16.5t14.5 -20t6.5 -22.5v-525q0 -13 -86 -94t-93 -81h-342q-15 0 -28.5 20t-19.5 41l-131 339h-106q-85 0 -139.5 39t-54.5 111t54 110t138 38h302l-85 121q-11 15 -10.5 34t13.5 32l110 112q22 22 53 6zM370 945l146 -183 q17 -22 5 -47q-2 -2 -3.5 -4.5t-4 -4t-4 -2.5t-5 -2t-5 -1.5t-6 -0.5h-6h-6.5h-6h-475v-100h221q15 0 29 -20t20 -41l130 -339h294l106 89v503l-342 236zM1050 800h100q21 0 35.5 -14.5t14.5 -35.5v-500q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5 v500q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe129;" d="M550 1294q72 0 111 -55t39 -139v-106l339 -131q21 -6 41 -19.5t20 -28.5v-342q0 -7 -81 -90t-94 -83h-525q-17 0 -35.5 14t-28.5 28l-9 14l-230 363q-16 31 6 53l112 110q13 13 32 13.5t34 -11.5l121 -84v302q0 84 38 138t110 54zM600 972v203q0 21 -25 30.5t-50 0.5 t-25 -31v-456v-7v-6v-5.5t-0.5 -6t-1.5 -5t-2 -5t-2.5 -4t-4 -4t-4.5 -2.5q-25 -12 -47 5l-183 146l-83 -86l236 -339h503l89 100v293l-339 131q-21 6 -41 19.5t-20 28.5zM450 200h500q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-500 q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe130;" d="M350 1100h500q21 0 35.5 14.5t14.5 35.5v100q0 21 -14.5 35.5t-35.5 14.5h-500q-21 0 -35.5 -14.5t-14.5 -35.5v-100q0 -21 14.5 -35.5t35.5 -14.5zM600 306v-106q0 -84 -39 -139t-111 -55t-110 54t-38 138v302l-121 -84q-15 -12 -34 -11.5t-32 13.5l-112 110 q-22 22 -6 53l230 363q1 2 3.5 6t10.5 13.5t16.5 17t20 13.5t22.5 6h525q13 0 94 -83t81 -90v-342q0 -15 -20 -28.5t-41 -19.5zM308 900l-236 -339l83 -86l183 146q22 17 47 5q2 -1 4.5 -2.5t4 -4t2.5 -4t2 -5t1.5 -5t0.5 -6v-5.5v-6v-7v-456q0 -22 25 -31t50 0.5t25 30.5 v203q0 15 20 28.5t41 19.5l339 131v293l-89 100h-503z" />
<glyph unicode="&#xe131;" d="M600 1178q118 0 225 -45.5t184.5 -123t123 -184.5t45.5 -225t-45.5 -225t-123 -184.5t-184.5 -123t-225 -45.5t-225 45.5t-184.5 123t-123 184.5t-45.5 225t45.5 225t123 184.5t184.5 123t225 45.5zM914 632l-275 223q-16 13 -27.5 8t-11.5 -26v-137h-275 q-10 0 -17.5 -7.5t-7.5 -17.5v-150q0 -10 7.5 -17.5t17.5 -7.5h275v-137q0 -21 11.5 -26t27.5 8l275 223q16 13 16 32t-16 32z" />
<glyph unicode="&#xe132;" d="M600 1178q118 0 225 -45.5t184.5 -123t123 -184.5t45.5 -225t-45.5 -225t-123 -184.5t-184.5 -123t-225 -45.5t-225 45.5t-184.5 123t-123 184.5t-45.5 225t45.5 225t123 184.5t184.5 123t225 45.5zM561 855l-275 -223q-16 -13 -16 -32t16 -32l275 -223q16 -13 27.5 -8 t11.5 26v137h275q10 0 17.5 7.5t7.5 17.5v150q0 10 -7.5 17.5t-17.5 7.5h-275v137q0 21 -11.5 26t-27.5 -8z" />
<glyph unicode="&#xe133;" d="M600 1178q118 0 225 -45.5t184.5 -123t123 -184.5t45.5 -225t-45.5 -225t-123 -184.5t-184.5 -123t-225 -45.5t-225 45.5t-184.5 123t-123 184.5t-45.5 225t45.5 225t123 184.5t184.5 123t225 45.5zM855 639l-223 275q-13 16 -32 16t-32 -16l-223 -275q-13 -16 -8 -27.5 t26 -11.5h137v-275q0 -10 7.5 -17.5t17.5 -7.5h150q10 0 17.5 7.5t7.5 17.5v275h137q21 0 26 11.5t-8 27.5z" />
<glyph unicode="&#xe134;" d="M600 1178q118 0 225 -45.5t184.5 -123t123 -184.5t45.5 -225t-45.5 -225t-123 -184.5t-184.5 -123t-225 -45.5t-225 45.5t-184.5 123t-123 184.5t-45.5 225t45.5 225t123 184.5t184.5 123t225 45.5zM675 900h-150q-10 0 -17.5 -7.5t-7.5 -17.5v-275h-137q-21 0 -26 -11.5 t8 -27.5l223 -275q13 -16 32 -16t32 16l223 275q13 16 8 27.5t-26 11.5h-137v275q0 10 -7.5 17.5t-17.5 7.5z" />
<glyph unicode="&#xe135;" d="M600 1176q116 0 222.5 -46t184 -123.5t123.5 -184t46 -222.5t-46 -222.5t-123.5 -184t-184 -123.5t-222.5 -46t-222.5 46t-184 123.5t-123.5 184t-46 222.5t46 222.5t123.5 184t184 123.5t222.5 46zM627 1101q-15 -12 -36.5 -20.5t-35.5 -12t-43 -8t-39 -6.5 q-15 -3 -45.5 0t-45.5 -2q-20 -7 -51.5 -26.5t-34.5 -34.5q-3 -11 6.5 -22.5t8.5 -18.5q-3 -34 -27.5 -91t-29.5 -79q-9 -34 5 -93t8 -87q0 -9 17 -44.5t16 -59.5q12 0 23 -5t23.5 -15t19.5 -14q16 -8 33 -15t40.5 -15t34.5 -12q21 -9 52.5 -32t60 -38t57.5 -11 q7 -15 -3 -34t-22.5 -40t-9.5 -38q13 -21 23 -34.5t27.5 -27.5t36.5 -18q0 -7 -3.5 -16t-3.5 -14t5 -17q104 -2 221 112q30 29 46.5 47t34.5 49t21 63q-13 8 -37 8.5t-36 7.5q-15 7 -49.5 15t-51.5 19q-18 0 -41 -0.5t-43 -1.5t-42 -6.5t-38 -16.5q-51 -35 -66 -12 q-4 1 -3.5 25.5t0.5 25.5q-6 13 -26.5 17.5t-24.5 6.5q1 15 -0.5 30.5t-7 28t-18.5 11.5t-31 -21q-23 -25 -42 4q-19 28 -8 58q6 16 22 22q6 -1 26 -1.5t33.5 -4t19.5 -13.5q7 -12 18 -24t21.5 -20.5t20 -15t15.5 -10.5l5 -3q2 12 7.5 30.5t8 34.5t-0.5 32q-3 18 3.5 29 t18 22.5t15.5 24.5q6 14 10.5 35t8 31t15.5 22.5t34 22.5q-6 18 10 36q8 0 24 -1.5t24.5 -1.5t20 4.5t20.5 15.5q-10 23 -31 42.5t-37.5 29.5t-49 27t-43.5 23q0 1 2 8t3 11.5t1.5 10.5t-1 9.5t-4.5 4.5q31 -13 58.5 -14.5t38.5 2.5l12 5q5 28 -9.5 46t-36.5 24t-50 15 t-41 20q-18 -4 -37 0zM613 994q0 -17 8 -42t17 -45t9 -23q-8 1 -39.5 5.5t-52.5 10t-37 16.5q3 11 16 29.5t16 25.5q10 -10 19 -10t14 6t13.5 14.5t16.5 12.5z" />
<glyph unicode="&#xe136;" d="M756 1157q164 92 306 -9l-259 -138l145 -232l251 126q6 -89 -34 -156.5t-117 -110.5q-60 -34 -127 -39.5t-126 16.5l-596 -596q-15 -16 -36.5 -16t-36.5 16l-111 110q-15 15 -15 36.5t15 37.5l600 599q-34 101 5.5 201.5t135.5 154.5z" />
<glyph unicode="&#xe137;" horiz-adv-x="1220" d="M100 1196h1000q41 0 70.5 -29.5t29.5 -70.5v-100q0 -41 -29.5 -70.5t-70.5 -29.5h-1000q-41 0 -70.5 29.5t-29.5 70.5v100q0 41 29.5 70.5t70.5 29.5zM1100 1096h-200v-100h200v100zM100 796h1000q41 0 70.5 -29.5t29.5 -70.5v-100q0 -41 -29.5 -70.5t-70.5 -29.5h-1000 q-41 0 -70.5 29.5t-29.5 70.5v100q0 41 29.5 70.5t70.5 29.5zM1100 696h-500v-100h500v100zM100 396h1000q41 0 70.5 -29.5t29.5 -70.5v-100q0 -41 -29.5 -70.5t-70.5 -29.5h-1000q-41 0 -70.5 29.5t-29.5 70.5v100q0 41 29.5 70.5t70.5 29.5zM1100 296h-300v-100h300v100z " />
<glyph unicode="&#xe138;" d="M150 1200h900q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-900q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5zM700 500v-300l-200 -200v500l-350 500h900z" />
<glyph unicode="&#xe139;" d="M500 1200h200q41 0 70.5 -29.5t29.5 -70.5v-100h300q41 0 70.5 -29.5t29.5 -70.5v-400h-500v100h-200v-100h-500v400q0 41 29.5 70.5t70.5 29.5h300v100q0 41 29.5 70.5t70.5 29.5zM500 1100v-100h200v100h-200zM1200 400v-200q0 -41 -29.5 -70.5t-70.5 -29.5h-1000 q-41 0 -70.5 29.5t-29.5 70.5v200h1200z" />
<glyph unicode="&#xe140;" d="M50 1200h300q21 0 25 -10.5t-10 -24.5l-94 -94l199 -199q7 -8 7 -18t-7 -18l-106 -106q-8 -7 -18 -7t-18 7l-199 199l-94 -94q-14 -14 -24.5 -10t-10.5 25v300q0 21 14.5 35.5t35.5 14.5zM850 1200h300q21 0 35.5 -14.5t14.5 -35.5v-300q0 -21 -10.5 -25t-24.5 10l-94 94 l-199 -199q-8 -7 -18 -7t-18 7l-106 106q-7 8 -7 18t7 18l199 199l-94 94q-14 14 -10 24.5t25 10.5zM364 470l106 -106q7 -8 7 -18t-7 -18l-199 -199l94 -94q14 -14 10 -24.5t-25 -10.5h-300q-21 0 -35.5 14.5t-14.5 35.5v300q0 21 10.5 25t24.5 -10l94 -94l199 199 q8 7 18 7t18 -7zM1071 271l94 94q14 14 24.5 10t10.5 -25v-300q0 -21 -14.5 -35.5t-35.5 -14.5h-300q-21 0 -25 10.5t10 24.5l94 94l-199 199q-7 8 -7 18t7 18l106 106q8 7 18 7t18 -7z" />
<glyph unicode="&#xe141;" d="M596 1192q121 0 231.5 -47.5t190 -127t127 -190t47.5 -231.5t-47.5 -231.5t-127 -190.5t-190 -127t-231.5 -47t-231.5 47t-190.5 127t-127 190.5t-47 231.5t47 231.5t127 190t190.5 127t231.5 47.5zM596 1010q-112 0 -207.5 -55.5t-151 -151t-55.5 -207.5t55.5 -207.5 t151 -151t207.5 -55.5t207.5 55.5t151 151t55.5 207.5t-55.5 207.5t-151 151t-207.5 55.5zM454.5 905q22.5 0 38.5 -16t16 -38.5t-16 -39t-38.5 -16.5t-38.5 16.5t-16 39t16 38.5t38.5 16zM754.5 905q22.5 0 38.5 -16t16 -38.5t-16 -39t-38 -16.5q-14 0 -29 10l-55 -145 q17 -23 17 -51q0 -36 -25.5 -61.5t-61.5 -25.5t-61.5 25.5t-25.5 61.5q0 32 20.5 56.5t51.5 29.5l122 126l1 1q-9 14 -9 28q0 23 16 39t38.5 16zM345.5 709q22.5 0 38.5 -16t16 -38.5t-16 -38.5t-38.5 -16t-38.5 16t-16 38.5t16 38.5t38.5 16zM854.5 709q22.5 0 38.5 -16 t16 -38.5t-16 -38.5t-38.5 -16t-38.5 16t-16 38.5t16 38.5t38.5 16z" />
<glyph unicode="&#xe142;" d="M546 173l469 470q91 91 99 192q7 98 -52 175.5t-154 94.5q-22 4 -47 4q-34 0 -66.5 -10t-56.5 -23t-55.5 -38t-48 -41.5t-48.5 -47.5q-376 -375 -391 -390q-30 -27 -45 -41.5t-37.5 -41t-32 -46.5t-16 -47.5t-1.5 -56.5q9 -62 53.5 -95t99.5 -33q74 0 125 51l548 548 q36 36 20 75q-7 16 -21.5 26t-32.5 10q-26 0 -50 -23q-13 -12 -39 -38l-341 -338q-15 -15 -35.5 -15.5t-34.5 13.5t-14 34.5t14 34.5q327 333 361 367q35 35 67.5 51.5t78.5 16.5q14 0 29 -1q44 -8 74.5 -35.5t43.5 -68.5q14 -47 2 -96.5t-47 -84.5q-12 -11 -32 -32 t-79.5 -81t-114.5 -115t-124.5 -123.5t-123 -119.5t-96.5 -89t-57 -45q-56 -27 -120 -27q-70 0 -129 32t-93 89q-48 78 -35 173t81 163l511 511q71 72 111 96q91 55 198 55q80 0 152 -33q78 -36 129.5 -103t66.5 -154q17 -93 -11 -183.5t-94 -156.5l-482 -476 q-15 -15 -36 -16t-37 14t-17.5 34t14.5 35z" />
<glyph unicode="&#xe143;" d="M649 949q48 68 109.5 104t121.5 38.5t118.5 -20t102.5 -64t71 -100.5t27 -123q0 -57 -33.5 -117.5t-94 -124.5t-126.5 -127.5t-150 -152.5t-146 -174q-62 85 -145.5 174t-150 152.5t-126.5 127.5t-93.5 124.5t-33.5 117.5q0 64 28 123t73 100.5t104 64t119 20 t120.5 -38.5t104.5 -104zM896 972q-33 0 -64.5 -19t-56.5 -46t-47.5 -53.5t-43.5 -45.5t-37.5 -19t-36 19t-40 45.5t-43 53.5t-54 46t-65.5 19q-67 0 -122.5 -55.5t-55.5 -132.5q0 -23 13.5 -51t46 -65t57.5 -63t76 -75l22 -22q15 -14 44 -44t50.5 -51t46 -44t41 -35t23 -12 t23.5 12t42.5 36t46 44t52.5 52t44 43q4 4 12 13q43 41 63.5 62t52 55t46 55t26 46t11.5 44q0 79 -53 133.5t-120 54.5z" />
<glyph unicode="&#xe144;" d="M776.5 1214q93.5 0 159.5 -66l141 -141q66 -66 66 -160q0 -42 -28 -95.5t-62 -87.5l-29 -29q-31 53 -77 99l-18 18l95 95l-247 248l-389 -389l212 -212l-105 -106l-19 18l-141 141q-66 66 -66 159t66 159l283 283q65 66 158.5 66zM600 706l105 105q10 -8 19 -17l141 -141 q66 -66 66 -159t-66 -159l-283 -283q-66 -66 -159 -66t-159 66l-141 141q-66 66 -66 159.5t66 159.5l55 55q29 -55 75 -102l18 -17l-95 -95l247 -248l389 389z" />
<glyph unicode="&#xe145;" d="M603 1200q85 0 162 -15t127 -38t79 -48t29 -46v-953q0 -41 -29.5 -70.5t-70.5 -29.5h-600q-41 0 -70.5 29.5t-29.5 70.5v953q0 21 30 46.5t81 48t129 37.5t163 15zM300 1000v-700h600v700h-600zM600 254q-43 0 -73.5 -30.5t-30.5 -73.5t30.5 -73.5t73.5 -30.5t73.5 30.5 t30.5 73.5t-30.5 73.5t-73.5 30.5z" />
<glyph unicode="&#xe146;" d="M902 1185l283 -282q15 -15 15 -36t-14.5 -35.5t-35.5 -14.5t-35 15l-36 35l-279 -267v-300l-212 210l-308 -307l-280 -203l203 280l307 308l-210 212h300l267 279l-35 36q-15 14 -15 35t14.5 35.5t35.5 14.5t35 -15z" />
<glyph unicode="&#xe148;" d="M700 1248v-78q38 -5 72.5 -14.5t75.5 -31.5t71 -53.5t52 -84t24 -118.5h-159q-4 36 -10.5 59t-21 45t-40 35.5t-64.5 20.5v-307l64 -13q34 -7 64 -16.5t70 -32t67.5 -52.5t47.5 -80t20 -112q0 -139 -89 -224t-244 -97v-77h-100v79q-150 16 -237 103q-40 40 -52.5 93.5 t-15.5 139.5h139q5 -77 48.5 -126t117.5 -65v335l-27 8q-46 14 -79 26.5t-72 36t-63 52t-40 72.5t-16 98q0 70 25 126t67.5 92t94.5 57t110 27v77h100zM600 754v274q-29 -4 -50 -11t-42 -21.5t-31.5 -41.5t-10.5 -65q0 -29 7 -50.5t16.5 -34t28.5 -22.5t31.5 -14t37.5 -10 q9 -3 13 -4zM700 547v-310q22 2 42.5 6.5t45 15.5t41.5 27t29 42t12 59.5t-12.5 59.5t-38 44.5t-53 31t-66.5 24.5z" />
<glyph unicode="&#xe149;" d="M561 1197q84 0 160.5 -40t123.5 -109.5t47 -147.5h-153q0 40 -19.5 71.5t-49.5 48.5t-59.5 26t-55.5 9q-37 0 -79 -14.5t-62 -35.5q-41 -44 -41 -101q0 -26 13.5 -63t26.5 -61t37 -66q6 -9 9 -14h241v-100h-197q8 -50 -2.5 -115t-31.5 -95q-45 -62 -99 -112 q34 10 83 17.5t71 7.5q32 1 102 -16t104 -17q83 0 136 30l50 -147q-31 -19 -58 -30.5t-55 -15.5t-42 -4.5t-46 -0.5q-23 0 -76 17t-111 32.5t-96 11.5q-39 -3 -82 -16t-67 -25l-23 -11l-55 145q4 3 16 11t15.5 10.5t13 9t15.5 12t14.5 14t17.5 18.5q48 55 54 126.5 t-30 142.5h-221v100h166q-23 47 -44 104q-7 20 -12 41.5t-6 55.5t6 66.5t29.5 70.5t58.5 71q97 88 263 88z" />
<glyph unicode="&#xe150;" d="M400 300h150q21 0 25 -11t-10 -25l-230 -250q-14 -15 -35 -15t-35 15l-230 250q-14 14 -10 25t25 11h150v900h200v-900zM935 1184l230 -249q14 -14 10 -24.5t-25 -10.5h-150v-900h-200v900h-150q-21 0 -25 10.5t10 24.5l230 249q14 15 35 15t35 -15z" />
<glyph unicode="&#xe151;" d="M1000 700h-100v100h-100v-100h-100v500h300v-500zM400 300h150q21 0 25 -11t-10 -25l-230 -250q-14 -15 -35 -15t-35 15l-230 250q-14 14 -10 25t25 11h150v900h200v-900zM801 1100v-200h100v200h-100zM1000 350l-200 -250h200v-100h-300v150l200 250h-200v100h300v-150z " />
<glyph unicode="&#xe152;" d="M400 300h150q21 0 25 -11t-10 -25l-230 -250q-14 -15 -35 -15t-35 15l-230 250q-14 14 -10 25t25 11h150v900h200v-900zM1000 1050l-200 -250h200v-100h-300v150l200 250h-200v100h300v-150zM1000 0h-100v100h-100v-100h-100v500h300v-500zM801 400v-200h100v200h-100z " />
<glyph unicode="&#xe153;" d="M400 300h150q21 0 25 -11t-10 -25l-230 -250q-14 -15 -35 -15t-35 15l-230 250q-14 14 -10 25t25 11h150v900h200v-900zM1000 700h-100v400h-100v100h200v-500zM1100 0h-100v100h-200v400h300v-500zM901 400v-200h100v200h-100z" />
<glyph unicode="&#xe154;" d="M400 300h150q21 0 25 -11t-10 -25l-230 -250q-14 -15 -35 -15t-35 15l-230 250q-14 14 -10 25t25 11h150v900h200v-900zM1100 700h-100v100h-200v400h300v-500zM901 1100v-200h100v200h-100zM1000 0h-100v400h-100v100h200v-500z" />
<glyph unicode="&#xe155;" d="M400 300h150q21 0 25 -11t-10 -25l-230 -250q-14 -15 -35 -15t-35 15l-230 250q-14 14 -10 25t25 11h150v900h200v-900zM900 1000h-200v200h200v-200zM1000 700h-300v200h300v-200zM1100 400h-400v200h400v-200zM1200 100h-500v200h500v-200z" />
<glyph unicode="&#xe156;" d="M400 300h150q21 0 25 -11t-10 -25l-230 -250q-14 -15 -35 -15t-35 15l-230 250q-14 14 -10 25t25 11h150v900h200v-900zM1200 1000h-500v200h500v-200zM1100 700h-400v200h400v-200zM1000 400h-300v200h300v-200zM900 100h-200v200h200v-200z" />
<glyph unicode="&#xe157;" d="M350 1100h400q162 0 256 -93.5t94 -256.5v-400q0 -165 -93.5 -257.5t-256.5 -92.5h-400q-165 0 -257.5 92.5t-92.5 257.5v400q0 165 92.5 257.5t257.5 92.5zM800 900h-500q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5 v500q0 41 -29.5 70.5t-70.5 29.5z" />
<glyph unicode="&#xe158;" d="M350 1100h400q165 0 257.5 -92.5t92.5 -257.5v-400q0 -165 -92.5 -257.5t-257.5 -92.5h-400q-163 0 -256.5 92.5t-93.5 257.5v400q0 163 94 256.5t256 93.5zM800 900h-500q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5 v500q0 41 -29.5 70.5t-70.5 29.5zM440 770l253 -190q17 -12 17 -30t-17 -30l-253 -190q-16 -12 -28 -6.5t-12 26.5v400q0 21 12 26.5t28 -6.5z" />
<glyph unicode="&#xe159;" d="M350 1100h400q163 0 256.5 -94t93.5 -256v-400q0 -165 -92.5 -257.5t-257.5 -92.5h-400q-165 0 -257.5 92.5t-92.5 257.5v400q0 163 92.5 256.5t257.5 93.5zM800 900h-500q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5 v500q0 41 -29.5 70.5t-70.5 29.5zM350 700h400q21 0 26.5 -12t-6.5 -28l-190 -253q-12 -17 -30 -17t-30 17l-190 253q-12 16 -6.5 28t26.5 12z" />
<glyph unicode="&#xe160;" d="M350 1100h400q165 0 257.5 -92.5t92.5 -257.5v-400q0 -163 -92.5 -256.5t-257.5 -93.5h-400q-163 0 -256.5 94t-93.5 256v400q0 165 92.5 257.5t257.5 92.5zM800 900h-500q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5 v500q0 41 -29.5 70.5t-70.5 29.5zM580 693l190 -253q12 -16 6.5 -28t-26.5 -12h-400q-21 0 -26.5 12t6.5 28l190 253q12 17 30 17t30 -17z" />
<glyph unicode="&#xe161;" d="M550 1100h400q165 0 257.5 -92.5t92.5 -257.5v-400q0 -165 -92.5 -257.5t-257.5 -92.5h-400q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5h450q41 0 70.5 29.5t29.5 70.5v500q0 41 -29.5 70.5t-70.5 29.5h-450q-21 0 -35.5 14.5t-14.5 35.5v100 q0 21 14.5 35.5t35.5 14.5zM338 867l324 -284q16 -14 16 -33t-16 -33l-324 -284q-16 -14 -27 -9t-11 26v150h-250q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5h250v150q0 21 11 26t27 -9z" />
<glyph unicode="&#xe162;" d="M793 1182l9 -9q8 -10 5 -27q-3 -11 -79 -225.5t-78 -221.5l300 1q24 0 32.5 -17.5t-5.5 -35.5q-1 0 -133.5 -155t-267 -312.5t-138.5 -162.5q-12 -15 -26 -15h-9l-9 8q-9 11 -4 32q2 9 42 123.5t79 224.5l39 110h-302q-23 0 -31 19q-10 21 6 41q75 86 209.5 237.5 t228 257t98.5 111.5q9 16 25 16h9z" />
<glyph unicode="&#xe163;" d="M350 1100h400q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-450q-41 0 -70.5 -29.5t-29.5 -70.5v-500q0 -41 29.5 -70.5t70.5 -29.5h450q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-165 0 -257.5 92.5t-92.5 257.5v400 q0 165 92.5 257.5t257.5 92.5zM938 867l324 -284q16 -14 16 -33t-16 -33l-324 -284q-16 -14 -27 -9t-11 26v150h-250q-21 0 -35.5 14.5t-14.5 35.5v200q0 21 14.5 35.5t35.5 14.5h250v150q0 21 11 26t27 -9z" />
<glyph unicode="&#xe164;" d="M750 1200h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -10.5 -25t-24.5 10l-109 109l-312 -312q-15 -15 -35.5 -15t-35.5 15l-141 141q-15 15 -15 35.5t15 35.5l312 312l-109 109q-14 14 -10 24.5t25 10.5zM456 900h-156q-41 0 -70.5 -29.5t-29.5 -70.5v-500 q0 -41 29.5 -70.5t70.5 -29.5h500q41 0 70.5 29.5t29.5 70.5v148l200 200v-298q0 -165 -93.5 -257.5t-256.5 -92.5h-400q-165 0 -257.5 92.5t-92.5 257.5v400q0 165 92.5 257.5t257.5 92.5h300z" />
<glyph unicode="&#xe165;" d="M600 1186q119 0 227.5 -46.5t187 -125t125 -187t46.5 -227.5t-46.5 -227.5t-125 -187t-187 -125t-227.5 -46.5t-227.5 46.5t-187 125t-125 187t-46.5 227.5t46.5 227.5t125 187t187 125t227.5 46.5zM600 1022q-115 0 -212 -56.5t-153.5 -153.5t-56.5 -212t56.5 -212 t153.5 -153.5t212 -56.5t212 56.5t153.5 153.5t56.5 212t-56.5 212t-153.5 153.5t-212 56.5zM600 794q80 0 137 -57t57 -137t-57 -137t-137 -57t-137 57t-57 137t57 137t137 57z" />
<glyph unicode="&#xe166;" d="M450 1200h200q21 0 35.5 -14.5t14.5 -35.5v-350h245q20 0 25 -11t-9 -26l-383 -426q-14 -15 -33.5 -15t-32.5 15l-379 426q-13 15 -8.5 26t25.5 11h250v350q0 21 14.5 35.5t35.5 14.5zM50 300h1000q21 0 35.5 -14.5t14.5 -35.5v-250h-1100v250q0 21 14.5 35.5t35.5 14.5z M900 200v-50h100v50h-100z" />
<glyph unicode="&#xe167;" d="M583 1182l378 -435q14 -15 9 -31t-26 -16h-244v-250q0 -20 -17 -35t-39 -15h-200q-20 0 -32 14.5t-12 35.5v250h-250q-20 0 -25.5 16.5t8.5 31.5l383 431q14 16 33.5 17t33.5 -14zM50 300h1000q21 0 35.5 -14.5t14.5 -35.5v-250h-1100v250q0 21 14.5 35.5t35.5 14.5z M900 200v-50h100v50h-100z" />
<glyph unicode="&#xe168;" d="M396 723l369 369q7 7 17.5 7t17.5 -7l139 -139q7 -8 7 -18.5t-7 -17.5l-525 -525q-7 -8 -17.5 -8t-17.5 8l-292 291q-7 8 -7 18t7 18l139 139q8 7 18.5 7t17.5 -7zM50 300h1000q21 0 35.5 -14.5t14.5 -35.5v-250h-1100v250q0 21 14.5 35.5t35.5 14.5zM900 200v-50h100v50 h-100z" />
<glyph unicode="&#xe169;" d="M135 1023l142 142q14 14 35 14t35 -14l77 -77l-212 -212l-77 76q-14 15 -14 36t14 35zM655 855l210 210q14 14 24.5 10t10.5 -25l-2 -599q-1 -20 -15.5 -35t-35.5 -15l-597 -1q-21 0 -25 10.5t10 24.5l208 208l-154 155l212 212zM50 300h1000q21 0 35.5 -14.5t14.5 -35.5 v-250h-1100v250q0 21 14.5 35.5t35.5 14.5zM900 200v-50h100v50h-100z" />
<glyph unicode="&#xe170;" d="M350 1200l599 -2q20 -1 35 -15.5t15 -35.5l1 -597q0 -21 -10.5 -25t-24.5 10l-208 208l-155 -154l-212 212l155 154l-210 210q-14 14 -10 24.5t25 10.5zM524 512l-76 -77q-15 -14 -36 -14t-35 14l-142 142q-14 14 -14 35t14 35l77 77zM50 300h1000q21 0 35.5 -14.5 t14.5 -35.5v-250h-1100v250q0 21 14.5 35.5t35.5 14.5zM900 200v-50h100v50h-100z" />
<glyph unicode="&#xe171;" d="M1200 103l-483 276l-314 -399v423h-399l1196 796v-1096zM483 424v-230l683 953z" />
<glyph unicode="&#xe172;" d="M1100 1000v-850q0 -21 -14.5 -35.5t-35.5 -14.5h-150v400h-700v-400h-150q-21 0 -35.5 14.5t-14.5 35.5v1000q0 20 14.5 35t35.5 15h250v-300h500v300h100zM700 1000h-100v200h100v-200z" />
<glyph unicode="&#xe173;" d="M1100 1000l-2 -149l-299 -299l-95 95q-9 9 -21.5 9t-21.5 -9l-149 -147h-312v-400h-150q-21 0 -35.5 14.5t-14.5 35.5v1000q0 20 14.5 35t35.5 15h250v-300h500v300h100zM700 1000h-100v200h100v-200zM1132 638l106 -106q7 -7 7 -17.5t-7 -17.5l-420 -421q-8 -7 -18 -7 t-18 7l-202 203q-8 7 -8 17.5t8 17.5l106 106q7 8 17.5 8t17.5 -8l79 -79l297 297q7 7 17.5 7t17.5 -7z" />
<glyph unicode="&#xe174;" d="M1100 1000v-269l-103 -103l-134 134q-15 15 -33.5 16.5t-34.5 -12.5l-266 -266h-329v-400h-150q-21 0 -35.5 14.5t-14.5 35.5v1000q0 20 14.5 35t35.5 15h250v-300h500v300h100zM700 1000h-100v200h100v-200zM1202 572l70 -70q15 -15 15 -35.5t-15 -35.5l-131 -131 l131 -131q15 -15 15 -35.5t-15 -35.5l-70 -70q-15 -15 -35.5 -15t-35.5 15l-131 131l-131 -131q-15 -15 -35.5 -15t-35.5 15l-70 70q-15 15 -15 35.5t15 35.5l131 131l-131 131q-15 15 -15 35.5t15 35.5l70 70q15 15 35.5 15t35.5 -15l131 -131l131 131q15 15 35.5 15 t35.5 -15z" />
<glyph unicode="&#xe175;" d="M1100 1000v-300h-350q-21 0 -35.5 -14.5t-14.5 -35.5v-150h-500v-400h-150q-21 0 -35.5 14.5t-14.5 35.5v1000q0 20 14.5 35t35.5 15h250v-300h500v300h100zM700 1000h-100v200h100v-200zM850 600h100q21 0 35.5 -14.5t14.5 -35.5v-250h150q21 0 25 -10.5t-10 -24.5 l-230 -230q-14 -14 -35 -14t-35 14l-230 230q-14 14 -10 24.5t25 10.5h150v250q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe176;" d="M1100 1000v-400l-165 165q-14 15 -35 15t-35 -15l-263 -265h-402v-400h-150q-21 0 -35.5 14.5t-14.5 35.5v1000q0 20 14.5 35t35.5 15h250v-300h500v300h100zM700 1000h-100v200h100v-200zM935 565l230 -229q14 -15 10 -25.5t-25 -10.5h-150v-250q0 -20 -14.5 -35 t-35.5 -15h-100q-21 0 -35.5 15t-14.5 35v250h-150q-21 0 -25 10.5t10 25.5l230 229q14 15 35 15t35 -15z" />
<glyph unicode="&#xe177;" d="M50 1100h1100q21 0 35.5 -14.5t14.5 -35.5v-150h-1200v150q0 21 14.5 35.5t35.5 14.5zM1200 800v-550q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5v550h1200zM100 500v-200h400v200h-400z" />
<glyph unicode="&#xe178;" d="M935 1165l248 -230q14 -14 14 -35t-14 -35l-248 -230q-14 -14 -24.5 -10t-10.5 25v150h-400v200h400v150q0 21 10.5 25t24.5 -10zM200 800h-50q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5h50v-200zM400 800h-100v200h100v-200zM18 435l247 230 q14 14 24.5 10t10.5 -25v-150h400v-200h-400v-150q0 -21 -10.5 -25t-24.5 10l-247 230q-15 14 -15 35t15 35zM900 300h-100v200h100v-200zM1000 500h51q20 0 34.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-34.5 -14.5h-51v200z" />
<glyph unicode="&#xe179;" d="M862 1073l276 116q25 18 43.5 8t18.5 -41v-1106q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v397q-4 1 -11 5t-24 17.5t-30 29t-24 42t-11 56.5v359q0 31 18.5 65t43.5 52zM550 1200q22 0 34.5 -12.5t14.5 -24.5l1 -13v-450q0 -28 -10.5 -59.5 t-25 -56t-29 -45t-25.5 -31.5l-10 -11v-447q0 -21 -14.5 -35.5t-35.5 -14.5h-200q-21 0 -35.5 14.5t-14.5 35.5v447q-4 4 -11 11.5t-24 30.5t-30 46t-24 55t-11 60v450q0 2 0.5 5.5t4 12t8.5 15t14.5 12t22.5 5.5q20 0 32.5 -12.5t14.5 -24.5l3 -13v-350h100v350v5.5t2.5 12 t7 15t15 12t25.5 5.5q23 0 35.5 -12.5t13.5 -24.5l1 -13v-350h100v350q0 2 0.5 5.5t3 12t7 15t15 12t24.5 5.5z" />
<glyph unicode="&#xe180;" d="M1200 1100v-56q-4 0 -11 -0.5t-24 -3t-30 -7.5t-24 -15t-11 -24v-888q0 -22 25 -34.5t50 -13.5l25 -2v-56h-400v56q75 0 87.5 6.5t12.5 43.5v394h-500v-394q0 -37 12.5 -43.5t87.5 -6.5v-56h-400v56q4 0 11 0.5t24 3t30 7.5t24 15t11 24v888q0 22 -25 34.5t-50 13.5 l-25 2v56h400v-56q-75 0 -87.5 -6.5t-12.5 -43.5v-394h500v394q0 37 -12.5 43.5t-87.5 6.5v56h400z" />
<glyph unicode="&#xe181;" d="M675 1000h375q21 0 35.5 -14.5t14.5 -35.5v-150h-105l-295 -98v98l-200 200h-400l100 100h375zM100 900h300q41 0 70.5 -29.5t29.5 -70.5v-500q0 -41 -29.5 -70.5t-70.5 -29.5h-300q-41 0 -70.5 29.5t-29.5 70.5v500q0 41 29.5 70.5t70.5 29.5zM100 800v-200h300v200 h-300zM1100 535l-400 -133v163l400 133v-163zM100 500v-200h300v200h-300zM1100 398v-248q0 -21 -14.5 -35.5t-35.5 -14.5h-375l-100 -100h-375l-100 100h400l200 200h105z" />
<glyph unicode="&#xe182;" d="M17 1007l162 162q17 17 40 14t37 -22l139 -194q14 -20 11 -44.5t-20 -41.5l-119 -118q102 -142 228 -268t267 -227l119 118q17 17 42.5 19t44.5 -12l192 -136q19 -14 22.5 -37.5t-13.5 -40.5l-163 -162q-3 -1 -9.5 -1t-29.5 2t-47.5 6t-62.5 14.5t-77.5 26.5t-90 42.5 t-101.5 60t-111 83t-119 108.5q-74 74 -133.5 150.5t-94.5 138.5t-60 119.5t-34.5 100t-15 74.5t-4.5 48z" />
<glyph unicode="&#xe183;" d="M600 1100q92 0 175 -10.5t141.5 -27t108.5 -36.5t81.5 -40t53.5 -37t31 -27l9 -10v-200q0 -21 -14.5 -33t-34.5 -9l-202 34q-20 3 -34.5 20t-14.5 38v146q-141 24 -300 24t-300 -24v-146q0 -21 -14.5 -38t-34.5 -20l-202 -34q-20 -3 -34.5 9t-14.5 33v200q3 4 9.5 10.5 t31 26t54 37.5t80.5 39.5t109 37.5t141 26.5t175 10.5zM600 795q56 0 97 -9.5t60 -23.5t30 -28t12 -24l1 -10v-50l365 -303q14 -15 24.5 -40t10.5 -45v-212q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5v212q0 20 10.5 45t24.5 40l365 303v50 q0 4 1 10.5t12 23t30 29t60 22.5t97 10z" />
<glyph unicode="&#xe184;" d="M1100 700l-200 -200h-600l-200 200v500h200v-200h200v200h200v-200h200v200h200v-500zM250 400h700q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-12l137 -100h-950l137 100h-12q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5zM50 100h1100q21 0 35.5 -14.5 t14.5 -35.5v-50h-1200v50q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe185;" d="M700 1100h-100q-41 0 -70.5 -29.5t-29.5 -70.5v-1000h300v1000q0 41 -29.5 70.5t-70.5 29.5zM1100 800h-100q-41 0 -70.5 -29.5t-29.5 -70.5v-700h300v700q0 41 -29.5 70.5t-70.5 29.5zM400 0h-300v400q0 41 29.5 70.5t70.5 29.5h100q41 0 70.5 -29.5t29.5 -70.5v-400z " />
<glyph unicode="&#xe186;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM500 700h-200v-100h200v-300h-300v100h200v100h-200v300h300v-100zM900 700v-300l-100 -100h-200v500h200z M700 700v-300h100v300h-100z" />
<glyph unicode="&#xe187;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM500 300h-100v200h-100v-200h-100v500h100v-200h100v200h100v-500zM900 700v-300l-100 -100h-200v500h200z M700 700v-300h100v300h-100z" />
<glyph unicode="&#xe188;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM500 700h-200v-300h200v-100h-300v500h300v-100zM900 700h-200v-300h200v-100h-300v500h300v-100z" />
<glyph unicode="&#xe189;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM500 400l-300 150l300 150v-300zM900 550l-300 -150v300z" />
<glyph unicode="&#xe190;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM900 300h-700v500h700v-500zM800 700h-130q-38 0 -66.5 -43t-28.5 -108t27 -107t68 -42h130v300zM300 700v-300 h130q41 0 68 42t27 107t-28.5 108t-66.5 43h-130z" />
<glyph unicode="&#xe191;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM500 700h-200v-100h200v-300h-300v100h200v100h-200v300h300v-100zM900 300h-100v400h-100v100h200v-500z M700 300h-100v100h100v-100z" />
<glyph unicode="&#xe192;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM300 700h200v-400h-300v500h100v-100zM900 300h-100v400h-100v100h200v-500zM300 600v-200h100v200h-100z M700 300h-100v100h100v-100z" />
<glyph unicode="&#xe193;" d="M200 1100h700q124 0 212 -88t88 -212v-500q0 -124 -88 -212t-212 -88h-700q-124 0 -212 88t-88 212v500q0 124 88 212t212 88zM100 900v-700h900v700h-900zM500 500l-199 -200h-100v50l199 200v150h-200v100h300v-300zM900 300h-100v400h-100v100h200v-500zM701 300h-100 v100h100v-100z" />
<glyph unicode="&#xe194;" d="M600 1191q120 0 229.5 -47t188.5 -126t126 -188.5t47 -229.5t-47 -229.5t-126 -188.5t-188.5 -126t-229.5 -47t-229.5 47t-188.5 126t-126 188.5t-47 229.5t47 229.5t126 188.5t188.5 126t229.5 47zM600 1021q-114 0 -211 -56.5t-153.5 -153.5t-56.5 -211t56.5 -211 t153.5 -153.5t211 -56.5t211 56.5t153.5 153.5t56.5 211t-56.5 211t-153.5 153.5t-211 56.5zM800 700h-300v-200h300v-100h-300l-100 100v200l100 100h300v-100z" />
<glyph unicode="&#xe195;" d="M600 1191q120 0 229.5 -47t188.5 -126t126 -188.5t47 -229.5t-47 -229.5t-126 -188.5t-188.5 -126t-229.5 -47t-229.5 47t-188.5 126t-126 188.5t-47 229.5t47 229.5t126 188.5t188.5 126t229.5 47zM600 1021q-114 0 -211 -56.5t-153.5 -153.5t-56.5 -211t56.5 -211 t153.5 -153.5t211 -56.5t211 56.5t153.5 153.5t56.5 211t-56.5 211t-153.5 153.5t-211 56.5zM800 700v-100l-50 -50l100 -100v-50h-100l-100 100h-150v-100h-100v400h300zM500 700v-100h200v100h-200z" />
<glyph unicode="&#xe197;" d="M503 1089q110 0 200.5 -59.5t134.5 -156.5q44 14 90 14q120 0 205 -86.5t85 -207t-85 -207t-205 -86.5h-128v250q0 21 -14.5 35.5t-35.5 14.5h-300q-21 0 -35.5 -14.5t-14.5 -35.5v-250h-222q-80 0 -136 57.5t-56 136.5q0 69 43 122.5t108 67.5q-2 19 -2 37q0 100 49 185 t134 134t185 49zM525 500h150q10 0 17.5 -7.5t7.5 -17.5v-275h137q21 0 26 -11.5t-8 -27.5l-223 -244q-13 -16 -32 -16t-32 16l-223 244q-13 16 -8 27.5t26 11.5h137v275q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe198;" d="M502 1089q110 0 201 -59.5t135 -156.5q43 15 89 15q121 0 206 -86.5t86 -206.5q0 -99 -60 -181t-150 -110l-378 360q-13 16 -31.5 16t-31.5 -16l-381 -365h-9q-79 0 -135.5 57.5t-56.5 136.5q0 69 43 122.5t108 67.5q-2 19 -2 38q0 100 49 184.5t133.5 134t184.5 49.5z M632 467l223 -228q13 -16 8 -27.5t-26 -11.5h-137v-275q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v275h-137q-21 0 -26 11.5t8 27.5q199 204 223 228q19 19 31.5 19t32.5 -19z" />
<glyph unicode="&#xe199;" d="M700 100v100h400l-270 300h170l-270 300h170l-300 333l-300 -333h170l-270 -300h170l-270 -300h400v-100h-50q-21 0 -35.5 -14.5t-14.5 -35.5v-50h400v50q0 21 -14.5 35.5t-35.5 14.5h-50z" />
<glyph unicode="&#xe200;" d="M600 1179q94 0 167.5 -56.5t99.5 -145.5q89 -6 150.5 -71.5t61.5 -155.5q0 -61 -29.5 -112.5t-79.5 -82.5q9 -29 9 -55q0 -74 -52.5 -126.5t-126.5 -52.5q-55 0 -100 30v-251q21 0 35.5 -14.5t14.5 -35.5v-50h-300v50q0 21 14.5 35.5t35.5 14.5v251q-45 -30 -100 -30 q-74 0 -126.5 52.5t-52.5 126.5q0 18 4 38q-47 21 -75.5 65t-28.5 97q0 74 52.5 126.5t126.5 52.5q5 0 23 -2q0 2 -1 10t-1 13q0 116 81.5 197.5t197.5 81.5z" />
<glyph unicode="&#xe201;" d="M1010 1010q111 -111 150.5 -260.5t0 -299t-150.5 -260.5q-83 -83 -191.5 -126.5t-218.5 -43.5t-218.5 43.5t-191.5 126.5q-111 111 -150.5 260.5t0 299t150.5 260.5q83 83 191.5 126.5t218.5 43.5t218.5 -43.5t191.5 -126.5zM476 1065q-4 0 -8 -1q-121 -34 -209.5 -122.5 t-122.5 -209.5q-4 -12 2.5 -23t18.5 -14l36 -9q3 -1 7 -1q23 0 29 22q27 96 98 166q70 71 166 98q11 3 17.5 13.5t3.5 22.5l-9 35q-3 13 -14 19q-7 4 -15 4zM512 920q-4 0 -9 -2q-80 -24 -138.5 -82.5t-82.5 -138.5q-4 -13 2 -24t19 -14l34 -9q4 -1 8 -1q22 0 28 21 q18 58 58.5 98.5t97.5 58.5q12 3 18 13.5t3 21.5l-9 35q-3 12 -14 19q-7 4 -15 4zM719.5 719.5q-49.5 49.5 -119.5 49.5t-119.5 -49.5t-49.5 -119.5t49.5 -119.5t119.5 -49.5t119.5 49.5t49.5 119.5t-49.5 119.5zM855 551q-22 0 -28 -21q-18 -58 -58.5 -98.5t-98.5 -57.5 q-11 -4 -17 -14.5t-3 -21.5l9 -35q3 -12 14 -19q7 -4 15 -4q4 0 9 2q80 24 138.5 82.5t82.5 138.5q4 13 -2.5 24t-18.5 14l-34 9q-4 1 -8 1zM1000 515q-23 0 -29 -22q-27 -96 -98 -166q-70 -71 -166 -98q-11 -3 -17.5 -13.5t-3.5 -22.5l9 -35q3 -13 14 -19q7 -4 15 -4 q4 0 8 1q121 34 209.5 122.5t122.5 209.5q4 12 -2.5 23t-18.5 14l-36 9q-3 1 -7 1z" />
<glyph unicode="&#xe202;" d="M700 800h300v-380h-180v200h-340v-200h-380v755q0 10 7.5 17.5t17.5 7.5h575v-400zM1000 900h-200v200zM700 300h162l-212 -212l-212 212h162v200h100v-200zM520 0h-395q-10 0 -17.5 7.5t-7.5 17.5v395zM1000 220v-195q0 -10 -7.5 -17.5t-17.5 -7.5h-195z" />
<glyph unicode="&#xe203;" d="M700 800h300v-520l-350 350l-550 -550v1095q0 10 7.5 17.5t17.5 7.5h575v-400zM1000 900h-200v200zM862 200h-162v-200h-100v200h-162l212 212zM480 0h-355q-10 0 -17.5 7.5t-7.5 17.5v55h380v-80zM1000 80v-55q0 -10 -7.5 -17.5t-17.5 -7.5h-155v80h180z" />
<glyph unicode="&#xe204;" d="M1162 800h-162v-200h100l100 -100h-300v300h-162l212 212zM200 800h200q27 0 40 -2t29.5 -10.5t23.5 -30t7 -57.5h300v-100h-600l-200 -350v450h100q0 36 7 57.5t23.5 30t29.5 10.5t40 2zM800 400h240l-240 -400h-800l300 500h500v-100z" />
<glyph unicode="&#xe205;" d="M650 1100h100q21 0 35.5 -14.5t14.5 -35.5v-50h50q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-300q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5h50v50q0 21 14.5 35.5t35.5 14.5zM1000 850v150q41 0 70.5 -29.5t29.5 -70.5v-800 q0 -41 -29.5 -70.5t-70.5 -29.5h-600q-1 0 -20 4l246 246l-326 326v324q0 41 29.5 70.5t70.5 29.5v-150q0 -62 44 -106t106 -44h300q62 0 106 44t44 106zM412 250l-212 -212v162h-200v100h200v162z" />
<glyph unicode="&#xe206;" d="M450 1100h100q21 0 35.5 -14.5t14.5 -35.5v-50h50q21 0 35.5 -14.5t14.5 -35.5v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-300q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5h50v50q0 21 14.5 35.5t35.5 14.5zM800 850v150q41 0 70.5 -29.5t29.5 -70.5v-500 h-200v-300h200q0 -36 -7 -57.5t-23.5 -30t-29.5 -10.5t-40 -2h-600q-41 0 -70.5 29.5t-29.5 70.5v800q0 41 29.5 70.5t70.5 29.5v-150q0 -62 44 -106t106 -44h300q62 0 106 44t44 106zM1212 250l-212 -212v162h-200v100h200v162z" />
<glyph unicode="&#xe209;" d="M658 1197l637 -1104q23 -38 7 -65.5t-60 -27.5h-1276q-44 0 -60 27.5t7 65.5l637 1104q22 39 54 39t54 -39zM704 800h-208q-20 0 -32 -14.5t-8 -34.5l58 -302q4 -20 21.5 -34.5t37.5 -14.5h54q20 0 37.5 14.5t21.5 34.5l58 302q4 20 -8 34.5t-32 14.5zM500 300v-100h200 v100h-200z" />
<glyph unicode="&#xe210;" d="M425 1100h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5zM425 800h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5 t17.5 7.5zM825 800h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5zM25 500h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v150 q0 10 7.5 17.5t17.5 7.5zM425 500h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5zM825 500h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5 v150q0 10 7.5 17.5t17.5 7.5zM25 200h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5zM425 200h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5 t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5zM825 200h250q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-250q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe211;" d="M700 1200h100v-200h-100v-100h350q62 0 86.5 -39.5t-3.5 -94.5l-66 -132q-41 -83 -81 -134h-772q-40 51 -81 134l-66 132q-28 55 -3.5 94.5t86.5 39.5h350v100h-100v200h100v100h200v-100zM250 400h700q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-12l137 -100 h-950l138 100h-13q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5zM50 100h1100q21 0 35.5 -14.5t14.5 -35.5v-50h-1200v50q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe212;" d="M600 1300q40 0 68.5 -29.5t28.5 -70.5h-194q0 41 28.5 70.5t68.5 29.5zM443 1100h314q18 -37 18 -75q0 -8 -3 -25h328q41 0 44.5 -16.5t-30.5 -38.5l-175 -145h-678l-178 145q-34 22 -29 38.5t46 16.5h328q-3 17 -3 25q0 38 18 75zM250 700h700q21 0 35.5 -14.5 t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-150v-200l275 -200h-950l275 200v200h-150q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5zM50 100h1100q21 0 35.5 -14.5t14.5 -35.5v-50h-1200v50q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe213;" d="M600 1181q75 0 128 -53t53 -128t-53 -128t-128 -53t-128 53t-53 128t53 128t128 53zM602 798h46q34 0 55.5 -28.5t21.5 -86.5q0 -76 39 -183h-324q39 107 39 183q0 58 21.5 86.5t56.5 28.5h45zM250 400h700q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-13 l138 -100h-950l137 100h-12q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5zM50 100h1100q21 0 35.5 -14.5t14.5 -35.5v-50h-1200v50q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe214;" d="M600 1300q47 0 92.5 -53.5t71 -123t25.5 -123.5q0 -78 -55.5 -133.5t-133.5 -55.5t-133.5 55.5t-55.5 133.5q0 62 34 143l144 -143l111 111l-163 163q34 26 63 26zM602 798h46q34 0 55.5 -28.5t21.5 -86.5q0 -76 39 -183h-324q39 107 39 183q0 58 21.5 86.5t56.5 28.5h45 zM250 400h700q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-13l138 -100h-950l137 100h-12q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5zM50 100h1100q21 0 35.5 -14.5t14.5 -35.5v-50h-1200v50q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe215;" d="M600 1200l300 -161v-139h-300q0 -57 18.5 -108t50 -91.5t63 -72t70 -67.5t57.5 -61h-530q-60 83 -90.5 177.5t-30.5 178.5t33 164.5t87.5 139.5t126 96.5t145.5 41.5v-98zM250 400h700q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-13l138 -100h-950l137 100 h-12q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5zM50 100h1100q21 0 35.5 -14.5t14.5 -35.5v-50h-1200v50q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe216;" d="M600 1300q41 0 70.5 -29.5t29.5 -70.5v-78q46 -26 73 -72t27 -100v-50h-400v50q0 54 27 100t73 72v78q0 41 29.5 70.5t70.5 29.5zM400 800h400q54 0 100 -27t72 -73h-172v-100h200v-100h-200v-100h200v-100h-200v-100h200q0 -83 -58.5 -141.5t-141.5 -58.5h-400 q-83 0 -141.5 58.5t-58.5 141.5v400q0 83 58.5 141.5t141.5 58.5z" />
<glyph unicode="&#xe218;" d="M150 1100h900q21 0 35.5 -14.5t14.5 -35.5v-500q0 -21 -14.5 -35.5t-35.5 -14.5h-900q-21 0 -35.5 14.5t-14.5 35.5v500q0 21 14.5 35.5t35.5 14.5zM125 400h950q10 0 17.5 -7.5t7.5 -17.5v-50q0 -10 -7.5 -17.5t-17.5 -7.5h-283l224 -224q13 -13 13 -31.5t-13 -32 t-31.5 -13.5t-31.5 13l-88 88h-524l-87 -88q-13 -13 -32 -13t-32 13.5t-13 32t13 31.5l224 224h-289q-10 0 -17.5 7.5t-7.5 17.5v50q0 10 7.5 17.5t17.5 7.5zM541 300l-100 -100h324l-100 100h-124z" />
<glyph unicode="&#xe219;" d="M200 1100h800q83 0 141.5 -58.5t58.5 -141.5v-200h-100q0 41 -29.5 70.5t-70.5 29.5h-250q-41 0 -70.5 -29.5t-29.5 -70.5h-100q0 41 -29.5 70.5t-70.5 29.5h-250q-41 0 -70.5 -29.5t-29.5 -70.5h-100v200q0 83 58.5 141.5t141.5 58.5zM100 600h1000q41 0 70.5 -29.5 t29.5 -70.5v-300h-1200v300q0 41 29.5 70.5t70.5 29.5zM300 100v-50q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v50h200zM1100 100v-50q0 -21 -14.5 -35.5t-35.5 -14.5h-100q-21 0 -35.5 14.5t-14.5 35.5v50h200z" />
<glyph unicode="&#xe221;" d="M480 1165l682 -683q31 -31 31 -75.5t-31 -75.5l-131 -131h-481l-517 518q-32 31 -32 75.5t32 75.5l295 296q31 31 75.5 31t76.5 -31zM108 794l342 -342l303 304l-341 341zM250 100h800q21 0 35.5 -14.5t14.5 -35.5v-50h-900v50q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe223;" d="M1057 647l-189 506q-8 19 -27.5 33t-40.5 14h-400q-21 0 -40.5 -14t-27.5 -33l-189 -506q-8 -19 1.5 -33t30.5 -14h625v-150q0 -21 14.5 -35.5t35.5 -14.5t35.5 14.5t14.5 35.5v150h125q21 0 30.5 14t1.5 33zM897 0h-595v50q0 21 14.5 35.5t35.5 14.5h50v50 q0 21 14.5 35.5t35.5 14.5h48v300h200v-300h47q21 0 35.5 -14.5t14.5 -35.5v-50h50q21 0 35.5 -14.5t14.5 -35.5v-50z" />
<glyph unicode="&#xe224;" d="M900 800h300v-575q0 -10 -7.5 -17.5t-17.5 -7.5h-375v591l-300 300v84q0 10 7.5 17.5t17.5 7.5h375v-400zM1200 900h-200v200zM400 600h300v-575q0 -10 -7.5 -17.5t-17.5 -7.5h-650q-10 0 -17.5 7.5t-7.5 17.5v950q0 10 7.5 17.5t17.5 7.5h375v-400zM700 700h-200v200z " />
<glyph unicode="&#xe225;" d="M484 1095h195q75 0 146 -32.5t124 -86t89.5 -122.5t48.5 -142q18 -14 35 -20q31 -10 64.5 6.5t43.5 48.5q10 34 -15 71q-19 27 -9 43q5 8 12.5 11t19 -1t23.5 -16q41 -44 39 -105q-3 -63 -46 -106.5t-104 -43.5h-62q-7 -55 -35 -117t-56 -100l-39 -234q-3 -20 -20 -34.5 t-38 -14.5h-100q-21 0 -33 14.5t-9 34.5l12 70q-49 -14 -91 -14h-195q-24 0 -65 8l-11 -64q-3 -20 -20 -34.5t-38 -14.5h-100q-21 0 -33 14.5t-9 34.5l26 157q-84 74 -128 175l-159 53q-19 7 -33 26t-14 40v50q0 21 14.5 35.5t35.5 14.5h124q11 87 56 166l-111 95 q-16 14 -12.5 23.5t24.5 9.5h203q116 101 250 101zM675 1000h-250q-10 0 -17.5 -7.5t-7.5 -17.5v-50q0 -10 7.5 -17.5t17.5 -7.5h250q10 0 17.5 7.5t7.5 17.5v50q0 10 -7.5 17.5t-17.5 7.5z" />
<glyph unicode="&#xe226;" d="M641 900l423 247q19 8 42 2.5t37 -21.5l32 -38q14 -15 12.5 -36t-17.5 -34l-139 -120h-390zM50 1100h106q67 0 103 -17t66 -71l102 -212h823q21 0 35.5 -14.5t14.5 -35.5v-50q0 -21 -14 -40t-33 -26l-737 -132q-23 -4 -40 6t-26 25q-42 67 -100 67h-300q-62 0 -106 44 t-44 106v200q0 62 44 106t106 44zM173 928h-80q-19 0 -28 -14t-9 -35v-56q0 -51 42 -51h134q16 0 21.5 8t5.5 24q0 11 -16 45t-27 51q-18 28 -43 28zM550 727q-32 0 -54.5 -22.5t-22.5 -54.5t22.5 -54.5t54.5 -22.5t54.5 22.5t22.5 54.5t-22.5 54.5t-54.5 22.5zM130 389 l152 130q18 19 34 24t31 -3.5t24.5 -17.5t25.5 -28q28 -35 50.5 -51t48.5 -13l63 5l48 -179q13 -61 -3.5 -97.5t-67.5 -79.5l-80 -69q-47 -40 -109 -35.5t-103 51.5l-130 151q-40 47 -35.5 109.5t51.5 102.5zM380 377l-102 -88q-31 -27 2 -65l37 -43q13 -15 27.5 -19.5 t31.5 6.5l61 53q19 16 14 49q-2 20 -12 56t-17 45q-11 12 -19 14t-23 -8z" />
<glyph unicode="&#xe227;" d="M625 1200h150q10 0 17.5 -7.5t7.5 -17.5v-109q79 -33 131 -87.5t53 -128.5q1 -46 -15 -84.5t-39 -61t-46 -38t-39 -21.5l-17 -6q6 0 15 -1.5t35 -9t50 -17.5t53 -30t50 -45t35.5 -64t14.5 -84q0 -59 -11.5 -105.5t-28.5 -76.5t-44 -51t-49.5 -31.5t-54.5 -16t-49.5 -6.5 t-43.5 -1v-75q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v75h-100v-75q0 -10 -7.5 -17.5t-17.5 -7.5h-150q-10 0 -17.5 7.5t-7.5 17.5v75h-175q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5h75v600h-75q-10 0 -17.5 7.5t-7.5 17.5v150 q0 10 7.5 17.5t17.5 7.5h175v75q0 10 7.5 17.5t17.5 7.5h150q10 0 17.5 -7.5t7.5 -17.5v-75h100v75q0 10 7.5 17.5t17.5 7.5zM400 900v-200h263q28 0 48.5 10.5t30 25t15 29t5.5 25.5l1 10q0 4 -0.5 11t-6 24t-15 30t-30 24t-48.5 11h-263zM400 500v-200h363q28 0 48.5 10.5 t30 25t15 29t5.5 25.5l1 10q0 4 -0.5 11t-6 24t-15 30t-30 24t-48.5 11h-363z" />
<glyph unicode="&#xe230;" d="M212 1198h780q86 0 147 -61t61 -147v-416q0 -51 -18 -142.5t-36 -157.5l-18 -66q-29 -87 -93.5 -146.5t-146.5 -59.5h-572q-82 0 -147 59t-93 147q-8 28 -20 73t-32 143.5t-20 149.5v416q0 86 61 147t147 61zM600 1045q-70 0 -132.5 -11.5t-105.5 -30.5t-78.5 -41.5 t-57 -45t-36 -41t-20.5 -30.5l-6 -12l156 -243h560l156 243q-2 5 -6 12.5t-20 29.5t-36.5 42t-57 44.5t-79 42t-105 29.5t-132.5 12zM762 703h-157l195 261z" />
<glyph unicode="&#xe231;" d="M475 1300h150q103 0 189 -86t86 -189v-500q0 -41 -42 -83t-83 -42h-450q-41 0 -83 42t-42 83v500q0 103 86 189t189 86zM700 300v-225q0 -21 -27 -48t-48 -27h-150q-21 0 -48 27t-27 48v225h300z" />
<glyph unicode="&#xe232;" d="M475 1300h96q0 -150 89.5 -239.5t239.5 -89.5v-446q0 -41 -42 -83t-83 -42h-450q-41 0 -83 42t-42 83v500q0 103 86 189t189 86zM700 300v-225q0 -21 -27 -48t-48 -27h-150q-21 0 -48 27t-27 48v225h300z" />
<glyph unicode="&#xe233;" d="M1294 767l-638 -283l-378 170l-78 -60v-224l100 -150v-199l-150 148l-150 -149v200l100 150v250q0 4 -0.5 10.5t0 9.5t1 8t3 8t6.5 6l47 40l-147 65l642 283zM1000 380l-350 -166l-350 166v147l350 -165l350 165v-147z" />
<glyph unicode="&#xe234;" d="M250 800q62 0 106 -44t44 -106t-44 -106t-106 -44t-106 44t-44 106t44 106t106 44zM650 800q62 0 106 -44t44 -106t-44 -106t-106 -44t-106 44t-44 106t44 106t106 44zM1050 800q62 0 106 -44t44 -106t-44 -106t-106 -44t-106 44t-44 106t44 106t106 44z" />
<glyph unicode="&#xe235;" d="M550 1100q62 0 106 -44t44 -106t-44 -106t-106 -44t-106 44t-44 106t44 106t106 44zM550 700q62 0 106 -44t44 -106t-44 -106t-106 -44t-106 44t-44 106t44 106t106 44zM550 300q62 0 106 -44t44 -106t-44 -106t-106 -44t-106 44t-44 106t44 106t106 44z" />
<glyph unicode="&#xe236;" d="M125 1100h950q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-950q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5zM125 700h950q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-950q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5 t17.5 7.5zM125 300h950q10 0 17.5 -7.5t7.5 -17.5v-150q0 -10 -7.5 -17.5t-17.5 -7.5h-950q-10 0 -17.5 7.5t-7.5 17.5v150q0 10 7.5 17.5t17.5 7.5z" />
<glyph unicode="&#xe237;" d="M350 1200h500q162 0 256 -93.5t94 -256.5v-500q0 -165 -93.5 -257.5t-256.5 -92.5h-500q-165 0 -257.5 92.5t-92.5 257.5v500q0 165 92.5 257.5t257.5 92.5zM900 1000h-600q-41 0 -70.5 -29.5t-29.5 -70.5v-600q0 -41 29.5 -70.5t70.5 -29.5h600q41 0 70.5 29.5 t29.5 70.5v600q0 41 -29.5 70.5t-70.5 29.5zM350 900h500q21 0 35.5 -14.5t14.5 -35.5v-300q0 -21 -14.5 -35.5t-35.5 -14.5h-500q-21 0 -35.5 14.5t-14.5 35.5v300q0 21 14.5 35.5t35.5 14.5zM400 800v-200h400v200h-400z" />
<glyph unicode="&#xe238;" d="M150 1100h1000q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-50v-200h50q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-50v-200h50q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5t-35.5 -14.5h-50v-200h50q21 0 35.5 -14.5t14.5 -35.5t-14.5 -35.5 t-35.5 -14.5h-1000q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5h50v200h-50q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5h50v200h-50q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5h50v200h-50q-21 0 -35.5 14.5t-14.5 35.5t14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe239;" d="M650 1187q87 -67 118.5 -156t0 -178t-118.5 -155q-87 66 -118.5 155t0 178t118.5 156zM300 800q124 0 212 -88t88 -212q-124 0 -212 88t-88 212zM1000 800q0 -124 -88 -212t-212 -88q0 124 88 212t212 88zM300 500q124 0 212 -88t88 -212q-124 0 -212 88t-88 212z M1000 500q0 -124 -88 -212t-212 -88q0 124 88 212t212 88zM700 199v-144q0 -21 -14.5 -35.5t-35.5 -14.5t-35.5 14.5t-14.5 35.5v142q40 -4 43 -4q17 0 57 6z" />
<glyph unicode="&#xe240;" d="M745 878l69 19q25 6 45 -12l298 -295q11 -11 15 -26.5t-2 -30.5q-5 -14 -18 -23.5t-28 -9.5h-8q1 0 1 -13q0 -29 -2 -56t-8.5 -62t-20 -63t-33 -53t-51 -39t-72.5 -14h-146q-184 0 -184 288q0 24 10 47q-20 4 -62 4t-63 -4q11 -24 11 -47q0 -288 -184 -288h-142 q-48 0 -84.5 21t-56 51t-32 71.5t-16 75t-3.5 68.5q0 13 2 13h-7q-15 0 -27.5 9.5t-18.5 23.5q-6 15 -2 30.5t15 25.5l298 296q20 18 46 11l76 -19q20 -5 30.5 -22.5t5.5 -37.5t-22.5 -31t-37.5 -5l-51 12l-182 -193h891l-182 193l-44 -12q-20 -5 -37.5 6t-22.5 31t6 37.5 t31 22.5z" />
<glyph unicode="&#xe241;" d="M1200 900h-50q0 21 -4 37t-9.5 26.5t-18 17.5t-22 11t-28.5 5.5t-31 2t-37 0.5h-200v-850q0 -22 25 -34.5t50 -13.5l25 -2v-100h-400v100q4 0 11 0.5t24 3t30 7t24 15t11 24.5v850h-200q-25 0 -37 -0.5t-31 -2t-28.5 -5.5t-22 -11t-18 -17.5t-9.5 -26.5t-4 -37h-50v300 h1000v-300zM500 450h-25q0 15 -4 24.5t-9 14.5t-17 7.5t-20 3t-25 0.5h-100v-425q0 -11 12.5 -17.5t25.5 -7.5h12v-50h-200v50q50 0 50 25v425h-100q-17 0 -25 -0.5t-20 -3t-17 -7.5t-9 -14.5t-4 -24.5h-25v150h500v-150z" />
<glyph unicode="&#xe242;" d="M1000 300v50q-25 0 -55 32q-14 14 -25 31t-16 27l-4 11l-289 747h-69l-300 -754q-18 -35 -39 -56q-9 -9 -24.5 -18.5t-26.5 -14.5l-11 -5v-50h273v50q-49 0 -78.5 21.5t-11.5 67.5l69 176h293l61 -166q13 -34 -3.5 -66.5t-55.5 -32.5v-50h312zM412 691l134 342l121 -342 h-255zM1100 150v-100q0 -21 -14.5 -35.5t-35.5 -14.5h-1000q-21 0 -35.5 14.5t-14.5 35.5v100q0 21 14.5 35.5t35.5 14.5h1000q21 0 35.5 -14.5t14.5 -35.5z" />
<glyph unicode="&#xe243;" d="M50 1200h1100q21 0 35.5 -14.5t14.5 -35.5v-1100q0 -21 -14.5 -35.5t-35.5 -14.5h-1100q-21 0 -35.5 14.5t-14.5 35.5v1100q0 21 14.5 35.5t35.5 14.5zM611 1118h-70q-13 0 -18 -12l-299 -753q-17 -32 -35 -51q-18 -18 -56 -34q-12 -5 -12 -18v-50q0 -8 5.5 -14t14.5 -6 h273q8 0 14 6t6 14v50q0 8 -6 14t-14 6q-55 0 -71 23q-10 14 0 39l63 163h266l57 -153q11 -31 -6 -55q-12 -17 -36 -17q-8 0 -14 -6t-6 -14v-50q0 -8 6 -14t14 -6h313q8 0 14 6t6 14v50q0 7 -5.5 13t-13.5 7q-17 0 -42 25q-25 27 -40 63h-1l-288 748q-5 12 -19 12zM639 611 h-197l103 264z" />
<glyph unicode="&#xe244;" d="M1200 1100h-1200v100h1200v-100zM50 1000h400q21 0 35.5 -14.5t14.5 -35.5v-900q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v900q0 21 14.5 35.5t35.5 14.5zM650 1000h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400 q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5zM700 900v-300h300v300h-300z" />
<glyph unicode="&#xe245;" d="M50 1200h400q21 0 35.5 -14.5t14.5 -35.5v-900q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v900q0 21 14.5 35.5t35.5 14.5zM650 700h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v400 q0 21 14.5 35.5t35.5 14.5zM700 600v-300h300v300h-300zM1200 0h-1200v100h1200v-100z" />
<glyph unicode="&#xe246;" d="M50 1000h400q21 0 35.5 -14.5t14.5 -35.5v-350h100v150q0 21 14.5 35.5t35.5 14.5h400q21 0 35.5 -14.5t14.5 -35.5v-150h100v-100h-100v-150q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v150h-100v-350q0 -21 -14.5 -35.5t-35.5 -14.5h-400 q-21 0 -35.5 14.5t-14.5 35.5v800q0 21 14.5 35.5t35.5 14.5zM700 700v-300h300v300h-300z" />
<glyph unicode="&#xe247;" d="M100 0h-100v1200h100v-1200zM250 1100h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5zM300 1000v-300h300v300h-300zM250 500h900q21 0 35.5 -14.5t14.5 -35.5v-400 q0 -21 -14.5 -35.5t-35.5 -14.5h-900q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe248;" d="M600 1100h150q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-150v-100h450q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-900q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5h350v100h-150q-21 0 -35.5 14.5 t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5h150v100h100v-100zM400 1000v-300h300v300h-300z" />
<glyph unicode="&#xe249;" d="M1200 0h-100v1200h100v-1200zM550 1100h400q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-400q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5zM600 1000v-300h300v300h-300zM50 500h900q21 0 35.5 -14.5t14.5 -35.5v-400 q0 -21 -14.5 -35.5t-35.5 -14.5h-900q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5z" />
<glyph unicode="&#xe250;" d="M865 565l-494 -494q-23 -23 -41 -23q-14 0 -22 13.5t-8 38.5v1000q0 25 8 38.5t22 13.5q18 0 41 -23l494 -494q14 -14 14 -35t-14 -35z" />
<glyph unicode="&#xe251;" d="M335 635l494 494q29 29 50 20.5t21 -49.5v-1000q0 -41 -21 -49.5t-50 20.5l-494 494q-14 14 -14 35t14 35z" />
<glyph unicode="&#xe252;" d="M100 900h1000q41 0 49.5 -21t-20.5 -50l-494 -494q-14 -14 -35 -14t-35 14l-494 494q-29 29 -20.5 50t49.5 21z" />
<glyph unicode="&#xe253;" d="M635 865l494 -494q29 -29 20.5 -50t-49.5 -21h-1000q-41 0 -49.5 21t20.5 50l494 494q14 14 35 14t35 -14z" />
<glyph unicode="&#xe254;" d="M700 741v-182l-692 -323v221l413 193l-413 193v221zM1200 0h-800v200h800v-200z" />
<glyph unicode="&#xe255;" d="M1200 900h-200v-100h200v-100h-300v300h200v100h-200v100h300v-300zM0 700h50q0 21 4 37t9.5 26.5t18 17.5t22 11t28.5 5.5t31 2t37 0.5h100v-550q0 -22 -25 -34.5t-50 -13.5l-25 -2v-100h400v100q-4 0 -11 0.5t-24 3t-30 7t-24 15t-11 24.5v550h100q25 0 37 -0.5t31 -2 t28.5 -5.5t22 -11t18 -17.5t9.5 -26.5t4 -37h50v300h-800v-300z" />
<glyph unicode="&#xe256;" d="M800 700h-50q0 21 -4 37t-9.5 26.5t-18 17.5t-22 11t-28.5 5.5t-31 2t-37 0.5h-100v-550q0 -22 25 -34.5t50 -14.5l25 -1v-100h-400v100q4 0 11 0.5t24 3t30 7t24 15t11 24.5v550h-100q-25 0 -37 -0.5t-31 -2t-28.5 -5.5t-22 -11t-18 -17.5t-9.5 -26.5t-4 -37h-50v300 h800v-300zM1100 200h-200v-100h200v-100h-300v300h200v100h-200v100h300v-300z" />
<glyph unicode="&#xe257;" d="M701 1098h160q16 0 21 -11t-7 -23l-464 -464l464 -464q12 -12 7 -23t-21 -11h-160q-13 0 -23 9l-471 471q-7 8 -7 18t7 18l471 471q10 9 23 9z" />
<glyph unicode="&#xe258;" d="M339 1098h160q13 0 23 -9l471 -471q7 -8 7 -18t-7 -18l-471 -471q-10 -9 -23 -9h-160q-16 0 -21 11t7 23l464 464l-464 464q-12 12 -7 23t21 11z" />
<glyph unicode="&#xe259;" d="M1087 882q11 -5 11 -21v-160q0 -13 -9 -23l-471 -471q-8 -7 -18 -7t-18 7l-471 471q-9 10 -9 23v160q0 16 11 21t23 -7l464 -464l464 464q12 12 23 7z" />
<glyph unicode="&#xe260;" d="M618 993l471 -471q9 -10 9 -23v-160q0 -16 -11 -21t-23 7l-464 464l-464 -464q-12 -12 -23 -7t-11 21v160q0 13 9 23l471 471q8 7 18 7t18 -7z" />
<glyph unicode="&#xf8ff;" d="M1000 1200q0 -124 -88 -212t-212 -88q0 124 88 212t212 88zM450 1000h100q21 0 40 -14t26 -33l79 -194q5 1 16 3q34 6 54 9.5t60 7t65.5 1t61 -10t56.5 -23t42.5 -42t29 -64t5 -92t-19.5 -121.5q-1 -7 -3 -19.5t-11 -50t-20.5 -73t-32.5 -81.5t-46.5 -83t-64 -70 t-82.5 -50q-13 -5 -42 -5t-65.5 2.5t-47.5 2.5q-14 0 -49.5 -3.5t-63 -3.5t-43.5 7q-57 25 -104.5 78.5t-75 111.5t-46.5 112t-26 90l-7 35q-15 63 -18 115t4.5 88.5t26 64t39.5 43.5t52 25.5t58.5 13t62.5 2t59.5 -4.5t55.5 -8l-147 192q-12 18 -5.5 30t27.5 12z" />
<glyph unicode="&#x1f511;" d="M250 1200h600q21 0 35.5 -14.5t14.5 -35.5v-400q0 -21 -14.5 -35.5t-35.5 -14.5h-150v-500l-255 -178q-19 -9 -32 -1t-13 29v650h-150q-21 0 -35.5 14.5t-14.5 35.5v400q0 21 14.5 35.5t35.5 14.5zM400 1100v-100h300v100h-300z" />
<glyph unicode="&#x1f6aa;" d="M250 1200h750q39 0 69.5 -40.5t30.5 -84.5v-933l-700 -117v950l600 125h-700v-1000h-100v1025q0 23 15.5 49t34.5 26zM500 525v-100l100 20v100z" />
</font>
</defs></svg> ) format('svg')}.glyphicon{position:relative;top:1px;display:inline-block;font-family:'Glyphicons Halflings';font-style:normal;font-weight:400;line-height:1;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.glyphicon-asterisk:before{content:"\2a"}.glyphicon-plus:before{content:"\2b"}.glyphicon-eur:before,.glyphicon-euro:before{content:"\20ac"}.glyphicon-minus:before{content:"\2212"}.glyphicon-cloud:before{content:"\2601"}.glyphicon-envelope:before{content:"\2709"}.glyphicon-pencil:before{content:"\270f"}.glyphicon-glass:before{content:"\e001"}.glyphicon-music:before{content:"\e002"}.glyphicon-search:before{content:"\e003"}.glyphicon-heart:before{content:"\e005"}.glyphicon-star:before{content:"\e006"}.glyphicon-star-empty:before{content:"\e007"}.glyphicon-user:before{content:"\e008"}.glyphicon-film:before{content:"\e009"}.glyphicon-th-large:before{content:"\e010"}.glyphicon-th:before{content:"\e011"}.glyphicon-th-list:before{content:"\e012"}.glyphicon-ok:before{content:"\e013"}.glyphicon-remove:before{content:"\e014"}.glyphicon-zoom-in:before{content:"\e015"}.glyphicon-zoom-out:before{content:"\e016"}.glyphicon-off:before{content:"\e017"}.glyphicon-signal:before{content:"\e018"}.glyphicon-cog:before{content:"\e019"}.glyphicon-trash:before{content:"\e020"}.glyphicon-home:before{content:"\e021"}.glyphicon-file:before{content:"\e022"}.glyphicon-time:before{content:"\e023"}.glyphicon-road:before{content:"\e024"}.glyphicon-download-alt:before{content:"\e025"}.glyphicon-download:before{content:"\e026"}.glyphicon-upload:before{content:"\e027"}.glyphicon-inbox:before{content:"\e028"}.glyphicon-play-circle:before{content:"\e029"}.glyphicon-repeat:before{content:"\e030"}.glyphicon-refresh:before{content:"\e031"}.glyphicon-list-alt:before{content:"\e032"}.glyphicon-lock:before{content:"\e033"}.glyphicon-flag:before{content:"\e034"}.glyphicon-headphones:before{content:"\e035"}.glyphicon-volume-off:before{content:"\e036"}.glyphicon-volume-down:before{content:"\e037"}.glyphicon-volume-up:before{content:"\e038"}.glyphicon-qrcode:before{content:"\e039"}.glyphicon-barcode:before{content:"\e040"}.glyphicon-tag:before{content:"\e041"}.glyphicon-tags:before{content:"\e042"}.glyphicon-book:before{content:"\e043"}.glyphicon-bookmark:before{content:"\e044"}.glyphicon-print:before{content:"\e045"}.glyphicon-camera:before{content:"\e046"}.glyphicon-font:before{content:"\e047"}.glyphicon-bold:before{content:"\e048"}.glyphicon-italic:before{content:"\e049"}.glyphicon-text-height:before{content:"\e050"}.glyphicon-text-width:before{content:"\e051"}.glyphicon-align-left:before{content:"\e052"}.glyphicon-align-center:before{content:"\e053"}.glyphicon-align-right:before{content:"\e054"}.glyphicon-align-justify:before{content:"\e055"}.glyphicon-list:before{content:"\e056"}.glyphicon-indent-left:before{content:"\e057"}.glyphicon-indent-right:before{content:"\e058"}.glyphicon-facetime-video:before{content:"\e059"}.glyphicon-picture:before{content:"\e060"}.glyphicon-map-marker:before{content:"\e062"}.glyphicon-adjust:before{content:"\e063"}.glyphicon-tint:before{content:"\e064"}.glyphicon-edit:before{content:"\e065"}.glyphicon-share:before{content:"\e066"}.glyphicon-check:before{content:"\e067"}.glyphicon-move:before{content:"\e068"}.glyphicon-step-backward:before{content:"\e069"}.glyphicon-fast-backward:before{content:"\e070"}.glyphicon-backward:before{content:"\e071"}.glyphicon-play:before{content:"\e072"}.glyphicon-pause:before{content:"\e073"}.glyphicon-stop:before{content:"\e074"}.glyphicon-forward:before{content:"\e075"}.glyphicon-fast-forward:before{content:"\e076"}.glyphicon-step-forward:before{content:"\e077"}.glyphicon-eject:before{content:"\e078"}.glyphicon-chevron-left:before{content:"\e079"}.glyphicon-chevron-right:before{content:"\e080"}.glyphicon-plus-sign:before{content:"\e081"}.glyphicon-minus-sign:before{content:"\e082"}.glyphicon-remove-sign:before{content:"\e083"}.glyphicon-ok-sign:before{content:"\e084"}.glyphicon-question-sign:before{content:"\e085"}.glyphicon-info-sign:before{content:"\e086"}.glyphicon-screenshot:before{content:"\e087"}.glyphicon-remove-circle:before{content:"\e088"}.glyphicon-ok-circle:before{content:"\e089"}.glyphicon-ban-circle:before{content:"\e090"}.glyphicon-arrow-left:before{content:"\e091"}.glyphicon-arrow-right:before{content:"\e092"}.glyphicon-arrow-up:before{content:"\e093"}.glyphicon-arrow-down:before{content:"\e094"}.glyphicon-share-alt:before{content:"\e095"}.glyphicon-resize-full:before{content:"\e096"}.glyphicon-resize-small:before{content:"\e097"}.glyphicon-exclamation-sign:before{content:"\e101"}.glyphicon-gift:before{content:"\e102"}.glyphicon-leaf:before{content:"\e103"}.glyphicon-fire:before{content:"\e104"}.glyphicon-eye-open:before{content:"\e105"}.glyphicon-eye-close:before{content:"\e106"}.glyphicon-warning-sign:before{content:"\e107"}.glyphicon-plane:before{content:"\e108"}.glyphicon-calendar:before{content:"\e109"}.glyphicon-random:before{content:"\e110"}.glyphicon-comment:before{content:"\e111"}.glyphicon-magnet:before{content:"\e112"}.glyphicon-chevron-up:before{content:"\e113"}.glyphicon-chevron-down:before{content:"\e114"}.glyphicon-retweet:before{content:"\e115"}.glyphicon-shopping-cart:before{content:"\e116"}.glyphicon-folder-close:before{content:"\e117"}.glyphicon-folder-open:before{content:"\e118"}.glyphicon-resize-vertical:before{content:"\e119"}.glyphicon-resize-horizontal:before{content:"\e120"}.glyphicon-hdd:before{content:"\e121"}.glyphicon-bullhorn:before{content:"\e122"}.glyphicon-bell:before{content:"\e123"}.glyphicon-certificate:before{content:"\e124"}.glyphicon-thumbs-up:before{content:"\e125"}.glyphicon-thumbs-down:before{content:"\e126"}.glyphicon-hand-right:before{content:"\e127"}.glyphicon-hand-left:before{content:"\e128"}.glyphicon-hand-up:before{content:"\e129"}.glyphicon-hand-down:before{content:"\e130"}.glyphicon-circle-arrow-right:before{content:"\e131"}.glyphicon-circle-arrow-left:before{content:"\e132"}.glyphicon-circle-arrow-up:before{content:"\e133"}.glyphicon-circle-arrow-down:before{content:"\e134"}.glyphicon-globe:before{content:"\e135"}.glyphicon-wrench:before{content:"\e136"}.glyphicon-tasks:before{content:"\e137"}.glyphicon-filter:before{content:"\e138"}.glyphicon-briefcase:before{content:"\e139"}.glyphicon-fullscreen:before{content:"\e140"}.glyphicon-dashboard:before{content:"\e141"}.glyphicon-paperclip:before{content:"\e142"}.glyphicon-heart-empty:before{content:"\e143"}.glyphicon-link:before{content:"\e144"}.glyphicon-phone:before{content:"\e145"}.glyphicon-pushpin:before{content:"\e146"}.glyphicon-usd:before{content:"\e148"}.glyphicon-gbp:before{content:"\e149"}.glyphicon-sort:before{content:"\e150"}.glyphicon-sort-by-alphabet:before{content:"\e151"}.glyphicon-sort-by-alphabet-alt:before{content:"\e152"}.glyphicon-sort-by-order:before{content:"\e153"}.glyphicon-sort-by-order-alt:before{content:"\e154"}.glyphicon-sort-by-attributes:before{content:"\e155"}.glyphicon-sort-by-attributes-alt:before{content:"\e156"}.glyphicon-unchecked:before{content:"\e157"}.glyphicon-expand:before{content:"\e158"}.glyphicon-collapse-down:before{content:"\e159"}.glyphicon-collapse-up:before{content:"\e160"}.glyphicon-log-in:before{content:"\e161"}.glyphicon-flash:before{content:"\e162"}.glyphicon-log-out:before{content:"\e163"}.glyphicon-new-window:before{content:"\e164"}.glyphicon-record:before{content:"\e165"}.glyphicon-save:before{content:"\e166"}.glyphicon-open:before{content:"\e167"}.glyphicon-saved:before{content:"\e168"}.glyphicon-import:before{content:"\e169"}.glyphicon-export:before{content:"\e170"}.glyphicon-send:before{content:"\e171"}.glyphicon-floppy-disk:before{content:"\e172"}.glyphicon-floppy-saved:before{content:"\e173"}.glyphicon-floppy-remove:before{content:"\e174"}.glyphicon-floppy-save:before{content:"\e175"}.glyphicon-floppy-open:before{content:"\e176"}.glyphicon-credit-card:before{content:"\e177"}.glyphicon-transfer:before{content:"\e178"}.glyphicon-cutlery:before{content:"\e179"}.glyphicon-header:before{content:"\e180"}.glyphicon-compressed:before{content:"\e181"}.glyphicon-earphone:before{content:"\e182"}.glyphicon-phone-alt:before{content:"\e183"}.glyphicon-tower:before{content:"\e184"}.glyphicon-stats:before{content:"\e185"}.glyphicon-sd-video:before{content:"\e186"}.glyphicon-hd-video:before{content:"\e187"}.glyphicon-subtitles:before{content:"\e188"}.glyphicon-sound-stereo:before{content:"\e189"}.glyphicon-sound-dolby:before{content:"\e190"}.glyphicon-sound-5-1:before{content:"\e191"}.glyphicon-sound-6-1:before{content:"\e192"}.glyphicon-sound-7-1:before{content:"\e193"}.glyphicon-copyright-mark:before{content:"\e194"}.glyphicon-registration-mark:before{content:"\e195"}.glyphicon-cloud-download:before{content:"\e197"}.glyphicon-cloud-upload:before{content:"\e198"}.glyphicon-tree-conifer:before{content:"\e199"}.glyphicon-tree-deciduous:before{content:"\e200"}.glyphicon-cd:before{content:"\e201"}.glyphicon-save-file:before{content:"\e202"}.glyphicon-open-file:before{content:"\e203"}.glyphicon-level-up:before{content:"\e204"}.glyphicon-copy:before{content:"\e205"}.glyphicon-paste:before{content:"\e206"}.glyphicon-alert:before{content:"\e209"}.glyphicon-equalizer:before{content:"\e210"}.glyphicon-king:before{content:"\e211"}.glyphicon-queen:before{content:"\e212"}.glyphicon-pawn:before{content:"\e213"}.glyphicon-bishop:before{content:"\e214"}.glyphicon-knight:before{content:"\e215"}.glyphicon-baby-formula:before{content:"\e216"}.glyphicon-tent:before{content:"\26fa"}.glyphicon-blackboard:before{content:"\e218"}.glyphicon-bed:before{content:"\e219"}.glyphicon-apple:before{content:"\f8ff"}.glyphicon-erase:before{content:"\e221"}.glyphicon-hourglass:before{content:"\231b"}.glyphicon-lamp:before{content:"\e223"}.glyphicon-duplicate:before{content:"\e224"}.glyphicon-piggy-bank:before{content:"\e225"}.glyphicon-scissors:before{content:"\e226"}.glyphicon-bitcoin:before{content:"\e227"}.glyphicon-btc:before{content:"\e227"}.glyphicon-xbt:before{content:"\e227"}.glyphicon-yen:before{content:"\00a5"}.glyphicon-jpy:before{content:"\00a5"}.glyphicon-ruble:before{content:"\20bd"}.glyphicon-rub:before{content:"\20bd"}.glyphicon-scale:before{content:"\e230"}.glyphicon-ice-lolly:before{content:"\e231"}.glyphicon-ice-lolly-tasted:before{content:"\e232"}.glyphicon-education:before{content:"\e233"}.glyphicon-option-horizontal:before{content:"\e234"}.glyphicon-option-vertical:before{content:"\e235"}.glyphicon-menu-hamburger:before{content:"\e236"}.glyphicon-modal-window:before{content:"\e237"}.glyphicon-oil:before{content:"\e238"}.glyphicon-grain:before{content:"\e239"}.glyphicon-sunglasses:before{content:"\e240"}.glyphicon-text-size:before{content:"\e241"}.glyphicon-text-color:before{content:"\e242"}.glyphicon-text-background:before{content:"\e243"}.glyphicon-object-align-top:before{content:"\e244"}.glyphicon-object-align-bottom:before{content:"\e245"}.glyphicon-object-align-horizontal:before{content:"\e246"}.glyphicon-object-align-left:before{content:"\e247"}.glyphicon-object-align-vertical:before{content:"\e248"}.glyphicon-object-align-right:before{content:"\e249"}.glyphicon-triangle-right:before{content:"\e250"}.glyphicon-triangle-left:before{content:"\e251"}.glyphicon-triangle-bottom:before{content:"\e252"}.glyphicon-triangle-top:before{content:"\e253"}.glyphicon-console:before{content:"\e254"}.glyphicon-superscript:before{content:"\e255"}.glyphicon-subscript:before{content:"\e256"}.glyphicon-menu-left:before{content:"\e257"}.glyphicon-menu-right:before{content:"\e258"}.glyphicon-menu-down:before{content:"\e259"}.glyphicon-menu-up:before{content:"\e260"}*{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}:after,:before{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}html{font-size:10px;-webkit-tap-highlight-color:rgba(0,0,0,0)}body{font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;font-size:14px;line-height:1.42857143;color:#333;background-color:#fff}button,input,select,textarea{font-family:inherit;font-size:inherit;line-height:inherit}a{color:#337ab7;text-decoration:none}a:focus,a:hover{color:#23527c;text-decoration:underline}a:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}figure{margin:0}img{vertical-align:middle}.carousel-inner>.item>a>img,.carousel-inner>.item>img,.img-responsive,.thumbnail a>img,.thumbnail>img{display:block;max-width:100%;height:auto}.img-rounded{border-radius:6px}.img-thumbnail{display:inline-block;max-width:100%;height:auto;padding:4px;line-height:1.42857143;background-color:#fff;border:1px solid #ddd;border-radius:4px;-webkit-transition:all .2s ease-in-out;-o-transition:all .2s ease-in-out;transition:all .2s ease-in-out}.img-circle{border-radius:50%}hr{margin-top:20px;margin-bottom:20px;border:0;border-top:1px solid #eee}.sr-only{position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;clip:rect(0,0,0,0);border:0}.sr-only-focusable:active,.sr-only-focusable:focus{position:static;width:auto;height:auto;margin:0;overflow:visible;clip:auto}[role=button]{cursor:pointer}.h1,.h2,.h3,.h4,.h5,.h6,h1,h2,h3,h4,h5,h6{font-family:inherit;font-weight:500;line-height:1.1;color:inherit}.h1 .small,.h1 small,.h2 .small,.h2 small,.h3 .small,.h3 small,.h4 .small,.h4 small,.h5 .small,.h5 small,.h6 .small,.h6 small,h1 .small,h1 small,h2 .small,h2 small,h3 .small,h3 small,h4 .small,h4 small,h5 .small,h5 small,h6 .small,h6 small{font-weight:400;line-height:1;color:#777}.h1,.h2,.h3,h1,h2,h3{margin-top:20px;margin-bottom:10px}.h1 .small,.h1 small,.h2 .small,.h2 small,.h3 .small,.h3 small,h1 .small,h1 small,h2 .small,h2 small,h3 .small,h3 small{font-size:65%}.h4,.h5,.h6,h4,h5,h6{margin-top:10px;margin-bottom:10px}.h4 .small,.h4 small,.h5 .small,.h5 small,.h6 .small,.h6 small,h4 .small,h4 small,h5 .small,h5 small,h6 .small,h6 small{font-size:75%}.h1,h1{font-size:36px}.h2,h2{font-size:30px}.h3,h3{font-size:24px}.h4,h4{font-size:18px}.h5,h5{font-size:14px}.h6,h6{font-size:12px}p{margin:0 0 10px}.lead{margin-bottom:20px;font-size:16px;font-weight:300;line-height:1.4}@media (min-width:768px){.lead{font-size:21px}}.small,small{font-size:85%}.mark,mark{padding:.2em;background-color:#fcf8e3}.text-left{text-align:left}.text-right{text-align:right}.text-center{text-align:center}.text-justify{text-align:justify}.text-nowrap{white-space:nowrap}.text-lowercase{text-transform:lowercase}.text-uppercase{text-transform:uppercase}.text-capitalize{text-transform:capitalize}.text-muted{color:#777}.text-primary{color:#337ab7}a.text-primary:focus,a.text-primary:hover{color:#286090}.text-success{color:#3c763d}a.text-success:focus,a.text-success:hover{color:#2b542c}.text-info{color:#31708f}a.text-info:focus,a.text-info:hover{color:#245269}.text-warning{color:#8a6d3b}a.text-warning:focus,a.text-warning:hover{color:#66512c}.text-danger{color:#a94442}a.text-danger:focus,a.text-danger:hover{color:#843534}.bg-primary{color:#fff;background-color:#337ab7}a.bg-primary:focus,a.bg-primary:hover{background-color:#286090}.bg-success{background-color:#dff0d8}a.bg-success:focus,a.bg-success:hover{background-color:#c1e2b3}.bg-info{background-color:#d9edf7}a.bg-info:focus,a.bg-info:hover{background-color:#afd9ee}.bg-warning{background-color:#fcf8e3}a.bg-warning:focus,a.bg-warning:hover{background-color:#f7ecb5}.bg-danger{background-color:#f2dede}a.bg-danger:focus,a.bg-danger:hover{background-color:#e4b9b9}.page-header{padding-bottom:9px;margin:40px 0 20px;border-bottom:1px solid #eee}ol,ul{margin-top:0;margin-bottom:10px}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}.list-unstyled{padding-left:0;list-style:none}.list-inline{padding-left:0;margin-left:-5px;list-style:none}.list-inline>li{display:inline-block;padding-right:5px;padding-left:5px}dl{margin-top:0;margin-bottom:20px}dd,dt{line-height:1.42857143}dt{font-weight:700}dd{margin-left:0}@media (min-width:768px){.dl-horizontal dt{float:left;width:160px;overflow:hidden;clear:left;text-align:right;text-overflow:ellipsis;white-space:nowrap}.dl-horizontal dd{margin-left:180px}}abbr[data-original-title],abbr[title]{cursor:help;border-bottom:1px dotted #777}.initialism{font-size:90%;text-transform:uppercase}blockquote{padding:10px 20px;margin:0 0 20px;font-size:17.5px;border-left:5px solid #eee}blockquote ol:last-child,blockquote p:last-child,blockquote ul:last-child{margin-bottom:0}blockquote .small,blockquote footer,blockquote small{display:block;font-size:80%;line-height:1.42857143;color:#777}blockquote .small:before,blockquote footer:before,blockquote small:before{content:'\2014 \00A0'}.blockquote-reverse,blockquote.pull-right{padding-right:15px;padding-left:0;text-align:right;border-right:5px solid #eee;border-left:0}.blockquote-reverse .small:before,.blockquote-reverse footer:before,.blockquote-reverse small:before,blockquote.pull-right .small:before,blockquote.pull-right footer:before,blockquote.pull-right small:before{content:''}.blockquote-reverse .small:after,.blockquote-reverse footer:after,.blockquote-reverse small:after,blockquote.pull-right .small:after,blockquote.pull-right footer:after,blockquote.pull-right small:after{content:'\00A0 \2014'}address{margin-bottom:20px;font-style:normal;line-height:1.42857143}code,kbd,pre,samp{font-family:monospace}code{padding:2px 4px;font-size:90%;color:#c7254e;background-color:#f9f2f4;border-radius:4px}kbd{padding:2px 4px;font-size:90%;color:#fff;background-color:#333;border-radius:3px;-webkit-box-shadow:inset 0 -1px 0 rgba(0,0,0,.25);box-shadow:inset 0 -1px 0 rgba(0,0,0,.25)}kbd kbd{padding:0;font-size:100%;font-weight:700;-webkit-box-shadow:none;box-shadow:none}pre{display:block;padding:9.5px;margin:0 0 10px;font-size:13px;line-height:1.42857143;color:#333;word-break:break-all;word-wrap:break-word;background-color:#f5f5f5;border:1px solid #ccc;border-radius:4px}pre code{padding:0;font-size:inherit;color:inherit;white-space:pre-wrap;background-color:transparent;border-radius:0}.pre-scrollable{max-height:340px;overflow-y:scroll}.container{padding-right:15px;padding-left:15px;margin-right:auto;margin-left:auto}@media (min-width:768px){.container{width:750px}}@media (min-width:992px){.container{width:970px}}@media (min-width:1200px){.container{width:1170px}}.container-fluid{padding-right:15px;padding-left:15px;margin-right:auto;margin-left:auto}.row{margin-right:-15px;margin-left:-15px}.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9{position:relative;min-height:1px;padding-right:15px;padding-left:15px}.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9{float:left}.col-xs-12{width:100%}.col-xs-11{width:91.66666667%}.col-xs-10{width:83.33333333%}.col-xs-9{width:75%}.col-xs-8{width:66.66666667%}.col-xs-7{width:58.33333333%}.col-xs-6{width:50%}.col-xs-5{width:41.66666667%}.col-xs-4{width:33.33333333%}.col-xs-3{width:25%}.col-xs-2{width:16.66666667%}.col-xs-1{width:8.33333333%}.col-xs-pull-12{right:100%}.col-xs-pull-11{right:91.66666667%}.col-xs-pull-10{right:83.33333333%}.col-xs-pull-9{right:75%}.col-xs-pull-8{right:66.66666667%}.col-xs-pull-7{right:58.33333333%}.col-xs-pull-6{right:50%}.col-xs-pull-5{right:41.66666667%}.col-xs-pull-4{right:33.33333333%}.col-xs-pull-3{right:25%}.col-xs-pull-2{right:16.66666667%}.col-xs-pull-1{right:8.33333333%}.col-xs-pull-0{right:auto}.col-xs-push-12{left:100%}.col-xs-push-11{left:91.66666667%}.col-xs-push-10{left:83.33333333%}.col-xs-push-9{left:75%}.col-xs-push-8{left:66.66666667%}.col-xs-push-7{left:58.33333333%}.col-xs-push-6{left:50%}.col-xs-push-5{left:41.66666667%}.col-xs-push-4{left:33.33333333%}.col-xs-push-3{left:25%}.col-xs-push-2{left:16.66666667%}.col-xs-push-1{left:8.33333333%}.col-xs-push-0{left:auto}.col-xs-offset-12{margin-left:100%}.col-xs-offset-11{margin-left:91.66666667%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-0{margin-left:0}@media (min-width:768px){.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9{float:left}.col-sm-12{width:100%}.col-sm-11{width:91.66666667%}.col-sm-10{width:83.33333333%}.col-sm-9{width:75%}.col-sm-8{width:66.66666667%}.col-sm-7{width:58.33333333%}.col-sm-6{width:50%}.col-sm-5{width:41.66666667%}.col-sm-4{width:33.33333333%}.col-sm-3{width:25%}.col-sm-2{width:16.66666667%}.col-sm-1{width:8.33333333%}.col-sm-pull-12{right:100%}.col-sm-pull-11{right:91.66666667%}.col-sm-pull-10{right:83.33333333%}.col-sm-pull-9{right:75%}.col-sm-pull-8{right:66.66666667%}.col-sm-pull-7{right:58.33333333%}.col-sm-pull-6{right:50%}.col-sm-pull-5{right:41.66666667%}.col-sm-pull-4{right:33.33333333%}.col-sm-pull-3{right:25%}.col-sm-pull-2{right:16.66666667%}.col-sm-pull-1{right:8.33333333%}.col-sm-pull-0{right:auto}.col-sm-push-12{left:100%}.col-sm-push-11{left:91.66666667%}.col-sm-push-10{left:83.33333333%}.col-sm-push-9{left:75%}.col-sm-push-8{left:66.66666667%}.col-sm-push-7{left:58.33333333%}.col-sm-push-6{left:50%}.col-sm-push-5{left:41.66666667%}.col-sm-push-4{left:33.33333333%}.col-sm-push-3{left:25%}.col-sm-push-2{left:16.66666667%}.col-sm-push-1{left:8.33333333%}.col-sm-push-0{left:auto}.col-sm-offset-12{margin-left:100%}.col-sm-offset-11{margin-left:91.66666667%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-0{margin-left:0}}@media (min-width:992px){.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9{float:left}.col-md-12{width:100%}.col-md-11{width:91.66666667%}.col-md-10{width:83.33333333%}.col-md-9{width:75%}.col-md-8{width:66.66666667%}.col-md-7{width:58.33333333%}.col-md-6{width:50%}.col-md-5{width:41.66666667%}.col-md-4{width:33.33333333%}.col-md-3{width:25%}.col-md-2{width:16.66666667%}.col-md-1{width:8.33333333%}.col-md-pull-12{right:100%}.col-md-pull-11{right:91.66666667%}.col-md-pull-10{right:83.33333333%}.col-md-pull-9{right:75%}.col-md-pull-8{right:66.66666667%}.col-md-pull-7{right:58.33333333%}.col-md-pull-6{right:50%}.col-md-pull-5{right:41.66666667%}.col-md-pull-4{right:33.33333333%}.col-md-pull-3{right:25%}.col-md-pull-2{right:16.66666667%}.col-md-pull-1{right:8.33333333%}.col-md-pull-0{right:auto}.col-md-push-12{left:100%}.col-md-push-11{left:91.66666667%}.col-md-push-10{left:83.33333333%}.col-md-push-9{left:75%}.col-md-push-8{left:66.66666667%}.col-md-push-7{left:58.33333333%}.col-md-push-6{left:50%}.col-md-push-5{left:41.66666667%}.col-md-push-4{left:33.33333333%}.col-md-push-3{left:25%}.col-md-push-2{left:16.66666667%}.col-md-push-1{left:8.33333333%}.col-md-push-0{left:auto}.col-md-offset-12{margin-left:100%}.col-md-offset-11{margin-left:91.66666667%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-9{margin-left:75%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-6{margin-left:50%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-3{margin-left:25%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-0{margin-left:0}}@media (min-width:1200px){.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9{float:left}.col-lg-12{width:100%}.col-lg-11{width:91.66666667%}.col-lg-10{width:83.33333333%}.col-lg-9{width:75%}.col-lg-8{width:66.66666667%}.col-lg-7{width:58.33333333%}.col-lg-6{width:50%}.col-lg-5{width:41.66666667%}.col-lg-4{width:33.33333333%}.col-lg-3{width:25%}.col-lg-2{width:16.66666667%}.col-lg-1{width:8.33333333%}.col-lg-pull-12{right:100%}.col-lg-pull-11{right:91.66666667%}.col-lg-pull-10{right:83.33333333%}.col-lg-pull-9{right:75%}.col-lg-pull-8{right:66.66666667%}.col-lg-pull-7{right:58.33333333%}.col-lg-pull-6{right:50%}.col-lg-pull-5{right:41.66666667%}.col-lg-pull-4{right:33.33333333%}.col-lg-pull-3{right:25%}.col-lg-pull-2{right:16.66666667%}.col-lg-pull-1{right:8.33333333%}.col-lg-pull-0{right:auto}.col-lg-push-12{left:100%}.col-lg-push-11{left:91.66666667%}.col-lg-push-10{left:83.33333333%}.col-lg-push-9{left:75%}.col-lg-push-8{left:66.66666667%}.col-lg-push-7{left:58.33333333%}.col-lg-push-6{left:50%}.col-lg-push-5{left:41.66666667%}.col-lg-push-4{left:33.33333333%}.col-lg-push-3{left:25%}.col-lg-push-2{left:16.66666667%}.col-lg-push-1{left:8.33333333%}.col-lg-push-0{left:auto}.col-lg-offset-12{margin-left:100%}.col-lg-offset-11{margin-left:91.66666667%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-0{margin-left:0}}table{background-color:transparent}caption{padding-top:8px;padding-bottom:8px;color:#777;text-align:left}th{}.table{width:100%;max-width:100%;margin-bottom:20px}.table>tbody>tr>td,.table>tbody>tr>th,.table>tfoot>tr>td,.table>tfoot>tr>th,.table>thead>tr>td,.table>thead>tr>th{padding:8px;line-height:1.42857143;vertical-align:top;border-top:1px solid #ddd}.table>thead>tr>th{vertical-align:bottom;border-bottom:2px solid #ddd}.table>caption+thead>tr:first-child>td,.table>caption+thead>tr:first-child>th,.table>colgroup+thead>tr:first-child>td,.table>colgroup+thead>tr:first-child>th,.table>thead:first-child>tr:first-child>td,.table>thead:first-child>tr:first-child>th{border-top:0}.table>tbody+tbody{border-top:2px solid #ddd}.table .table{background-color:#fff}.table-condensed>tbody>tr>td,.table-condensed>tbody>tr>th,.table-condensed>tfoot>tr>td,.table-condensed>tfoot>tr>th,.table-condensed>thead>tr>td,.table-condensed>thead>tr>th{padding:5px}.table-bordered{border:1px solid #ddd}.table-bordered>tbody>tr>td,.table-bordered>tbody>tr>th,.table-bordered>tfoot>tr>td,.table-bordered>tfoot>tr>th,.table-bordered>thead>tr>td,.table-bordered>thead>tr>th{border:1px solid #ddd}.table-bordered>thead>tr>td,.table-bordered>thead>tr>th{border-bottom-width:2px}.table-striped>tbody>tr:nth-of-type(odd){background-color:#f9f9f9}.table-hover>tbody>tr:hover{background-color:#f5f5f5}table col[class*=col-]{position:static;display:table-column;float:none}table td[class*=col-],table th[class*=col-]{position:static;display:table-cell;float:none}.table>tbody>tr.active>td,.table>tbody>tr.active>th,.table>tbody>tr>td.active,.table>tbody>tr>th.active,.table>tfoot>tr.active>td,.table>tfoot>tr.active>th,.table>tfoot>tr>td.active,.table>tfoot>tr>th.active,.table>thead>tr.active>td,.table>thead>tr.active>th,.table>thead>tr>td.active,.table>thead>tr>th.active{background-color:#f5f5f5}.table-hover>tbody>tr.active:hover>td,.table-hover>tbody>tr.active:hover>th,.table-hover>tbody>tr:hover>.active,.table-hover>tbody>tr>td.active:hover,.table-hover>tbody>tr>th.active:hover{background-color:#e8e8e8}.table>tbody>tr.success>td,.table>tbody>tr.success>th,.table>tbody>tr>td.success,.table>tbody>tr>th.success,.table>tfoot>tr.success>td,.table>tfoot>tr.success>th,.table>tfoot>tr>td.success,.table>tfoot>tr>th.success,.table>thead>tr.success>td,.table>thead>tr.success>th,.table>thead>tr>td.success,.table>thead>tr>th.success{background-color:#dff0d8}.table-hover>tbody>tr.success:hover>td,.table-hover>tbody>tr.success:hover>th,.table-hover>tbody>tr:hover>.success,.table-hover>tbody>tr>td.success:hover,.table-hover>tbody>tr>th.success:hover{background-color:#d0e9c6}.table>tbody>tr.info>td,.table>tbody>tr.info>th,.table>tbody>tr>td.info,.table>tbody>tr>th.info,.table>tfoot>tr.info>td,.table>tfoot>tr.info>th,.table>tfoot>tr>td.info,.table>tfoot>tr>th.info,.table>thead>tr.info>td,.table>thead>tr.info>th,.table>thead>tr>td.info,.table>thead>tr>th.info{background-color:#d9edf7}.table-hover>tbody>tr.info:hover>td,.table-hover>tbody>tr.info:hover>th,.table-hover>tbody>tr:hover>.info,.table-hover>tbody>tr>td.info:hover,.table-hover>tbody>tr>th.info:hover{background-color:#c4e3f3}.table>tbody>tr.warning>td,.table>tbody>tr.warning>th,.table>tbody>tr>td.warning,.table>tbody>tr>th.warning,.table>tfoot>tr.warning>td,.table>tfoot>tr.warning>th,.table>tfoot>tr>td.warning,.table>tfoot>tr>th.warning,.table>thead>tr.warning>td,.table>thead>tr.warning>th,.table>thead>tr>td.warning,.table>thead>tr>th.warning{background-color:#fcf8e3}.table-hover>tbody>tr.warning:hover>td,.table-hover>tbody>tr.warning:hover>th,.table-hover>tbody>tr:hover>.warning,.table-hover>tbody>tr>td.warning:hover,.table-hover>tbody>tr>th.warning:hover{background-color:#faf2cc}.table>tbody>tr.danger>td,.table>tbody>tr.danger>th,.table>tbody>tr>td.danger,.table>tbody>tr>th.danger,.table>tfoot>tr.danger>td,.table>tfoot>tr.danger>th,.table>tfoot>tr>td.danger,.table>tfoot>tr>th.danger,.table>thead>tr.danger>td,.table>thead>tr.danger>th,.table>thead>tr>td.danger,.table>thead>tr>th.danger{background-color:#f2dede}.table-hover>tbody>tr.danger:hover>td,.table-hover>tbody>tr.danger:hover>th,.table-hover>tbody>tr:hover>.danger,.table-hover>tbody>tr>td.danger:hover,.table-hover>tbody>tr>th.danger:hover{background-color:#ebcccc}.table-responsive{min-height:.01%;overflow-x:auto}@media screen and (max-width:767px){.table-responsive{width:100%;margin-bottom:15px;overflow-y:hidden;-ms-overflow-style:-ms-autohiding-scrollbar;border:1px solid #ddd}.table-responsive>.table{margin-bottom:0}.table-responsive>.table>tbody>tr>td,.table-responsive>.table>tbody>tr>th,.table-responsive>.table>tfoot>tr>td,.table-responsive>.table>tfoot>tr>th,.table-responsive>.table>thead>tr>td,.table-responsive>.table>thead>tr>th{white-space:nowrap}.table-responsive>.table-bordered{border:0}.table-responsive>.table-bordered>tbody>tr>td:first-child,.table-responsive>.table-bordered>tbody>tr>th:first-child,.table-responsive>.table-bordered>tfoot>tr>td:first-child,.table-responsive>.table-bordered>tfoot>tr>th:first-child,.table-responsive>.table-bordered>thead>tr>td:first-child,.table-responsive>.table-bordered>thead>tr>th:first-child{border-left:0}.table-responsive>.table-bordered>tbody>tr>td:last-child,.table-responsive>.table-bordered>tbody>tr>th:last-child,.table-responsive>.table-bordered>tfoot>tr>td:last-child,.table-responsive>.table-bordered>tfoot>tr>th:last-child,.table-responsive>.table-bordered>thead>tr>td:last-child,.table-responsive>.table-bordered>thead>tr>th:last-child{border-right:0}.table-responsive>.table-bordered>tbody>tr:last-child>td,.table-responsive>.table-bordered>tbody>tr:last-child>th,.table-responsive>.table-bordered>tfoot>tr:last-child>td,.table-responsive>.table-bordered>tfoot>tr:last-child>th{border-bottom:0}}fieldset{min-width:0;padding:0;margin:0;border:0}legend{display:block;width:100%;padding:0;margin-bottom:20px;font-size:21px;line-height:inherit;color:#333;border:0;border-bottom:1px solid #e5e5e5}label{display:inline-block;max-width:100%;margin-bottom:5px;font-weight:700}input[type=search]{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}input[type=checkbox],input[type=radio]{margin:4px 0 0;margin-top:1px\9;line-height:normal}input[type=file]{display:block}input[type=range]{display:block;width:100%}select[multiple],select[size]{height:auto}input[type=file]:focus,input[type=checkbox]:focus,input[type=radio]:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}output{display:block;padding-top:7px;font-size:14px;line-height:1.42857143;color:#555}.form-control{display:block;width:100%;height:34px;padding:6px 12px;font-size:14px;line-height:1.42857143;color:#555;background-color:#fff;background-image:none;border:1px solid #ccc;border-radius:4px;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075);box-shadow:inset 0 1px 1px rgba(0,0,0,.075);-webkit-transition:border-color ease-in-out .15s,-webkit-box-shadow ease-in-out .15s;-o-transition:border-color ease-in-out .15s,box-shadow ease-in-out .15s;transition:border-color ease-in-out .15s,box-shadow ease-in-out .15s}.form-control:focus{border-color:#66afe9;outline:0;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 8px rgba(102,175,233,.6);box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 8px rgba(102,175,233,.6)}.form-control::-moz-placeholder{color:#999;opacity:1}.form-control:-ms-input-placeholder{color:#999}.form-control::-webkit-input-placeholder{color:#999}.form-control[disabled],.form-control[readonly],fieldset[disabled] .form-control{background-color:#eee;opacity:1}.form-control[disabled],fieldset[disabled] .form-control{cursor:not-allowed}textarea.form-control{height:auto}input[type=search]{-webkit-appearance:none}@media screen and (-webkit-min-device-pixel-ratio:0){input[type=date].form-control,input[type=time].form-control,input[type=datetime-local].form-control,input[type=month].form-control{line-height:34px}.input-group-sm input[type=date],.input-group-sm input[type=time],.input-group-sm input[type=datetime-local],.input-group-sm input[type=month],input[type=date].input-sm,input[type=time].input-sm,input[type=datetime-local].input-sm,input[type=month].input-sm{line-height:30px}.input-group-lg input[type=date],.input-group-lg input[type=time],.input-group-lg input[type=datetime-local],.input-group-lg input[type=month],input[type=date].input-lg,input[type=time].input-lg,input[type=datetime-local].input-lg,input[type=month].input-lg{line-height:46px}}.form-group{margin-bottom:15px}.checkbox,.radio{position:relative;display:block;margin-top:10px;margin-bottom:10px}.checkbox label,.radio label{min-height:20px;padding-left:20px;margin-bottom:0;font-weight:400;cursor:pointer}.checkbox input[type=checkbox],.checkbox-inline input[type=checkbox],.radio input[type=radio],.radio-inline input[type=radio]{position:absolute;margin-top:4px\9;margin-left:-20px}.checkbox+.checkbox,.radio+.radio{margin-top:-5px}.checkbox-inline,.radio-inline{position:relative;display:inline-block;padding-left:20px;margin-bottom:0;font-weight:400;vertical-align:middle;cursor:pointer}.checkbox-inline+.checkbox-inline,.radio-inline+.radio-inline{margin-top:0;margin-left:10px}fieldset[disabled] input[type=checkbox],fieldset[disabled] input[type=radio],input[type=checkbox].disabled,input[type=checkbox][disabled],input[type=radio].disabled,input[type=radio][disabled]{cursor:not-allowed}.checkbox-inline.disabled,.radio-inline.disabled,fieldset[disabled] .checkbox-inline,fieldset[disabled] .radio-inline{cursor:not-allowed}.checkbox.disabled label,.radio.disabled label,fieldset[disabled] .checkbox label,fieldset[disabled] .radio label{cursor:not-allowed}.form-control-static{min-height:34px;padding-top:7px;padding-bottom:7px;margin-bottom:0}.form-control-static.input-lg,.form-control-static.input-sm{padding-right:0;padding-left:0}.input-sm{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}select.input-sm{height:30px;line-height:30px}select[multiple].input-sm,textarea.input-sm{height:auto}.form-group-sm .form-control{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}.form-group-sm select.form-control{height:30px;line-height:30px}.form-group-sm select[multiple].form-control,.form-group-sm textarea.form-control{height:auto}.form-group-sm .form-control-static{height:30px;min-height:32px;padding:6px 10px;font-size:12px;line-height:1.5}.input-lg{height:46px;padding:10px 16px;font-size:18px;line-height:1.3333333;border-radius:6px}select.input-lg{height:46px;line-height:46px}select[multiple].input-lg,textarea.input-lg{height:auto}.form-group-lg .form-control{height:46px;padding:10px 16px;font-size:18px;line-height:1.3333333;border-radius:6px}.form-group-lg select.form-control{height:46px;line-height:46px}.form-group-lg select[multiple].form-control,.form-group-lg textarea.form-control{height:auto}.form-group-lg .form-control-static{height:46px;min-height:38px;padding:11px 16px;font-size:18px;line-height:1.3333333}.has-feedback{position:relative}.has-feedback .form-control{padding-right:42.5px}.form-control-feedback{position:absolute;top:0;right:0;z-index:2;display:block;width:34px;height:34px;line-height:34px;text-align:center;pointer-events:none}.form-group-lg .form-control+.form-control-feedback,.input-group-lg+.form-control-feedback,.input-lg+.form-control-feedback{width:46px;height:46px;line-height:46px}.form-group-sm .form-control+.form-control-feedback,.input-group-sm+.form-control-feedback,.input-sm+.form-control-feedback{width:30px;height:30px;line-height:30px}.has-success .checkbox,.has-success .checkbox-inline,.has-success .control-label,.has-success .help-block,.has-success .radio,.has-success .radio-inline,.has-success.checkbox label,.has-success.checkbox-inline label,.has-success.radio label,.has-success.radio-inline label{color:#3c763d}.has-success .form-control{border-color:#3c763d;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075);box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-success .form-control:focus{border-color:#2b542c;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #67b168;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #67b168}.has-success .input-group-addon{color:#3c763d;background-color:#dff0d8;border-color:#3c763d}.has-success .form-control-feedback{color:#3c763d}.has-warning .checkbox,.has-warning .checkbox-inline,.has-warning .control-label,.has-warning .help-block,.has-warning .radio,.has-warning .radio-inline,.has-warning.checkbox label,.has-warning.checkbox-inline label,.has-warning.radio label,.has-warning.radio-inline label{color:#8a6d3b}.has-warning .form-control{border-color:#8a6d3b;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075);box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-warning .form-control:focus{border-color:#66512c;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #c0a16b;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #c0a16b}.has-warning .input-group-addon{color:#8a6d3b;background-color:#fcf8e3;border-color:#8a6d3b}.has-warning .form-control-feedback{color:#8a6d3b}.has-error .checkbox,.has-error .checkbox-inline,.has-error .control-label,.has-error .help-block,.has-error .radio,.has-error .radio-inline,.has-error.checkbox label,.has-error.checkbox-inline label,.has-error.radio label,.has-error.radio-inline label{color:#a94442}.has-error .form-control{border-color:#a94442;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075);box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-error .form-control:focus{border-color:#843534;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #ce8483;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #ce8483}.has-error .input-group-addon{color:#a94442;background-color:#f2dede;border-color:#a94442}.has-error .form-control-feedback{color:#a94442}.has-feedback label~.form-control-feedback{top:25px}.has-feedback label.sr-only~.form-control-feedback{top:0}.help-block{display:block;margin-top:5px;margin-bottom:10px;color:#737373}@media (min-width:768px){.form-inline .form-group{display:inline-block;margin-bottom:0;vertical-align:middle}.form-inline .form-control{display:inline-block;width:auto;vertical-align:middle}.form-inline .form-control-static{display:inline-block}.form-inline .input-group{display:inline-table;vertical-align:middle}.form-inline .input-group .form-control,.form-inline .input-group .input-group-addon,.form-inline .input-group .input-group-btn{width:auto}.form-inline .input-group>.form-control{width:100%}.form-inline .control-label{margin-bottom:0;vertical-align:middle}.form-inline .checkbox,.form-inline .radio{display:inline-block;margin-top:0;margin-bottom:0;vertical-align:middle}.form-inline .checkbox label,.form-inline .radio label{padding-left:0}.form-inline .checkbox input[type=checkbox],.form-inline .radio input[type=radio]{position:relative;margin-left:0}.form-inline .has-feedback .form-control-feedback{top:0}}.form-horizontal .checkbox,.form-horizontal .checkbox-inline,.form-horizontal .radio,.form-horizontal .radio-inline{padding-top:7px;margin-top:0;margin-bottom:0}.form-horizontal .checkbox,.form-horizontal .radio{min-height:27px}.form-horizontal .form-group{margin-right:-15px;margin-left:-15px}@media (min-width:768px){.form-horizontal .control-label{padding-top:7px;margin-bottom:0;text-align:right}}.form-horizontal .has-feedback .form-control-feedback{right:15px}@media (min-width:768px){.form-horizontal .form-group-lg .control-label{padding-top:14.33px;font-size:18px}}@media (min-width:768px){.form-horizontal .form-group-sm .control-label{padding-top:6px;font-size:12px}}.btn{display:inline-block;padding:6px 12px;margin-bottom:0;font-size:14px;font-weight:400;line-height:1.42857143;text-align:center;white-space:nowrap;vertical-align:middle;-ms-touch-action:manipulation;touch-action:manipulation;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;background-image:none;border:1px solid transparent;border-radius:4px}.btn.active.focus,.btn.active:focus,.btn.focus,.btn:active.focus,.btn:active:focus,.btn:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}.btn.focus,.btn:focus,.btn:hover{color:#333;text-decoration:none}.btn.active,.btn:active{background-image:none;outline:0;-webkit-box-shadow:inset 0 3px 5px rgba(0,0,0,.125);box-shadow:inset 0 3px 5px rgba(0,0,0,.125)}.btn.disabled,.btn[disabled],fieldset[disabled] .btn{cursor:not-allowed;filter:alpha(opacity=65);-webkit-box-shadow:none;box-shadow:none;opacity:.65}a.btn.disabled,fieldset[disabled] a.btn{pointer-events:none}.btn-default{color:#333;background-color:#fff;border-color:#ccc}.btn-default.focus,.btn-default:focus{color:#333;background-color:#e6e6e6;border-color:#8c8c8c}.btn-default:hover{color:#333;background-color:#e6e6e6;border-color:#adadad}.btn-default.active,.btn-default:active,.open>.dropdown-toggle.btn-default{color:#333;background-color:#e6e6e6;border-color:#adadad}.btn-default.active.focus,.btn-default.active:focus,.btn-default.active:hover,.btn-default:active.focus,.btn-default:active:focus,.btn-default:active:hover,.open>.dropdown-toggle.btn-default.focus,.open>.dropdown-toggle.btn-default:focus,.open>.dropdown-toggle.btn-default:hover{color:#333;background-color:#d4d4d4;border-color:#8c8c8c}.btn-default.active,.btn-default:active,.open>.dropdown-toggle.btn-default{background-image:none}.btn-default.disabled,.btn-default.disabled.active,.btn-default.disabled.focus,.btn-default.disabled:active,.btn-default.disabled:focus,.btn-default.disabled:hover,.btn-default[disabled],.btn-default[disabled].active,.btn-default[disabled].focus,.btn-default[disabled]:active,.btn-default[disabled]:focus,.btn-default[disabled]:hover,fieldset[disabled] .btn-default,fieldset[disabled] .btn-default.active,fieldset[disabled] .btn-default.focus,fieldset[disabled] .btn-default:active,fieldset[disabled] .btn-default:focus,fieldset[disabled] .btn-default:hover{background-color:#fff;border-color:#ccc}.btn-default .badge{color:#fff;background-color:#333}.btn-primary{color:#fff;background-color:#337ab7;border-color:#2e6da4}.btn-primary.focus,.btn-primary:focus{color:#fff;background-color:#286090;border-color:#122b40}.btn-primary:hover{color:#fff;background-color:#286090;border-color:#204d74}.btn-primary.active,.btn-primary:active,.open>.dropdown-toggle.btn-primary{color:#fff;background-color:#286090;border-color:#204d74}.btn-primary.active.focus,.btn-primary.active:focus,.btn-primary.active:hover,.btn-primary:active.focus,.btn-primary:active:focus,.btn-primary:active:hover,.open>.dropdown-toggle.btn-primary.focus,.open>.dropdown-toggle.btn-primary:focus,.open>.dropdown-toggle.btn-primary:hover{color:#fff;background-color:#204d74;border-color:#122b40}.btn-primary.active,.btn-primary:active,.open>.dropdown-toggle.btn-primary{background-image:none}.btn-primary.disabled,.btn-primary.disabled.active,.btn-primary.disabled.focus,.btn-primary.disabled:active,.btn-primary.disabled:focus,.btn-primary.disabled:hover,.btn-primary[disabled],.btn-primary[disabled].active,.btn-primary[disabled].focus,.btn-primary[disabled]:active,.btn-primary[disabled]:focus,.btn-primary[disabled]:hover,fieldset[disabled] .btn-primary,fieldset[disabled] .btn-primary.active,fieldset[disabled] .btn-primary.focus,fieldset[disabled] .btn-primary:active,fieldset[disabled] .btn-primary:focus,fieldset[disabled] .btn-primary:hover{background-color:#337ab7;border-color:#2e6da4}.btn-primary .badge{color:#337ab7;background-color:#fff}.btn-success{color:#fff;background-color:#5cb85c;border-color:#4cae4c}.btn-success.focus,.btn-success:focus{color:#fff;background-color:#449d44;border-color:#255625}.btn-success:hover{color:#fff;background-color:#449d44;border-color:#398439}.btn-success.active,.btn-success:active,.open>.dropdown-toggle.btn-success{color:#fff;background-color:#449d44;border-color:#398439}.btn-success.active.focus,.btn-success.active:focus,.btn-success.active:hover,.btn-success:active.focus,.btn-success:active:focus,.btn-success:active:hover,.open>.dropdown-toggle.btn-success.focus,.open>.dropdown-toggle.btn-success:focus,.open>.dropdown-toggle.btn-success:hover{color:#fff;background-color:#398439;border-color:#255625}.btn-success.active,.btn-success:active,.open>.dropdown-toggle.btn-success{background-image:none}.btn-success.disabled,.btn-success.disabled.active,.btn-success.disabled.focus,.btn-success.disabled:active,.btn-success.disabled:focus,.btn-success.disabled:hover,.btn-success[disabled],.btn-success[disabled].active,.btn-success[disabled].focus,.btn-success[disabled]:active,.btn-success[disabled]:focus,.btn-success[disabled]:hover,fieldset[disabled] .btn-success,fieldset[disabled] .btn-success.active,fieldset[disabled] .btn-success.focus,fieldset[disabled] .btn-success:active,fieldset[disabled] .btn-success:focus,fieldset[disabled] .btn-success:hover{background-color:#5cb85c;border-color:#4cae4c}.btn-success .badge{color:#5cb85c;background-color:#fff}.btn-info{color:#fff;background-color:#5bc0de;border-color:#46b8da}.btn-info.focus,.btn-info:focus{color:#fff;background-color:#31b0d5;border-color:#1b6d85}.btn-info:hover{color:#fff;background-color:#31b0d5;border-color:#269abc}.btn-info.active,.btn-info:active,.open>.dropdown-toggle.btn-info{color:#fff;background-color:#31b0d5;border-color:#269abc}.btn-info.active.focus,.btn-info.active:focus,.btn-info.active:hover,.btn-info:active.focus,.btn-info:active:focus,.btn-info:active:hover,.open>.dropdown-toggle.btn-info.focus,.open>.dropdown-toggle.btn-info:focus,.open>.dropdown-toggle.btn-info:hover{color:#fff;background-color:#269abc;border-color:#1b6d85}.btn-info.active,.btn-info:active,.open>.dropdown-toggle.btn-info{background-image:none}.btn-info.disabled,.btn-info.disabled.active,.btn-info.disabled.focus,.btn-info.disabled:active,.btn-info.disabled:focus,.btn-info.disabled:hover,.btn-info[disabled],.btn-info[disabled].active,.btn-info[disabled].focus,.btn-info[disabled]:active,.btn-info[disabled]:focus,.btn-info[disabled]:hover,fieldset[disabled] .btn-info,fieldset[disabled] .btn-info.active,fieldset[disabled] .btn-info.focus,fieldset[disabled] .btn-info:active,fieldset[disabled] .btn-info:focus,fieldset[disabled] .btn-info:hover{background-color:#5bc0de;border-color:#46b8da}.btn-info .badge{color:#5bc0de;background-color:#fff}.btn-warning{color:#fff;background-color:#f0ad4e;border-color:#eea236}.btn-warning.focus,.btn-warning:focus{color:#fff;background-color:#ec971f;border-color:#985f0d}.btn-warning:hover{color:#fff;background-color:#ec971f;border-color:#d58512}.btn-warning.active,.btn-warning:active,.open>.dropdown-toggle.btn-warning{color:#fff;background-color:#ec971f;border-color:#d58512}.btn-warning.active.focus,.btn-warning.active:focus,.btn-warning.active:hover,.btn-warning:active.focus,.btn-warning:active:focus,.btn-warning:active:hover,.open>.dropdown-toggle.btn-warning.focus,.open>.dropdown-toggle.btn-warning:focus,.open>.dropdown-toggle.btn-warning:hover{color:#fff;background-color:#d58512;border-color:#985f0d}.btn-warning.active,.btn-warning:active,.open>.dropdown-toggle.btn-warning{background-image:none}.btn-warning.disabled,.btn-warning.disabled.active,.btn-warning.disabled.focus,.btn-warning.disabled:active,.btn-warning.disabled:focus,.btn-warning.disabled:hover,.btn-warning[disabled],.btn-warning[disabled].active,.btn-warning[disabled].focus,.btn-warning[disabled]:active,.btn-warning[disabled]:focus,.btn-warning[disabled]:hover,fieldset[disabled] .btn-warning,fieldset[disabled] .btn-warning.active,fieldset[disabled] .btn-warning.focus,fieldset[disabled] .btn-warning:active,fieldset[disabled] .btn-warning:focus,fieldset[disabled] .btn-warning:hover{background-color:#f0ad4e;border-color:#eea236}.btn-warning .badge{color:#f0ad4e;background-color:#fff}.btn-danger{color:#fff;background-color:#d9534f;border-color:#d43f3a}.btn-danger.focus,.btn-danger:focus{color:#fff;background-color:#c9302c;border-color:#761c19}.btn-danger:hover{color:#fff;background-color:#c9302c;border-color:#ac2925}.btn-danger.active,.btn-danger:active,.open>.dropdown-toggle.btn-danger{color:#fff;background-color:#c9302c;border-color:#ac2925}.btn-danger.active.focus,.btn-danger.active:focus,.btn-danger.active:hover,.btn-danger:active.focus,.btn-danger:active:focus,.btn-danger:active:hover,.open>.dropdown-toggle.btn-danger.focus,.open>.dropdown-toggle.btn-danger:focus,.open>.dropdown-toggle.btn-danger:hover{color:#fff;background-color:#ac2925;border-color:#761c19}.btn-danger.active,.btn-danger:active,.open>.dropdown-toggle.btn-danger{background-image:none}.btn-danger.disabled,.btn-danger.disabled.active,.btn-danger.disabled.focus,.btn-danger.disabled:active,.btn-danger.disabled:focus,.btn-danger.disabled:hover,.btn-danger[disabled],.btn-danger[disabled].active,.btn-danger[disabled].focus,.btn-danger[disabled]:active,.btn-danger[disabled]:focus,.btn-danger[disabled]:hover,fieldset[disabled] .btn-danger,fieldset[disabled] .btn-danger.active,fieldset[disabled] .btn-danger.focus,fieldset[disabled] .btn-danger:active,fieldset[disabled] .btn-danger:focus,fieldset[disabled] .btn-danger:hover{background-color:#d9534f;border-color:#d43f3a}.btn-danger .badge{color:#d9534f;background-color:#fff}.btn-link{font-weight:400;color:#337ab7;border-radius:0}.btn-link,.btn-link.active,.btn-link:active,.btn-link[disabled],fieldset[disabled] .btn-link{background-color:transparent;-webkit-box-shadow:none;box-shadow:none}.btn-link,.btn-link:active,.btn-link:focus,.btn-link:hover{border-color:transparent}.btn-link:focus,.btn-link:hover{color:#23527c;text-decoration:underline;background-color:transparent}.btn-link[disabled]:focus,.btn-link[disabled]:hover,fieldset[disabled] .btn-link:focus,fieldset[disabled] .btn-link:hover{color:#777;text-decoration:none}.btn-group-lg>.btn,.btn-lg{padding:10px 16px;font-size:18px;line-height:1.3333333;border-radius:6px}.btn-group-sm>.btn,.btn-sm{padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}.btn-group-xs>.btn,.btn-xs{padding:1px 5px;font-size:12px;line-height:1.5;border-radius:3px}.btn-block{display:block;width:100%}.btn-block+.btn-block{margin-top:5px}input[type=button].btn-block,input[type=reset].btn-block,input[type=submit].btn-block{width:100%}.fade{opacity:0;-webkit-transition:opacity .15s linear;-o-transition:opacity .15s linear;transition:opacity .15s linear}.fade.in{opacity:1}.collapse{display:none}.collapse.in{display:block}tr.collapse.in{display:table-row}tbody.collapse.in{display:table-row-group}.collapsing{position:relative;height:0;overflow:hidden;-webkit-transition-timing-function:ease;-o-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-duration:.35s;-o-transition-duration:.35s;transition-duration:.35s;-webkit-transition-property:height,visibility;-o-transition-property:height,visibility;transition-property:height,visibility}.caret{display:inline-block;width:0;height:0;margin-left:2px;vertical-align:middle;border-top:4px dashed;border-top:4px solid\9;border-right:4px solid transparent;border-left:4px solid transparent}.dropdown,.dropup{position:relative}.dropdown-toggle:focus{outline:0}.dropdown-menu{position:absolute;top:100%;left:0;z-index:1000;display:none;float:left;min-width:160px;padding:5px 0;margin:2px 0 0;font-size:14px;text-align:left;list-style:none;background-color:#fff;-webkit-background-clip:padding-box;background-clip:padding-box;border:1px solid #ccc;border:1px solid rgba(0,0,0,.15);border-radius:4px;-webkit-box-shadow:0 6px 12px rgba(0,0,0,.175);box-shadow:0 6px 12px rgba(0,0,0,.175)}.dropdown-menu.pull-right{right:0;left:auto}.dropdown-menu .divider{height:1px;margin:9px 0;overflow:hidden;background-color:#e5e5e5}.dropdown-menu>li>a{display:block;padding:3px 20px;clear:both;font-weight:400;line-height:1.42857143;color:#333;white-space:nowrap}.dropdown-menu>li>a:focus,.dropdown-menu>li>a:hover{color:#262626;text-decoration:none;background-color:#f5f5f5}.dropdown-menu>.active>a,.dropdown-menu>.active>a:focus,.dropdown-menu>.active>a:hover{color:#fff;text-decoration:none;background-color:#337ab7;outline:0}.dropdown-menu>.disabled>a,.dropdown-menu>.disabled>a:focus,.dropdown-menu>.disabled>a:hover{color:#777}.dropdown-menu>.disabled>a:focus,.dropdown-menu>.disabled>a:hover{text-decoration:none;cursor:not-allowed;background-color:transparent;background-image:none;filter:progid:DXImageTransform.Microsoft.gradient(enabled=false)}.open>.dropdown-menu{display:block}.open>a{outline:0}.dropdown-menu-right{right:0;left:auto}.dropdown-menu-left{right:auto;left:0}.dropdown-header{display:block;padding:3px 20px;font-size:12px;line-height:1.42857143;color:#777;white-space:nowrap}.dropdown-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;z-index:990}.pull-right>.dropdown-menu{right:0;left:auto}.dropup .caret,.navbar-fixed-bottom .dropdown .caret{content:"";border-top:0;border-bottom:4px dashed;border-bottom:4px solid\9}.dropup .dropdown-menu,.navbar-fixed-bottom .dropdown .dropdown-menu{top:auto;bottom:100%;margin-bottom:2px}@media (min-width:768px){.navbar-right .dropdown-menu{right:0;left:auto}.navbar-right .dropdown-menu-left{right:auto;left:0}}.btn-group,.btn-group-vertical{position:relative;display:inline-block;vertical-align:middle}.btn-group-vertical>.btn,.btn-group>.btn{position:relative;float:left}.btn-group-vertical>.btn.active,.btn-group-vertical>.btn:active,.btn-group-vertical>.btn:focus,.btn-group-vertical>.btn:hover,.btn-group>.btn.active,.btn-group>.btn:active,.btn-group>.btn:focus,.btn-group>.btn:hover{z-index:2}.btn-group .btn+.btn,.btn-group .btn+.btn-group,.btn-group .btn-group+.btn,.btn-group .btn-group+.btn-group{margin-left:-1px}.btn-toolbar{margin-left:-5px}.btn-toolbar .btn,.btn-toolbar .btn-group,.btn-toolbar .input-group{float:left}.btn-toolbar>.btn,.btn-toolbar>.btn-group,.btn-toolbar>.input-group{margin-left:5px}.btn-group>.btn:not(:first-child):not(:last-child):not(.dropdown-toggle){border-radius:0}.btn-group>.btn:first-child{margin-left:0}.btn-group>.btn:first-child:not(:last-child):not(.dropdown-toggle){border-top-right-radius:0;border-bottom-right-radius:0}.btn-group>.btn:last-child:not(:first-child),.btn-group>.dropdown-toggle:not(:first-child){border-top-left-radius:0;border-bottom-left-radius:0}.btn-group>.btn-group{float:left}.btn-group>.btn-group:not(:first-child):not(:last-child)>.btn{border-radius:0}.btn-group>.btn-group:first-child:not(:last-child)>.btn:last-child,.btn-group>.btn-group:first-child:not(:last-child)>.dropdown-toggle{border-top-right-radius:0;border-bottom-right-radius:0}.btn-group>.btn-group:last-child:not(:first-child)>.btn:first-child{border-top-left-radius:0;border-bottom-left-radius:0}.btn-group .dropdown-toggle:active,.btn-group.open .dropdown-toggle{outline:0}.btn-group>.btn+.dropdown-toggle{padding-right:8px;padding-left:8px}.btn-group>.btn-lg+.dropdown-toggle{padding-right:12px;padding-left:12px}.btn-group.open .dropdown-toggle{-webkit-box-shadow:inset 0 3px 5px rgba(0,0,0,.125);box-shadow:inset 0 3px 5px rgba(0,0,0,.125)}.btn-group.open .dropdown-toggle.btn-link{-webkit-box-shadow:none;box-shadow:none}.btn .caret{margin-left:0}.btn-lg .caret{border-width:5px 5px 0;border-bottom-width:0}.dropup .btn-lg .caret{border-width:0 5px 5px}.btn-group-vertical>.btn,.btn-group-vertical>.btn-group,.btn-group-vertical>.btn-group>.btn{display:block;float:none;width:100%;max-width:100%}.btn-group-vertical>.btn-group>.btn{float:none}.btn-group-vertical>.btn+.btn,.btn-group-vertical>.btn+.btn-group,.btn-group-vertical>.btn-group+.btn,.btn-group-vertical>.btn-group+.btn-group{margin-top:-1px;margin-left:0}.btn-group-vertical>.btn:not(:first-child):not(:last-child){border-radius:0}.btn-group-vertical>.btn:first-child:not(:last-child){border-top-right-radius:4px;border-bottom-right-radius:0;border-bottom-left-radius:0}.btn-group-vertical>.btn:last-child:not(:first-child){border-top-left-radius:0;border-top-right-radius:0;border-bottom-left-radius:4px}.btn-group-vertical>.btn-group:not(:first-child):not(:last-child)>.btn{border-radius:0}.btn-group-vertical>.btn-group:first-child:not(:last-child)>.btn:last-child,.btn-group-vertical>.btn-group:first-child:not(:last-child)>.dropdown-toggle{border-bottom-right-radius:0;border-bottom-left-radius:0}.btn-group-vertical>.btn-group:last-child:not(:first-child)>.btn:first-child{border-top-left-radius:0;border-top-right-radius:0}.btn-group-justified{display:table;width:100%;table-layout:fixed;border-collapse:separate}.btn-group-justified>.btn,.btn-group-justified>.btn-group{display:table-cell;float:none;width:1%}.btn-group-justified>.btn-group .btn{width:100%}.btn-group-justified>.btn-group .dropdown-menu{left:auto}[data-toggle=buttons]>.btn input[type=checkbox],[data-toggle=buttons]>.btn input[type=radio],[data-toggle=buttons]>.btn-group>.btn input[type=checkbox],[data-toggle=buttons]>.btn-group>.btn input[type=radio]{position:absolute;clip:rect(0,0,0,0);pointer-events:none}.input-group{position:relative;display:table;border-collapse:separate}.input-group[class*=col-]{float:none;padding-right:0;padding-left:0}.input-group .form-control{position:relative;z-index:2;float:left;width:100%;margin-bottom:0}.input-group-lg>.form-control,.input-group-lg>.input-group-addon,.input-group-lg>.input-group-btn>.btn{height:46px;padding:10px 16px;font-size:18px;line-height:1.3333333;border-radius:6px}select.input-group-lg>.form-control,select.input-group-lg>.input-group-addon,select.input-group-lg>.input-group-btn>.btn{height:46px;line-height:46px}select[multiple].input-group-lg>.form-control,select[multiple].input-group-lg>.input-group-addon,select[multiple].input-group-lg>.input-group-btn>.btn,textarea.input-group-lg>.form-control,textarea.input-group-lg>.input-group-addon,textarea.input-group-lg>.input-group-btn>.btn{height:auto}.input-group-sm>.form-control,.input-group-sm>.input-group-addon,.input-group-sm>.input-group-btn>.btn{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}select.input-group-sm>.form-control,select.input-group-sm>.input-group-addon,select.input-group-sm>.input-group-btn>.btn{height:30px;line-height:30px}select[multiple].input-group-sm>.form-control,select[multiple].input-group-sm>.input-group-addon,select[multiple].input-group-sm>.input-group-btn>.btn,textarea.input-group-sm>.form-control,textarea.input-group-sm>.input-group-addon,textarea.input-group-sm>.input-group-btn>.btn{height:auto}.input-group .form-control,.input-group-addon,.input-group-btn{display:table-cell}.input-group .form-control:not(:first-child):not(:last-child),.input-group-addon:not(:first-child):not(:last-child),.input-group-btn:not(:first-child):not(:last-child){border-radius:0}.input-group-addon,.input-group-btn{width:1%;white-space:nowrap;vertical-align:middle}.input-group-addon{padding:6px 12px;font-size:14px;font-weight:400;line-height:1;color:#555;text-align:center;background-color:#eee;border:1px solid #ccc;border-radius:4px}.input-group-addon.input-sm{padding:5px 10px;font-size:12px;border-radius:3px}.input-group-addon.input-lg{padding:10px 16px;font-size:18px;border-radius:6px}.input-group-addon input[type=checkbox],.input-group-addon input[type=radio]{margin-top:0}.input-group .form-control:first-child,.input-group-addon:first-child,.input-group-btn:first-child>.btn,.input-group-btn:first-child>.btn-group>.btn,.input-group-btn:first-child>.dropdown-toggle,.input-group-btn:last-child>.btn-group:not(:last-child)>.btn,.input-group-btn:last-child>.btn:not(:last-child):not(.dropdown-toggle){border-top-right-radius:0;border-bottom-right-radius:0}.input-group-addon:first-child{border-right:0}.input-group .form-control:last-child,.input-group-addon:last-child,.input-group-btn:first-child>.btn-group:not(:first-child)>.btn,.input-group-btn:first-child>.btn:not(:first-child),.input-group-btn:last-child>.btn,.input-group-btn:last-child>.btn-group>.btn,.input-group-btn:last-child>.dropdown-toggle{border-top-left-radius:0;border-bottom-left-radius:0}.input-group-addon:last-child{border-left:0}.input-group-btn{position:relative;font-size:0;white-space:nowrap}.input-group-btn>.btn{position:relative}.input-group-btn>.btn+.btn{margin-left:-1px}.input-group-btn>.btn:active,.input-group-btn>.btn:focus,.input-group-btn>.btn:hover{z-index:2}.input-group-btn:first-child>.btn,.input-group-btn:first-child>.btn-group{margin-right:-1px}.input-group-btn:last-child>.btn,.input-group-btn:last-child>.btn-group{z-index:2;margin-left:-1px}.nav{padding-left:0;margin-bottom:0;list-style:none}.nav>li{position:relative;display:block}.nav>li>a{position:relative;display:block;padding:10px 15px}.nav>li>a:focus,.nav>li>a:hover{text-decoration:none;background-color:#eee}.nav>li.disabled>a{color:#777}.nav>li.disabled>a:focus,.nav>li.disabled>a:hover{color:#777;text-decoration:none;cursor:not-allowed;background-color:transparent}.nav .open>a,.nav .open>a:focus,.nav .open>a:hover{background-color:#eee;border-color:#337ab7}.nav .nav-divider{height:1px;margin:9px 0;overflow:hidden;background-color:#e5e5e5}.nav>li>a>img{max-width:none}.nav-tabs{border-bottom:1px solid #ddd}.nav-tabs>li{float:left;margin-bottom:-1px}.nav-tabs>li>a{margin-right:2px;line-height:1.42857143;border:1px solid transparent;border-radius:4px 4px 0 0}.nav-tabs>li>a:hover{border-color:#eee #eee #ddd}.nav-tabs>li.active>a,.nav-tabs>li.active>a:focus,.nav-tabs>li.active>a:hover{color:#555;cursor:default;background-color:#fff;border:1px solid #ddd;border-bottom-color:transparent}.nav-tabs.nav-justified{width:100%;border-bottom:0}.nav-tabs.nav-justified>li{float:none}.nav-tabs.nav-justified>li>a{margin-bottom:5px;text-align:center}.nav-tabs.nav-justified>.dropdown .dropdown-menu{top:auto;left:auto}@media (min-width:768px){.nav-tabs.nav-justified>li{display:table-cell;width:1%}.nav-tabs.nav-justified>li>a{margin-bottom:0}}.nav-tabs.nav-justified>li>a{margin-right:0;border-radius:4px}.nav-tabs.nav-justified>.active>a,.nav-tabs.nav-justified>.active>a:focus,.nav-tabs.nav-justified>.active>a:hover{border:1px solid #ddd}@media (min-width:768px){.nav-tabs.nav-justified>li>a{border-bottom:1px solid #ddd;border-radius:4px 4px 0 0}.nav-tabs.nav-justified>.active>a,.nav-tabs.nav-justified>.active>a:focus,.nav-tabs.nav-justified>.active>a:hover{border-bottom-color:#fff}}.nav-pills>li{float:left}.nav-pills>li>a{border-radius:4px}.nav-pills>li+li{margin-left:2px}.nav-pills>li.active>a,.nav-pills>li.active>a:focus,.nav-pills>li.active>a:hover{color:#fff;background-color:#337ab7}.nav-stacked>li{float:none}.nav-stacked>li+li{margin-top:2px;margin-left:0}.nav-justified{width:100%}.nav-justified>li{float:none}.nav-justified>li>a{margin-bottom:5px;text-align:center}.nav-justified>.dropdown .dropdown-menu{top:auto;left:auto}@media (min-width:768px){.nav-justified>li{display:table-cell;width:1%}.nav-justified>li>a{margin-bottom:0}}.nav-tabs-justified{border-bottom:0}.nav-tabs-justified>li>a{margin-right:0;border-radius:4px}.nav-tabs-justified>.active>a,.nav-tabs-justified>.active>a:focus,.nav-tabs-justified>.active>a:hover{border:1px solid #ddd}@media (min-width:768px){.nav-tabs-justified>li>a{border-bottom:1px solid #ddd;border-radius:4px 4px 0 0}.nav-tabs-justified>.active>a,.nav-tabs-justified>.active>a:focus,.nav-tabs-justified>.active>a:hover{border-bottom-color:#fff}}.tab-content>.tab-pane{display:none}.tab-content>.active{display:block}.nav-tabs .dropdown-menu{margin-top:-1px;border-top-left-radius:0;border-top-right-radius:0}.navbar{position:relative;min-height:50px;margin-bottom:20px;border:1px solid transparent}@media (min-width:768px){.navbar{border-radius:4px}}@media (min-width:768px){.navbar-header{float:left}}.navbar-collapse{padding-right:15px;padding-left:15px;overflow-x:visible;-webkit-overflow-scrolling:touch;border-top:1px solid transparent;-webkit-box-shadow:inset 0 1px 0 rgba(255,255,255,.1);box-shadow:inset 0 1px 0 rgba(255,255,255,.1)}.navbar-collapse.in{overflow-y:auto}@media (min-width:768px){.navbar-collapse{width:auto;border-top:0;-webkit-box-shadow:none;box-shadow:none}.navbar-collapse.collapse{display:block!important;height:auto!important;padding-bottom:0;overflow:visible!important}.navbar-collapse.in{overflow-y:visible}.navbar-fixed-bottom .navbar-collapse,.navbar-fixed-top .navbar-collapse,.navbar-static-top .navbar-collapse{padding-right:0;padding-left:0}}.navbar-fixed-bottom .navbar-collapse,.navbar-fixed-top .navbar-collapse{max-height:340px}@media (max-device-width:480px) and (orientation:landscape){.navbar-fixed-bottom .navbar-collapse,.navbar-fixed-top .navbar-collapse{max-height:200px}}.container-fluid>.navbar-collapse,.container-fluid>.navbar-header,.container>.navbar-collapse,.container>.navbar-header{margin-right:-15px;margin-left:-15px}@media (min-width:768px){.container-fluid>.navbar-collapse,.container-fluid>.navbar-header,.container>.navbar-collapse,.container>.navbar-header{margin-right:0;margin-left:0}}.navbar-static-top{z-index:1000;border-width:0 0 1px}@media (min-width:768px){.navbar-static-top{border-radius:0}}.navbar-fixed-bottom,.navbar-fixed-top{position:fixed;right:0;left:0;z-index:1030}@media (min-width:768px){.navbar-fixed-bottom,.navbar-fixed-top{border-radius:0}}.navbar-fixed-top{top:0;border-width:0 0 1px}.navbar-fixed-bottom{bottom:0;margin-bottom:0;border-width:1px 0 0}.navbar-brand{float:left;height:50px;padding:15px 15px;font-size:18px;line-height:20px}.navbar-brand:focus,.navbar-brand:hover{text-decoration:none}.navbar-brand>img{display:block}@media (min-width:768px){.navbar>.container .navbar-brand,.navbar>.container-fluid .navbar-brand{margin-left:-15px}}.navbar-toggle{position:relative;float:right;padding:9px 10px;margin-top:8px;margin-right:15px;margin-bottom:8px;background-color:transparent;background-image:none;border:1px solid transparent;border-radius:4px}.navbar-toggle:focus{outline:0}.navbar-toggle .icon-bar{display:block;width:22px;height:2px;border-radius:1px}.navbar-toggle .icon-bar+.icon-bar{margin-top:4px}@media (min-width:768px){.navbar-toggle{display:none}}.navbar-nav{margin:7.5px -15px}.navbar-nav>li>a{padding-top:10px;padding-bottom:10px;line-height:20px}@media (max-width:767px){.navbar-nav .open .dropdown-menu{position:static;float:none;width:auto;margin-top:0;background-color:transparent;border:0;-webkit-box-shadow:none;box-shadow:none}.navbar-nav .open .dropdown-menu .dropdown-header,.navbar-nav .open .dropdown-menu>li>a{padding:5px 15px 5px 25px}.navbar-nav .open .dropdown-menu>li>a{line-height:20px}.navbar-nav .open .dropdown-menu>li>a:focus,.navbar-nav .open .dropdown-menu>li>a:hover{background-image:none}}@media (min-width:768px){.navbar-nav{float:left;margin:0}.navbar-nav>li{float:left}.navbar-nav>li>a{padding-top:15px;padding-bottom:15px}}.navbar-form{padding:10px 15px;margin-top:8px;margin-right:-15px;margin-bottom:8px;margin-left:-15px;border-top:1px solid transparent;border-bottom:1px solid transparent;-webkit-box-shadow:inset 0 1px 0 rgba(255,255,255,.1),0 1px 0 rgba(255,255,255,.1);box-shadow:inset 0 1px 0 rgba(255,255,255,.1),0 1px 0 rgba(255,255,255,.1)}@media (min-width:768px){.navbar-form .form-group{display:inline-block;margin-bottom:0;vertical-align:middle}.navbar-form .form-control{display:inline-block;width:auto;vertical-align:middle}.navbar-form .form-control-static{display:inline-block}.navbar-form .input-group{display:inline-table;vertical-align:middle}.navbar-form .input-group .form-control,.navbar-form .input-group .input-group-addon,.navbar-form .input-group .input-group-btn{width:auto}.navbar-form .input-group>.form-control{width:100%}.navbar-form .control-label{margin-bottom:0;vertical-align:middle}.navbar-form .checkbox,.navbar-form .radio{display:inline-block;margin-top:0;margin-bottom:0;vertical-align:middle}.navbar-form .checkbox label,.navbar-form .radio label{padding-left:0}.navbar-form .checkbox input[type=checkbox],.navbar-form .radio input[type=radio]{position:relative;margin-left:0}.navbar-form .has-feedback .form-control-feedback{top:0}}@media (max-width:767px){.navbar-form .form-group{margin-bottom:5px}.navbar-form .form-group:last-child{margin-bottom:0}}@media (min-width:768px){.navbar-form{width:auto;padding-top:0;padding-bottom:0;margin-right:0;margin-left:0;border:0;-webkit-box-shadow:none;box-shadow:none}}.navbar-nav>li>.dropdown-menu{margin-top:0;border-top-left-radius:0;border-top-right-radius:0}.navbar-fixed-bottom .navbar-nav>li>.dropdown-menu{margin-bottom:0;border-top-left-radius:4px;border-top-right-radius:4px;border-bottom-right-radius:0;border-bottom-left-radius:0}.navbar-btn{margin-top:8px;margin-bottom:8px}.navbar-btn.btn-sm{margin-top:10px;margin-bottom:10px}.navbar-btn.btn-xs{margin-top:14px;margin-bottom:14px}.navbar-text{margin-top:15px;margin-bottom:15px}@media (min-width:768px){.navbar-text{float:left;margin-right:15px;margin-left:15px}}@media (min-width:768px){.navbar-left{float:left!important}.navbar-right{float:right!important;margin-right:-15px}.navbar-right~.navbar-right{margin-right:0}}.navbar-default{background-color:#f8f8f8;border-color:#e7e7e7}.navbar-default .navbar-brand{color:#777}.navbar-default .navbar-brand:focus,.navbar-default .navbar-brand:hover{color:#5e5e5e;background-color:transparent}.navbar-default .navbar-text{color:#777}.navbar-default .navbar-nav>li>a{color:#777}.navbar-default .navbar-nav>li>a:focus,.navbar-default .navbar-nav>li>a:hover{color:#333;background-color:transparent}.navbar-default .navbar-nav>.active>a,.navbar-default .navbar-nav>.active>a:focus,.navbar-default .navbar-nav>.active>a:hover{color:#555;background-color:#e7e7e7}.navbar-default .navbar-nav>.disabled>a,.navbar-default .navbar-nav>.disabled>a:focus,.navbar-default .navbar-nav>.disabled>a:hover{color:#ccc;background-color:transparent}.navbar-default .navbar-toggle{border-color:#ddd}.navbar-default .navbar-toggle:focus,.navbar-default .navbar-toggle:hover{background-color:#ddd}.navbar-default .navbar-toggle .icon-bar{background-color:#888}.navbar-default .navbar-collapse,.navbar-default .navbar-form{border-color:#e7e7e7}.navbar-default .navbar-nav>.open>a,.navbar-default .navbar-nav>.open>a:focus,.navbar-default .navbar-nav>.open>a:hover{color:#555;background-color:#e7e7e7}@media (max-width:767px){.navbar-default .navbar-nav .open .dropdown-menu>li>a{color:#777}.navbar-default .navbar-nav .open .dropdown-menu>li>a:focus,.navbar-default .navbar-nav .open .dropdown-menu>li>a:hover{color:#333;background-color:transparent}.navbar-default .navbar-nav .open .dropdown-menu>.active>a,.navbar-default .navbar-nav .open .dropdown-menu>.active>a:focus,.navbar-default .navbar-nav .open .dropdown-menu>.active>a:hover{color:#555;background-color:#e7e7e7}.navbar-default .navbar-nav .open .dropdown-menu>.disabled>a,.navbar-default .navbar-nav .open .dropdown-menu>.disabled>a:focus,.navbar-default .navbar-nav .open .dropdown-menu>.disabled>a:hover{color:#ccc;background-color:transparent}}.navbar-default .navbar-link{color:#777}.navbar-default .navbar-link:hover{color:#333}.navbar-default .btn-link{color:#777}.navbar-default .btn-link:focus,.navbar-default .btn-link:hover{color:#333}.navbar-default .btn-link[disabled]:focus,.navbar-default .btn-link[disabled]:hover,fieldset[disabled] .navbar-default .btn-link:focus,fieldset[disabled] .navbar-default .btn-link:hover{color:#ccc}.navbar-inverse{background-color:#222;border-color:#080808}.navbar-inverse .navbar-brand{color:#9d9d9d}.navbar-inverse .navbar-brand:focus,.navbar-inverse .navbar-brand:hover{color:#fff;background-color:transparent}.navbar-inverse .navbar-text{color:#9d9d9d}.navbar-inverse .navbar-nav>li>a{color:#9d9d9d}.navbar-inverse .navbar-nav>li>a:focus,.navbar-inverse .navbar-nav>li>a:hover{color:#fff;background-color:transparent}.navbar-inverse .navbar-nav>.active>a,.navbar-inverse .navbar-nav>.active>a:focus,.navbar-inverse .navbar-nav>.active>a:hover{color:#fff;background-color:#080808}.navbar-inverse .navbar-nav>.disabled>a,.navbar-inverse .navbar-nav>.disabled>a:focus,.navbar-inverse .navbar-nav>.disabled>a:hover{color:#444;background-color:transparent}.navbar-inverse .navbar-toggle{border-color:#333}.navbar-inverse .navbar-toggle:focus,.navbar-inverse .navbar-toggle:hover{background-color:#333}.navbar-inverse .navbar-toggle .icon-bar{background-color:#fff}.navbar-inverse .navbar-collapse,.navbar-inverse .navbar-form{border-color:#101010}.navbar-inverse .navbar-nav>.open>a,.navbar-inverse .navbar-nav>.open>a:focus,.navbar-inverse .navbar-nav>.open>a:hover{color:#fff;background-color:#080808}@media (max-width:767px){.navbar-inverse .navbar-nav .open .dropdown-menu>.dropdown-header{border-color:#080808}.navbar-inverse .navbar-nav .open .dropdown-menu .divider{background-color:#080808}.navbar-inverse .navbar-nav .open .dropdown-menu>li>a{color:#9d9d9d}.navbar-inverse .navbar-nav .open .dropdown-menu>li>a:focus,.navbar-inverse .navbar-nav .open .dropdown-menu>li>a:hover{color:#fff;background-color:transparent}.navbar-inverse .navbar-nav .open .dropdown-menu>.active>a,.navbar-inverse .navbar-nav .open .dropdown-menu>.active>a:focus,.navbar-inverse .navbar-nav .open .dropdown-menu>.active>a:hover{color:#fff;background-color:#080808}.navbar-inverse .navbar-nav .open .dropdown-menu>.disabled>a,.navbar-inverse .navbar-nav .open .dropdown-menu>.disabled>a:focus,.navbar-inverse .navbar-nav .open .dropdown-menu>.disabled>a:hover{color:#444;background-color:transparent}}.navbar-inverse .navbar-link{color:#9d9d9d}.navbar-inverse .navbar-link:hover{color:#fff}.navbar-inverse .btn-link{color:#9d9d9d}.navbar-inverse .btn-link:focus,.navbar-inverse .btn-link:hover{color:#fff}.navbar-inverse .btn-link[disabled]:focus,.navbar-inverse .btn-link[disabled]:hover,fieldset[disabled] .navbar-inverse .btn-link:focus,fieldset[disabled] .navbar-inverse .btn-link:hover{color:#444}.breadcrumb{padding:8px 15px;margin-bottom:20px;list-style:none;background-color:#f5f5f5;border-radius:4px}.breadcrumb>li{display:inline-block}.breadcrumb>li+li:before{padding:0 5px;color:#ccc;content:"/\00a0"}.breadcrumb>.active{color:#777}.pagination{display:inline-block;padding-left:0;margin:20px 0;border-radius:4px}.pagination>li{display:inline}.pagination>li>a,.pagination>li>span{position:relative;float:left;padding:6px 12px;margin-left:-1px;line-height:1.42857143;color:#337ab7;text-decoration:none;background-color:#fff;border:1px solid #ddd}.pagination>li:first-child>a,.pagination>li:first-child>span{margin-left:0;border-top-left-radius:4px;border-bottom-left-radius:4px}.pagination>li:last-child>a,.pagination>li:last-child>span{border-top-right-radius:4px;border-bottom-right-radius:4px}.pagination>li>a:focus,.pagination>li>a:hover,.pagination>li>span:focus,.pagination>li>span:hover{z-index:3;color:#23527c;background-color:#eee;border-color:#ddd}.pagination>.active>a,.pagination>.active>a:focus,.pagination>.active>a:hover,.pagination>.active>span,.pagination>.active>span:focus,.pagination>.active>span:hover{z-index:2;color:#fff;cursor:default;background-color:#337ab7;border-color:#337ab7}.pagination>.disabled>a,.pagination>.disabled>a:focus,.pagination>.disabled>a:hover,.pagination>.disabled>span,.pagination>.disabled>span:focus,.pagination>.disabled>span:hover{color:#777;cursor:not-allowed;background-color:#fff;border-color:#ddd}.pagination-lg>li>a,.pagination-lg>li>span{padding:10px 16px;font-size:18px;line-height:1.3333333}.pagination-lg>li:first-child>a,.pagination-lg>li:first-child>span{border-top-left-radius:6px;border-bottom-left-radius:6px}.pagination-lg>li:last-child>a,.pagination-lg>li:last-child>span{border-top-right-radius:6px;border-bottom-right-radius:6px}.pagination-sm>li>a,.pagination-sm>li>span{padding:5px 10px;font-size:12px;line-height:1.5}.pagination-sm>li:first-child>a,.pagination-sm>li:first-child>span{border-top-left-radius:3px;border-bottom-left-radius:3px}.pagination-sm>li:last-child>a,.pagination-sm>li:last-child>span{border-top-right-radius:3px;border-bottom-right-radius:3px}.pager{padding-left:0;margin:20px 0;text-align:center;list-style:none}.pager li{display:inline}.pager li>a,.pager li>span{display:inline-block;padding:5px 14px;background-color:#fff;border:1px solid #ddd;border-radius:15px}.pager li>a:focus,.pager li>a:hover{text-decoration:none;background-color:#eee}.pager .next>a,.pager .next>span{float:right}.pager .previous>a,.pager .previous>span{float:left}.pager .disabled>a,.pager .disabled>a:focus,.pager .disabled>a:hover,.pager .disabled>span{color:#777;cursor:not-allowed;background-color:#fff}.label{display:inline;padding:.2em .6em .3em;font-size:75%;font-weight:700;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:baseline;border-radius:.25em}a.label:focus,a.label:hover{color:#fff;text-decoration:none;cursor:pointer}.label:empty{display:none}.btn .label{position:relative;top:-1px}.label-default{background-color:#777}.label-default[href]:focus,.label-default[href]:hover{background-color:#5e5e5e}.label-primary{background-color:#337ab7}.label-primary[href]:focus,.label-primary[href]:hover{background-color:#286090}.label-success{background-color:#5cb85c}.label-success[href]:focus,.label-success[href]:hover{background-color:#449d44}.label-info{background-color:#5bc0de}.label-info[href]:focus,.label-info[href]:hover{background-color:#31b0d5}.label-warning{background-color:#f0ad4e}.label-warning[href]:focus,.label-warning[href]:hover{background-color:#ec971f}.label-danger{background-color:#d9534f}.label-danger[href]:focus,.label-danger[href]:hover{background-color:#c9302c}.badge{display:inline-block;min-width:10px;padding:3px 7px;font-size:12px;font-weight:700;line-height:1;color:#fff;text-align:center;white-space:nowrap;vertical-align:middle;background-color:#777;border-radius:10px}.badge:empty{display:none}.btn .badge{position:relative;top:-1px}.btn-group-xs>.btn .badge,.btn-xs .badge{top:0;padding:1px 5px}a.badge:focus,a.badge:hover{color:#fff;text-decoration:none;cursor:pointer}.list-group-item.active>.badge,.nav-pills>.active>a>.badge{color:#337ab7;background-color:#fff}.list-group-item>.badge{float:right}.list-group-item>.badge+.badge{margin-right:5px}.nav-pills>li>a>.badge{margin-left:3px}.jumbotron{padding-top:30px;padding-bottom:30px;margin-bottom:30px;color:inherit;background-color:#eee}.jumbotron .h1,.jumbotron h1{color:inherit}.jumbotron p{margin-bottom:15px;font-size:21px;font-weight:200}.jumbotron>hr{border-top-color:#d5d5d5}.container .jumbotron,.container-fluid .jumbotron{border-radius:6px}.jumbotron .container{max-width:100%}@media screen and (min-width:768px){.jumbotron{padding-top:48px;padding-bottom:48px}.container .jumbotron,.container-fluid .jumbotron{padding-right:60px;padding-left:60px}.jumbotron .h1,.jumbotron h1{font-size:63px}}.thumbnail{display:block;padding:4px;margin-bottom:20px;line-height:1.42857143;background-color:#fff;border:1px solid #ddd;border-radius:4px;-webkit-transition:border .2s ease-in-out;-o-transition:border .2s ease-in-out;transition:border .2s ease-in-out}.thumbnail a>img,.thumbnail>img{margin-right:auto;margin-left:auto}a.thumbnail.active,a.thumbnail:focus,a.thumbnail:hover{border-color:#337ab7}.thumbnail .caption{padding:9px;color:#333}.alert{padding:15px;margin-bottom:20px;border:1px solid transparent;border-radius:4px}.alert h4{margin-top:0;color:inherit}.alert .alert-link{font-weight:700}.alert>p,.alert>ul{margin-bottom:0}.alert>p+p{margin-top:5px}.alert-dismissable,.alert-dismissible{padding-right:35px}.alert-dismissable .close,.alert-dismissible .close{position:relative;top:-2px;right:-21px;color:inherit}.alert-success{color:#3c763d;background-color:#dff0d8;border-color:#d6e9c6}.alert-success hr{border-top-color:#c9e2b3}.alert-success .alert-link{color:#2b542c}.alert-info{color:#31708f;background-color:#d9edf7;border-color:#bce8f1}.alert-info hr{border-top-color:#a6e1ec}.alert-info .alert-link{color:#245269}.alert-warning{color:#8a6d3b;background-color:#fcf8e3;border-color:#faebcc}.alert-warning hr{border-top-color:#f7e1b5}.alert-warning .alert-link{color:#66512c}.alert-danger{color:#a94442;background-color:#f2dede;border-color:#ebccd1}.alert-danger hr{border-top-color:#e4b9c0}.alert-danger .alert-link{color:#843534}@-webkit-keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}@-o-keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}@keyframes progress-bar-stripes{from{background-position:40px 0}to{background-position:0 0}}.progress{height:20px;margin-bottom:20px;overflow:hidden;background-color:#f5f5f5;border-radius:4px;-webkit-box-shadow:inset 0 1px 2px rgba(0,0,0,.1);box-shadow:inset 0 1px 2px rgba(0,0,0,.1)}.progress-bar{float:left;width:0;height:100%;font-size:12px;line-height:20px;color:#fff;text-align:center;background-color:#337ab7;-webkit-box-shadow:inset 0 -1px 0 rgba(0,0,0,.15);box-shadow:inset 0 -1px 0 rgba(0,0,0,.15);-webkit-transition:width .6s ease;-o-transition:width .6s ease;transition:width .6s ease}.progress-bar-striped,.progress-striped .progress-bar{background-image:-webkit-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:-o-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);-webkit-background-size:40px 40px;background-size:40px 40px}.progress-bar.active,.progress.active .progress-bar{-webkit-animation:progress-bar-stripes 2s linear infinite;-o-animation:progress-bar-stripes 2s linear infinite;animation:progress-bar-stripes 2s linear infinite}.progress-bar-success{background-color:#5cb85c}.progress-striped .progress-bar-success{background-image:-webkit-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:-o-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent)}.progress-bar-info{background-color:#5bc0de}.progress-striped .progress-bar-info{background-image:-webkit-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:-o-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent)}.progress-bar-warning{background-color:#f0ad4e}.progress-striped .progress-bar-warning{background-image:-webkit-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:-o-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent)}.progress-bar-danger{background-color:#d9534f}.progress-striped .progress-bar-danger{background-image:-webkit-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:-o-linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent);background-image:linear-gradient(45deg,rgba(255,255,255,.15) 25%,transparent 25%,transparent 50%,rgba(255,255,255,.15) 50%,rgba(255,255,255,.15) 75%,transparent 75%,transparent)}.media{margin-top:15px}.media:first-child{margin-top:0}.media,.media-body{overflow:hidden;zoom:1}.media-body{width:10000px}.media-object{display:block}.media-object.img-thumbnail{max-width:none}.media-right,.media>.pull-right{padding-left:10px}.media-left,.media>.pull-left{padding-right:10px}.media-body,.media-left,.media-right{display:table-cell;vertical-align:top}.media-middle{vertical-align:middle}.media-bottom{vertical-align:bottom}.media-heading{margin-top:0;margin-bottom:5px}.media-list{padding-left:0;list-style:none}.list-group{padding-left:0;margin-bottom:20px}.list-group-item{position:relative;display:block;padding:10px 15px;margin-bottom:-1px;background-color:#fff;border:1px solid #ddd}.list-group-item:first-child{border-top-left-radius:4px;border-top-right-radius:4px}.list-group-item:last-child{margin-bottom:0;border-bottom-right-radius:4px;border-bottom-left-radius:4px}a.list-group-item,button.list-group-item{color:#555}a.list-group-item .list-group-item-heading,button.list-group-item .list-group-item-heading{color:#333}a.list-group-item:focus,a.list-group-item:hover,button.list-group-item:focus,button.list-group-item:hover{color:#555;text-decoration:none;background-color:#f5f5f5}button.list-group-item{width:100%;text-align:left}.list-group-item.disabled,.list-group-item.disabled:focus,.list-group-item.disabled:hover{color:#777;cursor:not-allowed;background-color:#eee}.list-group-item.disabled .list-group-item-heading,.list-group-item.disabled:focus .list-group-item-heading,.list-group-item.disabled:hover .list-group-item-heading{color:inherit}.list-group-item.disabled .list-group-item-text,.list-group-item.disabled:focus .list-group-item-text,.list-group-item.disabled:hover .list-group-item-text{color:#777}.list-group-item.active,.list-group-item.active:focus,.list-group-item.active:hover{z-index:2;color:#fff;background-color:#337ab7;border-color:#337ab7}.list-group-item.active .list-group-item-heading,.list-group-item.active .list-group-item-heading>.small,.list-group-item.active .list-group-item-heading>small,.list-group-item.active:focus .list-group-item-heading,.list-group-item.active:focus .list-group-item-heading>.small,.list-group-item.active:focus .list-group-item-heading>small,.list-group-item.active:hover .list-group-item-heading,.list-group-item.active:hover .list-group-item-heading>.small,.list-group-item.active:hover .list-group-item-heading>small{color:inherit}.list-group-item.active .list-group-item-text,.list-group-item.active:focus .list-group-item-text,.list-group-item.active:hover .list-group-item-text{color:#c7ddef}.list-group-item-success{color:#3c763d;background-color:#dff0d8}a.list-group-item-success,button.list-group-item-success{color:#3c763d}a.list-group-item-success .list-group-item-heading,button.list-group-item-success .list-group-item-heading{color:inherit}a.list-group-item-success:focus,a.list-group-item-success:hover,button.list-group-item-success:focus,button.list-group-item-success:hover{color:#3c763d;background-color:#d0e9c6}a.list-group-item-success.active,a.list-group-item-success.active:focus,a.list-group-item-success.active:hover,button.list-group-item-success.active,button.list-group-item-success.active:focus,button.list-group-item-success.active:hover{color:#fff;background-color:#3c763d;border-color:#3c763d}.list-group-item-info{color:#31708f;background-color:#d9edf7}a.list-group-item-info,button.list-group-item-info{color:#31708f}a.list-group-item-info .list-group-item-heading,button.list-group-item-info .list-group-item-heading{color:inherit}a.list-group-item-info:focus,a.list-group-item-info:hover,button.list-group-item-info:focus,button.list-group-item-info:hover{color:#31708f;background-color:#c4e3f3}a.list-group-item-info.active,a.list-group-item-info.active:focus,a.list-group-item-info.active:hover,button.list-group-item-info.active,button.list-group-item-info.active:focus,button.list-group-item-info.active:hover{color:#fff;background-color:#31708f;border-color:#31708f}.list-group-item-warning{color:#8a6d3b;background-color:#fcf8e3}a.list-group-item-warning,button.list-group-item-warning{color:#8a6d3b}a.list-group-item-warning .list-group-item-heading,button.list-group-item-warning .list-group-item-heading{color:inherit}a.list-group-item-warning:focus,a.list-group-item-warning:hover,button.list-group-item-warning:focus,button.list-group-item-warning:hover{color:#8a6d3b;background-color:#faf2cc}a.list-group-item-warning.active,a.list-group-item-warning.active:focus,a.list-group-item-warning.active:hover,button.list-group-item-warning.active,button.list-group-item-warning.active:focus,button.list-group-item-warning.active:hover{color:#fff;background-color:#8a6d3b;border-color:#8a6d3b}.list-group-item-danger{color:#a94442;background-color:#f2dede}a.list-group-item-danger,button.list-group-item-danger{color:#a94442}a.list-group-item-danger .list-group-item-heading,button.list-group-item-danger .list-group-item-heading{color:inherit}a.list-group-item-danger:focus,a.list-group-item-danger:hover,button.list-group-item-danger:focus,button.list-group-item-danger:hover{color:#a94442;background-color:#ebcccc}a.list-group-item-danger.active,a.list-group-item-danger.active:focus,a.list-group-item-danger.active:hover,button.list-group-item-danger.active,button.list-group-item-danger.active:focus,button.list-group-item-danger.active:hover{color:#fff;background-color:#a94442;border-color:#a94442}.list-group-item-heading{margin-top:0;margin-bottom:5px}.list-group-item-text{margin-bottom:0;line-height:1.3}.panel{margin-bottom:20px;background-color:#fff;border:1px solid transparent;border-radius:4px;-webkit-box-shadow:0 1px 1px rgba(0,0,0,.05);box-shadow:0 1px 1px rgba(0,0,0,.05)}.panel-body{padding:15px}.panel-heading{padding:10px 15px;border-bottom:1px solid transparent;border-top-left-radius:3px;border-top-right-radius:3px}.panel-heading>.dropdown .dropdown-toggle{color:inherit}.panel-title{margin-top:0;margin-bottom:0;font-size:16px;color:inherit}.panel-title>.small,.panel-title>.small>a,.panel-title>a,.panel-title>small,.panel-title>small>a{color:inherit}.panel-footer{padding:10px 15px;background-color:#f5f5f5;border-top:1px solid #ddd;border-bottom-right-radius:3px;border-bottom-left-radius:3px}.panel>.list-group,.panel>.panel-collapse>.list-group{margin-bottom:0}.panel>.list-group .list-group-item,.panel>.panel-collapse>.list-group .list-group-item{border-width:1px 0;border-radius:0}.panel>.list-group:first-child .list-group-item:first-child,.panel>.panel-collapse>.list-group:first-child .list-group-item:first-child{border-top:0;border-top-left-radius:3px;border-top-right-radius:3px}.panel>.list-group:last-child .list-group-item:last-child,.panel>.panel-collapse>.list-group:last-child .list-group-item:last-child{border-bottom:0;border-bottom-right-radius:3px;border-bottom-left-radius:3px}.panel>.panel-heading+.panel-collapse>.list-group .list-group-item:first-child{border-top-left-radius:0;border-top-right-radius:0}.panel-heading+.list-group .list-group-item:first-child{border-top-width:0}.list-group+.panel-footer{border-top-width:0}.panel>.panel-collapse>.table,.panel>.table,.panel>.table-responsive>.table{margin-bottom:0}.panel>.panel-collapse>.table caption,.panel>.table caption,.panel>.table-responsive>.table caption{padding-right:15px;padding-left:15px}.panel>.table-responsive:first-child>.table:first-child,.panel>.table:first-child{border-top-left-radius:3px;border-top-right-radius:3px}.panel>.table-responsive:first-child>.table:first-child>tbody:first-child>tr:first-child,.panel>.table-responsive:first-child>.table:first-child>thead:first-child>tr:first-child,.panel>.table:first-child>tbody:first-child>tr:first-child,.panel>.table:first-child>thead:first-child>tr:first-child{border-top-left-radius:3px;border-top-right-radius:3px}.panel>.table-responsive:first-child>.table:first-child>tbody:first-child>tr:first-child td:first-child,.panel>.table-responsive:first-child>.table:first-child>tbody:first-child>tr:first-child th:first-child,.panel>.table-responsive:first-child>.table:first-child>thead:first-child>tr:first-child td:first-child,.panel>.table-responsive:first-child>.table:first-child>thead:first-child>tr:first-child th:first-child,.panel>.table:first-child>tbody:first-child>tr:first-child td:first-child,.panel>.table:first-child>tbody:first-child>tr:first-child th:first-child,.panel>.table:first-child>thead:first-child>tr:first-child td:first-child,.panel>.table:first-child>thead:first-child>tr:first-child th:first-child{border-top-left-radius:3px}.panel>.table-responsive:first-child>.table:first-child>tbody:first-child>tr:first-child td:last-child,.panel>.table-responsive:first-child>.table:first-child>tbody:first-child>tr:first-child th:last-child,.panel>.table-responsive:first-child>.table:first-child>thead:first-child>tr:first-child td:last-child,.panel>.table-responsive:first-child>.table:first-child>thead:first-child>tr:first-child th:last-child,.panel>.table:first-child>tbody:first-child>tr:first-child td:last-child,.panel>.table:first-child>tbody:first-child>tr:first-child th:last-child,.panel>.table:first-child>thead:first-child>tr:first-child td:last-child,.panel>.table:first-child>thead:first-child>tr:first-child th:last-child{border-top-right-radius:3px}.panel>.table-responsive:last-child>.table:last-child,.panel>.table:last-child{border-bottom-right-radius:3px;border-bottom-left-radius:3px}.panel>.table-responsive:last-child>.table:last-child>tbody:last-child>tr:last-child,.panel>.table-responsive:last-child>.table:last-child>tfoot:last-child>tr:last-child,.panel>.table:last-child>tbody:last-child>tr:last-child,.panel>.table:last-child>tfoot:last-child>tr:last-child{border-bottom-right-radius:3px;border-bottom-left-radius:3px}.panel>.table-responsive:last-child>.table:last-child>tbody:last-child>tr:last-child td:first-child,.panel>.table-responsive:last-child>.table:last-child>tbody:last-child>tr:last-child th:first-child,.panel>.table-responsive:last-child>.table:last-child>tfoot:last-child>tr:last-child td:first-child,.panel>.table-responsive:last-child>.table:last-child>tfoot:last-child>tr:last-child th:first-child,.panel>.table:last-child>tbody:last-child>tr:last-child td:first-child,.panel>.table:last-child>tbody:last-child>tr:last-child th:first-child,.panel>.table:last-child>tfoot:last-child>tr:last-child td:first-child,.panel>.table:last-child>tfoot:last-child>tr:last-child th:first-child{border-bottom-left-radius:3px}.panel>.table-responsive:last-child>.table:last-child>tbody:last-child>tr:last-child td:last-child,.panel>.table-responsive:last-child>.table:last-child>tbody:last-child>tr:last-child th:last-child,.panel>.table-responsive:last-child>.table:last-child>tfoot:last-child>tr:last-child td:last-child,.panel>.table-responsive:last-child>.table:last-child>tfoot:last-child>tr:last-child th:last-child,.panel>.table:last-child>tbody:last-child>tr:last-child td:last-child,.panel>.table:last-child>tbody:last-child>tr:last-child th:last-child,.panel>.table:last-child>tfoot:last-child>tr:last-child td:last-child,.panel>.table:last-child>tfoot:last-child>tr:last-child th:last-child{border-bottom-right-radius:3px}.panel>.panel-body+.table,.panel>.panel-body+.table-responsive,.panel>.table+.panel-body,.panel>.table-responsive+.panel-body{border-top:1px solid #ddd}.panel>.table>tbody:first-child>tr:first-child td,.panel>.table>tbody:first-child>tr:first-child th{border-top:0}.panel>.table-bordered,.panel>.table-responsive>.table-bordered{border:0}.panel>.table-bordered>tbody>tr>td:first-child,.panel>.table-bordered>tbody>tr>th:first-child,.panel>.table-bordered>tfoot>tr>td:first-child,.panel>.table-bordered>tfoot>tr>th:first-child,.panel>.table-bordered>thead>tr>td:first-child,.panel>.table-bordered>thead>tr>th:first-child,.panel>.table-responsive>.table-bordered>tbody>tr>td:first-child,.panel>.table-responsive>.table-bordered>tbody>tr>th:first-child,.panel>.table-responsive>.table-bordered>tfoot>tr>td:first-child,.panel>.table-responsive>.table-bordered>tfoot>tr>th:first-child,.panel>.table-responsive>.table-bordered>thead>tr>td:first-child,.panel>.table-responsive>.table-bordered>thead>tr>th:first-child{border-left:0}.panel>.table-bordered>tbody>tr>td:last-child,.panel>.table-bordered>tbody>tr>th:last-child,.panel>.table-bordered>tfoot>tr>td:last-child,.panel>.table-bordered>tfoot>tr>th:last-child,.panel>.table-bordered>thead>tr>td:last-child,.panel>.table-bordered>thead>tr>th:last-child,.panel>.table-responsive>.table-bordered>tbody>tr>td:last-child,.panel>.table-responsive>.table-bordered>tbody>tr>th:last-child,.panel>.table-responsive>.table-bordered>tfoot>tr>td:last-child,.panel>.table-responsive>.table-bordered>tfoot>tr>th:last-child,.panel>.table-responsive>.table-bordered>thead>tr>td:last-child,.panel>.table-responsive>.table-bordered>thead>tr>th:last-child{border-right:0}.panel>.table-bordered>tbody>tr:first-child>td,.panel>.table-bordered>tbody>tr:first-child>th,.panel>.table-bordered>thead>tr:first-child>td,.panel>.table-bordered>thead>tr:first-child>th,.panel>.table-responsive>.table-bordered>tbody>tr:first-child>td,.panel>.table-responsive>.table-bordered>tbody>tr:first-child>th,.panel>.table-responsive>.table-bordered>thead>tr:first-child>td,.panel>.table-responsive>.table-bordered>thead>tr:first-child>th{border-bottom:0}.panel>.table-bordered>tbody>tr:last-child>td,.panel>.table-bordered>tbody>tr:last-child>th,.panel>.table-bordered>tfoot>tr:last-child>td,.panel>.table-bordered>tfoot>tr:last-child>th,.panel>.table-responsive>.table-bordered>tbody>tr:last-child>td,.panel>.table-responsive>.table-bordered>tbody>tr:last-child>th,.panel>.table-responsive>.table-bordered>tfoot>tr:last-child>td,.panel>.table-responsive>.table-bordered>tfoot>tr:last-child>th{border-bottom:0}.panel>.table-responsive{margin-bottom:0;border:0}.panel-group{margin-bottom:20px}.panel-group .panel{margin-bottom:0;border-radius:4px}.panel-group .panel+.panel{margin-top:5px}.panel-group .panel-heading{border-bottom:0}.panel-group .panel-heading+.panel-collapse>.list-group,.panel-group .panel-heading+.panel-collapse>.panel-body{border-top:1px solid #ddd}.panel-group .panel-footer{border-top:0}.panel-group .panel-footer+.panel-collapse .panel-body{border-bottom:1px solid #ddd}.panel-default{border-color:#ddd}.panel-default>.panel-heading{color:#333;background-color:#f5f5f5;border-color:#ddd}.panel-default>.panel-heading+.panel-collapse>.panel-body{border-top-color:#ddd}.panel-default>.panel-heading .badge{color:#f5f5f5;background-color:#333}.panel-default>.panel-footer+.panel-collapse>.panel-body{border-bottom-color:#ddd}.panel-primary{border-color:#337ab7}.panel-primary>.panel-heading{color:#fff;background-color:#337ab7;border-color:#337ab7}.panel-primary>.panel-heading+.panel-collapse>.panel-body{border-top-color:#337ab7}.panel-primary>.panel-heading .badge{color:#337ab7;background-color:#fff}.panel-primary>.panel-footer+.panel-collapse>.panel-body{border-bottom-color:#337ab7}.panel-success{border-color:#d6e9c6}.panel-success>.panel-heading{color:#3c763d;background-color:#dff0d8;border-color:#d6e9c6}.panel-success>.panel-heading+.panel-collapse>.panel-body{border-top-color:#d6e9c6}.panel-success>.panel-heading .badge{color:#dff0d8;background-color:#3c763d}.panel-success>.panel-footer+.panel-collapse>.panel-body{border-bottom-color:#d6e9c6}.panel-info{border-color:#bce8f1}.panel-info>.panel-heading{color:#31708f;background-color:#d9edf7;border-color:#bce8f1}.panel-info>.panel-heading+.panel-collapse>.panel-body{border-top-color:#bce8f1}.panel-info>.panel-heading .badge{color:#d9edf7;background-color:#31708f}.panel-info>.panel-footer+.panel-collapse>.panel-body{border-bottom-color:#bce8f1}.panel-warning{border-color:#faebcc}.panel-warning>.panel-heading{color:#8a6d3b;background-color:#fcf8e3;border-color:#faebcc}.panel-warning>.panel-heading+.panel-collapse>.panel-body{border-top-color:#faebcc}.panel-warning>.panel-heading .badge{color:#fcf8e3;background-color:#8a6d3b}.panel-warning>.panel-footer+.panel-collapse>.panel-body{border-bottom-color:#faebcc}.panel-danger{border-color:#ebccd1}.panel-danger>.panel-heading{color:#a94442;background-color:#f2dede;border-color:#ebccd1}.panel-danger>.panel-heading+.panel-collapse>.panel-body{border-top-color:#ebccd1}.panel-danger>.panel-heading .badge{color:#f2dede;background-color:#a94442}.panel-danger>.panel-footer+.panel-collapse>.panel-body{border-bottom-color:#ebccd1}.embed-responsive{position:relative;display:block;height:0;padding:0;overflow:hidden}.embed-responsive .embed-responsive-item,.embed-responsive embed,.embed-responsive iframe,.embed-responsive object,.embed-responsive video{position:absolute;top:0;bottom:0;left:0;width:100%;height:100%;border:0}.embed-responsive-16by9{padding-bottom:56.25%}.embed-responsive-4by3{padding-bottom:75%}.well{min-height:20px;padding:19px;margin-bottom:20px;background-color:#f5f5f5;border:1px solid #e3e3e3;border-radius:4px;-webkit-box-shadow:inset 0 1px 1px rgba(0,0,0,.05);box-shadow:inset 0 1px 1px rgba(0,0,0,.05)}.well blockquote{border-color:#ddd;border-color:rgba(0,0,0,.15)}.well-lg{padding:24px;border-radius:6px}.well-sm{padding:9px;border-radius:3px}.close{float:right;font-size:21px;font-weight:700;line-height:1;color:#000;text-shadow:0 1px 0 #fff;filter:alpha(opacity=20);opacity:.2}.close:focus,.close:hover{color:#000;text-decoration:none;cursor:pointer;filter:alpha(opacity=50);opacity:.5}button.close{-webkit-appearance:none;padding:0;cursor:pointer;background:0 0;border:0}.modal-open{overflow:hidden}.modal{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1050;display:none;overflow:hidden;-webkit-overflow-scrolling:touch;outline:0}.modal.fade .modal-dialog{-webkit-transition:-webkit-transform .3s ease-out;-o-transition:-o-transform .3s ease-out;transition:transform .3s ease-out;-webkit-transform:translate(0,-25%);-ms-transform:translate(0,-25%);-o-transform:translate(0,-25%);transform:translate(0,-25%)}.modal.in .modal-dialog{-webkit-transform:translate(0,0);-ms-transform:translate(0,0);-o-transform:translate(0,0);transform:translate(0,0)}.modal-open .modal{overflow-x:hidden;overflow-y:auto}.modal-dialog{position:relative;width:auto;margin:10px}.modal-content{position:relative;background-color:#fff;-webkit-background-clip:padding-box;background-clip:padding-box;border:1px solid #999;border:1px solid rgba(0,0,0,.2);border-radius:6px;outline:0;-webkit-box-shadow:0 3px 9px rgba(0,0,0,.5);box-shadow:0 3px 9px rgba(0,0,0,.5)}.modal-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1040;background-color:#000}.modal-backdrop.fade{filter:alpha(opacity=0);opacity:0}.modal-backdrop.in{filter:alpha(opacity=50);opacity:.5}.modal-header{min-height:16.43px;padding:15px;border-bottom:1px solid #e5e5e5}.modal-header .close{margin-top:-2px}.modal-title{margin:0;line-height:1.42857143}.modal-body{position:relative;padding:15px}.modal-footer{padding:15px;text-align:right;border-top:1px solid #e5e5e5}.modal-footer .btn+.btn{margin-bottom:0;margin-left:5px}.modal-footer .btn-group .btn+.btn{margin-left:-1px}.modal-footer .btn-block+.btn-block{margin-left:0}.modal-scrollbar-measure{position:absolute;top:-9999px;width:50px;height:50px;overflow:scroll}@media (min-width:768px){.modal-dialog{width:600px;margin:30px auto}.modal-content{-webkit-box-shadow:0 5px 15px rgba(0,0,0,.5);box-shadow:0 5px 15px rgba(0,0,0,.5)}.modal-sm{width:300px}}@media (min-width:992px){.modal-lg{width:900px}}.tooltip{position:absolute;z-index:1070;display:block;font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;font-size:12px;font-style:normal;font-weight:400;line-height:1.42857143;text-align:left;text-align:start;text-decoration:none;text-shadow:none;text-transform:none;letter-spacing:normal;word-break:normal;word-spacing:normal;word-wrap:normal;white-space:normal;filter:alpha(opacity=0);opacity:0;line-break:auto}.tooltip.in{filter:alpha(opacity=90);opacity:.9}.tooltip.top{padding:5px 0;margin-top:-3px}.tooltip.right{padding:0 5px;margin-left:3px}.tooltip.bottom{padding:5px 0;margin-top:3px}.tooltip.left{padding:0 5px;margin-left:-3px}.tooltip-inner{max-width:200px;padding:3px 8px;color:#fff;text-align:center;background-color:#000;border-radius:4px}.tooltip-arrow{position:absolute;width:0;height:0;border-color:transparent;border-style:solid}.tooltip.top .tooltip-arrow{bottom:0;left:50%;margin-left:-5px;border-width:5px 5px 0;border-top-color:#000}.tooltip.top-left .tooltip-arrow{right:5px;bottom:0;margin-bottom:-5px;border-width:5px 5px 0;border-top-color:#000}.tooltip.top-right .tooltip-arrow{bottom:0;left:5px;margin-bottom:-5px;border-width:5px 5px 0;border-top-color:#000}.tooltip.right .tooltip-arrow{top:50%;left:0;margin-top:-5px;border-width:5px 5px 5px 0;border-right-color:#000}.tooltip.left .tooltip-arrow{top:50%;right:0;margin-top:-5px;border-width:5px 0 5px 5px;border-left-color:#000}.tooltip.bottom .tooltip-arrow{top:0;left:50%;margin-left:-5px;border-width:0 5px 5px;border-bottom-color:#000}.tooltip.bottom-left .tooltip-arrow{top:0;right:5px;margin-top:-5px;border-width:0 5px 5px;border-bottom-color:#000}.tooltip.bottom-right .tooltip-arrow{top:0;left:5px;margin-top:-5px;border-width:0 5px 5px;border-bottom-color:#000}.popover{position:absolute;top:0;left:0;z-index:1060;display:none;max-width:276px;padding:1px;font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;font-size:14px;font-style:normal;font-weight:400;line-height:1.42857143;text-align:left;text-align:start;text-decoration:none;text-shadow:none;text-transform:none;letter-spacing:normal;word-break:normal;word-spacing:normal;word-wrap:normal;white-space:normal;background-color:#fff;-webkit-background-clip:padding-box;background-clip:padding-box;border:1px solid #ccc;border:1px solid rgba(0,0,0,.2);border-radius:6px;-webkit-box-shadow:0 5px 10px rgba(0,0,0,.2);box-shadow:0 5px 10px rgba(0,0,0,.2);line-break:auto}.popover.top{margin-top:-10px}.popover.right{margin-left:10px}.popover.bottom{margin-top:10px}.popover.left{margin-left:-10px}.popover-title{padding:8px 14px;margin:0;font-size:14px;background-color:#f7f7f7;border-bottom:1px solid #ebebeb;border-radius:5px 5px 0 0}.popover-content{padding:9px 14px}.popover>.arrow,.popover>.arrow:after{position:absolute;display:block;width:0;height:0;border-color:transparent;border-style:solid}.popover>.arrow{border-width:11px}.popover>.arrow:after{content:"";border-width:10px}.popover.top>.arrow{bottom:-11px;left:50%;margin-left:-11px;border-top-color:#999;border-top-color:rgba(0,0,0,.25);border-bottom-width:0}.popover.top>.arrow:after{bottom:1px;margin-left:-10px;content:" ";border-top-color:#fff;border-bottom-width:0}.popover.right>.arrow{top:50%;left:-11px;margin-top:-11px;border-right-color:#999;border-right-color:rgba(0,0,0,.25);border-left-width:0}.popover.right>.arrow:after{bottom:-10px;left:1px;content:" ";border-right-color:#fff;border-left-width:0}.popover.bottom>.arrow{top:-11px;left:50%;margin-left:-11px;border-top-width:0;border-bottom-color:#999;border-bottom-color:rgba(0,0,0,.25)}.popover.bottom>.arrow:after{top:1px;margin-left:-10px;content:" ";border-top-width:0;border-bottom-color:#fff}.popover.left>.arrow{top:50%;right:-11px;margin-top:-11px;border-right-width:0;border-left-color:#999;border-left-color:rgba(0,0,0,.25)}.popover.left>.arrow:after{right:1px;bottom:-10px;content:" ";border-right-width:0;border-left-color:#fff}.carousel{position:relative}.carousel-inner{position:relative;width:100%;overflow:hidden}.carousel-inner>.item{position:relative;display:none;-webkit-transition:.6s ease-in-out left;-o-transition:.6s ease-in-out left;transition:.6s ease-in-out left}.carousel-inner>.item>a>img,.carousel-inner>.item>img{line-height:1}@media all and (transform-3d),(-webkit-transform-3d){.carousel-inner>.item{-webkit-transition:-webkit-transform .6s ease-in-out;-o-transition:-o-transform .6s ease-in-out;transition:transform .6s ease-in-out;-webkit-backface-visibility:hidden;backface-visibility:hidden;-webkit-perspective:1000px;perspective:1000px}.carousel-inner>.item.active.right,.carousel-inner>.item.next{left:0;-webkit-transform:translate3d(100%,0,0);transform:translate3d(100%,0,0)}.carousel-inner>.item.active.left,.carousel-inner>.item.prev{left:0;-webkit-transform:translate3d(-100%,0,0);transform:translate3d(-100%,0,0)}.carousel-inner>.item.active,.carousel-inner>.item.next.left,.carousel-inner>.item.prev.right{left:0;-webkit-transform:translate3d(0,0,0);transform:translate3d(0,0,0)}}.carousel-inner>.active,.carousel-inner>.next,.carousel-inner>.prev{display:block}.carousel-inner>.active{left:0}.carousel-inner>.next,.carousel-inner>.prev{position:absolute;top:0;width:100%}.carousel-inner>.next{left:100%}.carousel-inner>.prev{left:-100%}.carousel-inner>.next.left,.carousel-inner>.prev.right{left:0}.carousel-inner>.active.left{left:-100%}.carousel-inner>.active.right{left:100%}.carousel-control{position:absolute;top:0;bottom:0;left:0;width:15%;font-size:20px;color:#fff;text-align:center;text-shadow:0 1px 2px rgba(0,0,0,.6);filter:alpha(opacity=50);opacity:.5}.carousel-control.left{background-image:-webkit-linear-gradient(left,rgba(0,0,0,.5) 0,rgba(0,0,0,.0001) 100%);background-image:-o-linear-gradient(left,rgba(0,0,0,.5) 0,rgba(0,0,0,.0001) 100%);background-image:-webkit-gradient(linear,left top,right top,from(rgba(0,0,0,.5)),to(rgba(0,0,0,.0001)));background-image:linear-gradient(to right,rgba(0,0,0,.5) 0,rgba(0,0,0,.0001) 100%);filter:progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);background-repeat:repeat-x}.carousel-control.right{right:0;left:auto;background-image:-webkit-linear-gradient(left,rgba(0,0,0,.0001) 0,rgba(0,0,0,.5) 100%);background-image:-o-linear-gradient(left,rgba(0,0,0,.0001) 0,rgba(0,0,0,.5) 100%);background-image:-webkit-gradient(linear,left top,right top,from(rgba(0,0,0,.0001)),to(rgba(0,0,0,.5)));background-image:linear-gradient(to right,rgba(0,0,0,.0001) 0,rgba(0,0,0,.5) 100%);filter:progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);background-repeat:repeat-x}.carousel-control:focus,.carousel-control:hover{color:#fff;text-decoration:none;filter:alpha(opacity=90);outline:0;opacity:.9}.carousel-control .glyphicon-chevron-left,.carousel-control .glyphicon-chevron-right,.carousel-control .icon-next,.carousel-control .icon-prev{position:absolute;top:50%;z-index:5;display:inline-block;margin-top:-10px}.carousel-control .glyphicon-chevron-left,.carousel-control .icon-prev{left:50%;margin-left:-10px}.carousel-control .glyphicon-chevron-right,.carousel-control .icon-next{right:50%;margin-right:-10px}.carousel-control .icon-next,.carousel-control .icon-prev{width:20px;height:20px;font-family:serif;line-height:1}.carousel-control .icon-prev:before{content:'\2039'}.carousel-control .icon-next:before{content:'\203a'}.carousel-indicators{position:absolute;bottom:10px;left:50%;z-index:15;width:60%;padding-left:0;margin-left:-30%;text-align:center;list-style:none}.carousel-indicators li{display:inline-block;width:10px;height:10px;margin:1px;text-indent:-999px;cursor:pointer;background-color:#000\9;background-color:rgba(0,0,0,0);border:1px solid #fff;border-radius:10px}.carousel-indicators .active{width:12px;height:12px;margin:0;background-color:#fff}.carousel-caption{position:absolute;right:15%;bottom:20px;left:15%;z-index:10;padding-top:20px;padding-bottom:20px;color:#fff;text-align:center;text-shadow:0 1px 2px rgba(0,0,0,.6)}.carousel-caption .btn{text-shadow:none}@media screen and (min-width:768px){.carousel-control .glyphicon-chevron-left,.carousel-control .glyphicon-chevron-right,.carousel-control .icon-next,.carousel-control .icon-prev{width:30px;height:30px;margin-top:-15px;font-size:30px}.carousel-control .glyphicon-chevron-left,.carousel-control .icon-prev{margin-left:-15px}.carousel-control .glyphicon-chevron-right,.carousel-control .icon-next{margin-right:-15px}.carousel-caption{right:20%;left:20%;padding-bottom:30px}.carousel-indicators{bottom:20px}}.btn-group-vertical>.btn-group:after,.btn-group-vertical>.btn-group:before,.btn-toolbar:after,.btn-toolbar:before,.clearfix:after,.clearfix:before,.container-fluid:after,.container-fluid:before,.container:after,.container:before,.dl-horizontal dd:after,.dl-horizontal dd:before,.form-horizontal .form-group:after,.form-horizontal .form-group:before,.modal-footer:after,.modal-footer:before,.nav:after,.nav:before,.navbar-collapse:after,.navbar-collapse:before,.navbar-header:after,.navbar-header:before,.navbar:after,.navbar:before,.pager:after,.pager:before,.panel-body:after,.panel-body:before,.row:after,.row:before{display:table;content:" "}.btn-group-vertical>.btn-group:after,.btn-toolbar:after,.clearfix:after,.container-fluid:after,.container:after,.dl-horizontal dd:after,.form-horizontal .form-group:after,.modal-footer:after,.nav:after,.navbar-collapse:after,.navbar-header:after,.navbar:after,.pager:after,.panel-body:after,.row:after{clear:both}.center-block{display:block;margin-right:auto;margin-left:auto}.pull-right{float:right!important}.pull-left{float:left!important}.hide{display:none!important}.show{display:block!important}.invisible{visibility:hidden}.text-hide{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0}.hidden{display:none!important}.affix{position:fixed}@-ms-viewport{width:device-width}.visible-lg,.visible-md,.visible-sm,.visible-xs{display:none!important}.visible-lg-block,.visible-lg-inline,.visible-lg-inline-block,.visible-md-block,.visible-md-inline,.visible-md-inline-block,.visible-sm-block,.visible-sm-inline,.visible-sm-inline-block,.visible-xs-block,.visible-xs-inline,.visible-xs-inline-block{display:none!important}@media (max-width:767px){.visible-xs{display:block!important}table.visible-xs{display:table!important}tr.visible-xs{display:table-row!important}td.visible-xs,th.visible-xs{display:table-cell!important}}@media (max-width:767px){.visible-xs-block{display:block!important}}@media (max-width:767px){.visible-xs-inline{display:inline!important}}@media (max-width:767px){.visible-xs-inline-block{display:inline-block!important}}@media (min-width:768px) and (max-width:991px){.visible-sm{display:block!important}table.visible-sm{display:table!important}tr.visible-sm{display:table-row!important}td.visible-sm,th.visible-sm{display:table-cell!important}}@media (min-width:768px) and (max-width:991px){.visible-sm-block{display:block!important}}@media (min-width:768px) and (max-width:991px){.visible-sm-inline{display:inline!important}}@media (min-width:768px) and (max-width:991px){.visible-sm-inline-block{display:inline-block!important}}@media (min-width:992px) and (max-width:1199px){.visible-md{display:block!important}table.visible-md{display:table!important}tr.visible-md{display:table-row!important}td.visible-md,th.visible-md{display:table-cell!important}}@media (min-width:992px) and (max-width:1199px){.visible-md-block{display:block!important}}@media (min-width:992px) and (max-width:1199px){.visible-md-inline{display:inline!important}}@media (min-width:992px) and (max-width:1199px){.visible-md-inline-block{display:inline-block!important}}@media (min-width:1200px){.visible-lg{display:block!important}table.visible-lg{display:table!important}tr.visible-lg{display:table-row!important}td.visible-lg,th.visible-lg{display:table-cell!important}}@media (min-width:1200px){.visible-lg-block{display:block!important}}@media (min-width:1200px){.visible-lg-inline{display:inline!important}}@media (min-width:1200px){.visible-lg-inline-block{display:inline-block!important}}@media (max-width:767px){.hidden-xs{display:none!important}}@media (min-width:768px) and (max-width:991px){.hidden-sm{display:none!important}}@media (min-width:992px) and (max-width:1199px){.hidden-md{display:none!important}}@media (min-width:1200px){.hidden-lg{display:none!important}}.visible-print{display:none!important}@media print{.visible-print{display:block!important}table.visible-print{display:table!important}tr.visible-print{display:table-row!important}td.visible-print,th.visible-print{display:table-cell!important}}.visible-print-block{display:none!important}@media print{.visible-print-block{display:block!important}}.visible-print-inline{display:none!important}@media print{.visible-print-inline{display:inline!important}}.visible-print-inline-block{display:none!important}@media print{.visible-print-inline-block{display:inline-block!important}}@media print{.hidden-print{display:none!important}}
</style>
<script>/*!
 * Bootstrap v3.3.5 (http://getbootstrap.com)
 * Copyright 2011-2015 Twitter, Inc.
 * Licensed under the MIT license
 */
if("undefined"==typeof jQuery)throw new Error("Bootstrap's JavaScript requires jQuery");+function(a){"use strict";var b=a.fn.jquery.split(" ")[0].split(".");if(b[0]<2&&b[1]<9||1==b[0]&&9==b[1]&&b[2]<1)throw new Error("Bootstrap's JavaScript requires jQuery version 1.9.1 or higher")}(jQuery),+function(a){"use strict";function b(){var a=document.createElement("bootstrap"),b={WebkitTransition:"webkitTransitionEnd",MozTransition:"transitionend",OTransition:"oTransitionEnd otransitionend",transition:"transitionend"};for(var c in b)if(void 0!==a.style[c])return{end:b[c]};return!1}a.fn.emulateTransitionEnd=function(b){var c=!1,d=this;a(this).one("bsTransitionEnd",function(){c=!0});var e=function(){c||a(d).trigger(a.support.transition.end)};return setTimeout(e,b),this},a(function(){a.support.transition=b(),a.support.transition&&(a.event.special.bsTransitionEnd={bindType:a.support.transition.end,delegateType:a.support.transition.end,handle:function(b){return a(b.target).is(this)?b.handleObj.handler.apply(this,arguments):void 0}})})}(jQuery),+function(a){"use strict";function b(b){return this.each(function(){var c=a(this),e=c.data("bs.alert");e||c.data("bs.alert",e=new d(this)),"string"==typeof b&&e[b].call(c)})}var c='[data-dismiss="alert"]',d=function(b){a(b).on("click",c,this.close)};d.VERSION="3.3.5",d.TRANSITION_DURATION=150,d.prototype.close=function(b){function c(){g.detach().trigger("closed.bs.alert").remove()}var e=a(this),f=e.attr("data-target");f||(f=e.attr("href"),f=f&&f.replace(/.*(?=#[^\s]*$)/,""));var g=a(f);b&&b.preventDefault(),g.length||(g=e.closest(".alert")),g.trigger(b=a.Event("close.bs.alert")),b.isDefaultPrevented()||(g.removeClass("in"),a.support.transition&&g.hasClass("fade")?g.one("bsTransitionEnd",c).emulateTransitionEnd(d.TRANSITION_DURATION):c())};var e=a.fn.alert;a.fn.alert=b,a.fn.alert.Constructor=d,a.fn.alert.noConflict=function(){return a.fn.alert=e,this},a(document).on("click.bs.alert.data-api",c,d.prototype.close)}(jQuery),+function(a){"use strict";function b(b){return this.each(function(){var d=a(this),e=d.data("bs.button"),f="object"==typeof b&&b;e||d.data("bs.button",e=new c(this,f)),"toggle"==b?e.toggle():b&&e.setState(b)})}var c=function(b,d){this.$element=a(b),this.options=a.extend({},c.DEFAULTS,d),this.isLoading=!1};c.VERSION="3.3.5",c.DEFAULTS={loadingText:"loading..."},c.prototype.setState=function(b){var c="disabled",d=this.$element,e=d.is("input")?"val":"html",f=d.data();b+="Text",null==f.resetText&&d.data("resetText",d[e]()),setTimeout(a.proxy(function(){d[e](null==f[b]?this.options[b]:f[b]),"loadingText"==b?(this.isLoading=!0,d.addClass(c).attr(c,c)):this.isLoading&&(this.isLoading=!1,d.removeClass(c).removeAttr(c))},this),0)},c.prototype.toggle=function(){var a=!0,b=this.$element.closest('[data-toggle="buttons"]');if(b.length){var c=this.$element.find("input");"radio"==c.prop("type")?(c.prop("checked")&&(a=!1),b.find(".active").removeClass("active"),this.$element.addClass("active")):"checkbox"==c.prop("type")&&(c.prop("checked")!==this.$element.hasClass("active")&&(a=!1),this.$element.toggleClass("active")),c.prop("checked",this.$element.hasClass("active")),a&&c.trigger("change")}else this.$element.attr("aria-pressed",!this.$element.hasClass("active")),this.$element.toggleClass("active")};var d=a.fn.button;a.fn.button=b,a.fn.button.Constructor=c,a.fn.button.noConflict=function(){return a.fn.button=d,this},a(document).on("click.bs.button.data-api",'[data-toggle^="button"]',function(c){var d=a(c.target);d.hasClass("btn")||(d=d.closest(".btn")),b.call(d,"toggle"),a(c.target).is('input[type="radio"]')||a(c.target).is('input[type="checkbox"]')||c.preventDefault()}).on("focus.bs.button.data-api blur.bs.button.data-api",'[data-toggle^="button"]',function(b){a(b.target).closest(".btn").toggleClass("focus",/^focus(in)?$/.test(b.type))})}(jQuery),+function(a){"use strict";function b(b){return this.each(function(){var d=a(this),e=d.data("bs.carousel"),f=a.extend({},c.DEFAULTS,d.data(),"object"==typeof b&&b),g="string"==typeof b?b:f.slide;e||d.data("bs.carousel",e=new c(this,f)),"number"==typeof b?e.to(b):g?e[g]():f.interval&&e.pause().cycle()})}var c=function(b,c){this.$element=a(b),this.$indicators=this.$element.find(".carousel-indicators"),this.options=c,this.paused=null,this.sliding=null,this.interval=null,this.$active=null,this.$items=null,this.options.keyboard&&this.$element.on("keydown.bs.carousel",a.proxy(this.keydown,this)),"hover"==this.options.pause&&!("ontouchstart"in document.documentElement)&&this.$element.on("mouseenter.bs.carousel",a.proxy(this.pause,this)).on("mouseleave.bs.carousel",a.proxy(this.cycle,this))};c.VERSION="3.3.5",c.TRANSITION_DURATION=600,c.DEFAULTS={interval:5e3,pause:"hover",wrap:!0,keyboard:!0},c.prototype.keydown=function(a){if(!/input|textarea/i.test(a.target.tagName)){switch(a.which){case 37:this.prev();break;case 39:this.next();break;default:return}a.preventDefault()}},c.prototype.cycle=function(b){return b||(this.paused=!1),this.interval&&clearInterval(this.interval),this.options.interval&&!this.paused&&(this.interval=setInterval(a.proxy(this.next,this),this.options.interval)),this},c.prototype.getItemIndex=function(a){return this.$items=a.parent().children(".item"),this.$items.index(a||this.$active)},c.prototype.getItemForDirection=function(a,b){var c=this.getItemIndex(b),d="prev"==a&&0===c||"next"==a&&c==this.$items.length-1;if(d&&!this.options.wrap)return b;var e="prev"==a?-1:1,f=(c+e)%this.$items.length;return this.$items.eq(f)},c.prototype.to=function(a){var b=this,c=this.getItemIndex(this.$active=this.$element.find(".item.active"));return a>this.$items.length-1||0>a?void 0:this.sliding?this.$element.one("slid.bs.carousel",function(){b.to(a)}):c==a?this.pause().cycle():this.slide(a>c?"next":"prev",this.$items.eq(a))},c.prototype.pause=function(b){return b||(this.paused=!0),this.$element.find(".next, .prev").length&&a.support.transition&&(this.$element.trigger(a.support.transition.end),this.cycle(!0)),this.interval=clearInterval(this.interval),this},c.prototype.next=function(){return this.sliding?void 0:this.slide("next")},c.prototype.prev=function(){return this.sliding?void 0:this.slide("prev")},c.prototype.slide=function(b,d){var e=this.$element.find(".item.active"),f=d||this.getItemForDirection(b,e),g=this.interval,h="next"==b?"left":"right",i=this;if(f.hasClass("active"))return this.sliding=!1;var j=f[0],k=a.Event("slide.bs.carousel",{relatedTarget:j,direction:h});if(this.$element.trigger(k),!k.isDefaultPrevented()){if(this.sliding=!0,g&&this.pause(),this.$indicators.length){this.$indicators.find(".active").removeClass("active");var l=a(this.$indicators.children()[this.getItemIndex(f)]);l&&l.addClass("active")}var m=a.Event("slid.bs.carousel",{relatedTarget:j,direction:h});return a.support.transition&&this.$element.hasClass("slide")?(f.addClass(b),f[0].offsetWidth,e.addClass(h),f.addClass(h),e.one("bsTransitionEnd",function(){f.removeClass([b,h].join(" ")).addClass("active"),e.removeClass(["active",h].join(" ")),i.sliding=!1,setTimeout(function(){i.$element.trigger(m)},0)}).emulateTransitionEnd(c.TRANSITION_DURATION)):(e.removeClass("active"),f.addClass("active"),this.sliding=!1,this.$element.trigger(m)),g&&this.cycle(),this}};var d=a.fn.carousel;a.fn.carousel=b,a.fn.carousel.Constructor=c,a.fn.carousel.noConflict=function(){return a.fn.carousel=d,this};var e=function(c){var d,e=a(this),f=a(e.attr("data-target")||(d=e.attr("href"))&&d.replace(/.*(?=#[^\s]+$)/,""));if(f.hasClass("carousel")){var g=a.extend({},f.data(),e.data()),h=e.attr("data-slide-to");h&&(g.interval=!1),b.call(f,g),h&&f.data("bs.carousel").to(h),c.preventDefault()}};a(document).on("click.bs.carousel.data-api","[data-slide]",e).on("click.bs.carousel.data-api","[data-slide-to]",e),a(window).on("load",function(){a('[data-ride="carousel"]').each(function(){var c=a(this);b.call(c,c.data())})})}(jQuery),+function(a){"use strict";function b(b){var c,d=b.attr("data-target")||(c=b.attr("href"))&&c.replace(/.*(?=#[^\s]+$)/,"");return a(d)}function c(b){return this.each(function(){var c=a(this),e=c.data("bs.collapse"),f=a.extend({},d.DEFAULTS,c.data(),"object"==typeof b&&b);!e&&f.toggle&&/show|hide/.test(b)&&(f.toggle=!1),e||c.data("bs.collapse",e=new d(this,f)),"string"==typeof b&&e[b]()})}var d=function(b,c){this.$element=a(b),this.options=a.extend({},d.DEFAULTS,c),this.$trigger=a('[data-toggle="collapse"][href="#'+b.id+'"],[data-toggle="collapse"][data-target="#'+b.id+'"]'),this.transitioning=null,this.options.parent?this.$parent=this.getParent():this.addAriaAndCollapsedClass(this.$element,this.$trigger),this.options.toggle&&this.toggle()};d.VERSION="3.3.5",d.TRANSITION_DURATION=350,d.DEFAULTS={toggle:!0},d.prototype.dimension=function(){var a=this.$element.hasClass("width");return a?"width":"height"},d.prototype.show=function(){if(!this.transitioning&&!this.$element.hasClass("in")){var b,e=this.$parent&&this.$parent.children(".panel").children(".in, .collapsing");if(!(e&&e.length&&(b=e.data("bs.collapse"),b&&b.transitioning))){var f=a.Event("show.bs.collapse");if(this.$element.trigger(f),!f.isDefaultPrevented()){e&&e.length&&(c.call(e,"hide"),b||e.data("bs.collapse",null));var g=this.dimension();this.$element.removeClass("collapse").addClass("collapsing")[g](0).attr("aria-expanded",!0),this.$trigger.removeClass("collapsed").attr("aria-expanded",!0),this.transitioning=1;var h=function(){this.$element.removeClass("collapsing").addClass("collapse in")[g](""),this.transitioning=0,this.$element.trigger("shown.bs.collapse")};if(!a.support.transition)return h.call(this);var i=a.camelCase(["scroll",g].join("-"));this.$element.one("bsTransitionEnd",a.proxy(h,this)).emulateTransitionEnd(d.TRANSITION_DURATION)[g](this.$element[0][i])}}}},d.prototype.hide=function(){if(!this.transitioning&&this.$element.hasClass("in")){var b=a.Event("hide.bs.collapse");if(this.$element.trigger(b),!b.isDefaultPrevented()){var c=this.dimension();this.$element[c](this.$element[c]())[0].offsetHeight,this.$element.addClass("collapsing").removeClass("collapse in").attr("aria-expanded",!1),this.$trigger.addClass("collapsed").attr("aria-expanded",!1),this.transitioning=1;var e=function(){this.transitioning=0,this.$element.removeClass("collapsing").addClass("collapse").trigger("hidden.bs.collapse")};return a.support.transition?void this.$element[c](0).one("bsTransitionEnd",a.proxy(e,this)).emulateTransitionEnd(d.TRANSITION_DURATION):e.call(this)}}},d.prototype.toggle=function(){this[this.$element.hasClass("in")?"hide":"show"]()},d.prototype.getParent=function(){return a(this.options.parent).find('[data-toggle="collapse"][data-parent="'+this.options.parent+'"]').each(a.proxy(function(c,d){var e=a(d);this.addAriaAndCollapsedClass(b(e),e)},this)).end()},d.prototype.addAriaAndCollapsedClass=function(a,b){var c=a.hasClass("in");a.attr("aria-expanded",c),b.toggleClass("collapsed",!c).attr("aria-expanded",c)};var e=a.fn.collapse;a.fn.collapse=c,a.fn.collapse.Constructor=d,a.fn.collapse.noConflict=function(){return a.fn.collapse=e,this},a(document).on("click.bs.collapse.data-api",'[data-toggle="collapse"]',function(d){var e=a(this);e.attr("data-target")||d.preventDefault();var f=b(e),g=f.data("bs.collapse"),h=g?"toggle":e.data();c.call(f,h)})}(jQuery),+function(a){"use strict";function b(b){var c=b.attr("data-target");c||(c=b.attr("href"),c=c&&/#[A-Za-z]/.test(c)&&c.replace(/.*(?=#[^\s]*$)/,""));var d=c&&a(c);return d&&d.length?d:b.parent()}function c(c){c&&3===c.which||(a(e).remove(),a(f).each(function(){var d=a(this),e=b(d),f={relatedTarget:this};e.hasClass("open")&&(c&&"click"==c.type&&/input|textarea/i.test(c.target.tagName)&&a.contains(e[0],c.target)||(e.trigger(c=a.Event("hide.bs.dropdown",f)),c.isDefaultPrevented()||(d.attr("aria-expanded","false"),e.removeClass("open").trigger("hidden.bs.dropdown",f))))}))}function d(b){return this.each(function(){var c=a(this),d=c.data("bs.dropdown");d||c.data("bs.dropdown",d=new g(this)),"string"==typeof b&&d[b].call(c)})}var e=".dropdown-backdrop",f='[data-toggle="dropdown"]',g=function(b){a(b).on("click.bs.dropdown",this.toggle)};g.VERSION="3.3.5",g.prototype.toggle=function(d){var e=a(this);if(!e.is(".disabled, :disabled")){var f=b(e),g=f.hasClass("open");if(c(),!g){"ontouchstart"in document.documentElement&&!f.closest(".navbar-nav").length&&a(document.createElement("div")).addClass("dropdown-backdrop").insertAfter(a(this)).on("click",c);var h={relatedTarget:this};if(f.trigger(d=a.Event("show.bs.dropdown",h)),d.isDefaultPrevented())return;e.trigger("focus").attr("aria-expanded","true"),f.toggleClass("open").trigger("shown.bs.dropdown",h)}return!1}},g.prototype.keydown=function(c){if(/(38|40|27|32)/.test(c.which)&&!/input|textarea/i.test(c.target.tagName)){var d=a(this);if(c.preventDefault(),c.stopPropagation(),!d.is(".disabled, :disabled")){var e=b(d),g=e.hasClass("open");if(!g&&27!=c.which||g&&27==c.which)return 27==c.which&&e.find(f).trigger("focus"),d.trigger("click");var h=" li:not(.disabled):visible a",i=e.find(".dropdown-menu"+h);if(i.length){var j=i.index(c.target);38==c.which&&j>0&&j--,40==c.which&&j<i.length-1&&j++,~j||(j=0),i.eq(j).trigger("focus")}}}};var h=a.fn.dropdown;a.fn.dropdown=d,a.fn.dropdown.Constructor=g,a.fn.dropdown.noConflict=function(){return a.fn.dropdown=h,this},a(document).on("click.bs.dropdown.data-api",c).on("click.bs.dropdown.data-api",".dropdown form",function(a){a.stopPropagation()}).on("click.bs.dropdown.data-api",f,g.prototype.toggle).on("keydown.bs.dropdown.data-api",f,g.prototype.keydown).on("keydown.bs.dropdown.data-api",".dropdown-menu",g.prototype.keydown)}(jQuery),+function(a){"use strict";function b(b,d){return this.each(function(){var e=a(this),f=e.data("bs.modal"),g=a.extend({},c.DEFAULTS,e.data(),"object"==typeof b&&b);f||e.data("bs.modal",f=new c(this,g)),"string"==typeof b?f[b](d):g.show&&f.show(d)})}var c=function(b,c){this.options=c,this.$body=a(document.body),this.$element=a(b),this.$dialog=this.$element.find(".modal-dialog"),this.$backdrop=null,this.isShown=null,this.originalBodyPad=null,this.scrollbarWidth=0,this.ignoreBackdropClick=!1,this.options.remote&&this.$element.find(".modal-content").load(this.options.remote,a.proxy(function(){this.$element.trigger("loaded.bs.modal")},this))};c.VERSION="3.3.5",c.TRANSITION_DURATION=300,c.BACKDROP_TRANSITION_DURATION=150,c.DEFAULTS={backdrop:!0,keyboard:!0,show:!0},c.prototype.toggle=function(a){return this.isShown?this.hide():this.show(a)},c.prototype.show=function(b){var d=this,e=a.Event("show.bs.modal",{relatedTarget:b});this.$element.trigger(e),this.isShown||e.isDefaultPrevented()||(this.isShown=!0,this.checkScrollbar(),this.setScrollbar(),this.$body.addClass("modal-open"),this.escape(),this.resize(),this.$element.on("click.dismiss.bs.modal",'[data-dismiss="modal"]',a.proxy(this.hide,this)),this.$dialog.on("mousedown.dismiss.bs.modal",function(){d.$element.one("mouseup.dismiss.bs.modal",function(b){a(b.target).is(d.$element)&&(d.ignoreBackdropClick=!0)})}),this.backdrop(function(){var e=a.support.transition&&d.$element.hasClass("fade");d.$element.parent().length||d.$element.appendTo(d.$body),d.$element.show().scrollTop(0),d.adjustDialog(),e&&d.$element[0].offsetWidth,d.$element.addClass("in"),d.enforceFocus();var f=a.Event("shown.bs.modal",{relatedTarget:b});e?d.$dialog.one("bsTransitionEnd",function(){d.$element.trigger("focus").trigger(f)}).emulateTransitionEnd(c.TRANSITION_DURATION):d.$element.trigger("focus").trigger(f)}))},c.prototype.hide=function(b){b&&b.preventDefault(),b=a.Event("hide.bs.modal"),this.$element.trigger(b),this.isShown&&!b.isDefaultPrevented()&&(this.isShown=!1,this.escape(),this.resize(),a(document).off("focusin.bs.modal"),this.$element.removeClass("in").off("click.dismiss.bs.modal").off("mouseup.dismiss.bs.modal"),this.$dialog.off("mousedown.dismiss.bs.modal"),a.support.transition&&this.$element.hasClass("fade")?this.$element.one("bsTransitionEnd",a.proxy(this.hideModal,this)).emulateTransitionEnd(c.TRANSITION_DURATION):this.hideModal())},c.prototype.enforceFocus=function(){a(document).off("focusin.bs.modal").on("focusin.bs.modal",a.proxy(function(a){this.$element[0]===a.target||this.$element.has(a.target).length||this.$element.trigger("focus")},this))},c.prototype.escape=function(){this.isShown&&this.options.keyboard?this.$element.on("keydown.dismiss.bs.modal",a.proxy(function(a){27==a.which&&this.hide()},this)):this.isShown||this.$element.off("keydown.dismiss.bs.modal")},c.prototype.resize=function(){this.isShown?a(window).on("resize.bs.modal",a.proxy(this.handleUpdate,this)):a(window).off("resize.bs.modal")},c.prototype.hideModal=function(){var a=this;this.$element.hide(),this.backdrop(function(){a.$body.removeClass("modal-open"),a.resetAdjustments(),a.resetScrollbar(),a.$element.trigger("hidden.bs.modal")})},c.prototype.removeBackdrop=function(){this.$backdrop&&this.$backdrop.remove(),this.$backdrop=null},c.prototype.backdrop=function(b){var d=this,e=this.$element.hasClass("fade")?"fade":"";if(this.isShown&&this.options.backdrop){var f=a.support.transition&&e;if(this.$backdrop=a(document.createElement("div")).addClass("modal-backdrop "+e).appendTo(this.$body),this.$element.on("click.dismiss.bs.modal",a.proxy(function(a){return this.ignoreBackdropClick?void(this.ignoreBackdropClick=!1):void(a.target===a.currentTarget&&("static"==this.options.backdrop?this.$element[0].focus():this.hide()))},this)),f&&this.$backdrop[0].offsetWidth,this.$backdrop.addClass("in"),!b)return;f?this.$backdrop.one("bsTransitionEnd",b).emulateTransitionEnd(c.BACKDROP_TRANSITION_DURATION):b()}else if(!this.isShown&&this.$backdrop){this.$backdrop.removeClass("in");var g=function(){d.removeBackdrop(),b&&b()};a.support.transition&&this.$element.hasClass("fade")?this.$backdrop.one("bsTransitionEnd",g).emulateTransitionEnd(c.BACKDROP_TRANSITION_DURATION):g()}else b&&b()},c.prototype.handleUpdate=function(){this.adjustDialog()},c.prototype.adjustDialog=function(){var a=this.$element[0].scrollHeight>document.documentElement.clientHeight;this.$element.css({paddingLeft:!this.bodyIsOverflowing&&a?this.scrollbarWidth:"",paddingRight:this.bodyIsOverflowing&&!a?this.scrollbarWidth:""})},c.prototype.resetAdjustments=function(){this.$element.css({paddingLeft:"",paddingRight:""})},c.prototype.checkScrollbar=function(){var a=window.innerWidth;if(!a){var b=document.documentElement.getBoundingClientRect();a=b.right-Math.abs(b.left)}this.bodyIsOverflowing=document.body.clientWidth<a,this.scrollbarWidth=this.measureScrollbar()},c.prototype.setScrollbar=function(){var a=parseInt(this.$body.css("padding-right")||0,10);this.originalBodyPad=document.body.style.paddingRight||"",this.bodyIsOverflowing&&this.$body.css("padding-right",a+this.scrollbarWidth)},c.prototype.resetScrollbar=function(){this.$body.css("padding-right",this.originalBodyPad)},c.prototype.measureScrollbar=function(){var a=document.createElement("div");a.className="modal-scrollbar-measure",this.$body.append(a);var b=a.offsetWidth-a.clientWidth;return this.$body[0].removeChild(a),b};var d=a.fn.modal;a.fn.modal=b,a.fn.modal.Constructor=c,a.fn.modal.noConflict=function(){return a.fn.modal=d,this},a(document).on("click.bs.modal.data-api",'[data-toggle="modal"]',function(c){var d=a(this),e=d.attr("href"),f=a(d.attr("data-target")||e&&e.replace(/.*(?=#[^\s]+$)/,"")),g=f.data("bs.modal")?"toggle":a.extend({remote:!/#/.test(e)&&e},f.data(),d.data());d.is("a")&&c.preventDefault(),f.one("show.bs.modal",function(a){a.isDefaultPrevented()||f.one("hidden.bs.modal",function(){d.is(":visible")&&d.trigger("focus")})}),b.call(f,g,this)})}(jQuery),+function(a){"use strict";function b(b){return this.each(function(){var d=a(this),e=d.data("bs.tooltip"),f="object"==typeof b&&b;(e||!/destroy|hide/.test(b))&&(e||d.data("bs.tooltip",e=new c(this,f)),"string"==typeof b&&e[b]())})}var c=function(a,b){this.type=null,this.options=null,this.enabled=null,this.timeout=null,this.hoverState=null,this.$element=null,this.inState=null,this.init("tooltip",a,b)};c.VERSION="3.3.5",c.TRANSITION_DURATION=150,c.DEFAULTS={animation:!0,placement:"top",selector:!1,template:'<div class="tooltip" role="tooltip"><div class="tooltip-arrow"></div><div class="tooltip-inner"></div></div>',trigger:"hover focus",title:"",delay:0,html:!1,container:!1,viewport:{selector:"body",padding:0}},c.prototype.init=function(b,c,d){if(this.enabled=!0,this.type=b,this.$element=a(c),this.options=this.getOptions(d),this.$viewport=this.options.viewport&&a(a.isFunction(this.options.viewport)?this.options.viewport.call(this,this.$element):this.options.viewport.selector||this.options.viewport),this.inState={click:!1,hover:!1,focus:!1},this.$element[0]instanceof document.constructor&&!this.options.selector)throw new Error("`selector` option must be specified when initializing "+this.type+" on the window.document object!");for(var e=this.options.trigger.split(" "),f=e.length;f--;){var g=e[f];if("click"==g)this.$element.on("click."+this.type,this.options.selector,a.proxy(this.toggle,this));else if("manual"!=g){var h="hover"==g?"mouseenter":"focusin",i="hover"==g?"mouseleave":"focusout";this.$element.on(h+"."+this.type,this.options.selector,a.proxy(this.enter,this)),this.$element.on(i+"."+this.type,this.options.selector,a.proxy(this.leave,this))}}this.options.selector?this._options=a.extend({},this.options,{trigger:"manual",selector:""}):this.fixTitle()},c.prototype.getDefaults=function(){return c.DEFAULTS},c.prototype.getOptions=function(b){return b=a.extend({},this.getDefaults(),this.$element.data(),b),b.delay&&"number"==typeof b.delay&&(b.delay={show:b.delay,hide:b.delay}),b},c.prototype.getDelegateOptions=function(){var b={},c=this.getDefaults();return this._options&&a.each(this._options,function(a,d){c[a]!=d&&(b[a]=d)}),b},c.prototype.enter=function(b){var c=b instanceof this.constructor?b:a(b.currentTarget).data("bs."+this.type);return c||(c=new this.constructor(b.currentTarget,this.getDelegateOptions()),a(b.currentTarget).data("bs."+this.type,c)),b instanceof a.Event&&(c.inState["focusin"==b.type?"focus":"hover"]=!0),c.tip().hasClass("in")||"in"==c.hoverState?void(c.hoverState="in"):(clearTimeout(c.timeout),c.hoverState="in",c.options.delay&&c.options.delay.show?void(c.timeout=setTimeout(function(){"in"==c.hoverState&&c.show()},c.options.delay.show)):c.show())},c.prototype.isInStateTrue=function(){for(var a in this.inState)if(this.inState[a])return!0;return!1},c.prototype.leave=function(b){var c=b instanceof this.constructor?b:a(b.currentTarget).data("bs."+this.type);return c||(c=new this.constructor(b.currentTarget,this.getDelegateOptions()),a(b.currentTarget).data("bs."+this.type,c)),b instanceof a.Event&&(c.inState["focusout"==b.type?"focus":"hover"]=!1),c.isInStateTrue()?void 0:(clearTimeout(c.timeout),c.hoverState="out",c.options.delay&&c.options.delay.hide?void(c.timeout=setTimeout(function(){"out"==c.hoverState&&c.hide()},c.options.delay.hide)):c.hide())},c.prototype.show=function(){var b=a.Event("show.bs."+this.type);if(this.hasContent()&&this.enabled){this.$element.trigger(b);var d=a.contains(this.$element[0].ownerDocument.documentElement,this.$element[0]);if(b.isDefaultPrevented()||!d)return;var e=this,f=this.tip(),g=this.getUID(this.type);this.setContent(),f.attr("id",g),this.$element.attr("aria-describedby",g),this.options.animation&&f.addClass("fade");var h="function"==typeof this.options.placement?this.options.placement.call(this,f[0],this.$element[0]):this.options.placement,i=/\s?auto?\s?/i,j=i.test(h);j&&(h=h.replace(i,"")||"top"),f.detach().css({top:0,left:0,display:"block"}).addClass(h).data("bs."+this.type,this),this.options.container?f.appendTo(this.options.container):f.insertAfter(this.$element),this.$element.trigger("inserted.bs."+this.type);var k=this.getPosition(),l=f[0].offsetWidth,m=f[0].offsetHeight;if(j){var n=h,o=this.getPosition(this.$viewport);h="bottom"==h&&k.bottom+m>o.bottom?"top":"top"==h&&k.top-m<o.top?"bottom":"right"==h&&k.right+l>o.width?"left":"left"==h&&k.left-l<o.left?"right":h,f.removeClass(n).addClass(h)}var p=this.getCalculatedOffset(h,k,l,m);this.applyPlacement(p,h);var q=function(){var a=e.hoverState;e.$element.trigger("shown.bs."+e.type),e.hoverState=null,"out"==a&&e.leave(e)};a.support.transition&&this.$tip.hasClass("fade")?f.one("bsTransitionEnd",q).emulateTransitionEnd(c.TRANSITION_DURATION):q()}},c.prototype.applyPlacement=function(b,c){var d=this.tip(),e=d[0].offsetWidth,f=d[0].offsetHeight,g=parseInt(d.css("margin-top"),10),h=parseInt(d.css("margin-left"),10);isNaN(g)&&(g=0),isNaN(h)&&(h=0),b.top+=g,b.left+=h,a.offset.setOffset(d[0],a.extend({using:function(a){d.css({top:Math.round(a.top),left:Math.round(a.left)})}},b),0),d.addClass("in");var i=d[0].offsetWidth,j=d[0].offsetHeight;"top"==c&&j!=f&&(b.top=b.top+f-j);var k=this.getViewportAdjustedDelta(c,b,i,j);k.left?b.left+=k.left:b.top+=k.top;var l=/top|bottom/.test(c),m=l?2*k.left-e+i:2*k.top-f+j,n=l?"offsetWidth":"offsetHeight";d.offset(b),this.replaceArrow(m,d[0][n],l)},c.prototype.replaceArrow=function(a,b,c){this.arrow().css(c?"left":"top",50*(1-a/b)+"%").css(c?"top":"left","")},c.prototype.setContent=function(){var a=this.tip(),b=this.getTitle();a.find(".tooltip-inner")[this.options.html?"html":"text"](b),a.removeClass("fade in top bottom left right")},c.prototype.hide=function(b){function d(){"in"!=e.hoverState&&f.detach(),e.$element.removeAttr("aria-describedby").trigger("hidden.bs."+e.type),b&&b()}var e=this,f=a(this.$tip),g=a.Event("hide.bs."+this.type);return this.$element.trigger(g),g.isDefaultPrevented()?void 0:(f.removeClass("in"),a.support.transition&&f.hasClass("fade")?f.one("bsTransitionEnd",d).emulateTransitionEnd(c.TRANSITION_DURATION):d(),this.hoverState=null,this)},c.prototype.fixTitle=function(){var a=this.$element;(a.attr("title")||"string"!=typeof a.attr("data-original-title"))&&a.attr("data-original-title",a.attr("title")||"").attr("title","")},c.prototype.hasContent=function(){return this.getTitle()},c.prototype.getPosition=function(b){b=b||this.$element;var c=b[0],d="BODY"==c.tagName,e=c.getBoundingClientRect();null==e.width&&(e=a.extend({},e,{width:e.right-e.left,height:e.bottom-e.top}));var f=d?{top:0,left:0}:b.offset(),g={scroll:d?document.documentElement.scrollTop||document.body.scrollTop:b.scrollTop()},h=d?{width:a(window).width(),height:a(window).height()}:null;return a.extend({},e,g,h,f)},c.prototype.getCalculatedOffset=function(a,b,c,d){return"bottom"==a?{top:b.top+b.height,left:b.left+b.width/2-c/2}:"top"==a?{top:b.top-d,left:b.left+b.width/2-c/2}:"left"==a?{top:b.top+b.height/2-d/2,left:b.left-c}:{top:b.top+b.height/2-d/2,left:b.left+b.width}},c.prototype.getViewportAdjustedDelta=function(a,b,c,d){var e={top:0,left:0};if(!this.$viewport)return e;var f=this.options.viewport&&this.options.viewport.padding||0,g=this.getPosition(this.$viewport);if(/right|left/.test(a)){var h=b.top-f-g.scroll,i=b.top+f-g.scroll+d;h<g.top?e.top=g.top-h:i>g.top+g.height&&(e.top=g.top+g.height-i)}else{var j=b.left-f,k=b.left+f+c;j<g.left?e.left=g.left-j:k>g.right&&(e.left=g.left+g.width-k)}return e},c.prototype.getTitle=function(){var a,b=this.$element,c=this.options;return a=b.attr("data-original-title")||("function"==typeof c.title?c.title.call(b[0]):c.title)},c.prototype.getUID=function(a){do a+=~~(1e6*Math.random());while(document.getElementById(a));return a},c.prototype.tip=function(){if(!this.$tip&&(this.$tip=a(this.options.template),1!=this.$tip.length))throw new Error(this.type+" `template` option must consist of exactly 1 top-level element!");return this.$tip},c.prototype.arrow=function(){return this.$arrow=this.$arrow||this.tip().find(".tooltip-arrow")},c.prototype.enable=function(){this.enabled=!0},c.prototype.disable=function(){this.enabled=!1},c.prototype.toggleEnabled=function(){this.enabled=!this.enabled},c.prototype.toggle=function(b){var c=this;b&&(c=a(b.currentTarget).data("bs."+this.type),c||(c=new this.constructor(b.currentTarget,this.getDelegateOptions()),a(b.currentTarget).data("bs."+this.type,c))),b?(c.inState.click=!c.inState.click,c.isInStateTrue()?c.enter(c):c.leave(c)):c.tip().hasClass("in")?c.leave(c):c.enter(c)},c.prototype.destroy=function(){var a=this;clearTimeout(this.timeout),this.hide(function(){a.$element.off("."+a.type).removeData("bs."+a.type),a.$tip&&a.$tip.detach(),a.$tip=null,a.$arrow=null,a.$viewport=null})};var d=a.fn.tooltip;a.fn.tooltip=b,a.fn.tooltip.Constructor=c,a.fn.tooltip.noConflict=function(){return a.fn.tooltip=d,this}}(jQuery),+function(a){"use strict";function b(b){return this.each(function(){var d=a(this),e=d.data("bs.popover"),f="object"==typeof b&&b;(e||!/destroy|hide/.test(b))&&(e||d.data("bs.popover",e=new c(this,f)),"string"==typeof b&&e[b]())})}var c=function(a,b){this.init("popover",a,b)};if(!a.fn.tooltip)throw new Error("Popover requires tooltip.js");c.VERSION="3.3.5",c.DEFAULTS=a.extend({},a.fn.tooltip.Constructor.DEFAULTS,{placement:"right",trigger:"click",content:"",template:'<div class="popover" role="tooltip"><div class="arrow"></div><h3 class="popover-title"></h3><div class="popover-content"></div></div>'}),c.prototype=a.extend({},a.fn.tooltip.Constructor.prototype),c.prototype.constructor=c,c.prototype.getDefaults=function(){return c.DEFAULTS},c.prototype.setContent=function(){var a=this.tip(),b=this.getTitle(),c=this.getContent();a.find(".popover-title")[this.options.html?"html":"text"](b),a.find(".popover-content").children().detach().end()[this.options.html?"string"==typeof c?"html":"append":"text"](c),a.removeClass("fade top bottom left right in"),a.find(".popover-title").html()||a.find(".popover-title").hide()},c.prototype.hasContent=function(){return this.getTitle()||this.getContent()},c.prototype.getContent=function(){var a=this.$element,b=this.options;return a.attr("data-content")||("function"==typeof b.content?b.content.call(a[0]):b.content)},c.prototype.arrow=function(){return this.$arrow=this.$arrow||this.tip().find(".arrow")};var d=a.fn.popover;a.fn.popover=b,a.fn.popover.Constructor=c,a.fn.popover.noConflict=function(){return a.fn.popover=d,this}}(jQuery),+function(a){"use strict";function b(c,d){this.$body=a(document.body),this.$scrollElement=a(a(c).is(document.body)?window:c),this.options=a.extend({},b.DEFAULTS,d),this.selector=(this.options.target||"")+" .nav li > a",this.offsets=[],this.targets=[],this.activeTarget=null,this.scrollHeight=0,this.$scrollElement.on("scroll.bs.scrollspy",a.proxy(this.process,this)),this.refresh(),this.process()}function c(c){return this.each(function(){var d=a(this),e=d.data("bs.scrollspy"),f="object"==typeof c&&c;e||d.data("bs.scrollspy",e=new b(this,f)),"string"==typeof c&&e[c]()})}b.VERSION="3.3.5",b.DEFAULTS={offset:10},b.prototype.getScrollHeight=function(){return this.$scrollElement[0].scrollHeight||Math.max(this.$body[0].scrollHeight,document.documentElement.scrollHeight)},b.prototype.refresh=function(){var b=this,c="offset",d=0;this.offsets=[],this.targets=[],this.scrollHeight=this.getScrollHeight(),a.isWindow(this.$scrollElement[0])||(c="position",d=this.$scrollElement.scrollTop()),this.$body.find(this.selector).map(function(){var b=a(this),e=b.data("target")||b.attr("href"),f=/^#./.test(e)&&a(e);return f&&f.length&&f.is(":visible")&&[[f[c]().top+d,e]]||null}).sort(function(a,b){return a[0]-b[0]}).each(function(){b.offsets.push(this[0]),b.targets.push(this[1])})},b.prototype.process=function(){var a,b=this.$scrollElement.scrollTop()+this.options.offset,c=this.getScrollHeight(),d=this.options.offset+c-this.$scrollElement.height(),e=this.offsets,f=this.targets,g=this.activeTarget;if(this.scrollHeight!=c&&this.refresh(),b>=d)return g!=(a=f[f.length-1])&&this.activate(a);if(g&&b<e[0])return this.activeTarget=null,this.clear();for(a=e.length;a--;)g!=f[a]&&b>=e[a]&&(void 0===e[a+1]||b<e[a+1])&&this.activate(f[a])},b.prototype.activate=function(b){this.activeTarget=b,this.clear();var c=this.selector+'[data-target="'+b+'"],'+this.selector+'[href="'+b+'"]',d=a(c).parents("li").addClass("active");d.parent(".dropdown-menu").length&&(d=d.closest("li.dropdown").addClass("active")),
d.trigger("activate.bs.scrollspy")},b.prototype.clear=function(){a(this.selector).parentsUntil(this.options.target,".active").removeClass("active")};var d=a.fn.scrollspy;a.fn.scrollspy=c,a.fn.scrollspy.Constructor=b,a.fn.scrollspy.noConflict=function(){return a.fn.scrollspy=d,this},a(window).on("load.bs.scrollspy.data-api",function(){a('[data-spy="scroll"]').each(function(){var b=a(this);c.call(b,b.data())})})}(jQuery),+function(a){"use strict";function b(b){return this.each(function(){var d=a(this),e=d.data("bs.tab");e||d.data("bs.tab",e=new c(this)),"string"==typeof b&&e[b]()})}var c=function(b){this.element=a(b)};c.VERSION="3.3.5",c.TRANSITION_DURATION=150,c.prototype.show=function(){var b=this.element,c=b.closest("ul:not(.dropdown-menu)"),d=b.data("target");if(d||(d=b.attr("href"),d=d&&d.replace(/.*(?=#[^\s]*$)/,"")),!b.parent("li").hasClass("active")){var e=c.find(".active:last a"),f=a.Event("hide.bs.tab",{relatedTarget:b[0]}),g=a.Event("show.bs.tab",{relatedTarget:e[0]});if(e.trigger(f),b.trigger(g),!g.isDefaultPrevented()&&!f.isDefaultPrevented()){var h=a(d);this.activate(b.closest("li"),c),this.activate(h,h.parent(),function(){e.trigger({type:"hidden.bs.tab",relatedTarget:b[0]}),b.trigger({type:"shown.bs.tab",relatedTarget:e[0]})})}}},c.prototype.activate=function(b,d,e){function f(){g.removeClass("active").find("> .dropdown-menu > .active").removeClass("active").end().find('[data-toggle="tab"]').attr("aria-expanded",!1),b.addClass("active").find('[data-toggle="tab"]').attr("aria-expanded",!0),h?(b[0].offsetWidth,b.addClass("in")):b.removeClass("fade"),b.parent(".dropdown-menu").length&&b.closest("li.dropdown").addClass("active").end().find('[data-toggle="tab"]').attr("aria-expanded",!0),e&&e()}var g=d.find("> .active"),h=e&&a.support.transition&&(g.length&&g.hasClass("fade")||!!d.find("> .fade").length);g.length&&h?g.one("bsTransitionEnd",f).emulateTransitionEnd(c.TRANSITION_DURATION):f(),g.removeClass("in")};var d=a.fn.tab;a.fn.tab=b,a.fn.tab.Constructor=c,a.fn.tab.noConflict=function(){return a.fn.tab=d,this};var e=function(c){c.preventDefault(),b.call(a(this),"show")};a(document).on("click.bs.tab.data-api",'[data-toggle="tab"]',e).on("click.bs.tab.data-api",'[data-toggle="pill"]',e)}(jQuery),+function(a){"use strict";function b(b){return this.each(function(){var d=a(this),e=d.data("bs.affix"),f="object"==typeof b&&b;e||d.data("bs.affix",e=new c(this,f)),"string"==typeof b&&e[b]()})}var c=function(b,d){this.options=a.extend({},c.DEFAULTS,d),this.$target=a(this.options.target).on("scroll.bs.affix.data-api",a.proxy(this.checkPosition,this)).on("click.bs.affix.data-api",a.proxy(this.checkPositionWithEventLoop,this)),this.$element=a(b),this.affixed=null,this.unpin=null,this.pinnedOffset=null,this.checkPosition()};c.VERSION="3.3.5",c.RESET="affix affix-top affix-bottom",c.DEFAULTS={offset:0,target:window},c.prototype.getState=function(a,b,c,d){var e=this.$target.scrollTop(),f=this.$element.offset(),g=this.$target.height();if(null!=c&&"top"==this.affixed)return c>e?"top":!1;if("bottom"==this.affixed)return null!=c?e+this.unpin<=f.top?!1:"bottom":a-d>=e+g?!1:"bottom";var h=null==this.affixed,i=h?e:f.top,j=h?g:b;return null!=c&&c>=e?"top":null!=d&&i+j>=a-d?"bottom":!1},c.prototype.getPinnedOffset=function(){if(this.pinnedOffset)return this.pinnedOffset;this.$element.removeClass(c.RESET).addClass("affix");var a=this.$target.scrollTop(),b=this.$element.offset();return this.pinnedOffset=b.top-a},c.prototype.checkPositionWithEventLoop=function(){setTimeout(a.proxy(this.checkPosition,this),1)},c.prototype.checkPosition=function(){if(this.$element.is(":visible")){var b=this.$element.height(),d=this.options.offset,e=d.top,f=d.bottom,g=Math.max(a(document).height(),a(document.body).height());"object"!=typeof d&&(f=e=d),"function"==typeof e&&(e=d.top(this.$element)),"function"==typeof f&&(f=d.bottom(this.$element));var h=this.getState(g,b,e,f);if(this.affixed!=h){null!=this.unpin&&this.$element.css("top","");var i="affix"+(h?"-"+h:""),j=a.Event(i+".bs.affix");if(this.$element.trigger(j),j.isDefaultPrevented())return;this.affixed=h,this.unpin="bottom"==h?this.getPinnedOffset():null,this.$element.removeClass(c.RESET).addClass(i).trigger(i.replace("affix","affixed")+".bs.affix")}"bottom"==h&&this.$element.offset({top:g-b-f})}};var d=a.fn.affix;a.fn.affix=b,a.fn.affix.Constructor=c,a.fn.affix.noConflict=function(){return a.fn.affix=d,this},a(window).on("load",function(){a('[data-spy="affix"]').each(function(){var c=a(this),d=c.data();d.offset=d.offset||{},null!=d.offsetBottom&&(d.offset.bottom=d.offsetBottom),null!=d.offsetTop&&(d.offset.top=d.offsetTop),b.call(c,d)})})}(jQuery);</script>
<script>/**
* @preserve HTML5 Shiv 3.7.2 | @afarkas @jdalton @jon_neal @rem | MIT/GPL2 Licensed
*/
// Only run this code in IE 8
if (!!window.navigator.userAgent.match("MSIE 8")) {
!function(a,b){function c(a,b){var c=a.createElement("p"),d=a.getElementsByTagName("head")[0]||a.documentElement;return c.innerHTML="x<style>"+b+"</style>",d.insertBefore(c.lastChild,d.firstChild)}function d(){var a=t.elements;return"string"==typeof a?a.split(" "):a}function e(a,b){var c=t.elements;"string"!=typeof c&&(c=c.join(" ")),"string"!=typeof a&&(a=a.join(" ")),t.elements=c+" "+a,j(b)}function f(a){var b=s[a[q]];return b||(b={},r++,a[q]=r,s[r]=b),b}function g(a,c,d){if(c||(c=b),l)return c.createElement(a);d||(d=f(c));var e;return e=d.cache[a]?d.cache[a].cloneNode():p.test(a)?(d.cache[a]=d.createElem(a)).cloneNode():d.createElem(a),!e.canHaveChildren||o.test(a)||e.tagUrn?e:d.frag.appendChild(e)}function h(a,c){if(a||(a=b),l)return a.createDocumentFragment();c=c||f(a);for(var e=c.frag.cloneNode(),g=0,h=d(),i=h.length;i>g;g++)e.createElement(h[g]);return e}function i(a,b){b.cache||(b.cache={},b.createElem=a.createElement,b.createFrag=a.createDocumentFragment,b.frag=b.createFrag()),a.createElement=function(c){return t.shivMethods?g(c,a,b):b.createElem(c)},a.createDocumentFragment=Function("h,f","return function(){var n=f.cloneNode(),c=n.createElement;h.shivMethods&&("+d().join().replace(/[\w\-:]+/g,function(a){return b.createElem(a),b.frag.createElement(a),'c("'+a+'")'})+");return n}")(t,b.frag)}function j(a){a||(a=b);var d=f(a);return!t.shivCSS||k||d.hasCSS||(d.hasCSS=!!c(a,"article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}template{display:none}")),l||i(a,d),a}var k,l,m="3.7.2",n=a.html5||{},o=/^<|^(?:button|map|select|textarea|object|iframe|option|optgroup)$/i,p=/^(?:a|b|code|div|fieldset|h1|h2|h3|h4|h5|h6|i|label|li|ol|p|q|span|strong|style|table|tbody|td|th|tr|ul)$/i,q="_html5shiv",r=0,s={};!function(){try{var a=b.createElement("a");a.innerHTML="<xyz></xyz>",k="hidden"in a,l=1==a.childNodes.length||function(){b.createElement("a");var a=b.createDocumentFragment();return"undefined"==typeof a.cloneNode||"undefined"==typeof a.createDocumentFragment||"undefined"==typeof a.createElement}()}catch(c){k=!0,l=!0}}();var t={elements:n.elements||"abbr article aside audio bdi canvas data datalist details dialog figcaption figure footer header hgroup main mark meter nav output picture progress section summary template time video",version:m,shivCSS:n.shivCSS!==!1,supportsUnknownElements:l,shivMethods:n.shivMethods!==!1,type:"default",shivDocument:j,createElement:g,createDocumentFragment:h,addElements:e};a.html5=t,j(b)}(this,document);
};
</script>
<script>/*! Respond.js v1.4.2: min/max-width media query polyfill * Copyright 2013 Scott Jehl
 * Licensed under https://github.com/scottjehl/Respond/blob/master/LICENSE-MIT
 *  */

// Only run this code in IE 8
if (!!window.navigator.userAgent.match("MSIE 8")) {
!function(a){"use strict";a.matchMedia=a.matchMedia||function(a){var b,c=a.documentElement,d=c.firstElementChild||c.firstChild,e=a.createElement("body"),f=a.createElement("div");return f.id="mq-test-1",f.style.cssText="position:absolute;top:-100em",e.style.background="none",e.appendChild(f),function(a){return f.innerHTML='&shy;<style media="'+a+'"> #mq-test-1 { width: 42px; }</style>',c.insertBefore(e,d),b=42===f.offsetWidth,c.removeChild(e),{matches:b,media:a}}}(a.document)}(this),function(a){"use strict";function b(){u(!0)}var c={};a.respond=c,c.update=function(){};var d=[],e=function(){var b=!1;try{b=new a.XMLHttpRequest}catch(c){b=new a.ActiveXObject("Microsoft.XMLHTTP")}return function(){return b}}(),f=function(a,b){var c=e();c&&(c.open("GET",a,!0),c.onreadystatechange=function(){4!==c.readyState||200!==c.status&&304!==c.status||b(c.responseText)},4!==c.readyState&&c.send(null))};if(c.ajax=f,c.queue=d,c.regex={media:/@media[^\{]+\{([^\{\}]*\{[^\}\{]*\})+/gi,keyframes:/@(?:\-(?:o|moz|webkit)\-)?keyframes[^\{]+\{(?:[^\{\}]*\{[^\}\{]*\})+[^\}]*\}/gi,urls:/(url\()['"]?([^\/\)'"][^:\)'"]+)['"]?(\))/g,findStyles:/@media *([^\{]+)\{([\S\s]+?)$/,only:/(only\s+)?([a-zA-Z]+)\s?/,minw:/\([\s]*min\-width\s*:[\s]*([\s]*[0-9\.]+)(px|em)[\s]*\)/,maxw:/\([\s]*max\-width\s*:[\s]*([\s]*[0-9\.]+)(px|em)[\s]*\)/},c.mediaQueriesSupported=a.matchMedia&&null!==a.matchMedia("only all")&&a.matchMedia("only all").matches,!c.mediaQueriesSupported){var g,h,i,j=a.document,k=j.documentElement,l=[],m=[],n=[],o={},p=30,q=j.getElementsByTagName("head")[0]||k,r=j.getElementsByTagName("base")[0],s=q.getElementsByTagName("link"),t=function(){var a,b=j.createElement("div"),c=j.body,d=k.style.fontSize,e=c&&c.style.fontSize,f=!1;return b.style.cssText="position:absolute;font-size:1em;width:1em",c||(c=f=j.createElement("body"),c.style.background="none"),k.style.fontSize="100%",c.style.fontSize="100%",c.appendChild(b),f&&k.insertBefore(c,k.firstChild),a=b.offsetWidth,f?k.removeChild(c):c.removeChild(b),k.style.fontSize=d,e&&(c.style.fontSize=e),a=i=parseFloat(a)},u=function(b){var c="clientWidth",d=k[c],e="CSS1Compat"===j.compatMode&&d||j.body[c]||d,f={},o=s[s.length-1],r=(new Date).getTime();if(b&&g&&p>r-g)return a.clearTimeout(h),h=a.setTimeout(u,p),void 0;g=r;for(var v in l)if(l.hasOwnProperty(v)){var w=l[v],x=w.minw,y=w.maxw,z=null===x,A=null===y,B="em";x&&(x=parseFloat(x)*(x.indexOf(B)>-1?i||t():1)),y&&(y=parseFloat(y)*(y.indexOf(B)>-1?i||t():1)),w.hasquery&&(z&&A||!(z||e>=x)||!(A||y>=e))||(f[w.media]||(f[w.media]=[]),f[w.media].push(m[w.rules]))}for(var C in n)n.hasOwnProperty(C)&&n[C]&&n[C].parentNode===q&&q.removeChild(n[C]);n.length=0;for(var D in f)if(f.hasOwnProperty(D)){var E=j.createElement("style"),F=f[D].join("\n");E.type="text/css",E.media=D,q.insertBefore(E,o.nextSibling),E.styleSheet?E.styleSheet.cssText=F:E.appendChild(j.createTextNode(F)),n.push(E)}},v=function(a,b,d){var e=a.replace(c.regex.keyframes,"").match(c.regex.media),f=e&&e.length||0;b=b.substring(0,b.lastIndexOf("/"));var g=function(a){return a.replace(c.regex.urls,"$1"+b+"$2$3")},h=!f&&d;b.length&&(b+="/"),h&&(f=1);for(var i=0;f>i;i++){var j,k,n,o;h?(j=d,m.push(g(a))):(j=e[i].match(c.regex.findStyles)&&RegExp.$1,m.push(RegExp.$2&&g(RegExp.$2))),n=j.split(","),o=n.length;for(var p=0;o>p;p++)k=n[p],l.push({media:k.split("(")[0].match(c.regex.only)&&RegExp.$2||"all",rules:m.length-1,hasquery:k.indexOf("(")>-1,minw:k.match(c.regex.minw)&&parseFloat(RegExp.$1)+(RegExp.$2||""),maxw:k.match(c.regex.maxw)&&parseFloat(RegExp.$1)+(RegExp.$2||"")})}u()},w=function(){if(d.length){var b=d.shift();f(b.href,function(c){v(c,b.href,b.media),o[b.href]=!0,a.setTimeout(function(){w()},0)})}},x=function(){for(var b=0;b<s.length;b++){var c=s[b],e=c.href,f=c.media,g=c.rel&&"stylesheet"===c.rel.toLowerCase();e&&g&&!o[e]&&(c.styleSheet&&c.styleSheet.rawCssText?(v(c.styleSheet.rawCssText,e,f),o[e]=!0):(!/^([a-zA-Z:]*\/\/)/.test(e)&&!r||e.replace(RegExp.$1,"").split("/")[0]===a.location.host)&&("//"===e.substring(0,2)&&(e=a.location.protocol+e),d.push({href:e,media:f})))}w()};x(),c.update=x,c.getEmValue=t,a.addEventListener?a.addEventListener("resize",b,!1):a.attachEvent&&a.attachEvent("onresize",b)}}(this);
};
</script>
<script>

/**
 * jQuery Plugin: Sticky Tabs
 *
 * @author Aidan Lister <aidan@php.net>
 * adapted by Ruben Arslan to activate parent tabs too
 * http://www.aidanlister.com/2014/03/persisting-the-tab-state-in-bootstrap/
 */
(function($) {
  "use strict";
  $.fn.rmarkdownStickyTabs = function() {
    var context = this;
    // Show the tab corresponding with the hash in the URL, or the first tab
    var showStuffFromHash = function() {
      var hash = window.location.hash;
      var selector = hash ? 'a[href="' + hash + '"]' : 'li.active > a';
      var $selector = $(selector, context);
      if($selector.data('toggle') === "tab") {
        $selector.tab('show');
        // walk up the ancestors of this element, show any hidden tabs
        $selector.parents('.section.tabset').each(function(i, elm) {
          var link = $('a[href="#' + $(elm).attr('id') + '"]');
          if(link.data('toggle') === "tab") {
            link.tab("show");
          }
        });
      }
    };


    // Set the correct tab when the page loads
    showStuffFromHash(context);

    // Set the correct tab when a user uses their back/forward button
    $(window).on('hashchange', function() {
      showStuffFromHash(context);
    });

    // Change the URL when tabs are clicked
    $('a', context).on('click', function(e) {
      history.pushState(null, null, this.href);
      showStuffFromHash(context);
    });

    return this;
  };
}(jQuery));

window.buildTabsets = function(tocID) {

  // build a tabset from a section div with the .tabset class
  function buildTabset(tabset) {

    // check for fade and pills options
    var fade = tabset.hasClass("tabset-fade");
    var pills = tabset.hasClass("tabset-pills");
    var navClass = pills ? "nav-pills" : "nav-tabs";

    // determine the heading level of the tabset and tabs
    var match = tabset.attr('class').match(/level(\d) /);
    if (match === null)
      return;
    var tabsetLevel = Number(match[1]);
    var tabLevel = tabsetLevel + 1;

    // find all subheadings immediately below
    var tabs = tabset.find("div.section.level" + tabLevel);
    if (!tabs.length)
      return;

    // create tablist and tab-content elements
    var tabList = $('<ul class="nav ' + navClass + '" role="tablist"></ul>');
    $(tabs[0]).before(tabList);
    var tabContent = $('<div class="tab-content"></div>');
    $(tabs[0]).before(tabContent);

    // build the tabset
    var activeTab = 0;
    tabs.each(function(i) {

      // get the tab div
      var tab = $(tabs[i]);

      // get the id then sanitize it for use with bootstrap tabs
      var id = tab.attr('id');

      // see if this is marked as the active tab
      if (tab.hasClass('active'))
        activeTab = i;

      // remove any table of contents entries associated with
      // this ID (since we'll be removing the heading element)
      $("div#" + tocID + " li a[href='#" + id + "']").parent().remove();

      // sanitize the id for use with bootstrap tabs
      id = id.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_');
      tab.attr('id', id);

      // get the heading element within it, grab it's text, then remove it
      var heading = tab.find('h' + tabLevel + ':first');
      var headingText = heading.html();
      heading.remove();

      // build and append the tab list item
      var a = $('<a role="tab" data-toggle="tab">' + headingText + '</a>');
      a.attr('href', '#' + id);
      a.attr('aria-controls', id);
      var li = $('<li role="presentation"></li>');
      li.append(a);
      tabList.append(li);

      // set it's attributes
      tab.attr('role', 'tabpanel');
      tab.addClass('tab-pane');
      tab.addClass('tabbed-pane');
      if (fade)
        tab.addClass('fade');

      // move it into the tab content div
      tab.detach().appendTo(tabContent);
    });

    // set active tab
    $(tabList.children('li')[activeTab]).addClass('active');
    var active = $(tabContent.children('div.section')[activeTab]);
    active.addClass('active');
    if (fade)
      active.addClass('in');

    if (tabset.hasClass("tabset-sticky"))
      tabset.rmarkdownStickyTabs();
  }

  // convert section divs with the .tabset class to tabsets
  var tabsets = $("div.section.tabset");
  tabsets.each(function(i) {
    buildTabset($(tabsets[i]));
  });
};

</script>
<style type="text/css">.hljs-literal {
color: #990073;
}
.hljs-number {
color: #099;
}
.hljs-comment {
color: #998;
font-style: italic;
}
.hljs-keyword {
color: #900;
font-weight: bold;
}
.hljs-string {
color: #d14;
}
</style>
<script src="data:application/javascript;base64,/*! highlight.js v9.12.0 | BSD3 License | git.io/hljslicense */
!function(e){var n="object"==typeof window&&window||"object"==typeof self&&self;"undefined"!=typeof exports?e(exports):n&&(n.hljs=e({}),"function"==typeof define&&define.amd&&define([],function(){return n.hljs}))}(function(e){function n(e){return e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/>/g,"&gt;")}function t(e){return e.nodeName.toLowerCase()}function r(e,n){var t=e&&e.exec(n);return t&&0===t.index}function a(e){return k.test(e)}function i(e){var n,t,r,i,o=e.className+" ";if(o+=e.parentNode?e.parentNode.className:"",t=B.exec(o))return w(t[1])?t[1]:"no-highlight";for(o=o.split(/\s+/),n=0,r=o.length;r>n;n++)if(i=o[n],a(i)||w(i))return i}function o(e){var n,t={},r=Array.prototype.slice.call(arguments,1);for(n in e)t[n]=e[n];return r.forEach(function(e){for(n in e)t[n]=e[n]}),t}function u(e){var n=[];return function r(e,a){for(var i=e.firstChild;i;i=i.nextSibling)3===i.nodeType?a+=i.nodeValue.length:1===i.nodeType&&(n.push({event:"start",offset:a,node:i}),a=r(i,a),t(i).match(/br|hr|img|input/)||n.push({event:"stop",offset:a,node:i}));return a}(e,0),n}function c(e,r,a){function i(){return e.length&&r.length?e[0].offset!==r[0].offset?e[0].offset<r[0].offset?e:r:"start"===r[0].event?e:r:e.length?e:r}function o(e){function r(e){return" "+e.nodeName+'="'+n(e.value).replace('"',"&quot;")+'"'}s+="<"+t(e)+E.map.call(e.attributes,r).join("")+">"}function u(e){s+="</"+t(e)+">"}function c(e){("start"===e.event?o:u)(e.node)}for(var l=0,s="",f=[];e.length||r.length;){var g=i();if(s+=n(a.substring(l,g[0].offset)),l=g[0].offset,g===e){f.reverse().forEach(u);do c(g.splice(0,1)[0]),g=i();while(g===e&&g.length&&g[0].offset===l);f.reverse().forEach(o)}else"start"===g[0].event?f.push(g[0].node):f.pop(),c(g.splice(0,1)[0])}return s+n(a.substr(l))}function l(e){return e.v&&!e.cached_variants&&(e.cached_variants=e.v.map(function(n){return o(e,{v:null},n)})),e.cached_variants||e.eW&&[o(e)]||[e]}function s(e){function n(e){return e&&e.source||e}function t(t,r){return new RegExp(n(t),"m"+(e.cI?"i":"")+(r?"g":""))}function r(a,i){if(!a.compiled){if(a.compiled=!0,a.k=a.k||a.bK,a.k){var o={},u=function(n,t){e.cI&&(t=t.toLowerCase()),t.split(" ").forEach(function(e){var t=e.split("|");o[t[0]]=[n,t[1]?Number(t[1]):1]})};"string"==typeof a.k?u("keyword",a.k):x(a.k).forEach(function(e){u(e,a.k[e])}),a.k=o}a.lR=t(a.l||/\w+/,!0),i&&(a.bK&&(a.b="\\b("+a.bK.split(" ").join("|")+")\\b"),a.b||(a.b=/\B|\b/),a.bR=t(a.b),a.e||a.eW||(a.e=/\B|\b/),a.e&&(a.eR=t(a.e)),a.tE=n(a.e)||"",a.eW&&i.tE&&(a.tE+=(a.e?"|":"")+i.tE)),a.i&&(a.iR=t(a.i)),null==a.r&&(a.r=1),a.c||(a.c=[]),a.c=Array.prototype.concat.apply([],a.c.map(function(e){return l("self"===e?a:e)})),a.c.forEach(function(e){r(e,a)}),a.starts&&r(a.starts,i);var c=a.c.map(function(e){return e.bK?"\\.?("+e.b+")\\.?":e.b}).concat([a.tE,a.i]).map(n).filter(Boolean);a.t=c.length?t(c.join("|"),!0):{exec:function(){return null}}}}r(e)}function f(e,t,a,i){function o(e,n){var t,a;for(t=0,a=n.c.length;a>t;t++)if(r(n.c[t].bR,e))return n.c[t]}function u(e,n){if(r(e.eR,n)){for(;e.endsParent&&e.parent;)e=e.parent;return e}return e.eW?u(e.parent,n):void 0}function c(e,n){return!a&&r(n.iR,e)}function l(e,n){var t=N.cI?n[0].toLowerCase():n[0];return e.k.hasOwnProperty(t)&&e.k[t]}function p(e,n,t,r){var a=r?"":I.classPrefix,i='<span class="'+a,o=t?"":C;return i+=e+'">',i+n+o}function h(){var e,t,r,a;if(!E.k)return n(k);for(a="",t=0,E.lR.lastIndex=0,r=E.lR.exec(k);r;)a+=n(k.substring(t,r.index)),e=l(E,r),e?(B+=e[1],a+=p(e[0],n(r[0]))):a+=n(r[0]),t=E.lR.lastIndex,r=E.lR.exec(k);return a+n(k.substr(t))}function d(){var e="string"==typeof E.sL;if(e&&!y[E.sL])return n(k);var t=e?f(E.sL,k,!0,x[E.sL]):g(k,E.sL.length?E.sL:void 0);return E.r>0&&(B+=t.r),e&&(x[E.sL]=t.top),p(t.language,t.value,!1,!0)}function b(){L+=null!=E.sL?d():h(),k=""}function v(e){L+=e.cN?p(e.cN,"",!0):"",E=Object.create(e,{parent:{value:E}})}function m(e,n){if(k+=e,null==n)return b(),0;var t=o(n,E);if(t)return t.skip?k+=n:(t.eB&&(k+=n),b(),t.rB||t.eB||(k=n)),v(t,n),t.rB?0:n.length;var r=u(E,n);if(r){var a=E;a.skip?k+=n:(a.rE||a.eE||(k+=n),b(),a.eE&&(k=n));do E.cN&&(L+=C),E.skip||(B+=E.r),E=E.parent;while(E!==r.parent);return r.starts&&v(r.starts,""),a.rE?0:n.length}if(c(n,E))throw new Error('Illegal lexeme "'+n+'" for mode "'+(E.cN||"<unnamed>")+'"');return k+=n,n.length||1}var N=w(e);if(!N)throw new Error('Unknown language: "'+e+'"');s(N);var R,E=i||N,x={},L="";for(R=E;R!==N;R=R.parent)R.cN&&(L=p(R.cN,"",!0)+L);var k="",B=0;try{for(var M,j,O=0;;){if(E.t.lastIndex=O,M=E.t.exec(t),!M)break;j=m(t.substring(O,M.index),M[0]),O=M.index+j}for(m(t.substr(O)),R=E;R.parent;R=R.parent)R.cN&&(L+=C);return{r:B,value:L,language:e,top:E}}catch(T){if(T.message&&-1!==T.message.indexOf("Illegal"))return{r:0,value:n(t)};throw T}}function g(e,t){t=t||I.languages||x(y);var r={r:0,value:n(e)},a=r;return t.filter(w).forEach(function(n){var t=f(n,e,!1);t.language=n,t.r>a.r&&(a=t),t.r>r.r&&(a=r,r=t)}),a.language&&(r.second_best=a),r}function p(e){return I.tabReplace||I.useBR?e.replace(M,function(e,n){return I.useBR&&"\n"===e?"<br>":I.tabReplace?n.replace(/\t/g,I.tabReplace):""}):e}function h(e,n,t){var r=n?L[n]:t,a=[e.trim()];return e.match(/\bhljs\b/)||a.push("hljs"),-1===e.indexOf(r)&&a.push(r),a.join(" ").trim()}function d(e){var n,t,r,o,l,s=i(e);a(s)||(I.useBR?(n=document.createElementNS("http://www.w3.org/1999/xhtml","div"),n.innerHTML=e.innerHTML.replace(/\n/g,"").replace(/<br[ \/]*>/g,"\n")):n=e,l=n.textContent,r=s?f(s,l,!0):g(l),t=u(n),t.length&&(o=document.createElementNS("http://www.w3.org/1999/xhtml","div"),o.innerHTML=r.value,r.value=c(t,u(o),l)),r.value=p(r.value),e.innerHTML=r.value,e.className=h(e.className,s,r.language),e.result={language:r.language,re:r.r},r.second_best&&(e.second_best={language:r.second_best.language,re:r.second_best.r}))}function b(e){I=o(I,e)}function v(){if(!v.called){v.called=!0;var e=document.querySelectorAll("pre code");E.forEach.call(e,d)}}function m(){addEventListener("DOMContentLoaded",v,!1),addEventListener("load",v,!1)}function N(n,t){var r=y[n]=t(e);r.aliases&&r.aliases.forEach(function(e){L[e]=n})}function R(){return x(y)}function w(e){return e=(e||"").toLowerCase(),y[e]||y[L[e]]}var E=[],x=Object.keys,y={},L={},k=/^(no-?highlight|plain|text)$/i,B=/\blang(?:uage)?-([\w-]+)\b/i,M=/((^(<[^>]+>|\t|)+|(?:\n)))/gm,C="</span>",I={classPrefix:"hljs-",tabReplace:null,useBR:!1,languages:void 0};return e.highlight=f,e.highlightAuto=g,e.fixMarkup=p,e.highlightBlock=d,e.configure=b,e.initHighlighting=v,e.initHighlightingOnLoad=m,e.registerLanguage=N,e.listLanguages=R,e.getLanguage=w,e.inherit=o,e.IR="[a-zA-Z]\\w*",e.UIR="[a-zA-Z_]\\w*",e.NR="\\b\\d+(\\.\\d+)?",e.CNR="(-?)(\\b0[xX][a-fA-F0-9]+|(\\b\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)",e.BNR="\\b(0b[01]+)",e.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|-|-=|/=|/|:|;|<<|<<=|<=|<|===|==|=|>>>=|>>=|>=|>>>|>>|>|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~",e.BE={b:"\\\\[\\s\\S]",r:0},e.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[e.BE]},e.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[e.BE]},e.PWM={b:/\b(a|an|the|are|I'm|isn't|don't|doesn't|won't|but|just|should|pretty|simply|enough|gonna|going|wtf|so|such|will|you|your|they|like|more)\b/},e.C=function(n,t,r){var a=e.inherit({cN:"comment",b:n,e:t,c:[]},r||{});return a.c.push(e.PWM),a.c.push({cN:"doctag",b:"(?:TODO|FIXME|NOTE|BUG|XXX):",r:0}),a},e.CLCM=e.C("//","$"),e.CBCM=e.C("/\\*","\\*/"),e.HCM=e.C("#","$"),e.NM={cN:"number",b:e.NR,r:0},e.CNM={cN:"number",b:e.CNR,r:0},e.BNM={cN:"number",b:e.BNR,r:0},e.CSSNM={cN:"number",b:e.NR+"(%|em|ex|ch|rem|vw|vh|vmin|vmax|cm|mm|in|pt|pc|px|deg|grad|rad|turn|s|ms|Hz|kHz|dpi|dpcm|dppx)?",r:0},e.RM={cN:"regexp",b:/\//,e:/\/[gimuy]*/,i:/\n/,c:[e.BE,{b:/\[/,e:/\]/,r:0,c:[e.BE]}]},e.TM={cN:"title",b:e.IR,r:0},e.UTM={cN:"title",b:e.UIR,r:0},e.METHOD_GUARD={b:"\\.\\s*"+e.UIR,r:0},e});hljs.registerLanguage("sql",function(e){var t=e.C("--","$");return{cI:!0,i:/[<>{}*#]/,c:[{bK:"begin end start commit rollback savepoint lock alter create drop rename call delete do handler insert load replace select truncate update set show pragma grant merge describe use explain help declare prepare execute deallocate release unlock purge reset change stop analyze cache flush optimize repair kill install uninstall checksum restore check backup revoke comment",e:/;/,eW:!0,l:/[\w\.]+/,k:{keyword:"abort abs absolute acc acce accep accept access accessed accessible account acos action activate add addtime admin administer advanced advise aes_decrypt aes_encrypt after agent aggregate ali alia alias allocate allow alter always analyze ancillary and any anydata anydataset anyschema anytype apply archive archived archivelog are as asc ascii asin assembly assertion associate asynchronous at atan atn2 attr attri attrib attribu attribut attribute attributes audit authenticated authentication authid authors auto autoallocate autodblink autoextend automatic availability avg backup badfile basicfile before begin beginning benchmark between bfile bfile_base big bigfile bin binary_double binary_float binlog bit_and bit_count bit_length bit_or bit_xor bitmap blob_base block blocksize body both bound buffer_cache buffer_pool build bulk by byte byteordermark bytes cache caching call calling cancel capacity cascade cascaded case cast catalog category ceil ceiling chain change changed char_base char_length character_length characters characterset charindex charset charsetform charsetid check checksum checksum_agg child choose chr chunk class cleanup clear client clob clob_base clone close cluster_id cluster_probability cluster_set clustering coalesce coercibility col collate collation collect colu colum column column_value columns columns_updated comment commit compact compatibility compiled complete composite_limit compound compress compute concat concat_ws concurrent confirm conn connec connect connect_by_iscycle connect_by_isleaf connect_by_root connect_time connection consider consistent constant constraint constraints constructor container content contents context contributors controlfile conv convert convert_tz corr corr_k corr_s corresponding corruption cos cost count count_big counted covar_pop covar_samp cpu_per_call cpu_per_session crc32 create creation critical cross cube cume_dist curdate current current_date current_time current_timestamp current_user cursor curtime customdatum cycle data database databases datafile datafiles datalength date_add date_cache date_format date_sub dateadd datediff datefromparts datename datepart datetime2fromparts day day_to_second dayname dayofmonth dayofweek dayofyear days db_role_change dbtimezone ddl deallocate declare decode decompose decrement decrypt deduplicate def defa defau defaul default defaults deferred defi defin define degrees delayed delegate delete delete_all delimited demand dense_rank depth dequeue des_decrypt des_encrypt des_key_file desc descr descri describ describe descriptor deterministic diagnostics difference dimension direct_load directory disable disable_all disallow disassociate discardfile disconnect diskgroup distinct distinctrow distribute distributed div do document domain dotnet double downgrade drop dumpfile duplicate duration each edition editionable editions element ellipsis else elsif elt empty enable enable_all enclosed encode encoding encrypt end end-exec endian enforced engine engines enqueue enterprise entityescaping eomonth error errors escaped evalname evaluate event eventdata events except exception exceptions exchange exclude excluding execu execut execute exempt exists exit exp expire explain export export_set extended extent external external_1 external_2 externally extract failed failed_login_attempts failover failure far fast feature_set feature_value fetch field fields file file_name_convert filesystem_like_logging final finish first first_value fixed flash_cache flashback floor flush following follows for forall force form forma format found found_rows freelist freelists freepools fresh from from_base64 from_days ftp full function general generated get get_format get_lock getdate getutcdate global global_name globally go goto grant grants greatest group group_concat group_id grouping grouping_id groups gtid_subtract guarantee guard handler hash hashkeys having hea head headi headin heading heap help hex hierarchy high high_priority hosts hour http id ident_current ident_incr ident_seed identified identity idle_time if ifnull ignore iif ilike ilm immediate import in include including increment index indexes indexing indextype indicator indices inet6_aton inet6_ntoa inet_aton inet_ntoa infile initial initialized initially initrans inmemory inner innodb input insert install instance instantiable instr interface interleaved intersect into invalidate invisible is is_free_lock is_ipv4 is_ipv4_compat is_not is_not_null is_used_lock isdate isnull isolation iterate java join json json_exists keep keep_duplicates key keys kill language large last last_day last_insert_id last_value lax lcase lead leading least leaves left len lenght length less level levels library like like2 like4 likec limit lines link list listagg little ln load load_file lob lobs local localtime localtimestamp locate locator lock locked log log10 log2 logfile logfiles logging logical logical_reads_per_call logoff logon logs long loop low low_priority lower lpad lrtrim ltrim main make_set makedate maketime managed management manual map mapping mask master master_pos_wait match matched materialized max maxextents maximize maxinstances maxlen maxlogfiles maxloghistory maxlogmembers maxsize maxtrans md5 measures median medium member memcompress memory merge microsecond mid migration min minextents minimum mining minus minute minvalue missing mod mode model modification modify module monitoring month months mount move movement multiset mutex name name_const names nan national native natural nav nchar nclob nested never new newline next nextval no no_write_to_binlog noarchivelog noaudit nobadfile nocheck nocompress nocopy nocycle nodelay nodiscardfile noentityescaping noguarantee nokeep nologfile nomapping nomaxvalue nominimize nominvalue nomonitoring none noneditionable nonschema noorder nopr nopro noprom nopromp noprompt norely noresetlogs noreverse normal norowdependencies noschemacheck noswitch not nothing notice notrim novalidate now nowait nth_value nullif nulls num numb numbe nvarchar nvarchar2 object ocicoll ocidate ocidatetime ociduration ociinterval ociloblocator ocinumber ociref ocirefcursor ocirowid ocistring ocitype oct octet_length of off offline offset oid oidindex old on online only opaque open operations operator optimal optimize option optionally or oracle oracle_date oradata ord ordaudio orddicom orddoc order ordimage ordinality ordvideo organization orlany orlvary out outer outfile outline output over overflow overriding package pad parallel parallel_enable parameters parent parse partial partition partitions pascal passing password password_grace_time password_lock_time password_reuse_max password_reuse_time password_verify_function patch path patindex pctincrease pctthreshold pctused pctversion percent percent_rank percentile_cont percentile_disc performance period period_add period_diff permanent physical pi pipe pipelined pivot pluggable plugin policy position post_transaction pow power pragma prebuilt precedes preceding precision prediction prediction_cost prediction_details prediction_probability prediction_set prepare present preserve prior priority private private_sga privileges procedural procedure procedure_analyze processlist profiles project prompt protection public publishingservername purge quarter query quick quiesce quota quotename radians raise rand range rank raw read reads readsize rebuild record records recover recovery recursive recycle redo reduced ref reference referenced references referencing refresh regexp_like register regr_avgx regr_avgy regr_count regr_intercept regr_r2 regr_slope regr_sxx regr_sxy reject rekey relational relative relaylog release release_lock relies_on relocate rely rem remainder rename repair repeat replace replicate replication required reset resetlogs resize resource respect restore restricted result result_cache resumable resume retention return returning returns reuse reverse revoke right rlike role roles rollback rolling rollup round row row_count rowdependencies rowid rownum rows rtrim rules safe salt sample save savepoint sb1 sb2 sb4 scan schema schemacheck scn scope scroll sdo_georaster sdo_topo_geometry search sec_to_time second section securefile security seed segment select self sequence sequential serializable server servererror session session_user sessions_per_user set sets settings sha sha1 sha2 share shared shared_pool short show shrink shutdown si_averagecolor si_colorhistogram si_featurelist si_positionalcolor si_stillimage si_texture siblings sid sign sin size size_t sizes skip slave sleep smalldatetimefromparts smallfile snapshot some soname sort soundex source space sparse spfile split sql sql_big_result sql_buffer_result sql_cache sql_calc_found_rows sql_small_result sql_variant_property sqlcode sqldata sqlerror sqlname sqlstate sqrt square standalone standby start starting startup statement static statistics stats_binomial_test stats_crosstab stats_ks_test stats_mode stats_mw_test stats_one_way_anova stats_t_test_ stats_t_test_indep stats_t_test_one stats_t_test_paired stats_wsr_test status std stddev stddev_pop stddev_samp stdev stop storage store stored str str_to_date straight_join strcmp strict string struct stuff style subdate subpartition subpartitions substitutable substr substring subtime subtring_index subtype success sum suspend switch switchoffset switchover sync synchronous synonym sys sys_xmlagg sysasm sysaux sysdate sysdatetimeoffset sysdba sysoper system system_user sysutcdatetime table tables tablespace tan tdo template temporary terminated tertiary_weights test than then thread through tier ties time time_format time_zone timediff timefromparts timeout timestamp timestampadd timestampdiff timezone_abbr timezone_minute timezone_region to to_base64 to_date to_days to_seconds todatetimeoffset trace tracking transaction transactional translate translation treat trigger trigger_nestlevel triggers trim truncate try_cast try_convert try_parse type ub1 ub2 ub4 ucase unarchived unbounded uncompress under undo unhex unicode uniform uninstall union unique unix_timestamp unknown unlimited unlock unpivot unrecoverable unsafe unsigned until untrusted unusable unused update updated upgrade upped upper upsert url urowid usable usage use use_stored_outlines user user_data user_resources users using utc_date utc_timestamp uuid uuid_short validate validate_password_strength validation valist value values var var_samp varcharc vari varia variab variabl variable variables variance varp varraw varrawc varray verify version versions view virtual visible void wait wallet warning warnings week weekday weekofyear wellformed when whene whenev wheneve whenever where while whitespace with within without work wrapped xdb xml xmlagg xmlattributes xmlcast xmlcolattval xmlelement xmlexists xmlforest xmlindex xmlnamespaces xmlpi xmlquery xmlroot xmlschema xmlserialize xmltable xmltype xor year year_to_month years yearweek",literal:"true false null",built_in:"array bigint binary bit blob boolean char character date dec decimal float int int8 integer interval number numeric real record serial serial8 smallint text varchar varying void"},c:[{cN:"string",b:"'",e:"'",c:[e.BE,{b:"''"}]},{cN:"string",b:'"',e:'"',c:[e.BE,{b:'""'}]},{cN:"string",b:"`",e:"`",c:[e.BE]},e.CNM,e.CBCM,t]},e.CBCM,t]}});hljs.registerLanguage("r",function(e){var r="([a-zA-Z]|\\.[a-zA-Z.])[a-zA-Z0-9._]*";return{c:[e.HCM,{b:r,l:r,k:{keyword:"function if in break next repeat else for return switch while try tryCatch stop warning require library attach detach source setMethod setGeneric setGroupGeneric setClass ...",literal:"NULL NA TRUE FALSE T F Inf NaN NA_integer_|10 NA_real_|10 NA_character_|10 NA_complex_|10"},r:0},{cN:"number",b:"0[xX][0-9a-fA-F]+[Li]?\\b",r:0},{cN:"number",b:"\\d+(?:[eE][+\\-]?\\d*)?L\\b",r:0},{cN:"number",b:"\\d+\\.(?!\\d)(?:i\\b)?",r:0},{cN:"number",b:"\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",r:0},{b:"`",e:"`",r:0},{cN:"string",c:[e.BE],v:[{b:'"',e:'"'},{b:"'",e:"'"}]}]}});hljs.registerLanguage("perl",function(e){var t="getpwent getservent quotemeta msgrcv scalar kill dbmclose undef lc ma syswrite tr send umask sysopen shmwrite vec qx utime local oct semctl localtime readpipe do return format read sprintf dbmopen pop getpgrp not getpwnam rewinddir qqfileno qw endprotoent wait sethostent bless s|0 opendir continue each sleep endgrent shutdown dump chomp connect getsockname die socketpair close flock exists index shmgetsub for endpwent redo lstat msgctl setpgrp abs exit select print ref gethostbyaddr unshift fcntl syscall goto getnetbyaddr join gmtime symlink semget splice x|0 getpeername recv log setsockopt cos last reverse gethostbyname getgrnam study formline endhostent times chop length gethostent getnetent pack getprotoent getservbyname rand mkdir pos chmod y|0 substr endnetent printf next open msgsnd readdir use unlink getsockopt getpriority rindex wantarray hex system getservbyport endservent int chr untie rmdir prototype tell listen fork shmread ucfirst setprotoent else sysseek link getgrgid shmctl waitpid unpack getnetbyname reset chdir grep split require caller lcfirst until warn while values shift telldir getpwuid my getprotobynumber delete and sort uc defined srand accept package seekdir getprotobyname semop our rename seek if q|0 chroot sysread setpwent no crypt getc chown sqrt write setnetent setpriority foreach tie sin msgget map stat getlogin unless elsif truncate exec keys glob tied closedirioctl socket readlink eval xor readline binmode setservent eof ord bind alarm pipe atan2 getgrent exp time push setgrent gt lt or ne m|0 break given say state when",r={cN:"subst",b:"[$@]\\{",e:"\\}",k:t},s={b:"->{",e:"}"},n={v:[{b:/\$\d/},{b:/[\$%@](\^\w\b|#\w+(::\w+)*|{\w+}|\w+(::\w*)*)/},{b:/[\$%@][^\s\w{]/,r:0}]},i=[e.BE,r,n],o=[n,e.HCM,e.C("^\\=\\w","\\=cut",{eW:!0}),s,{cN:"string",c:i,v:[{b:"q[qwxr]?\\s*\\(",e:"\\)",r:5},{b:"q[qwxr]?\\s*\\[",e:"\\]",r:5},{b:"q[qwxr]?\\s*\\{",e:"\\}",r:5},{b:"q[qwxr]?\\s*\\|",e:"\\|",r:5},{b:"q[qwxr]?\\s*\\<",e:"\\>",r:5},{b:"qw\\s+q",e:"q",r:5},{b:"'",e:"'",c:[e.BE]},{b:'"',e:'"'},{b:"`",e:"`",c:[e.BE]},{b:"{\\w+}",c:[],r:0},{b:"-?\\w+\\s*\\=\\>",c:[],r:0}]},{cN:"number",b:"(\\b0[0-7_]+)|(\\b0x[0-9a-fA-F_]+)|(\\b[1-9][0-9_]*(\\.[0-9_]+)?)|[0_]\\b",r:0},{b:"(\\/\\/|"+e.RSR+"|\\b(split|return|print|reverse|grep)\\b)\\s*",k:"split return print reverse grep",r:0,c:[e.HCM,{cN:"regexp",b:"(s|tr|y)/(\\\\.|[^/])*/(\\\\.|[^/])*/[a-z]*",r:10},{cN:"regexp",b:"(m|qr)?/",e:"/[a-z]*",c:[e.BE],r:0}]},{cN:"function",bK:"sub",e:"(\\s*\\(.*?\\))?[;{]",eE:!0,r:5,c:[e.TM]},{b:"-\\w\\b",r:0},{b:"^__DATA__$",e:"^__END__$",sL:"mojolicious",c:[{b:"^@@.*",e:"$",cN:"comment"}]}];return r.c=o,s.c=o,{aliases:["pl","pm"],l:/[\w\.]+/,k:t,c:o}});hljs.registerLanguage("ini",function(e){var b={cN:"string",c:[e.BE],v:[{b:"'''",e:"'''",r:10},{b:'"""',e:'"""',r:10},{b:'"',e:'"'},{b:"'",e:"'"}]};return{aliases:["toml"],cI:!0,i:/\S/,c:[e.C(";","$"),e.HCM,{cN:"section",b:/^\s*\[+/,e:/\]+/},{b:/^[a-z0-9\[\]_-]+\s*=\s*/,e:"$",rB:!0,c:[{cN:"attr",b:/[a-z0-9\[\]_-]+/},{b:/=/,eW:!0,r:0,c:[{cN:"literal",b:/\bon|off|true|false|yes|no\b/},{cN:"variable",v:[{b:/\$[\w\d"][\w\d_]*/},{b:/\$\{(.*?)}/}]},b,{cN:"number",b:/([\+\-]+)?[\d]+_[\d_]+/},e.NM]}]}]}});hljs.registerLanguage("diff",function(e){return{aliases:["patch"],c:[{cN:"meta",r:10,v:[{b:/^@@ +\-\d+,\d+ +\+\d+,\d+ +@@$/},{b:/^\*\*\* +\d+,\d+ +\*\*\*\*$/},{b:/^\-\-\- +\d+,\d+ +\-\-\-\-$/}]},{cN:"comment",v:[{b:/Index: /,e:/$/},{b:/={3,}/,e:/$/},{b:/^\-{3}/,e:/$/},{b:/^\*{3} /,e:/$/},{b:/^\+{3}/,e:/$/},{b:/\*{5}/,e:/\*{5}$/}]},{cN:"addition",b:"^\\+",e:"$"},{cN:"deletion",b:"^\\-",e:"$"},{cN:"addition",b:"^\\!",e:"$"}]}});hljs.registerLanguage("go",function(e){var t={keyword:"break default func interface select case map struct chan else goto package switch const fallthrough if range type continue for import return var go defer bool byte complex64 complex128 float32 float64 int8 int16 int32 int64 string uint8 uint16 uint32 uint64 int uint uintptr rune",literal:"true false iota nil",built_in:"append cap close complex copy imag len make new panic print println real recover delete"};return{aliases:["golang"],k:t,i:"</",c:[e.CLCM,e.CBCM,{cN:"string",v:[e.QSM,{b:"'",e:"[^\\\\]'"},{b:"`",e:"`"}]},{cN:"number",v:[{b:e.CNR+"[dflsi]",r:1},e.CNM]},{b:/:=/},{cN:"function",bK:"func",e:/\s*\{/,eE:!0,c:[e.TM,{cN:"params",b:/\(/,e:/\)/,k:t,i:/["']/}]}]}});hljs.registerLanguage("bash",function(e){var t={cN:"variable",v:[{b:/\$[\w\d#@][\w\d_]*/},{b:/\$\{(.*?)}/}]},s={cN:"string",b:/"/,e:/"/,c:[e.BE,t,{cN:"variable",b:/\$\(/,e:/\)/,c:[e.BE]}]},a={cN:"string",b:/'/,e:/'/};return{aliases:["sh","zsh"],l:/\b-?[a-z\._]+\b/,k:{keyword:"if then else elif fi for while in do done case esac function",literal:"true false",built_in:"break cd continue eval exec exit export getopts hash pwd readonly return shift test times trap umask unset alias bind builtin caller command declare echo enable help let local logout mapfile printf read readarray source type typeset ulimit unalias set shopt autoload bg bindkey bye cap chdir clone comparguments compcall compctl compdescribe compfiles compgroups compquote comptags comptry compvalues dirs disable disown echotc echoti emulate fc fg float functions getcap getln history integer jobs kill limit log noglob popd print pushd pushln rehash sched setcap setopt stat suspend ttyctl unfunction unhash unlimit unsetopt vared wait whence where which zcompile zformat zftp zle zmodload zparseopts zprof zpty zregexparse zsocket zstyle ztcp",_:"-ne -eq -lt -gt -f -d -e -s -l -a"},c:[{cN:"meta",b:/^#![^\n]+sh\s*$/,r:10},{cN:"function",b:/\w[\w\d_]*\s*\(\s*\)\s*\{/,rB:!0,c:[e.inherit(e.TM,{b:/\w[\w\d_]*/})],r:0},e.HCM,s,a,t]}});hljs.registerLanguage("python",function(e){var r={keyword:"and elif is global as in if from raise for except finally print import pass return exec else break not with class assert yield try while continue del or def lambda async await nonlocal|10 None True False",built_in:"Ellipsis NotImplemented"},b={cN:"meta",b:/^(>>>|\.\.\.) /},c={cN:"subst",b:/\{/,e:/\}/,k:r,i:/#/},a={cN:"string",c:[e.BE],v:[{b:/(u|b)?r?'''/,e:/'''/,c:[b],r:10},{b:/(u|b)?r?"""/,e:/"""/,c:[b],r:10},{b:/(fr|rf|f)'''/,e:/'''/,c:[b,c]},{b:/(fr|rf|f)"""/,e:/"""/,c:[b,c]},{b:/(u|r|ur)'/,e:/'/,r:10},{b:/(u|r|ur)"/,e:/"/,r:10},{b:/(b|br)'/,e:/'/},{b:/(b|br)"/,e:/"/},{b:/(fr|rf|f)'/,e:/'/,c:[c]},{b:/(fr|rf|f)"/,e:/"/,c:[c]},e.ASM,e.QSM]},s={cN:"number",r:0,v:[{b:e.BNR+"[lLjJ]?"},{b:"\\b(0o[0-7]+)[lLjJ]?"},{b:e.CNR+"[lLjJ]?"}]},i={cN:"params",b:/\(/,e:/\)/,c:["self",b,s,a]};return c.c=[a,s,b],{aliases:["py","gyp"],k:r,i:/(<\/|->|\?)|=>/,c:[b,s,a,e.HCM,{v:[{cN:"function",bK:"def"},{cN:"class",bK:"class"}],e:/:/,i:/[${=;\n,]/,c:[e.UTM,i,{b:/->/,eW:!0,k:"None"}]},{cN:"meta",b:/^[\t ]*@/,e:/$/},{b:/\b(print|exec)\(/}]}});hljs.registerLanguage("julia",function(e){var r={keyword:"in isa where baremodule begin break catch ccall const continue do else elseif end export false finally for function global if import importall let local macro module quote return true try using while type immutable abstract bitstype typealias ",literal:"true false ARGS C_NULL DevNull ENDIAN_BOM ENV I Inf Inf16 Inf32 Inf64 InsertionSort JULIA_HOME LOAD_PATH MergeSort NaN NaN16 NaN32 NaN64 PROGRAM_FILE QuickSort RoundDown RoundFromZero RoundNearest RoundNearestTiesAway RoundNearestTiesUp RoundToZero RoundUp STDERR STDIN STDOUT VERSION catalan e|0 eu|0 eulergamma golden im nothing pi γ π φ ",built_in:"ANY AbstractArray AbstractChannel AbstractFloat AbstractMatrix AbstractRNG AbstractSerializer AbstractSet AbstractSparseArray AbstractSparseMatrix AbstractSparseVector AbstractString AbstractUnitRange AbstractVecOrMat AbstractVector Any ArgumentError Array AssertionError Associative Base64DecodePipe Base64EncodePipe Bidiagonal BigFloat BigInt BitArray BitMatrix BitVector Bool BoundsError BufferStream CachingPool CapturedException CartesianIndex CartesianRange Cchar Cdouble Cfloat Channel Char Cint Cintmax_t Clong Clonglong ClusterManager Cmd CodeInfo Colon Complex Complex128 Complex32 Complex64 CompositeException Condition ConjArray ConjMatrix ConjVector Cptrdiff_t Cshort Csize_t Cssize_t Cstring Cuchar Cuint Cuintmax_t Culong Culonglong Cushort Cwchar_t Cwstring DataType Date DateFormat DateTime DenseArray DenseMatrix DenseVecOrMat DenseVector Diagonal Dict DimensionMismatch Dims DirectIndexString Display DivideError DomainError EOFError EachLine Enum Enumerate ErrorException Exception ExponentialBackOff Expr Factorization FileMonitor Float16 Float32 Float64 Function Future GlobalRef GotoNode HTML Hermitian IO IOBuffer IOContext IOStream IPAddr IPv4 IPv6 IndexCartesian IndexLinear IndexStyle InexactError InitError Int Int128 Int16 Int32 Int64 Int8 IntSet Integer InterruptException InvalidStateException Irrational KeyError LabelNode LinSpace LineNumberNode LoadError LowerTriangular MIME Matrix MersenneTwister Method MethodError MethodTable Module NTuple NewvarNode NullException Nullable Number ObjectIdDict OrdinalRange OutOfMemoryError OverflowError Pair ParseError PartialQuickSort PermutedDimsArray Pipe PollingFileWatcher ProcessExitedException Ptr QuoteNode RandomDevice Range RangeIndex Rational RawFD ReadOnlyMemoryError Real ReentrantLock Ref Regex RegexMatch RemoteChannel RemoteException RevString RoundingMode RowVector SSAValue SegmentationFault SerializationState Set SharedArray SharedMatrix SharedVector Signed SimpleVector Slot SlotNumber SparseMatrixCSC SparseVector StackFrame StackOverflowError StackTrace StepRange StepRangeLen StridedArray StridedMatrix StridedVecOrMat StridedVector String SubArray SubString SymTridiagonal Symbol Symmetric SystemError TCPSocket Task Text TextDisplay Timer Tridiagonal Tuple Type TypeError TypeMapEntry TypeMapLevel TypeName TypeVar TypedSlot UDPSocket UInt UInt128 UInt16 UInt32 UInt64 UInt8 UndefRefError UndefVarError UnicodeError UniformScaling Union UnionAll UnitRange Unsigned UpperTriangular Val Vararg VecElement VecOrMat Vector VersionNumber Void WeakKeyDict WeakRef WorkerConfig WorkerPool "},t="[A-Za-z_\\u00A1-\\uFFFF][A-Za-z_0-9\\u00A1-\\uFFFF]*",a={l:t,k:r,i:/<\//},n={cN:"number",b:/(\b0x[\d_]*(\.[\d_]*)?|0x\.\d[\d_]*)p[-+]?\d+|\b0[box][a-fA-F0-9][a-fA-F0-9_]*|(\b\d[\d_]*(\.[\d_]*)?|\.\d[\d_]*)([eEfF][-+]?\d+)?/,r:0},o={cN:"string",b:/'(.|\\[xXuU][a-zA-Z0-9]+)'/},i={cN:"subst",b:/\$\(/,e:/\)/,k:r},l={cN:"variable",b:"\\$"+t},c={cN:"string",c:[e.BE,i,l],v:[{b:/\w*"""/,e:/"""\w*/,r:10},{b:/\w*"/,e:/"\w*/}]},s={cN:"string",c:[e.BE,i,l],b:"`",e:"`"},d={cN:"meta",b:"@"+t},u={cN:"comment",v:[{b:"#=",e:"=#",r:10},{b:"#",e:"$"}]};return a.c=[n,o,c,s,d,u,e.HCM,{cN:"keyword",b:"\\b(((abstract|primitive)\\s+)type|(mutable\\s+)?struct)\\b"},{b:/<:/}],i.c=a.c,a});hljs.registerLanguage("coffeescript",function(e){var c={keyword:"in if for while finally new do return else break catch instanceof throw try this switch continue typeof delete debugger super yield import export from as default await then unless until loop of by when and or is isnt not",literal:"true false null undefined yes no on off",built_in:"npm require console print module global window document"},n="[A-Za-z$_][0-9A-Za-z$_]*",r={cN:"subst",b:/#\{/,e:/}/,k:c},i=[e.BNM,e.inherit(e.CNM,{starts:{e:"(\\s*/)?",r:0}}),{cN:"string",v:[{b:/'''/,e:/'''/,c:[e.BE]},{b:/'/,e:/'/,c:[e.BE]},{b:/"""/,e:/"""/,c:[e.BE,r]},{b:/"/,e:/"/,c:[e.BE,r]}]},{cN:"regexp",v:[{b:"///",e:"///",c:[r,e.HCM]},{b:"//[gim]*",r:0},{b:/\/(?![ *])(\\\/|.)*?\/[gim]*(?=\W|$)/}]},{b:"@"+n},{sL:"javascript",eB:!0,eE:!0,v:[{b:"```",e:"```"},{b:"`",e:"`"}]}];r.c=i;var s=e.inherit(e.TM,{b:n}),t="(\\(.*\\))?\\s*\\B[-=]>",o={cN:"params",b:"\\([^\\(]",rB:!0,c:[{b:/\(/,e:/\)/,k:c,c:["self"].concat(i)}]};return{aliases:["coffee","cson","iced"],k:c,i:/\/\*/,c:i.concat([e.C("###","###"),e.HCM,{cN:"function",b:"^\\s*"+n+"\\s*=\\s*"+t,e:"[-=]>",rB:!0,c:[s,o]},{b:/[:\(,=]\s*/,r:0,c:[{cN:"function",b:t,e:"[-=]>",rB:!0,c:[o]}]},{cN:"class",bK:"class",e:"$",i:/[:="\[\]]/,c:[{bK:"extends",eW:!0,i:/[:="\[\]]/,c:[s]},s]},{b:n+":",e:":",rB:!0,rE:!0,r:0}])}});hljs.registerLanguage("cpp",function(t){var e={cN:"keyword",b:"\\b[a-z\\d_]*_t\\b"},r={cN:"string",v:[{b:'(u8?|U)?L?"',e:'"',i:"\\n",c:[t.BE]},{b:'(u8?|U)?R"',e:'"',c:[t.BE]},{b:"'\\\\?.",e:"'",i:"."}]},s={cN:"number",v:[{b:"\\b(0b[01']+)"},{b:"(-?)\\b([\\d']+(\\.[\\d']*)?|\\.[\\d']+)(u|U|l|L|ul|UL|f|F|b|B)"},{b:"(-?)(\\b0[xX][a-fA-F0-9']+|(\\b[\\d']+(\\.[\\d']*)?|\\.[\\d']+)([eE][-+]?[\\d']+)?)"}],r:0},i={cN:"meta",b:/#\s*[a-z]+\b/,e:/$/,k:{"meta-keyword":"if else elif endif define undef warning error line pragma ifdef ifndef include"},c:[{b:/\\\n/,r:0},t.inherit(r,{cN:"meta-string"}),{cN:"meta-string",b:/<[^\n>]*>/,e:/$/,i:"\\n"},t.CLCM,t.CBCM]},a=t.IR+"\\s*\\(",c={keyword:"int float while private char catch import module export virtual operator sizeof dynamic_cast|10 typedef const_cast|10 const for static_cast|10 union namespace unsigned long volatile static protected bool template mutable if public friend do goto auto void enum else break extern using asm case typeid short reinterpret_cast|10 default double register explicit signed typename try this switch continue inline delete alignof constexpr decltype noexcept static_assert thread_local restrict _Bool complex _Complex _Imaginary atomic_bool atomic_char atomic_schar atomic_uchar atomic_short atomic_ushort atomic_int atomic_uint atomic_long atomic_ulong atomic_llong atomic_ullong new throw return and or not",built_in:"std string cin cout cerr clog stdin stdout stderr stringstream istringstream ostringstream auto_ptr deque list queue stack vector map set bitset multiset multimap unordered_set unordered_map unordered_multiset unordered_multimap array shared_ptr abort abs acos asin atan2 atan calloc ceil cosh cos exit exp fabs floor fmod fprintf fputs free frexp fscanf isalnum isalpha iscntrl isdigit isgraph islower isprint ispunct isspace isupper isxdigit tolower toupper labs ldexp log10 log malloc realloc memchr memcmp memcpy memset modf pow printf putchar puts scanf sinh sin snprintf sprintf sqrt sscanf strcat strchr strcmp strcpy strcspn strlen strncat strncmp strncpy strpbrk strrchr strspn strstr tanh tan vfprintf vprintf vsprintf endl initializer_list unique_ptr",literal:"true false nullptr NULL"},n=[e,t.CLCM,t.CBCM,s,r];return{aliases:["c","cc","h","c++","h++","hpp"],k:c,i:"</",c:n.concat([i,{b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:c,c:["self",e]},{b:t.IR+"::",k:c},{v:[{b:/=/,e:/;/},{b:/\(/,e:/\)/},{bK:"new throw return else",e:/;/}],k:c,c:n.concat([{b:/\(/,e:/\)/,k:c,c:n.concat(["self"]),r:0}]),r:0},{cN:"function",b:"("+t.IR+"[\\*&\\s]+)+"+a,rB:!0,e:/[{;=]/,eE:!0,k:c,i:/[^\w\s\*&]/,c:[{b:a,rB:!0,c:[t.TM],r:0},{cN:"params",b:/\(/,e:/\)/,k:c,r:0,c:[t.CLCM,t.CBCM,r,s,e]},t.CLCM,t.CBCM,i]},{cN:"class",bK:"class struct",e:/[{;:]/,c:[{b:/</,e:/>/,c:["self"]},t.TM]}]),exports:{preprocessor:i,strings:r,k:c}}});hljs.registerLanguage("ruby",function(e){var b="[a-zA-Z_]\\w*[!?=]?|[-+~]\\@|<<|>>|=~|===?|<=>|[<>]=?|\\*\\*|[-/+%^&*~`|]|\\[\\]=?",r={keyword:"and then defined module in return redo if BEGIN retry end for self when next until do begin unless END rescue else break undef not super class case require yield alias while ensure elsif or include attr_reader attr_writer attr_accessor",literal:"true false nil"},c={cN:"doctag",b:"@[A-Za-z]+"},a={b:"#<",e:">"},s=[e.C("#","$",{c:[c]}),e.C("^\\=begin","^\\=end",{c:[c],r:10}),e.C("^__END__","\\n$")],n={cN:"subst",b:"#\\{",e:"}",k:r},t={cN:"string",c:[e.BE,n],v:[{b:/'/,e:/'/},{b:/"/,e:/"/},{b:/`/,e:/`/},{b:"%[qQwWx]?\\(",e:"\\)"},{b:"%[qQwWx]?\\[",e:"\\]"},{b:"%[qQwWx]?{",e:"}"},{b:"%[qQwWx]?<",e:">"},{b:"%[qQwWx]?/",e:"/"},{b:"%[qQwWx]?%",e:"%"},{b:"%[qQwWx]?-",e:"-"},{b:"%[qQwWx]?\\|",e:"\\|"},{b:/\B\?(\\\d{1,3}|\\x[A-Fa-f0-9]{1,2}|\\u[A-Fa-f0-9]{4}|\\?\S)\b/},{b:/<<(-?)\w+$/,e:/^\s*\w+$/}]},i={cN:"params",b:"\\(",e:"\\)",endsParent:!0,k:r},d=[t,a,{cN:"class",bK:"class module",e:"$|;",i:/=/,c:[e.inherit(e.TM,{b:"[A-Za-z_]\\w*(::\\w+)*(\\?|\\!)?"}),{b:"<\\s*",c:[{b:"("+e.IR+"::)?"+e.IR}]}].concat(s)},{cN:"function",bK:"def",e:"$|;",c:[e.inherit(e.TM,{b:b}),i].concat(s)},{b:e.IR+"::"},{cN:"symbol",b:e.UIR+"(\\!|\\?)?:",r:0},{cN:"symbol",b:":(?!\\s)",c:[t,{b:b}],r:0},{cN:"number",b:"(\\b0[0-7_]+)|(\\b0x[0-9a-fA-F_]+)|(\\b[1-9][0-9_]*(\\.[0-9_]+)?)|[0_]\\b",r:0},{b:"(\\$\\W)|((\\$|\\@\\@?)(\\w+))"},{cN:"params",b:/\|/,e:/\|/,k:r},{b:"("+e.RSR+"|unless)\\s*",k:"unless",c:[a,{cN:"regexp",c:[e.BE,n],i:/\n/,v:[{b:"/",e:"/[a-z]*"},{b:"%r{",e:"}[a-z]*"},{b:"%r\\(",e:"\\)[a-z]*"},{b:"%r!",e:"![a-z]*"},{b:"%r\\[",e:"\\][a-z]*"}]}].concat(s),r:0}].concat(s);n.c=d,i.c=d;var l="[>?]>",o="[\\w#]+\\(\\w+\\):\\d+:\\d+>",u="(\\w+-)?\\d+\\.\\d+\\.\\d(p\\d+)?[^>]+>",w=[{b:/^\s*=>/,starts:{e:"$",c:d}},{cN:"meta",b:"^("+l+"|"+o+"|"+u+")",starts:{e:"$",c:d}}];return{aliases:["rb","gemspec","podspec","thor","irb"],k:r,i:/\/\*/,c:s.concat(w).concat(d)}});hljs.registerLanguage("yaml",function(e){var b="true false yes no null",a="^[ \\-]*",r="[a-zA-Z_][\\w\\-]*",t={cN:"attr",v:[{b:a+r+":"},{b:a+'"'+r+'":'},{b:a+"'"+r+"':"}]},c={cN:"template-variable",v:[{b:"{{",e:"}}"},{b:"%{",e:"}"}]},l={cN:"string",r:0,v:[{b:/'/,e:/'/},{b:/"/,e:/"/},{b:/\S+/}],c:[e.BE,c]};return{cI:!0,aliases:["yml","YAML","yaml"],c:[t,{cN:"meta",b:"^---s*$",r:10},{cN:"string",b:"[\\|>] *$",rE:!0,c:l.c,e:t.v[0].b},{b:"<%[%=-]?",e:"[%-]?%>",sL:"ruby",eB:!0,eE:!0,r:0},{cN:"type",b:"!!"+e.UIR},{cN:"meta",b:"&"+e.UIR+"$"},{cN:"meta",b:"\\*"+e.UIR+"$"},{cN:"bullet",b:"^ *-",r:0},e.HCM,{bK:b,k:{literal:b}},e.CNM,l]}});hljs.registerLanguage("css",function(e){var c="[a-zA-Z-][a-zA-Z0-9_-]*",t={b:/[A-Z\_\.\-]+\s*:/,rB:!0,e:";",eW:!0,c:[{cN:"attribute",b:/\S/,e:":",eE:!0,starts:{eW:!0,eE:!0,c:[{b:/[\w-]+\(/,rB:!0,c:[{cN:"built_in",b:/[\w-]+/},{b:/\(/,e:/\)/,c:[e.ASM,e.QSM]}]},e.CSSNM,e.QSM,e.ASM,e.CBCM,{cN:"number",b:"#[0-9A-Fa-f]+"},{cN:"meta",b:"!important"}]}}]};return{cI:!0,i:/[=\/|'\$]/,c:[e.CBCM,{cN:"selector-id",b:/#[A-Za-z0-9_-]+/},{cN:"selector-class",b:/\.[A-Za-z0-9_-]+/},{cN:"selector-attr",b:/\[/,e:/\]/,i:"$"},{cN:"selector-pseudo",b:/:(:)?[a-zA-Z0-9\_\-\+\(\)"'.]+/},{b:"@(font-face|page)",l:"[a-z-]+",k:"font-face page"},{b:"@",e:"[{;]",i:/:/,c:[{cN:"keyword",b:/\w+/},{b:/\s/,eW:!0,eE:!0,r:0,c:[e.ASM,e.QSM,e.CSSNM]}]},{cN:"selector-tag",b:c,r:0},{b:"{",e:"}",i:/\S/,c:[e.CBCM,t]}]}});hljs.registerLanguage("fortran",function(e){var t={cN:"params",b:"\\(",e:"\\)"},n={literal:".False. .True.",keyword:"kind do while private call intrinsic where elsewhere type endtype endmodule endselect endinterface end enddo endif if forall endforall only contains default return stop then public subroutine|10 function program .and. .or. .not. .le. .eq. .ge. .gt. .lt. goto save else use module select case access blank direct exist file fmt form formatted iostat name named nextrec number opened rec recl sequential status unformatted unit continue format pause cycle exit c_null_char c_alert c_backspace c_form_feed flush wait decimal round iomsg synchronous nopass non_overridable pass protected volatile abstract extends import non_intrinsic value deferred generic final enumerator class associate bind enum c_int c_short c_long c_long_long c_signed_char c_size_t c_int8_t c_int16_t c_int32_t c_int64_t c_int_least8_t c_int_least16_t c_int_least32_t c_int_least64_t c_int_fast8_t c_int_fast16_t c_int_fast32_t c_int_fast64_t c_intmax_t C_intptr_t c_float c_double c_long_double c_float_complex c_double_complex c_long_double_complex c_bool c_char c_null_ptr c_null_funptr c_new_line c_carriage_return c_horizontal_tab c_vertical_tab iso_c_binding c_loc c_funloc c_associated  c_f_pointer c_ptr c_funptr iso_fortran_env character_storage_size error_unit file_storage_size input_unit iostat_end iostat_eor numeric_storage_size output_unit c_f_procpointer ieee_arithmetic ieee_support_underflow_control ieee_get_underflow_mode ieee_set_underflow_mode newunit contiguous recursive pad position action delim readwrite eor advance nml interface procedure namelist include sequence elemental pure integer real character complex logical dimension allocatable|10 parameter external implicit|10 none double precision assign intent optional pointer target in out common equivalence data",built_in:"alog alog10 amax0 amax1 amin0 amin1 amod cabs ccos cexp clog csin csqrt dabs dacos dasin datan datan2 dcos dcosh ddim dexp dint dlog dlog10 dmax1 dmin1 dmod dnint dsign dsin dsinh dsqrt dtan dtanh float iabs idim idint idnint ifix isign max0 max1 min0 min1 sngl algama cdabs cdcos cdexp cdlog cdsin cdsqrt cqabs cqcos cqexp cqlog cqsin cqsqrt dcmplx dconjg derf derfc dfloat dgamma dimag dlgama iqint qabs qacos qasin qatan qatan2 qcmplx qconjg qcos qcosh qdim qerf qerfc qexp qgamma qimag qlgama qlog qlog10 qmax1 qmin1 qmod qnint qsign qsin qsinh qsqrt qtan qtanh abs acos aimag aint anint asin atan atan2 char cmplx conjg cos cosh exp ichar index int log log10 max min nint sign sin sinh sqrt tan tanh print write dim lge lgt lle llt mod nullify allocate deallocate adjustl adjustr all allocated any associated bit_size btest ceiling count cshift date_and_time digits dot_product eoshift epsilon exponent floor fraction huge iand ibclr ibits ibset ieor ior ishft ishftc lbound len_trim matmul maxexponent maxloc maxval merge minexponent minloc minval modulo mvbits nearest pack present product radix random_number random_seed range repeat reshape rrspacing scale scan selected_int_kind selected_real_kind set_exponent shape size spacing spread sum system_clock tiny transpose trim ubound unpack verify achar iachar transfer dble entry dprod cpu_time command_argument_count get_command get_command_argument get_environment_variable is_iostat_end ieee_arithmetic ieee_support_underflow_control ieee_get_underflow_mode ieee_set_underflow_mode is_iostat_eor move_alloc new_line selected_char_kind same_type_as extends_type_ofacosh asinh atanh bessel_j0 bessel_j1 bessel_jn bessel_y0 bessel_y1 bessel_yn erf erfc erfc_scaled gamma log_gamma hypot norm2 atomic_define atomic_ref execute_command_line leadz trailz storage_size merge_bits bge bgt ble blt dshiftl dshiftr findloc iall iany iparity image_index lcobound ucobound maskl maskr num_images parity popcnt poppar shifta shiftl shiftr this_image"};return{cI:!0,aliases:["f90","f95"],k:n,i:/\/\*/,c:[e.inherit(e.ASM,{cN:"string",r:0}),e.inherit(e.QSM,{cN:"string",r:0}),{cN:"function",bK:"subroutine function program",i:"[${=\\n]",c:[e.UTM,t]},e.C("!","$",{r:0}),{cN:"number",b:"(?=\\b|\\+|\\-|\\.)(?=\\.\\d|\\d)(?:\\d+)?(?:\\.?\\d*)(?:[de][+-]?\\d+)?\\b\\.?",r:0}]}});hljs.registerLanguage("awk",function(e){var r={cN:"variable",v:[{b:/\$[\w\d#@][\w\d_]*/},{b:/\$\{(.*?)}/}]},b="BEGIN END if else while do for in break continue delete next nextfile function func exit|10",n={cN:"string",c:[e.BE],v:[{b:/(u|b)?r?'''/,e:/'''/,r:10},{b:/(u|b)?r?"""/,e:/"""/,r:10},{b:/(u|r|ur)'/,e:/'/,r:10},{b:/(u|r|ur)"/,e:/"/,r:10},{b:/(b|br)'/,e:/'/},{b:/(b|br)"/,e:/"/},e.ASM,e.QSM]};return{k:{keyword:b},c:[r,n,e.RM,e.HCM,e.NM]}});hljs.registerLanguage("makefile",function(e){var i={cN:"variable",v:[{b:"\\$\\("+e.UIR+"\\)",c:[e.BE]},{b:/\$[@%<?\^\+\*]/}]},r={cN:"string",b:/"/,e:/"/,c:[e.BE,i]},a={cN:"variable",b:/\$\([\w-]+\s/,e:/\)/,k:{built_in:"subst patsubst strip findstring filter filter-out sort word wordlist firstword lastword dir notdir suffix basename addsuffix addprefix join wildcard realpath abspath error warning shell origin flavor foreach if or and call eval file value"},c:[i]},n={b:"^"+e.UIR+"\\s*[:+?]?=",i:"\\n",rB:!0,c:[{b:"^"+e.UIR,e:"[:+?]?=",eE:!0}]},t={cN:"meta",b:/^\.PHONY:/,e:/$/,k:{"meta-keyword":".PHONY"},l:/[\.\w]+/},l={cN:"section",b:/^[^\s]+:/,e:/$/,c:[i]};return{aliases:["mk","mak"],k:"define endef undefine ifdef ifndef ifeq ifneq else endif include -include sinclude override export unexport private vpath",l:/[\w-]+/,c:[e.HCM,i,r,a,n,t,l]}});hljs.registerLanguage("java",function(e){var a="[À-ʸa-zA-Z_$][À-ʸa-zA-Z_$0-9]*",t=a+"(<"+a+"(\\s*,\\s*"+a+")*>)?",r="false synchronized int abstract float private char boolean static null if const for true while long strictfp finally protected import native final void enum else break transient catch instanceof byte super volatile case assert short package default double public try this switch continue throws protected public private module requires exports do",s="\\b(0[bB]([01]+[01_]+[01]+|[01]+)|0[xX]([a-fA-F0-9]+[a-fA-F0-9_]+[a-fA-F0-9]+|[a-fA-F0-9]+)|(([\\d]+[\\d_]+[\\d]+|[\\d]+)(\\.([\\d]+[\\d_]+[\\d]+|[\\d]+))?|\\.([\\d]+[\\d_]+[\\d]+|[\\d]+))([eE][-+]?\\d+)?)[lLfF]?",c={cN:"number",b:s,r:0};return{aliases:["jsp"],k:r,i:/<\/|#/,c:[e.C("/\\*\\*","\\*/",{r:0,c:[{b:/\w+@/,r:0},{cN:"doctag",b:"@[A-Za-z]+"}]}),e.CLCM,e.CBCM,e.ASM,e.QSM,{cN:"class",bK:"class interface",e:/[{;=]/,eE:!0,k:"class interface",i:/[:"\[\]]/,c:[{bK:"extends implements"},e.UTM]},{bK:"new throw return else",r:0},{cN:"function",b:"("+t+"\\s+)+"+e.UIR+"\\s*\\(",rB:!0,e:/[{;=]/,eE:!0,k:r,c:[{b:e.UIR+"\\s*\\(",rB:!0,r:0,c:[e.UTM]},{cN:"params",b:/\(/,e:/\)/,k:r,r:0,c:[e.ASM,e.QSM,e.CNM,e.CBCM]},e.CLCM,e.CBCM]},c,{cN:"meta",b:"@[A-Za-z]+"}]}});hljs.registerLanguage("stan",function(e){return{c:[e.HCM,e.CLCM,e.CBCM,{b:e.UIR,l:e.UIR,k:{name:"for in while repeat until if then else",symbol:"bernoulli bernoulli_logit binomial binomial_logit beta_binomial hypergeometric categorical categorical_logit ordered_logistic neg_binomial neg_binomial_2 neg_binomial_2_log poisson poisson_log multinomial normal exp_mod_normal skew_normal student_t cauchy double_exponential logistic gumbel lognormal chi_square inv_chi_square scaled_inv_chi_square exponential inv_gamma weibull frechet rayleigh wiener pareto pareto_type_2 von_mises uniform multi_normal multi_normal_prec multi_normal_cholesky multi_gp multi_gp_cholesky multi_student_t gaussian_dlm_obs dirichlet lkj_corr lkj_corr_cholesky wishart inv_wishart","selector-tag":"int real vector simplex unit_vector ordered positive_ordered row_vector matrix cholesky_factor_corr cholesky_factor_cov corr_matrix cov_matrix",title:"functions model data parameters quantities transformed generated",literal:"true false"},r:0},{cN:"number",b:"0[xX][0-9a-fA-F]+[Li]?\\b",r:0},{cN:"number",b:"0[xX][0-9a-fA-F]+[Li]?\\b",r:0},{cN:"number",b:"\\d+(?:[eE][+\\-]?\\d*)?L\\b",r:0},{cN:"number",b:"\\d+\\.(?!\\d)(?:i\\b)?",r:0},{cN:"number",b:"\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",r:0}]}});hljs.registerLanguage("javascript",function(e){var r="[A-Za-z$_][0-9A-Za-z$_]*",t={keyword:"in of if for while finally var new function do return void else break catch instanceof with throw case default try this switch continue typeof delete let yield const export super debugger as async await static import from as",literal:"true false null undefined NaN Infinity",built_in:"eval isFinite isNaN parseFloat parseInt decodeURI decodeURIComponent encodeURI encodeURIComponent escape unescape Object Function Boolean Error EvalError InternalError RangeError ReferenceError StopIteration SyntaxError TypeError URIError Number Math Date String RegExp Array Float32Array Float64Array Int16Array Int32Array Int8Array Uint16Array Uint32Array Uint8Array Uint8ClampedArray ArrayBuffer DataView JSON Intl arguments require module console window document Symbol Set Map WeakSet WeakMap Proxy Reflect Promise"},a={cN:"number",v:[{b:"\\b(0[bB][01]+)"},{b:"\\b(0[oO][0-7]+)"},{b:e.CNR}],r:0},n={cN:"subst",b:"\\$\\{",e:"\\}",k:t,c:[]},c={cN:"string",b:"`",e:"`",c:[e.BE,n]};n.c=[e.ASM,e.QSM,c,a,e.RM];var s=n.c.concat([e.CBCM,e.CLCM]);return{aliases:["js","jsx"],k:t,c:[{cN:"meta",r:10,b:/^\s*['"]use (strict|asm)['"]/},{cN:"meta",b:/^#!/,e:/$/},e.ASM,e.QSM,c,e.CLCM,e.CBCM,a,{b:/[{,]\s*/,r:0,c:[{b:r+"\\s*:",rB:!0,r:0,c:[{cN:"attr",b:r,r:0}]}]},{b:"("+e.RSR+"|\\b(case|return|throw)\\b)\\s*",k:"return throw case",c:[e.CLCM,e.CBCM,e.RM,{cN:"function",b:"(\\(.*?\\)|"+r+")\\s*=>",rB:!0,e:"\\s*=>",c:[{cN:"params",v:[{b:r},{b:/\(\s*\)/},{b:/\(/,e:/\)/,eB:!0,eE:!0,k:t,c:s}]}]},{b:/</,e:/(\/\w+|\w+\/)>/,sL:"xml",c:[{b:/<\w+\s*\/>/,skip:!0},{b:/<\w+/,e:/(\/\w+|\w+\/)>/,skip:!0,c:[{b:/<\w+\s*\/>/,skip:!0},"self"]}]}],r:0},{cN:"function",bK:"function",e:/\{/,eE:!0,c:[e.inherit(e.TM,{b:r}),{cN:"params",b:/\(/,e:/\)/,eB:!0,eE:!0,c:s}],i:/\[|%/},{b:/\$[(.]/},e.METHOD_GUARD,{cN:"class",bK:"class",e:/[{;=]/,eE:!0,i:/[:"\[\]]/,c:[{bK:"extends"},e.UTM]},{bK:"constructor",e:/\{/,eE:!0}],i:/#(?!!)/}});hljs.registerLanguage("tex",function(c){var e={cN:"tag",b:/\\/,r:0,c:[{cN:"name",v:[{b:/[a-zA-Zа-яА-я]+[*]?/},{b:/[^a-zA-Zа-яА-я0-9]/}],starts:{eW:!0,r:0,c:[{cN:"string",v:[{b:/\[/,e:/\]/},{b:/\{/,e:/\}/}]},{b:/\s*=\s*/,eW:!0,r:0,c:[{cN:"number",b:/-?\d*\.?\d+(pt|pc|mm|cm|in|dd|cc|ex|em)?/}]}]}}]};return{c:[e,{cN:"formula",c:[e],r:0,v:[{b:/\$\$/,e:/\$\$/},{b:/\$/,e:/\$/}]},c.C("%","$",{r:0})]}});hljs.registerLanguage("xml",function(s){var e="[A-Za-z0-9\\._:-]+",t={eW:!0,i:/</,r:0,c:[{cN:"attr",b:e,r:0},{b:/=\s*/,r:0,c:[{cN:"string",endsParent:!0,v:[{b:/"/,e:/"/},{b:/'/,e:/'/},{b:/[^\s"'=<>`]+/}]}]}]};return{aliases:["html","xhtml","rss","atom","xjb","xsd","xsl","plist"],cI:!0,c:[{cN:"meta",b:"<!DOCTYPE",e:">",r:10,c:[{b:"\\[",e:"\\]"}]},s.C("<!--","-->",{r:10}),{b:"<\\!\\[CDATA\\[",e:"\\]\\]>",r:10},{b:/<\?(php)?/,e:/\?>/,sL:"php",c:[{b:"/\\*",e:"\\*/",skip:!0}]},{cN:"tag",b:"<style(?=\\s|>|$)",e:">",k:{name:"style"},c:[t],starts:{e:"</style>",rE:!0,sL:["css","xml"]}},{cN:"tag",b:"<script(?=\\s|>|$)",e:">",k:{name:"script"},c:[t],starts:{e:"</script>",rE:!0,sL:["actionscript","javascript","handlebars","xml"]}},{cN:"meta",v:[{b:/<\?xml/,e:/\?>/,r:10},{b:/<\?\w+/,e:/\?>/}]},{cN:"tag",b:"</?",e:"/?>",c:[{cN:"name",b:/[^\/><\s]+/,r:0},t]}]}});hljs.registerLanguage("markdown",function(e){return{aliases:["md","mkdown","mkd"],c:[{cN:"section",v:[{b:"^#{1,6}",e:"$"},{b:"^.+?\\n[=-]{2,}$"}]},{b:"<",e:">",sL:"xml",r:0},{cN:"bullet",b:"^([*+-]|(\\d+\\.))\\s+"},{cN:"strong",b:"[*_]{2}.+?[*_]{2}"},{cN:"emphasis",v:[{b:"\\*.+?\\*"},{b:"_.+?_",r:0}]},{cN:"quote",b:"^>\\s+",e:"$"},{cN:"code",v:[{b:"^```w*s*$",e:"^```s*$"},{b:"`.+?`"},{b:"^( {4}|	)",e:"$",r:0}]},{b:"^[-\\*]{3,}",e:"$"},{b:"\\[.+?\\][\\(\\[].*?[\\)\\]]",rB:!0,c:[{cN:"string",b:"\\[",e:"\\]",eB:!0,rE:!0,r:0},{cN:"link",b:"\\]\\(",e:"\\)",eB:!0,eE:!0},{cN:"symbol",b:"\\]\\[",e:"\\]",eB:!0,eE:!0}],r:10},{b:/^\[[^\n]+\]:/,rB:!0,c:[{cN:"symbol",b:/\[/,e:/\]/,eB:!0,eE:!0},{cN:"link",b:/:\s*/,e:/$/,eB:!0}]}]}});hljs.registerLanguage("json",function(e){var i={literal:"true false null"},n=[e.QSM,e.CNM],r={e:",",eW:!0,eE:!0,c:n,k:i},t={b:"{",e:"}",c:[{cN:"attr",b:/"/,e:/"/,c:[e.BE],i:"\\n"},e.inherit(r,{b:/:/})],i:"\\S"},c={b:"\\[",e:"\\]",c:[e.inherit(r)],i:"\\S"};return n.splice(n.length,0,t,c),{c:n,k:i,i:"\\S"}});"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type="text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">test</h1>
<h4 class="author">Adeline Shin</h4>
<h4 class="date">3/31/2020</h4>

</div>


<div id="soft-threshold" class="section level1">
<h1>soft threshold</h1>
<pre class="r"><code>soft_threshold = function(beta, r){
  return(sign(beta) * pmax(abs(beta) - r, 0))
}</code></pre>
</div>
<div id="path-wise-coordinate-descendent" class="section level1">
<h1>path-wise coordinate descendent</h1>
<p>this function is used to calculate beta by coordinate descendent, given a specific lambda and an initial beta vector. Since this function has no function to add the intercept and standardize data, please do not use the function directly. Try to use function logit_lasso instead.</p>
<pre class="r"><code>path_co_des = function(x, y, lambda, beta, tol = 0.01){
  
  # calculate the initial loglike loss
  loglike_loss_old = -sum((y * (x %*% beta) - log(1 + exp(x %*% beta)))) + lambda * sum(beta[2:length(beta)])
  
  step = 1
  
  # start to optimize iteratively
  while (TRUE) {
    
    # use a logical vector to denote the differences of each beta, if all the differences for beta are too small, then the vector &quot;changed&quot; will be all FALSE
    changed = rep(TRUE, ncol(x))
    
    # save the beta at last step. If some errors ocurr, we will return the beta we get at last step.
    beta_old = beta
    
    # coordinate descendent
    for (j in 1:ncol(x)) {
      
      # the formula is shown in lecture note
      p = 1 / (1 + exp(-(x %*% beta)))
      w = p * (1 - p)
      w[is_null(w)] = 0 # since the exp function may lead to infinity, let the NaN in w equals to 0
      z = x %*% beta * w + (y - p) # make a few changes in formula compared with that in lecture note to avoid divide by 0 error due to w = 0
      z_j = w * x[, -j] %*% beta[-j]
      beta_new = sum(x[, j] * (z - z_j)) / (t(w) %*% (x[, j]^2))
      if (j &gt; 1) # if beta is not intercept, use soft threshold
        beta_new = soft_threshold(beta_new, lambda)
      
      # if beta changes very little, set the changed flag to FALSE, if all beta changed flags are FALSE at each turn, end up the function and return beta. 
      if (abs(beta_new - beta[j]) &lt; tol) {
        changed[j] = FALSE
      }
      
      # save the new beta
      beta[j] = beta_new
    }
    
    ### coordinate descendent ends at this turn ###
    
    
    # calculate loglike loss after all the coeffient have been updated at this turn
    loglike_loss = -sum(w * (y * (x %*% beta) - log(1 + exp(x %*% beta)))) + lambda * sum(beta[2:length(beta)])

    
    print(paste(&quot;step = &quot;, step, &quot; lambda = &quot;, lambda, &quot; loss: &quot;, loglike_loss))
    
    # if some errors ocurr in loglike loss or this function optimizes at opposite direction, end up the function, and return the beta at previous step.
    if (is.na(loglike_loss) || loglike_loss_old &lt; loglike_loss)
      return(beta_old)
    
    # if all the changed flags equal to FALSE, end up the function, and return the beta at previous step.
    if (sum(changed) == 0) {
      return(beta_old)
    }
    
    # save loglike value at the current step. It will be used to compare at next step.
    loglike_loss_old = loglike_loss
    step = step + 1
  }
}</code></pre>
</div>
<div id="training-model" class="section level1">
<h1>training model</h1>
<p>This function is used as an interface to calculate the logit lasso model and sort the path of lambda. In this funtion, the original data need to be put in and intercept, standardization will be calculated automatically.</p>
<p>Input:</p>
<ul>
<li><p>x: original predictors without intercept and standarization</p></li>
<li><p>y: response variables</p></li>
<li><p>lambda: a scale or a vector of lambdas. The order can be arbitrary. This function will sort the descending order automatically. If parameter include_zero_lambda is TRUE, a zero lambda will append.</p></li>
<li><p>tol: threshold to put in function path_co_des and does not use in this function.</p></li>
<li><p>warm_start: If lambda is a vector will more than 1 element, the initial beta of each lambda will be the same as the result obtained with last lambda. If warm_start is FALSE, the initial beta will be 0 of any beta.</p></li>
<li><p>include_zero_lambda: If this parameter is TRUE, a zero lambda will be appended to parameter lambda. And lambda equals to 0 means no regularization.</p></li>
</ul>
<pre class="r"><code>logit_lasso = function(x, y, lambda, tol = 0.01, warm_start = FALSE, include_zero_lambda = TRUE){
  
  # if parameter &quot;include_zero_lambda&quot; equals to TRUE and no zero lambda in paramter &quot;lambda&quot;. Append zero lambda.
  if (include_zero_lambda &amp;&amp; sum(lambda == 0) == 0) {
    lambda = c(lambda, 0)
  }
  
  
  
  # this part is used to center the predictors and make each column unit which means the length of all column vectors equals to 1. 
  
  # also, due to the same sample space of training data and test data, the test data will be calculated by training data scale. So the scale will be returned at the end of this function.
  colmean = colMeans(x)
  colscale = c()
  for (i in 1:ncol(x)) { # standarize each columns
    x[, i] = x[, i] - colmean[i] # centered
    colscale = c(colscale, sqrt(sum(x[, i] * x[, i]))) # save scale
    x[, i] = x[, i] / sqrt(sum(x[, i] * x[, i])) # make column unit
  }
  
  # add intercept and initialize beta as all 0s
  x = cbind(rep(1, nrow(x)), x)
  beta = matrix(rep(0, ncol(x)))
  
  
  # in this part, we use path of lambda to calculate a list of beta with descending beta
  beta_list = list()
  
  # if &quot;lambda&quot; is a scale:
  if (length(lambda) == 1) {
    beta = path_co_des(x, y, lambda[1], beta, tol)
    beta_list[[paste(&quot;beta -&gt; lambda:&quot;, lambda[1])]] = beta
    
    # return &quot;lambda&quot;, &quot;beta_list&quot;, center factor &quot;colmean&quot;, unitization factor &quot;colscale&quot;
    return(list(lambda = lambda, beta = beta_list, colmean = colmean, colscale = colscale)) 
  }
  
  # if &quot;lambda&quot; is a vector:
  else{
    lambda = sort(lambda, decreasing = TRUE) # sort &quot;lambda&quot; by decreasing order
    for (k in 1:length(lambda)) { # for each lambda, calculate the beta
      if (warm_start)
        beta = path_co_des(x, y, lambda[k], beta, tol)
      else{
        beta = matrix(rep(0, ncol(x)))
        beta = path_co_des(x, y, lambda[k], beta, tol)
      }
      beta_list[[paste(&quot;beta -&gt; lambda:&quot;, lambda[k])]] = beta
    }
    
    # return &quot;lambda&quot;, &quot;beta_list&quot;, center factor &quot;colmean&quot;, unitization factor &quot;colscale&quot;
    return(list(lambda = lambda, beta = beta_list, colmean = colmean, colscale = colscale))
    
  }
}</code></pre>
</div>
<div id="prediction" class="section level1">
<h1>prediction</h1>
<p>this function is used to predict response variable given the model from function logit_lasso and orginal predictors.</p>
<p>input:</p>
<ul>
<li><p>model: the result list from function logit_lasso consist of lambda list lambda, beta list beta_list, center factor colmean, unitization factor colscale. If beta list beta_list consist of different coefficients from different lambda, then for each vector of coefficients, each vector of predicting response variable will be returned, to form a list of predicting response variable.</p></li>
<li><p>x: the original predictor variables.</p></li>
</ul>
<pre class="r"><code>predict = function(model, x){
  
  # use the scale factors for training data to standarize predicting data
  beta_list = model$beta
  colmean = model$colmean
  colscale = model$colscale
  predict_y_list = list()
  for (i in 1:ncol(x)) {
    x[, i] = x[, i] - colmean[i]
    x[, i] = x[, i] / colscale[i]
  }
  
  # add intercept
  x = cbind(rep(1, nrow(x)), x)
  
  # calculate the predicting reponse variable for each beta (or lambda)
  for (i in names(beta_list)) {
    # &quot;i&quot; is the value of lambda
    predict_y = 1 / (1 + exp(-x %*% beta_list[[i]]))
    predict_y[predict_y &lt; 0.5] = 0 
    predict_y[predict_y &gt;= 0.5] = 1
    predict_y_list[[i]] = predict_y
  }
  
  return(predict_y_list)
}</code></pre>
</div>
<div id="criterion-to-select-lambda" class="section level1">
<h1>criterion to select lambda</h1>
<p>This function is used to calculate the performance given predict_y and true_y. If predict_y_list is a list of predict_ys. Then for each predict_y, a performance score will be given, to form a list of scores.</p>
<p>input:</p>
<ul>
<li><p>true_y: dimension(1 * #samples), true response variable.</p></li>
<li><p>predict_y_list: dimension(#lambda * #samples), a list of predict_ys , for each predict_y, performance score will be calculate.</p></li>
<li><p>score: a function to calculate the performance score for a model, given predict_y and true_y</p></li>
</ul>
<pre class="r"><code>criterion = function(true_y, predict_y_list, score){
  result = list()
  for (i in names(predict_y_list)) {
    predict_y = predict_y_list[[i]]
    result[[i]] = score(true_y, predict_y)
  }
  return(result)
} 

#### a score function example --- Accuracy:

accuracy = function(true_y, predict_y){
  return(sum(true_y == predict_y) / length(true_y))
}</code></pre>
</div>
<div id="cross-validation" class="section level1">
<h1>cross validation</h1>
<p>This function is used to calculate different performance for different lambda by cross validationl. At each fold, this function will call functions logit_lasso, predict and criterion to calculate the cv performance for each lambda.</p>
<p>We first split the data to form training data and test data. At each fold, we calculate the predicting performance for each lambda. Also since the model is calculated by pair-wise coordinate descending, the warm start can be exploited at each fold. After the calculation of the final fold, the performances at all folds will be averaged for each lambda.</p>
<p>Input:</p>
<ul>
<li><p>x: orginal predictors. It will be split into training data and test data at each fold.</p></li>
<li><p>y: response variables. It will be split into training data and test data at each fold.</p></li>
<li><p>lambda: lambda list</p></li>
<li><p>model: a function to train model given x, y and lambda. In this file, logit_lasso will be put as this parameter.</p></li>
<li><p>predict: a function to predict response variable given predictors. In this file, funtion predict will be put as this parameter.</p></li>
<li><p>criterion: a function to calculate the performance given predict_y and true_y. In this file, funtion criterion will be put as this parameter.</p></li>
<li><p>score: the score function put in criterion function. Function accuracy is provided, other score function can be written by code runner.</p></li>
<li><p>n_fold: number of fold to cross validate.</p></li>
<li><p>tol: threshold used in function logit_lasso</p></li>
<li><p>warm_start: parameter used in function logit_lasso. If warm_start = TRUE, at each fold, the coefficients of lambdas will be calculated sequently.</p></li>
</ul>
<pre class="r"><code>cv = function(x, y, lambda, model, predict, criterion, score,  n_fold = 5, tol = 0.01, warm_start = FALSE){
  
  # collect the performance score at each fold for each lambda
  loss_fold = list()
  
  # cross validation
  for (fold in 0:(n_fold - 1)) {
    print(paste(&quot;fold: &quot;, fold + 1))
    
    # select the training  samples and test samples.
    test_index = ((fold * length(y) / n_fold) + 1):(((fold + 1) * length(y) / n_fold))
    train_index = (1:length(y))[-test_index]
    train_x = x[train_index, ]
    train_y = y[train_index]
    test_x = x[test_index, ]
    test_y = y[test_index]
    
    # train model
    train_model = model(train_x, train_y, lambda, tol, warm_start)
    
    # predict response variable
    predict_y = predict(train_model, test_x)
    
    # calculate the performance score
    test_loss = criterion(test_y, predict_y, score)
    
    # add the score
    loss_fold[[paste(&quot; fold &quot;, fold)]] = test_loss
    
  }
  
  
  
  
  # for each lambda, calculate an average performance score
  
  lambda_lost = list()
  
  for (l in names(loss_fold[[paste(&quot; fold &quot;, 0)]])) {
    lambda_lost[[l]] = loss_fold[[paste(&quot; fold &quot;, 0)]][[l]]
  }
  for (l in names(loss_fold[[paste(&quot; fold &quot;, fold)]])) {
    for (fold in 1:(n_fold - 1)) {
      lambda_lost[[l]] = lambda_lost[[l]] + loss_fold[[paste(&quot; fold &quot;, fold)]][[l]]
    }
    
    lambda_lost[[l]] = lambda_lost[[l]] / n_fold
  }
  
  # return lambda list and performance score
  return(list(lambda = lambda, performance = lambda_lost))
}</code></pre>
</div>
<div id="loading-the-data-and-run-main-code" class="section level1">
<h1>Loading the data and run main code</h1>
<pre class="r"><code># read data
cancer_data = read.csv(&quot;./breast-cancer.csv&quot;)

x = cancer_data %&gt;% select(-id, -diagnosis) %&gt;% as.matrix()

y = cancer_data %&gt;% 
  select(diagnosis) %&gt;% 
  mutate(diagnosis = as.integer(diagnosis) - 1) %&gt;% 
  as.matrix()




# cross validation by a vector of lambda

lambda = exp(seq(-10, 10, 0.01)) # rescale the log(lambda)

# here we can write another score function and replace function &quot;accuracy&quot;. The way to write can be found at the definition of function &quot;criterion&quot;

# in cv, standard mode cannot use warm start. But here a option is provided for users.
lambda_result = cv(x, y, lambda, logit_lasso, predict, criterion, accuracy, n_fold = 5, tol = 0.01, warm_start = T)</code></pre>
<pre><code>## [1] &quot;fold:  1&quot;
## [1] &quot;step =  1  lambda =  22026.4657948067  loss:  62.2712536167092&quot;
## [1] &quot;step =  2  lambda =  22026.4657948067  loss:  61.4517436350034&quot;
## [1] &quot;step =  3  lambda =  22026.4657948067  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  21807.2987982302  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  21590.3125497062  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  21375.4853504291  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  21162.7957175002  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  20952.2223817786  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  20743.7442857556  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  20537.3405814475  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  20332.9906283122  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  20130.6739911839  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  19930.3704382303  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  19732.0599389292  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  19535.7226620655  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  19341.3389737478  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  19148.8894354453  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  18958.3548020439  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  18769.7160199212  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  18582.9542250422  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  18398.0507410714  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  18214.9870775064  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  18033.7449278285  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  17854.3061676715  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  17676.65285301  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  17500.7672183642  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  17326.6316750244  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  17154.228809291  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  16983.5413807338  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  16814.5523204676  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  16647.2447294456  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  16481.6018767693  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  16317.6071980154  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  16155.2442935794  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  15994.4969270355  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  15835.3490235131  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  15677.7846680892  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  15521.788104197  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  15367.34373205  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  15214.4361070824  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  15063.0499384043  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  14913.1700872726  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  14764.7815655773  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  14617.8695343426  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  14472.4193022429  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  14328.4163241338  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  14185.8461995975  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  14044.6946715028  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  13904.9476245792  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  13766.5910840055  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  13629.6112140124  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  13493.9943164988  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  13359.7268296619  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  13226.7953266411  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  13095.1865141752  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12964.8872312735  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12835.8844478991  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12708.165263666  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12581.7169065495  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12456.5267316084  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12332.582219721  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12209.8709763327  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  12088.380730217  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11968.099332248  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11849.0147541856  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11731.1150874729  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11614.3885420449  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11498.8234451498  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11384.4082401816  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11271.1314855245  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11158.9818534085  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  11047.9481287771  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10938.0192081652  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10829.1840985891  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10721.4319164473  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10614.7518864317  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10509.1333404504  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10404.5657165607  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10301.0385579133  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10198.5415117058  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  10097.0643281483  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9996.59685943788  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9897.12905874391  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9798.65097920349  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9701.15277292652  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9604.6246900112  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9509.05707756873  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9414.44037875829  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9320.76513183108  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9228.02196918439  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9136.2016164247  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  9045.29489144014  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8955.29270348252  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8866.186052258  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8777.96602702724  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8690.62380571415  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8604.15065402384  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8518.53792456912  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8433.77705600563  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8349.85957217593  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8266.77708126167  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8184.52127494457  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8103.08392757538  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  8022.45689535158  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7942.63211550269  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7863.60160548423  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7785.35746217936  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7707.89186110851  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7631.19705564706  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7555.2653762505  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7480.08922968767  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7405.66109828121  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7331.97353915601  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7259.01918349469  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7186.79073580093  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7115.28097316979  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  7044.48274456536  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6974.38897010583  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6904.9926403553  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6836.286815623  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6768.26462526917  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6700.91926701811  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6634.24400627789  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6568.23217546683  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6502.87717334688  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6438.17246436333  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6374.1115779914  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6310.68810808902  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6247.8957122564  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6185.72811120158  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6124.17908811267  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6063.24248803609  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  6002.91221726102  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5943.18224271013  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5884.04659133616  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5825.49934952474  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5767.53466250285  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5710.14673375352  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5653.32982443602  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5597.07825281208  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5541.3863936777  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5486.2486778005  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5431.65959136299  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5377.61367541099  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5324.10552530792  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5271.12979019412  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5218.68117245197  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5166.754427176  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5115.34436164836  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5064.44583481972  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  5014.05375679492  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4964.16308832421  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4914.76884029913  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4865.86607325376  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4817.44989687059  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4769.51546949167  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4722.05799763432  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4675.07273551178  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4628.55498455872  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4582.50009296124  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4536.90345519183  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4491.76051154869  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4447.06674769987  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4402.8176942317  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4359.00892620198  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4315.63606269742  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4272.69476639549  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4230.1807431308  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4188.08974146558  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4146.4175522646  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4105.16000827419  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4064.31298370559  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  4023.87239382231  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3983.83419453164  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3944.19438198031  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3904.948992154  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3866.09410048106  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3827.62582143991  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3789.54030817061  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3751.83375209008  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3714.50238251129  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3677.5424662662  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3640.95030733235  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3604.72224646338  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3568.854660823  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3533.34396362276  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3498.18660376333  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3463.37906547946  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3428.91786798828  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3394.79956514135  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3361.02074507995  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3327.5780298939  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3294.46807528385  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3261.68757022671  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3229.23323664469  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3197.10182907735  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3165.29013435719  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3133.79497128823  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3102.61319032788  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3071.7416732721  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3041.17733294343  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  3010.91711288239  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2980.95798704173  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2951.29695948392  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2921.93106408148  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2892.85736422039  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2864.07295250646  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2835.57495047451  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2807.3605083006  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2779.42680451698  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2751.77104573003  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2724.39046634078  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2697.28232826851  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2670.44392067681  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2643.87255970255  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2617.5655881875  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2591.52037541257  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2565.7343168348  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2540.20483382683  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2514.92937341909  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2489.90540804446  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2465.13043528557  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2440.6019776245  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2416.31758219502  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2392.27482053738  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2368.47128835535  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2344.9046052759  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2321.57241461106  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2298.47238312234  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2275.60220078732  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2252.95958056872  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2230.54225818566  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2208.34799188721  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2186.37456222824  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2164.61977184748  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2143.08144524776  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2121.75742857847  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2100.64558942018  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2079.74381657137  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2059.05001983734  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2038.56212982119  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  2018.27809771681  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1998.19589510412  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1978.3135137461  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1958.62896538806  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1939.14028155876  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1919.84551337356  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1900.74273133958  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1881.83002516269  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1863.10550355652  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1844.56729405329  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1826.21354281661  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1808.04241445606  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1790.05209184367  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1772.24077593218  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1754.60668557515  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1737.14805734885  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1719.86314537592  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1702.75022115076  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1685.80757336666  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1669.03350774476  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1652.42634686448  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1635.98442999593  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1619.7061129337  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1603.58976783251  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1587.63378304445  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1571.83656295772  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1556.19652783716  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1540.71211366621  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1525.38177199056  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1510.20396976326  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1495.17718919145  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1480.29992758455  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1465.57069720398  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1450.98802511446  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1436.5504530366  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1422.25653720118  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1408.1048482047  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1394.09397086646  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1380.22250408705  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1366.48906070825  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1352.89226737426  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1339.43076439442  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1326.10320560722  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1312.90825824566  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1299.84460280403  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1286.91093290588  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1274.10595517346  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1261.4283890983  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1248.87696691325  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1236.45043346563  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1224.14754609174  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1211.96707449258  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1199.90780061084  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1187.9685185091  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1176.14803424917  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1164.4451657728  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1152.85874278339  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1141.38760662897  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1130.03061018637  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1118.78661774649  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1107.65450490071  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1096.63315842846  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1085.72147618592  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1074.91836699577  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1064.22275053809  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1053.63355724232  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1043.1497281803  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1032.7702149604  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1022.49397962264  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1012.31999453492  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  1002.24724229025  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  992.274715605024  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  982.401417218259  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  972.626359791883  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  962.948565812015  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  953.367067491183  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  943.88090667158  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  934.48913472921  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  925.19081247906  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  915.98501008115  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  906.870806947571  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  897.847291650418  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  888.913561830636  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  880.068724107804  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  871.311893990772  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  862.642195789238  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  854.058762526152  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  845.560735851038  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  837.147265954143  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  828.817511481469  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  820.570639450629  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  812.405825167543  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  804.322252143983  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  796.319112015905  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  788.395604462634  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  780.550937126804  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  772.784325535149  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  765.094993020038  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  757.482170641809  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  749.945097111883  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  742.483018716622  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  735.095189241974  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  727.780869898828  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  720.539329249161  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  713.369843132868  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  706.271694595365  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  699.244173815886  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  692.286578036491  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  685.39821149181  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  678.578385339442  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  671.826417591095  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  665.141633044362  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  658.523363215222  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  651.970946271172  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  645.483726965061  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  639.061056569553  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  632.702292812253  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  626.40679981149  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  620.173948012713  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  614.003114125553  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  607.893681061474  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  601.845037872081  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  595.856579688017  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  589.927707658468  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  584.057828891295  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  578.246356393726  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  572.492709013672  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  566.796311381596  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  561.156593852992  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  555.572992451403  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  550.044948812038  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  544.571910125929  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  539.153329084642  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  533.788663825562  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  528.477377877687  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  523.218940108002  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  518.012824668342  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  512.85851094283  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  507.755483495794  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  502.703232020238  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  497.701251286808  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  492.749041093256  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  487.846106214441  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  482.991956352786  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  478.186106089262  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  473.428074834835  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  468.717386782416  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  464.053570859277  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  459.436160679934  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  454.864694499525  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  450.338715167621  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  445.857770082518  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  441.421411145971  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  437.029194718392  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  432.680681574476  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  428.375436859286  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  424.113030044765  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  419.893034886674  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  415.715029381986  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  411.578595726666  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  407.483320273902  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  403.428793492735  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  399.41460992711  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  395.440368155324  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  391.505670749889  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  387.610124237784  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  383.753339061112  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  379.934929538142  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  376.154513824739  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  372.411713876182  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  368.706155409357  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  365.037467865329  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  361.405284372286  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  357.809241708853  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  354.248980267766  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  350.724144019914  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  347.234380478734  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  343.779340664966  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  340.358679071749  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  336.972053630071  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  333.619125674568  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  330.299559909649  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  327.013024375971  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  323.759190417243  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  320.537732647356  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  317.34832891785  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  314.190660285694  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  311.064410981393  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  307.969268377411  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  304.904922956909  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  301.87106828279  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  298.867400967061  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  295.893620640484  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  292.94942992255  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  290.034534391735  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  287.148642556054  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  284.291465823921  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  281.46271847528  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  278.66211763304  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  275.889383234783  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  273.144238004757  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  270.426407426153  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  267.735619713647  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  265.071605786227  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  262.434099240279  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  259.822836322951  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  257.237555905775  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  254.677999458555  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  252.143911023513  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  249.635037189694  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  247.151127067624  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  244.69193226422  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  242.257206857954  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  239.846707374255  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  237.460192761167  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  235.097424365239  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  232.758165907662  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  230.442183460642  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  228.149245424004  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  225.879122502033  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  223.631587680546  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  221.406416204187  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  219.203385553955  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  217.022275424948  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  214.862867704336  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  212.724946449547  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  210.608297866674  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  208.512710289096  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  206.437974156308  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  204.383881992968  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  202.350228388148  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  200.336809974792  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  198.343425409381  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  196.369875351799  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  194.415962445393  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  192.481491297246  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  190.56626845863  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  188.670102405666  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  186.792803520168  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  184.934184070684  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  183.094058193718  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  181.272241875151  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  179.468552931832  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  177.682810993364  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  175.914837484065  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  174.164455605111  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  172.431490316854  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  170.715768321323  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  169.017118044887  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  167.335369621104  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  165.67035487373  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  164.021907299902  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  162.389862053489  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  160.774055928607  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  159.174327343297  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  157.590516323367  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  156.022464486395  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  154.470015025891  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  152.933012695615  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  151.411303794053  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  149.904736149047  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  148.413159102577  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  146.936423495695  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  145.47438165361  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  144.02688737092  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  142.593795896989  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  141.174963921477  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  139.770249560003  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  138.379512339961  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  137.002613186469  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  135.639414408465  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  134.289779684936  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  132.953574051283  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  131.63066388583  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  130.320916896459  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  129.024202107378  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  127.740389846029  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  126.469351730115  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  125.210960654765  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  123.965090779824  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  122.731617517265  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  121.510417518735  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  120.301368663216  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  119.104350044814  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  117.919241960671  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  116.74592589899  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  115.584284527188  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  114.434201680159  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  113.29556234866  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  112.168252667809  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  111.052159905699  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  109.947172452124  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  108.853179807416  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  107.7700725714  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  106.697742432451  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  105.63608215666  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  104.584985577114  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  103.544347583281  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  102.514064110494  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  101.494032129546  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  100.484149636389  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  99.4843156419338  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  98.4944301619463  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  97.514394207054  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  96.5441097728447  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  95.5834798300663  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  94.6324083149241  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  93.6908001194741  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  92.7585610821118  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  91.8355979781567  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  90.9218185105295  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  90.0171313005218  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  89.1214458786587  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  88.2346726756515  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  87.3567230134411  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  86.4875090963295  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  85.6269440022007  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  84.774941673828  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  83.9314169102688  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  83.0962853583438  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  82.2694635042017  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  81.4508686649681  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  80.6404189804771  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  79.8380334050846  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  79.0436316995645  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  78.2571344230842  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  77.4784629252608  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  76.7075393382956  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  75.9442865691873  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  75.1886282920231  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  74.4404889403455  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  73.6997936995958  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  72.9664684996329  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  72.2404400073254  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  71.5216356192192  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  70.8099834542765  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  70.1054123466879  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  69.4078518387552  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  68.7172321738465  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  68.0334842894197  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  67.3565398101166  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  66.6863310409252  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  66.0227909604099  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  65.3658532140099  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  64.715452107403  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  64.0715225999366  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  63.4340002981233  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  62.8028214492017  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  62.1779229347609  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  61.5592422644286  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  60.9467175696222  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  60.340287597362  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  59.7398917041452  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  59.1454698498823  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  58.5569625918924  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  57.9743110789593  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  57.3974570454462  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  56.8263428054691  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  56.2609112471279  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  55.7011058267956  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  55.1468705634638  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  54.5981500331442  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  54.0548893633266  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  53.5170342274912  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  52.9845308396762  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  52.4573259490991  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  51.9353668348315  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  51.4186013005269  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  50.9069776692014  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  50.4004447780655  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  49.8989519734079  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  49.4024491055302  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  48.9108865237319  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  48.4242150713452  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  47.9423860808193  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  47.4653513688535  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  46.9930632315793  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  46.5254744397892  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  46.0625382342145  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  45.6042083208487  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  45.1504388663187  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  44.7011844933009  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  44.2564002759834  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  43.816041735574  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  43.3800648358516  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  42.948425978763  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  42.5210820000628  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  42.0979901649969  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  41.6791081640293  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  41.2643941086108  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  40.8538065269903  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  40.4473043600674  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  40.0448469572867  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  39.6463940725726  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  39.2519058603045  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  38.8613428713325  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  38.4746660490321  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  38.091836725399  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  37.7128166171818  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  37.3375678220537  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  36.9660528148225  loss:  61.4464148832075&quot;
## [1] &quot;step =  1  lambda =  36.598234443678  loss:  65.5599256616665&quot;
## [1] &quot;step =  2  lambda =  36.598234443678  loss:  62.2487235772453&quot;
## [1] &quot;step =  3  lambda =  36.598234443678  loss:  64.9074380417575&quot;
## [1] &quot;step =  1  lambda =  36.2340759264765  loss:  71.6444718907207&quot;
## [1] &quot;step =  2  lambda =  36.2340759264765  loss:  76.2372924420583&quot;
## [1] &quot;step =  1  lambda =  35.8735408470628  loss:  80.0220412718966&quot;
## [1] &quot;step =  2  lambda =  35.8735408470628  loss:  84.564928160548&quot;
## [1] &quot;step =  1  lambda =  35.5165931516285  loss:  87.5788215171192&quot;
## [1] &quot;step =  2  lambda =  35.5165931516285  loss:  91.3088223681928&quot;
## [1] &quot;step =  1  lambda =  35.1631971451066  loss:  94.5853038007082&quot;
## [1] &quot;step =  2  lambda =  35.1631971451066  loss:  97.7317482379222&quot;
## [1] &quot;step =  1  lambda =  34.813317487602  loss:  101.28395924573&quot;
## [1] &quot;step =  2  lambda =  34.813317487602  loss:  104.01465733266&quot;
## [1] &quot;step =  1  lambda =  34.4669191908574  loss:  109.643573375144&quot;
## [1] &quot;step =  2  lambda =  34.4669191908574  loss:  109.325065127274&quot;
## [1] &quot;step =  3  lambda =  34.4669191908574  loss:  109.724555203527&quot;
## [1] &quot;step =  1  lambda =  34.1239676147544  loss:  119.132977197321&quot;
## [1] &quot;step =  2  lambda =  34.1239676147544  loss:  119.069741814729&quot;
## [1] &quot;step =  3  lambda =  34.1239676147544  loss:  117.276110447394&quot;
## [1] &quot;step =  4  lambda =  34.1239676147544  loss:  116.357073795253&quot;
## [1] &quot;step =  5  lambda =  34.1239676147544  loss:  116.149544484064&quot;
## [1] &quot;step =  1  lambda =  33.7844284638495  loss:  126.044137217855&quot;
## [1] &quot;step =  2  lambda =  33.7844284638495  loss:  125.590244813849&quot;
## [1] &quot;step =  3  lambda =  33.7844284638495  loss:  125.69837424479&quot;
## [1] &quot;step =  1  lambda =  33.4482677839449  loss:  135.183800658228&quot;
## [1] &quot;step =  2  lambda =  33.4482677839449  loss:  134.811286197715&quot;
## [1] &quot;step =  3  lambda =  33.4482677839449  loss:  134.069943970646&quot;
## [1] &quot;step =  4  lambda =  33.4482677839449  loss:  133.468561020958&quot;
## [1] &quot;step =  5  lambda =  33.4482677839449  loss:  133.233416782063&quot;
## [1] &quot;step =  6  lambda =  33.4482677839449  loss:  133.176715022564&quot;
## [1] &quot;step =  1  lambda =  33.1154519586923  loss:  142.784929153035&quot;
## [1] &quot;step =  2  lambda =  33.1154519586923  loss:  142.373075117211&quot;
## [1] &quot;step =  3  lambda =  33.1154519586923  loss:  142.347760194073&quot;
## [1] &quot;step =  4  lambda =  33.1154519586923  loss:  141.888516363825&quot;
## [1] &quot;step =  5  lambda =  33.1154519586923  loss:  141.643987009779&quot;
## [1] &quot;step =  6  lambda =  33.1154519586923  loss:  141.556675461806&quot;
## [1] &quot;step =  1  lambda =  32.7859477062319  loss:  151.021160518033&quot;
## [1] &quot;step =  2  lambda =  32.7859477062319  loss:  150.650306763137&quot;
## [1] &quot;step =  3  lambda =  32.7859477062319  loss:  150.560282037758&quot;
## [1] &quot;step =  4  lambda =  32.7859477062319  loss:  150.149976177255&quot;
## [1] &quot;step =  5  lambda =  32.7859477062319  loss:  149.933074114194&quot;
## [1] &quot;step =  6  lambda =  32.7859477062319  loss:  149.847861915367&quot;
## [1] &quot;step =  1  lambda =  32.4597220758638  loss:  159.179465189686&quot;
## [1] &quot;step =  2  lambda =  32.4597220758638  loss:  158.849505284074&quot;
## [1] &quot;step =  3  lambda =  32.4597220758638  loss:  158.714780333161&quot;
## [1] &quot;step =  4  lambda =  32.4597220758638  loss:  158.349447518176&quot;
## [1] &quot;step =  5  lambda =  32.4597220758638  loss:  158.155376565911&quot;
## [1] &quot;step =  6  lambda =  32.4597220758638  loss:  158.072175312112&quot;
## [1] &quot;step =  1  lambda =  32.1367424447532  loss:  167.275503923544&quot;
## [1] &quot;step =  2  lambda =  32.1367424447532  loss:  166.988978510231&quot;
## [1] &quot;step =  3  lambda =  32.1367424447532  loss:  166.814789373112&quot;
## [1] &quot;step =  4  lambda =  32.1367424447532  loss:  166.490607496278&quot;
## [1] &quot;step =  5  lambda =  32.1367424447532  loss:  166.316528964484&quot;
## [1] &quot;step =  6  lambda =  32.1367424447532  loss:  166.235919236587&quot;
## [1] &quot;step =  7  lambda =  32.1367424447532  loss:  166.199909295429&quot;
## [1] &quot;step =  1  lambda =  31.8169765146677  loss:  175.286040924124&quot;
## [1] &quot;step =  2  lambda =  31.8169765146677  loss:  175.052956388105&quot;
## [1] &quot;step =  3  lambda =  31.8169765146677  loss:  174.862501637383&quot;
## [1] &quot;step =  4  lambda =  31.8169765146677  loss:  174.578617309815&quot;
## [1] &quot;step =  5  lambda =  31.8169765146677  loss:  174.422952755893&quot;
## [1] &quot;step =  6  lambda =  31.8169765146677  loss:  174.345749700147&quot;
## [1] &quot;step =  7  lambda =  31.8169765146677  loss:  174.308097067544&quot;
## [1] &quot;step =  1  lambda =  31.500392308748  loss:  183.275595676503&quot;
## [1] &quot;step =  2  lambda =  31.500392308748  loss:  183.090213954878&quot;
## [1] &quot;step =  3  lambda =  31.500392308748  loss:  182.871584263781&quot;
## [1] &quot;step =  4  lambda =  31.500392308748  loss:  182.622158127616&quot;
## [1] &quot;step =  5  lambda =  31.500392308748  loss:  182.482612557498&quot;
## [1] &quot;step =  6  lambda =  31.500392308748  loss:  182.409183823672&quot;
## [1] &quot;step =  7  lambda =  31.500392308748  loss:  182.370795262279&quot;
## [1] &quot;step =  1  lambda =  31.1869581683094  loss:  191.225823874061&quot;
## [1] &quot;step =  2  lambda =  31.1869581683094  loss:  191.086396856397&quot;
## [1] &quot;step =  3  lambda =  31.1869581683094  loss:  190.8477207948&quot;
## [1] &quot;step =  4  lambda =  31.1869581683094  loss:  190.628987612723&quot;
## [1] &quot;step =  5  lambda =  31.1869581683094  loss:  190.50361779674&quot;
## [1] &quot;step =  6  lambda =  31.1869581683094  loss:  190.434126541322&quot;
## [1] &quot;step =  7  lambda =  31.1869581683094  loss:  190.395663688767&quot;
## [1] &quot;step =  1  lambda =  30.876642749677  loss:  199.144797314894&quot;
## [1] &quot;step =  2  lambda =  30.876642749677  loss:  199.049212949718&quot;
## [1] &quot;step =  3  lambda =  30.876642749677  loss:  198.798652843584&quot;
## [1] &quot;step =  4  lambda =  30.876642749677  loss:  198.607533823415&quot;
## [1] &quot;step =  5  lambda =  30.876642749677  loss:  198.494561814764&quot;
## [1] &quot;step =  6  lambda =  30.876642749677  loss:  198.428993248633&quot;
## [1] &quot;step =  7  lambda =  30.876642749677  loss:  198.390913135899&quot;
## [1] &quot;step =  1  lambda =  30.5694150210502  loss:  207.04108493361&quot;
## [1] &quot;step =  2  lambda =  30.5694150210502  loss:  206.987574038438&quot;
## [1] &quot;step =  3  lambda =  30.5694150210502  loss:  206.73444097619&quot;
## [1] &quot;step =  4  lambda =  30.5694150210502  loss:  206.566639459122&quot;
## [1] &quot;step =  5  lambda =  30.5694150210502  loss:  206.464458015149&quot;
## [1] &quot;step =  6  lambda =  30.5694150210502  loss:  206.402709528104&quot;
## [1] &quot;step =  7  lambda =  30.5694150210502  loss:  206.365332588592&quot;
## [1] &quot;step =  1  lambda =  30.2652442594001  loss:  214.923776778596&quot;
## [1] &quot;step =  2  lambda =  30.2652442594001  loss:  214.909046536311&quot;
## [1] &quot;step =  3  lambda =  30.2652442594001  loss:  214.664916097054&quot;
## [1] &quot;step =  4  lambda =  30.2652442594001  loss:  214.514477428129&quot;
## [1] &quot;step =  5  lambda =  30.2652442594001  loss:  214.422121916311&quot;
## [1] &quot;step =  6  lambda =  30.2652442594001  loss:  214.364344170212&quot;
## [1] &quot;step =  7  lambda =  30.2652442594001  loss:  214.328081325756&quot;
## [1] &quot;step =  1  lambda =  29.964100047397  loss:  222.802290102962&quot;
## [1] &quot;step =  2  lambda =  29.964100047397  loss:  222.824669074364&quot;
## [1] &quot;step =  1  lambda =  29.6659522703689  loss:  231.236002077428&quot;
## [1] &quot;step =  2  lambda =  29.6659522703689  loss:  231.111743250627&quot;
## [1] &quot;step =  3  lambda =  29.6659522703689  loss:  230.75437325604&quot;
## [1] &quot;step =  4  lambda =  29.6659522703689  loss:  230.552040278781&quot;
## [1] &quot;step =  5  lambda =  29.6659522703689  loss:  230.431771545745&quot;
## [1] &quot;step =  6  lambda =  29.6659522703689  loss:  230.352498098508&quot;
## [1] &quot;step =  7  lambda =  29.6659522703689  loss:  230.299759318198&quot;
## [1] &quot;step =  8  lambda =  29.6659522703689  loss:  230.264575463936&quot;
## [1] &quot;step =  9  lambda =  29.6659522703689  loss:  230.241063063459&quot;
## [1] &quot;step =  1  lambda =  29.3707711132895  loss:  238.571895757092&quot;
## [1] &quot;step =  2  lambda =  29.3707711132895  loss:  238.677768180423&quot;
## [1] &quot;step =  1  lambda =  29.0785270577971  loss:  246.984659070913&quot;
## [1] &quot;step =  2  lambda =  29.0785270577971  loss:  246.985034240835&quot;
## [1] &quot;step =  1  lambda =  28.7891908792427  loss:  255.248483089157&quot;
## [1] &quot;step =  2  lambda =  28.7891908792427  loss:  255.208735594939&quot;
## [1] &quot;step =  3  lambda =  28.7891908792427  loss:  254.859678723962&quot;
## [1] &quot;step =  4  lambda =  28.7891908792427  loss:  254.626191673427&quot;
## [1] &quot;step =  5  lambda =  28.7891908792427  loss:  254.553063290943&quot;
## [1] &quot;step =  6  lambda =  28.7891908792427  loss:  254.480881117877&quot;
## [1] &quot;step =  7  lambda =  28.7891908792427  loss:  254.428149125118&quot;
## [1] &quot;step =  8  lambda =  28.7891908792427  loss:  254.390296755672&quot;
## [1] &quot;step =  9  lambda =  28.7891908792427  loss:  254.363114516716&quot;
## [1] &quot;step =  10  lambda =  28.7891908792427  loss:  254.34356782468&quot;
## [1] &quot;step =  1  lambda =  28.5027336437673  loss:  262.516930306545&quot;
## [1] &quot;step =  2  lambda =  28.5027336437673  loss:  262.767482750388&quot;
## [1] &quot;step =  1  lambda =  28.2191267054086  loss:  270.997743625066&quot;
## [1] &quot;step =  2  lambda =  28.2191267054086  loss:  271.22410470425&quot;
## [1] &quot;step =  1  lambda =  27.9383417032365  loss:  279.443941522064&quot;
## [1] &quot;step =  2  lambda =  27.9383417032365  loss:  279.690779237509&quot;
## [1] &quot;step =  1  lambda =  27.6603505585168  loss:  287.912579973029&quot;
## [1] &quot;step =  2  lambda =  27.6603505585168  loss:  288.230137047516&quot;
## [1] &quot;step =  1  lambda =  27.3851254719032  loss:  296.464494732761&quot;
## [1] &quot;step =  2  lambda =  27.3851254719032  loss:  296.895662286386&quot;
## [1] &quot;step =  1  lambda =  27.1126389206579  loss:  305.151888889167&quot;
## [1] &quot;step =  2  lambda =  27.1126389206579  loss:  305.73661638427&quot;
## [1] &quot;step =  1  lambda =  26.8428636558986  loss:  314.023042169003&quot;
## [1] &quot;step =  2  lambda =  26.8428636558986  loss:  314.802476080168&quot;
## [1] &quot;step =  1  lambda =  26.575772699874  loss:  323.126612040637&quot;
## [1] &quot;step =  2  lambda =  26.575772699874  loss:  324.146562214203&quot;
## [1] &quot;step =  1  lambda =  26.3113393432659  loss:  332.515171326527&quot;
## [1] &quot;step =  2  lambda =  26.3113393432659  loss:  333.829343310586&quot;
## [1] &quot;step =  1  lambda =  26.0495371425183  loss:  342.248446018243&quot;
## [1] &quot;step =  2  lambda =  26.0495371425183  loss:  343.921881494671&quot;
## [1] &quot;step =  1  lambda =  25.7903399171931  loss:  352.396707039662&quot;
## [1] &quot;step =  2  lambda =  25.7903399171931  loss:  354.509875724751&quot;
## [1] &quot;step =  1  lambda =  25.5337217473515  loss:  363.044760522722&quot;
## [1] &quot;step =  2  lambda =  25.5337217473515  loss:  365.698814159269&quot;
## [1] &quot;step =  1  lambda =  25.2796569709629  loss:  374.297039089569&quot;
## [1] &quot;step =  2  lambda =  25.2796569709629  loss:  377.620896472392&quot;
## [1] &quot;step =  1  lambda =  25.0281201813378  loss:  386.284444563644&quot;
## [1] &quot;step =  2  lambda =  25.0281201813378  loss:  390.444662245447&quot;
## [1] &quot;step =  1  lambda =  24.7790862245877  loss:  399.173864274495&quot;
## [1] &quot;step =  2  lambda =  24.7790862245877  loss:  404.388724179527&quot;
## [1] &quot;step =  1  lambda =  24.5325301971094  loss:  413.181738636693&quot;
## [1] &quot;step =  2  lambda =  24.5325301971094  loss:  419.741758278121&quot;
## [1] &quot;step =  1  lambda =  24.2884274430945  loss:  428.593798442581&quot;
## [1] &quot;step =  2  lambda =  24.2884274430945  loss:  436.892113127188&quot;
## [1] &quot;step =  1  lambda =  24.0467535520645  loss:  445.794278604798&quot;
## [1] &quot;step =  2  lambda =  24.0467535520645  loss:  456.372311877687&quot;
## [1] &quot;step =  1  lambda =  23.8074843564287  loss:  465.309790338591&quot;
## [1] &quot;step =  2  lambda =  23.8074843564287  loss:  478.926636063579&quot;
## [1] &quot;step =  1  lambda =  23.5705959290681  loss:  487.875891309418&quot;
## [1] &quot;step =  2  lambda =  23.5705959290681  loss:  505.614080524911&quot;
## [1] &quot;step =  1  lambda =  23.3360645809427  loss:  514.538408830182&quot;
## [1] &quot;step =  2  lambda =  23.3360645809427  loss:  537.963630467372&quot;
## [1] &quot;step =  1  lambda =  23.1038668587222  loss:  546.806136470934&quot;
## [1] &quot;step =  2  lambda =  23.1038668587222  loss:  578.200599949931&quot;
## [1] &quot;step =  1  lambda =  22.8739795424408  loss:  592.046357493875&quot;
## [1] &quot;step =  2  lambda =  22.8739795424408  loss:  648.806826637105&quot;
## [1] &quot;step =  1  lambda =  22.6463796431754  loss:  668.917169244813&quot;
## [1] &quot;step =  2  lambda =  22.6463796431754  loss:  785.244574020798&quot;
## [1] &quot;step =  1  lambda =  22.4210444007463  loss:  805.424159633187&quot;
## [1] &quot;step =  1  lambda =  22.1979512814416  loss:  826.906690813209&quot;
## [1] &quot;step =  1  lambda =  21.9770779757634  loss:  852.417106699429&quot;
## [1] &quot;step =  1  lambda =  21.7584023961971  loss:  877.275151470399&quot;
## [1] &quot;step =  1  lambda =  21.5419026750024  loss:  913.624955965229&quot;
## [1] &quot;step =  1  lambda =  21.3275571620269  loss:  951.944538587516&quot;
## [1] &quot;step =  1  lambda =  21.1153444225406  loss:  989.305718903878&quot;
## [1] &quot;step =  1  lambda =  20.9052432350928  loss:  1032.31903140278&quot;
## [1] &quot;step =  1  lambda =  20.6972325893895  loss:  1094.77339295601&quot;
## [1] &quot;step =  1  lambda =  20.4912916841929  loss:  1161.50276537588&quot;
## [1] &quot;step =  1  lambda =  20.2873999252409  loss:  1223.50002366942&quot;
## [1] &quot;step =  1  lambda =  20.0855369231877  loss:  1280.82185344898&quot;
## [1] &quot;step =  1  lambda =  19.8856824915647  loss:  1333.77090429988&quot;
## [1] &quot;step =  1  lambda =  19.6878166447624  loss:  1382.60829862845&quot;
## [1] &quot;step =  1  lambda =  19.4919195960311  loss:  1427.57732521083&quot;
## [1] &quot;step =  1  lambda =  19.2979717555028  loss:  1468.91427064458&quot;
## [1] &quot;step =  1  lambda =  19.1059537282317  loss:  1506.85081535024&quot;
## [1] &quot;step =  1  lambda =  18.915846312255  loss:  1541.61260831504&quot;
## [1] &quot;step =  1  lambda =  18.7276304966729  loss:  1577.91809039478&quot;
## [1] &quot;step =  1  lambda =  18.5412874597469  loss:  1614.04878533474&quot;
## [1] &quot;step =  1  lambda =  18.3567985670179  loss:  1646.17948832698&quot;
## [1] &quot;step =  1  lambda =  18.1741453694431  loss:  1678.09638100833&quot;
## [1] &quot;step =  1  lambda =  17.9933096015503  loss:  1706.96896223411&quot;
## [1] &quot;step =  1  lambda =  17.8142731796122  loss:  1732.3834690239&quot;
## [1] &quot;step =  1  lambda =  17.6370181998373  loss:  1754.81493718274&quot;
## [1] &quot;step =  1  lambda =  17.46152693658  loss:  1777.34498068297&quot;
## [1] &quot;step =  1  lambda =  17.2877818405676  loss:  1798.97802514342&quot;
## [1] &quot;step =  1  lambda =  17.1157655371459  loss:  1820.45443194722&quot;
## [1] &quot;step =  1  lambda =  16.945460824541  loss:  1839.66651246648&quot;
## [1] &quot;step =  1  lambda =  16.7768506721399  loss:  1856.80526931858&quot;
## [1] &quot;step =  1  lambda =  16.6099182187867  loss:  1872.03795811961&quot;
## [1] &quot;step =  1  lambda =  16.4446467710971  loss:  1885.58300647365&quot;
## [1] &quot;step =  1  lambda =  16.2810198017884  loss:  1897.73624464745&quot;
## [1] &quot;step =  1  lambda =  16.1190209480276  loss:  1908.3413003408&quot;
## [1] &quot;step =  1  lambda =  15.958634009794  loss:  1917.51207656726&quot;
## [1] &quot;step =  1  lambda =  15.7998429482604  loss:  1925.35345957199&quot;
## [1] &quot;step =  1  lambda =  15.6426318841882  loss:  1931.96219906082&quot;
## [1] &quot;step =  1  lambda =  15.4869850963399  loss:  1937.4276254453&quot;
## [1] &quot;step =  1  lambda =  15.3328870199072  loss:  1942.20454797797&quot;
## [1] &quot;step =  1  lambda =  15.1803222449539  loss:  1946.49532722861&quot;
## [1] &quot;step =  1  lambda =  15.0292755148754  loss:  1949.87867901085&quot;
## [1] &quot;step =  1  lambda =  14.8797317248728  loss:  1952.42198007519&quot;
## [1] &quot;step =  1  lambda =  14.7316759204426  loss:  1954.18708206249&quot;
## [1] &quot;step =  1  lambda =  14.5850932958808  loss:  1955.23075837513&quot;
## [1] &quot;step =  1  lambda =  14.4399691928029  loss:  1955.60512267896&quot;
## [1] &quot;step =  1  lambda =  14.2962890986776  loss:  1955.35801875481&quot;
## [1] &quot;step =  1  lambda =  14.1540386453758  loss:  1954.53338191959&quot;
## [1] &quot;step =  1  lambda =  14.0132036077336  loss:  1953.1715727056&quot;
## [1] &quot;step =  1  lambda =  13.8737699021299  loss:  1951.61925180729&quot;
## [1] &quot;step =  1  lambda =  13.7357235850779  loss:  1949.67117603699&quot;
## [1] &quot;step =  1  lambda =  13.5990508518309  loss:  1947.29217282338&quot;
## [1] &quot;step =  1  lambda =  13.4637380350017  loss:  1944.50944734087&quot;
## [1] &quot;step =  1  lambda =  13.3297716031958  loss:  1941.34810355896&quot;
## [1] &quot;step =  1  lambda =  13.1971381596584  loss:  1937.83132193674&quot;
## [1] &quot;step =  1  lambda =  13.0658244409346  loss:  1933.98052311998&quot;
## [1] &quot;step =  1  lambda =  12.9358173155431  loss:  1929.81551829165&quot;
## [1] &quot;step =  1  lambda =  12.807103782663  loss:  1925.35464693881&quot;
## [1] &quot;step =  1  lambda =  12.6796709708339  loss:  1921.24310949559&quot;
## [1] &quot;step =  1  lambda =  12.5535061366682  loss:  1917.10233561064&quot;
## [1] &quot;step =  1  lambda =  12.4285966635775  loss:  1912.72458173944&quot;
## [1] &quot;step =  1  lambda =  12.3049300605104  loss:  1908.12043742299&quot;
## [1] &quot;step =  1  lambda =  12.1824939607035  loss:  1903.29979706549&quot;
## [1] &quot;step =  1  lambda =  12.0612761204447  loss:  1899.27113113913&quot;
## [1] &quot;step =  1  lambda =  11.9412644178491  loss:  1895.45953561819&quot;
## [1] &quot;step =  1  lambda =  11.8224468516464  loss:  1891.41903931754&quot;
## [1] &quot;step =  1  lambda =  11.7048115399809  loss:  1887.15776353224&quot;
## [1] &quot;step =  1  lambda =  11.5883467192234  loss:  1882.68342766685&quot;
## [1] &quot;step =  1  lambda =  11.4730407427948  loss:  1878.00338223167&quot;
## [1] &quot;step =  1  lambda =  11.3588820800015  loss:  1873.12463895048&quot;
## [1] &quot;step =  1  lambda =  11.2458593148818  loss:  1868.05389823698&quot;
## [1] &quot;step =  1  lambda =  11.1339611450653  loss:  1862.79757427281&quot;
## [1] &quot;step =  1  lambda =  11.0231763806416  loss:  1857.36181789796&quot;
## [1] &quot;step =  1  lambda =  10.913493943042  loss:  1851.75253750427&quot;
## [1] &quot;step =  1  lambda =  10.8049028639313  loss:  1845.97541810396&quot;
## [1] &quot;step =  1  lambda =  10.6973922841111  loss:  1840.03593872883&quot;
## [1] &quot;step =  1  lambda =  10.5909514524338  loss:  1833.93938830062&quot;
## [1] &quot;step =  1  lambda =  10.4855697247276  loss:  1827.69088009971&quot;
## [1] &quot;step =  1  lambda =  10.3812365627318  loss:  1821.29536494727&quot;
## [1] &quot;step =  1  lambda =  10.2779415330434  loss:  1814.75764320558&quot;
## [1] &quot;step =  1  lambda =  10.1756743060733  loss:  1808.0823756916&quot;
## [1] &quot;step =  1  lambda =  10.0744246550136  loss:  1801.27409359058&quot;
## [1] &quot;step =  1  lambda =  9.97418245481473  loss:  1794.33720744917&quot;
## [1] &quot;step =  1  lambda =  9.87493768117319  loss:  1787.27601532075&quot;
## [1] &quot;step =  1  lambda =  9.77668040952892  loss:  1780.09471013007&quot;
## [1] &quot;step =  1  lambda =  9.67940081407284  loss:  1772.79738631909&quot;
## [1] &quot;step =  1  lambda =  9.58308916676438  loss:  1765.38804583111&quot;
## [1] &quot;step =  1  lambda =  9.48773583635853  loss:  1757.87060348644&quot;
## [1] &quot;step =  1  lambda =  9.39333128744278  loss:  1750.248891799&quot;
## [1] &quot;step =  1  lambda =  9.29986607948359  loss:  1742.52666527987&quot;
## [1] &quot;step =  1  lambda =  9.20733086588226  loss:  1734.70760427094&quot;
## [1] &quot;step =  1  lambda =  9.11571639304031  loss:  1726.79531834883&quot;
## [1] &quot;step =  1  lambda =  9.02501349943413  loss:  1718.79334933674&quot;
## [1] &quot;step =  1  lambda =  8.93521311469874  loss:  1710.70517395974&quot;
## [1] &quot;step =  1  lambda =  8.84630625872088  loss:  1702.53420617636&quot;
## [1] &quot;step =  1  lambda =  8.75828404074083  loss:  1694.28379921764&quot;
## [1] &quot;step =  1  lambda =  8.67113765846346  loss:  1685.95724736259&quot;
## [1] &quot;step =  1  lambda =  8.5848583971779  loss:  1677.55778747728&quot;
## [1] &quot;step =  1  lambda =  8.49943762888613  loss:  1669.08860034283&quot;
## [1] &quot;step =  1  lambda =  8.41486681144014  loss:  1660.55281179596&quot;
## [1] &quot;step =  1  lambda =  8.3311374876877  loss:  1651.95349370411&quot;
## [1] &quot;step =  1  lambda =  8.24824128462666  loss:  1643.29366479551&quot;
## [1] &quot;step =  1  lambda =  8.16616991256765  loss:  1634.57629136308&quot;
## [1] &quot;step =  1  lambda =  8.08491516430506  loss:  1624.40465398274&quot;
## [1] &quot;step =  1  lambda =  8.00446891429635  loss:  1613.02162288747&quot;
## [1] &quot;step =  1  lambda =  7.92482311784949  loss:  1601.67992189099&quot;
## [1] &quot;step =  1  lambda =  7.84596981031845  loss:  1590.37937231132&quot;
## [1] &quot;step =  1  lambda =  7.76790110630678  loss:  1579.11985471019&quot;
## [1] &quot;step =  1  lambda =  7.69060919887901  loss:  1567.90130513391&quot;
## [1] &quot;step =  1  lambda =  7.61408635877998  loss:  1556.7237112833&quot;
## [1] &quot;step =  1  lambda =  7.53832493366192  loss:  1545.58710867072&quot;
## [1] &quot;step =  1  lambda =  7.46331734731919  loss:  1534.49157681155&quot;
## [1] &quot;step =  1  lambda =  7.38905609893065  loss:  1523.43723548781&quot;
## [1] &quot;step =  1  lambda =  7.31553376230957  loss:  1512.33469958749&quot;
## [1] &quot;step =  1  lambda =  7.24274298516102  loss:  1501.26124696948&quot;
## [1] &quot;step =  1  lambda =  7.17067648834662  loss:  1490.22181726234&quot;
## [1] &quot;step =  1  lambda =  7.09932706515664  loss:  1479.21753219284&quot;
## [1] &quot;step =  1  lambda =  7.0286875805893  loss:  1468.24949519056&quot;
## [1] &quot;step =  1  lambda =  6.95875097063727  loss:  1457.31878922399&quot;
## [1] &quot;step =  1  lambda =  6.88951024158129  loss:  1446.42647486189&quot;
## [1] &quot;step =  1  lambda =  6.82095846929075  loss:  1435.57358855051&quot;
## [1] &quot;step =  1  lambda =  6.75308879853129  loss:  1424.76114109661&quot;
## [1] &quot;step =  1  lambda =  6.68589444227927  loss:  1413.990116346&quot;
## [1] &quot;step =  1  lambda =  6.61936868104308  loss:  1403.26147004708&quot;
## [1] &quot;step =  1  lambda =  6.55350486219115  loss:  1392.57612888862&quot;
## [1] &quot;step =  1  lambda =  6.48829639928672  loss:  1381.93498970153&quot;
## [1] &quot;step =  1  lambda =  6.42373677142913  loss:  1371.33891881376&quot;
## [1] &quot;step =  1  lambda =  6.35981952260183  loss:  1360.78875154837&quot;
## [1] &quot;step =  1  lambda =  6.29653826102666  loss:  1350.28529185454&quot;
## [1] &quot;step =  1  lambda =  6.23388665852472  loss:  1339.82931206176&quot;
## [1] &quot;step =  1  lambda =  6.17185844988355  loss:  1329.42155274789&quot;
## [1] &quot;step =  1  lambda =  6.11044743223061  loss:  1319.06272271171&quot;
## [1] &quot;step =  1  lambda =  6.04964746441295  loss:  1308.75349904152&quot;
## [1] &quot;step =  1  lambda =  5.98945246638312  loss:  1298.49452727116&quot;
## [1] &quot;step =  1  lambda =  5.92985641859114  loss:  1288.28642161562&quot;
## [1] &quot;step =  1  lambda =  5.8708533613826  loss:  1278.12976527859&quot;
## [1] &quot;step =  1  lambda =  5.81243739440259  loss:  1268.02511082475&quot;
## [1] &quot;step =  1  lambda =  5.75460267600573  loss:  1257.97298061&quot;
## [1] &quot;step =  1  lambda =  5.69734342267199  loss:  1247.97386726323&quot;
## [1] &quot;step =  1  lambda =  5.64065390842832  loss:  1238.02823421345&quot;
## [1] &quot;step =  1  lambda =  5.58452846427606  loss:  1228.13651625685&quot;
## [1] &quot;step =  1  lambda =  5.52896147762401  loss:  1218.29912015816&quot;
## [1] &quot;step =  1  lambda =  5.47394739172721  loss:  1208.51642528163&quot;
## [1] &quot;step =  1  lambda =  5.4194807051312  loss:  1198.78878424677&quot;
## [1] &quot;step =  1  lambda =  5.36555597112197  loss:  1189.11652360467&quot;
## [1] &quot;step =  1  lambda =  5.31216779718117  loss:  1179.49994453087&quot;
## [1] &quot;step =  1  lambda =  5.2593108444469  loss:  1169.93932353103&quot;
## [1] &quot;step =  1  lambda =  5.20697982717985  loss:  1160.26745103884&quot;
## [1] &quot;step =  1  lambda =  5.15516951223468  loss:  1150.28692326649&quot;
## [1] &quot;step =  1  lambda =  5.10387471853673  loss:  1140.37943246995&quot;
## [1] &quot;step =  1  lambda =  5.05309031656387  loss:  1130.54479160524&quot;
## [1] &quot;step =  1  lambda =  5.00281122783359  loss:  1120.78279887958&quot;
## [1] &quot;step =  1  lambda =  4.95303242439511  loss:  1111.09323848297&quot;
## [1] &quot;step =  1  lambda =  4.90374892832662  loss:  1101.47588130483&quot;
## [1] &quot;step =  1  lambda =  4.85495581123743  loss:  1091.93048563459&quot;
## [1] &quot;step =  1  lambda =  4.80664819377518  loss:  1082.45679784477&quot;
## [1] &quot;step =  1  lambda =  4.75882124513786  loss:  1073.05455305578&quot;
## [1] &quot;step =  1  lambda =  4.71147018259074  loss:  1063.66078842873&quot;
## [1] &quot;step =  1  lambda =  4.66459027098813  loss:  1054.24282542851&quot;
## [1] &quot;step =  1  lambda =  4.61817682229978  loss:  1044.89650017061&quot;
## [1] &quot;step =  1  lambda =  4.57222519514216  loss:  1035.6216034535&quot;
## [1] &quot;step =  1  lambda =  4.52673079431425  loss:  1026.41791310478&quot;
## [1] &quot;step =  1  lambda =  4.48168907033806  loss:  1017.28519468034&quot;
## [1] &quot;step =  1  lambda =  4.43709551900367  loss:  1008.22320213448&quot;
## [1] &quot;step =  1  lambda =  4.39294568091876  loss:  999.231678461751&quot;
## [1] &quot;step =  1  lambda =  4.34923514106274  loss:  990.310356311459&quot;
## [1] &quot;step =  1  lambda =  4.30595952834521  loss:  981.458958575584&quot;
## [1] &quot;step =  1  lambda =  4.26311451516882  loss:  972.677198951057&quot;
## [1] &quot;step =  1  lambda =  4.22069581699655  loss:  963.964782477274&quot;
## [1] &quot;step =  1  lambda =  4.17869919192325  loss:  955.321406049714&quot;
## [1] &quot;step =  1  lambda =  4.13712044025139  loss:  946.746758910557&quot;
## [1] &quot;step =  1  lambda =  4.09595540407118  loss:  938.240523117193&quot;
## [1] &quot;step =  1  lambda =  4.05519996684468  loss:  929.802373989465&quot;
## [1] &quot;step =  1  lambda =  4.0148500529942  loss:  921.431980536514&quot;
## [1] &quot;step =  1  lambda =  3.97490162749475  loss:  913.12900586404&quot;
## [1] &quot;step =  1  lambda =  3.93535069547048  loss:  904.893107562815&quot;
## [1] &quot;step =  1  lambda =  3.89619330179521  loss:  896.723938079216&quot;
## [1] &quot;step =  1  lambda =  3.85742553069697  loss:  888.621145068572&quot;
## [1] &quot;step =  1  lambda =  3.81904350536634  loss:  880.584371732031&quot;
## [1] &quot;step =  1  lambda =  3.78104338756878  loss:  872.613257137714&quot;
## [1] &quot;step =  1  lambda =  3.74342137726086  loss:  864.707436526819&quot;
## [1] &quot;step =  1  lambda =  3.7061737122102  loss:  856.866541605351&quot;
## [1] &quot;step =  1  lambda =  3.66929666761925  loss:  848.937446549878&quot;
## [1] &quot;step =  1  lambda =  3.63278655575281  loss:  840.939981419627&quot;
## [1] &quot;step =  1  lambda =  3.59663972556928  loss:  833.014577326645&quot;
## [1] &quot;step =  1  lambda =  3.56085256235552  loss:  825.160675451875&quot;
## [1] &quot;step =  1  lambda =  3.52542148736538  loss:  817.377717965529&quot;
## [1] &quot;step =  1  lambda =  3.49034295746184  loss:  809.665148199376&quot;
## [1] &quot;step =  1  lambda =  3.45561346476268  loss:  802.022410809229&quot;
## [1] &quot;step =  1  lambda =  3.42122953628968  loss:  794.448951928085&quot;
## [1] &quot;step =  1  lambda =  3.38718773362134  loss:  786.944219310289&quot;
## [1] &quot;step =  1  lambda =  3.35348465254903  loss:  779.507662467142&quot;
## [1] &quot;step =  1  lambda =  3.32011692273655  loss:  772.13873279431&quot;
## [1] &quot;step =  1  lambda =  3.28708120738312  loss:  764.836883691419&quot;
## [1] &quot;step =  1  lambda =  3.25437420288967  loss:  757.60157067419&quot;
## [1] &quot;step =  1  lambda =  3.2219926385285  loss:  750.488090662414&quot;
## [1] &quot;step =  1  lambda =  3.18993327611618  loss:  743.622887560277&quot;
## [1] &quot;step =  1  lambda =  3.15819290968977  loss:  736.816693227327&quot;
## [1] &quot;step =  1  lambda =  3.12676836518616  loss:  730.069068845907&quot;
## [1] &quot;step =  1  lambda =  3.09565650012471  loss:  723.379577110171&quot;
## [1] &quot;step =  1  lambda =  3.06485420329301  loss:  716.747782314299&quot;
## [1] &quot;step =  1  lambda =  3.03435839443567  loss:  710.173250434283&quot;
## [1] &quot;step =  1  lambda =  3.00416602394643  loss:  703.655549203635&quot;
## [1] &quot;step =  1  lambda =  2.97427407256306  loss:  697.194248183299&quot;
## [1] &quot;step =  1  lambda =  2.94467955106552  loss:  690.788918826129&quot;
## [1] &quot;step =  1  lambda =  2.915379499977  loss:  684.439134536186&quot;
## [1] &quot;step =  1  lambda =  2.88637098926796  loss:  678.144470723169&quot;
## [1] &quot;step =  1  lambda =  2.85765111806317  loss:  671.904504852245&quot;
## [1] &quot;step =  1  lambda =  2.82921701435156  loss:  665.718816489553&quot;
## [1] &quot;step =  1  lambda =  2.80106583469908  loss:  659.58698734362&quot;
## [1] &quot;step =  1  lambda =  2.7731947639643  loss:  653.508601302962&quot;
## [1] &quot;step =  1  lambda =  2.74560101501692  loss:  647.483244470065&quot;
## [1] &quot;step =  1  lambda =  2.71828182845905  loss:  641.510505192004&quot;
## [1] &quot;step =  1  lambda =  2.69123447234926  loss:  635.589974087884&quot;
## [1] &quot;step =  1  lambda =  2.66445624192942  loss:  629.721244073325&quot;
## [1] &quot;step =  1  lambda =  2.63794445935415  loss:  623.903910382166&quot;
## [1] &quot;step =  1  lambda =  2.61169647342312  loss:  618.137570585576&quot;
## [1] &quot;step =  1  lambda =  2.58570965931585  loss:  612.42182460875&quot;
## [1] &quot;step =  1  lambda =  2.55998141832927  loss:  606.756274745338&quot;
## [1] &quot;step =  1  lambda =  2.53450917761785  loss:  601.140525669781&quot;
## [1] &quot;step =  1  lambda =  2.5092903899363  loss:  595.574184447673&quot;
## [1] &quot;step =  1  lambda =  2.48432253338482  loss:  590.056860544315&quot;
## [1] &quot;step =  1  lambda =  2.45960311115695  loss:  584.588165831568&quot;
## [1] &quot;step =  1  lambda =  2.43512965128988  loss:  579.16771459314&quot;
## [1] &quot;step =  1  lambda =  2.41089970641721  loss:  573.795123528415&quot;
## [1] &quot;step =  1  lambda =  2.38691085352428  loss:  568.470011754934&quot;
## [1] &quot;step =  1  lambda =  2.36316069370579  loss:  563.192000809629&quot;
## [1] &quot;step =  1  lambda =  2.33964685192599  loss:  557.960714648909&quot;
## [1] &quot;step =  1  lambda =  2.31636697678109  loss:  552.775779647674&quot;
## [1] &quot;step =  1  lambda =  2.29331874026418  loss:  547.636824597359&quot;
## [1] &quot;step =  1  lambda =  2.27049983753241  loss:  542.543480703077&quot;
## [1] &quot;step =  1  lambda =  2.24790798667647  loss:  537.495381579923&quot;
## [1] &quot;step =  1  lambda =  2.22554092849247  loss:  532.49216324853&quot;
## [1] &quot;step =  1  lambda =  2.20339642625594  loss:  527.533464129921&quot;
## [1] &quot;step =  1  lambda =  2.1814722654982  loss:  522.618925039728&quot;
## [1] &quot;step =  1  lambda =  2.15976625378491  loss:  517.748189181831&quot;
## [1] &quot;step =  1  lambda =  2.13827622049682  loss:  512.920902141458&quot;
## [1] &quot;step =  1  lambda =  2.11700001661267  loss:  508.13671187781&quot;
## [1] &quot;step =  1  lambda =  2.09593551449437  loss:  503.395268716249&quot;
## [1] &quot;step =  1  lambda =  2.07508060767412  loss:  498.696225340081&quot;
## [1] &quot;step =  1  lambda =  2.05443321064389  loss:  494.039236781992&quot;
## [1] &quot;step =  1  lambda =  2.03399125864675  loss:  489.423960415151&quot;
## [1] &quot;step =  1  lambda =  2.01375270747048  loss:  484.850055944026&quot;
## [1] &quot;step =  1  lambda =  1.99371553324308  loss:  480.317185394941&quot;
## [1] &quot;step =  1  lambda =  1.97387773223045  loss:  475.798086784238&quot;
## [1] &quot;step =  1  lambda =  1.95423732063594  loss:  471.299865681216&quot;
## [1] &quot;step =  1  lambda =  1.93479233440203  loss:  466.843136527484&quot;
## [1] &quot;step =  1  lambda =  1.9155408290139  loss:  462.427534097364&quot;
## [1] &quot;step =  1  lambda =  1.89648087930495  loss:  458.052696323399&quot;
## [1] &quot;step =  1  lambda =  1.87761057926434  loss:  453.718264262564&quot;
## [1] &quot;step =  1  lambda =  1.85892804184634  loss:  449.423882063096&quot;
## [1] &quot;step =  1  lambda =  1.84043139878164  loss:  445.169196931918&quot;
## [1] &quot;step =  1  lambda =  1.82211880039051  loss:  440.953859102664&quot;
## [1] &quot;step =  1  lambda =  1.80398841539786  loss:  436.777521804276&quot;
## [1] &quot;step =  1  lambda =  1.78603843075007  loss:  432.639841230172&quot;
## [1] &quot;step =  1  lambda =  1.76826705143374  loss:  428.540476507967&quot;
## [1] &quot;step =  1  lambda =  1.7506725002961  loss:  424.479089669742&quot;
## [1] &quot;step =  1  lambda =  1.7332530178674  loss:  420.455345622842&quot;
## [1] &quot;step =  1  lambda =  1.71600686218486  loss:  416.468912121192&quot;
## [1] &quot;step =  1  lambda =  1.69893230861855  loss:  412.51945973712&quot;
## [1] &quot;step =  1  lambda =  1.68202764969889  loss:  408.606661833682&quot;
## [1] &quot;step =  1  lambda =  1.66529119494589  loss:  404.730194537462&quot;
## [1] &quot;step =  1  lambda =  1.64872127070013  loss:  400.889736711846&quot;
## [1] &quot;step =  1  lambda =  1.63231621995538  loss:  397.08496993076&quot;
## [1] &quot;step =  1  lambda =  1.61607440219289  loss:  393.315578452855&quot;
## [1] &quot;step =  1  lambda =  1.59999419321736  loss:  389.58124919612&quot;
## [1] &quot;step =  1  lambda =  1.58407398499448  loss:  385.881671712937&quot;
## [1] &quot;step =  1  lambda =  1.56831218549017  loss:  382.216538165539&quot;
## [1] &quot;step =  1  lambda =  1.55270721851134  loss:  378.585543301872&quot;
## [1] &quot;step =  1  lambda =  1.53725752354828  loss:  374.988384431859&quot;
## [1] &quot;step =  1  lambda =  1.52196155561863  loss:  371.424761404036&quot;
## [1] &quot;step =  1  lambda =  1.50681778511285  loss:  367.89437658257&quot;
## [1] &quot;step =  1  lambda =  1.49182469764127  loss:  364.396934824631&quot;
## [1] &quot;step =  1  lambda =  1.47698079388264  loss:  360.932143458127&quot;
## [1] &quot;step =  1  lambda =  1.46228458943423  loss:  357.499712259771&quot;
## [1] &quot;step =  1  lambda =  1.44773461466333  loss:  354.099353433488&quot;
## [1] &quot;step =  1  lambda =  1.43332941456034  loss:  350.73078158915&quot;
## [1] &quot;step =  1  lambda =  1.41906754859326  loss:  347.393713721619&quot;
## [1] &quot;step =  1  lambda =  1.40494759056359  loss:  344.0878691901&quot;
## [1] &quot;step =  1  lambda =  1.39096812846378  loss:  340.812969697795&quot;
## [1] &quot;step =  1  lambda =  1.37712776433596  loss:  337.56873927185&quot;
## [1] &quot;step =  1  lambda =  1.36342511413218  loss:  334.35490424357&quot;
## [1] &quot;step =  1  lambda =  1.349858807576  loss:  331.17119322893&quot;
## [1] &quot;step =  1  lambda =  1.33642748802547  loss:  328.017337109335&quot;
## [1] &quot;step =  1  lambda =  1.32312981233744  loss:  324.893069012648&quot;
## [1] &quot;step =  1  lambda =  1.30996445073325  loss:  321.798124294472&quot;
## [1] &quot;step =  1  lambda =  1.29693008666577  loss:  318.732240519668&quot;
## [1] &quot;step =  1  lambda =  1.28402541668774  loss:  315.695157444126&quot;
## [1] &quot;step =  1  lambda =  1.2712491503214  loss:  312.686616996757&quot;
## [1] &quot;step =  1  lambda =  1.25860000992948  loss:  309.706363261714&quot;
## [1] &quot;step =  1  lambda =  1.24607673058738  loss:  306.754142460834&quot;
## [1] &quot;step =  1  lambda =  1.23367805995674  loss:  303.829702936295&quot;
## [1] &quot;step =  1  lambda =  1.22140275816017  loss:  300.932795133478&quot;
## [1] &quot;step =  1  lambda =  1.20924959765725  loss:  298.06317158403&quot;
## [1] &quot;step =  1  lambda =  1.19721736312181  loss:  295.220586889135&quot;
## [1] &quot;step =  1  lambda =  1.18530485132037  loss:  292.40479770296&quot;
## [1] &quot;step =  1  lambda =  1.17351087099181  loss:  289.615562716302&quot;
## [1] &quot;step =  1  lambda =  1.16183424272828  loss:  286.852642640407&quot;
## [1] &quot;step =  1  lambda =  1.15027379885723  loss:  284.115800190977&quot;
## [1] &quot;step =  1  lambda =  1.13882838332462  loss:  281.404800072337&quot;
## [1] &quot;step =  1  lambda =  1.12749685157938  loss:  278.719408961781&quot;
## [1] &quot;step =  1  lambda =  1.11627807045887  loss:  276.059395494072&quot;
## [1] &quot;step =  1  lambda =  1.10517091807565  loss:  273.424530246113&quot;
## [1] &quot;step =  1  lambda =  1.09417428370521  loss:  270.814585721764&quot;
## [1] &quot;step =  1  lambda =  1.08328706767496  loss:  268.229336336817&quot;
## [1] &quot;step =  1  lambda =  1.07250818125422  loss:  265.668558404114&quot;
## [1] &quot;step =  1  lambda =  1.06183654654536  loss:  263.132030118814&quot;
## [1] &quot;step =  1  lambda =  1.05127109637602  loss:  260.619531543804&quot;
## [1] &quot;step =  1  lambda =  1.04081077419239  loss:  258.130844595235&quot;
## [1] &quot;step =  1  lambda =  1.03045453395352  loss:  255.665753028209&quot;
## [1] &quot;step =  1  lambda =  1.02020134002676  loss:  253.224042422588&quot;
## [1] &quot;step =  1  lambda =  1.01005016708417  loss:  250.80550016893&quot;
## [1] &quot;step =  1  lambda =  1  loss:  248.409915454559&quot;
## [1] &quot;step =  1  lambda =  0.990049833749168  loss:  246.03707924975&quot;
## [1] &quot;step =  1  lambda =  0.980198673306756  loss:  243.686784294039&quot;
## [1] &quot;step =  1  lambda =  0.970445533548509  loss:  241.358825082645&quot;
## [1] &quot;step =  1  lambda =  0.960789439152324  loss:  239.052997853019&quot;
## [1] &quot;step =  1  lambda =  0.951229424500715  loss:  236.76910057149&quot;
## [1] &quot;step =  1  lambda =  0.941764533584248  loss:  234.506932920034&quot;
## [1] &quot;step =  1  lambda =  0.932393819905948  loss:  232.266296283147&quot;
## [1] &quot;step =  1  lambda =  0.923116346386636  loss:  230.046993734821&quot;
## [1] &quot;step =  1  lambda =  0.913931185271228  loss:  227.84883002563&quot;
## [1] &quot;step =  1  lambda =  0.90483741803596  loss:  225.671611569914&quot;
## [1] &quot;step =  1  lambda =  0.895834135296529  loss:  223.515146433066&quot;
## [1] &quot;step =  1  lambda =  0.886920436717158  loss:  221.379244318914&quot;
## [1] &quot;step =  1  lambda =  0.878095430920562  loss:  219.263716557203&quot;
## [1] &quot;step =  1  lambda =  0.869358235398805  loss:  217.168376091171&quot;
## [1] &quot;step =  1  lambda =  0.860707976425057  loss:  215.09303746522&quot;
## [1] &quot;step =  1  lambda =  0.852143788966211  loss:  213.037516812671&quot;
## [1] &quot;step =  1  lambda =  0.843664816596384  loss:  211.001631843623&quot;
## [1] &quot;step =  1  lambda =  0.835270211411272  loss:  208.985201832889&quot;
## [1] &quot;step =  1  lambda =  0.826959133943363  loss:  206.988047608024&quot;
## [1] &quot;step =  1  lambda =  0.818730753077982  loss:  205.00999153744&quot;
## [1] &quot;step =  1  lambda =  0.810584245970188  loss:  203.050857518606&quot;
## [1] &quot;step =  1  lambda =  0.802518797962478  loss:  201.110470966327&quot;
## [1] &quot;step =  1  lambda =  0.794533602503334  loss:  199.188658801115&quot;
## [1] &quot;step =  1  lambda =  0.786627861066553  loss:  197.285249437628&quot;
## [1] &quot;step =  1  lambda =  0.778800783071405  loss:  195.400072773206&quot;
## [1] &quot;step =  1  lambda =  0.771051585803566  loss:  193.531736099228&quot;
## [1] &quot;step =  1  lambda =  0.763379494336853  loss:  191.68100401629&quot;
## [1] &quot;step =  1  lambda =  0.755783741455726  loss:  189.848042706899&quot;
## [1] &quot;step =  1  lambda =  0.748263567578566  loss:  188.032686971351&quot;
## [1] &quot;step =  1  lambda =  0.740818220681719  loss:  186.234773055917&quot;
## [1] &quot;step =  1  lambda =  0.733446956224289  loss:  184.454138641455&quot;
## [1] &quot;step =  1  lambda =  0.726149037073691  loss:  182.690622832114&quot;
## [1] &quot;step =  1  lambda =  0.718923733431926  loss:  180.944066144107&quot;
## [1] &quot;step =  1  lambda =  0.71177032276261  loss:  179.214310494565&quot;
## [1] &quot;step =  1  lambda =  0.704688089718714  loss:  177.501199190469&quot;
## [1] &quot;step =  1  lambda =  0.697676326071031  loss:  175.804576917652&quot;
## [1] &quot;step =  1  lambda =  0.690734330637355  loss:  174.124289729885&quot;
## [1] &quot;step =  1  lambda =  0.683861409212356  loss:  172.460185038031&quot;
## [1] &quot;step =  1  lambda =  0.677056874498164  loss:  170.812111599273&quot;
## [1] &quot;step =  1  lambda =  0.670320046035639  loss:  169.179919506421&quot;
## [1] &quot;step =  1  lambda =  0.663650250136319  loss:  167.563460177281&quot;
## [1] &quot;step =  1  lambda =  0.657046819815057  loss:  165.962586344109&quot;
## [1] &quot;step =  1  lambda =  0.650509094723317  loss:  164.377152043128&quot;
## [1] &quot;step =  1  lambda =  0.644036421083142  loss:  162.807012604117&quot;
## [1] &quot;step =  1  lambda =  0.637628151621774  loss:  161.252024640071&quot;
## [1] &quot;step =  1  lambda =  0.631283645506927  loss:  159.712046036933&quot;
## [1] &quot;step =  1  lambda =  0.6250022682827  loss:  158.186935943394&quot;
## [1] &quot;step =  1  lambda =  0.618783391806141  loss:  156.676554760757&quot;
## [1] &quot;step =  1  lambda =  0.612626394184416  loss:  155.18076413288&quot;
## [1] &quot;step =  1  lambda =  0.606530659712633  loss:  153.699426936176&quot;
## [1] &quot;step =  2  lambda =  0.606530659712633  loss:  173.285966388093&quot;
## [1] &quot;step =  1  lambda =  0.600495578812266  loss:  171.609199836344&quot;
## [1] &quot;step =  2  lambda =  0.600495578812266  loss:  184.481353320228&quot;
## [1] &quot;step =  1  lambda =  0.594520547970195  loss:  182.68539888464&quot;
## [1] &quot;step =  2  lambda =  0.594520547970195  loss:  195.137217763004&quot;
## [1] &quot;step =  1  lambda =  0.588604969678356  loss:  193.231299167601&quot;
## [1] &quot;step =  2  lambda =  0.588604969678356  loss:  203.933199257047&quot;
## [1] &quot;step =  1  lambda =  0.58274825237399  loss:  201.932809770148&quot;
## [1] &quot;step =  2  lambda =  0.58274825237399  loss:  210.962773596512&quot;
## [1] &quot;step =  1  lambda =  0.576949810380487  loss:  208.895468002476&quot;
## [1] &quot;step =  2  lambda =  0.576949810380487  loss:  216.294711117353&quot;
## [1] &quot;step =  1  lambda =  0.571209063848815  loss:  214.168607894056&quot;
## [1] &quot;step =  2  lambda =  0.571209063848815  loss:  219.668722535479&quot;
## [1] &quot;step =  1  lambda =  0.565525438699537  loss:  217.512377882977&quot;
## [1] &quot;step =  2  lambda =  0.565525438699537  loss:  222.296186749232&quot;
## [1] &quot;step =  1  lambda =  0.559898366565402  loss:  220.111844016441&quot;
## [1] &quot;step =  2  lambda =  0.559898366565402  loss:  225.170012663774&quot;
## [1] &quot;step =  1  lambda =  0.554327284734507  loss:  222.958229718974&quot;
## [1] &quot;step =  2  lambda =  0.554327284734507  loss:  226.998281451697&quot;
## [1] &quot;step =  1  lambda =  0.548811636094027  loss:  224.772172968358&quot;
## [1] &quot;step =  2  lambda =  0.548811636094027  loss:  228.740129787321&quot;
## [1] &quot;step =  1  lambda =  0.5433508690745  loss:  226.493530320618&quot;
## [1] &quot;step =  2  lambda =  0.5433508690745  loss:  230.452348559019&quot;
## [1] &quot;step =  1  lambda =  0.537944437594675  loss:  228.188229250145&quot;
## [1] &quot;step =  2  lambda =  0.537944437594675  loss:  231.780336060251&quot;
## [1] &quot;step =  1  lambda =  0.532591801006898  loss:  229.502099538801&quot;
## [1] &quot;step =  2  lambda =  0.532591801006898  loss:  232.935297235782&quot;
## [1] &quot;step =  1  lambda =  0.527292424043048  loss:  230.642590492356&quot;
## [1] &quot;step =  2  lambda =  0.527292424043048  loss:  233.916440027409&quot;
## [1] &quot;step =  1  lambda =  0.522045776761016  loss:  231.613583953615&quot;
## [1] &quot;step =  2  lambda =  0.522045776761016  loss:  234.747150931283&quot;
## [1] &quot;step =  1  lambda =  0.516851334491699  loss:  232.435660251371&quot;
## [1] &quot;step =  2  lambda =  0.516851334491699  loss:  235.452364016987&quot;
## [1] &quot;step =  1  lambda =  0.511708577786543  loss:  233.133503800058&quot;
## [1] &quot;step =  2  lambda =  0.511708577786543  loss:  236.056891791853&quot;
## [1] &quot;step =  1  lambda =  0.50661699236559  loss:  233.731680126318&quot;
## [1] &quot;step =  2  lambda =  0.50661699236559  loss:  236.580693204207&quot;
## [1] &quot;step =  1  lambda =  0.501576069066056  loss:  234.249949606026&quot;
## [1] &quot;step =  2  lambda =  0.501576069066056  loss:  237.038289407739&quot;
## [1] &quot;step =  1  lambda =  0.49658530379141  loss:  234.702689213437&quot;
## [1] &quot;step =  2  lambda =  0.49658530379141  loss:  237.414307974032&quot;
## [1] &quot;step =  1  lambda =  0.491644197460966  loss:  235.074952958895&quot;
## [1] &quot;step =  2  lambda =  0.491644197460966  loss:  237.710740700145&quot;
## [1] &quot;step =  1  lambda =  0.486752255959971  loss:  235.368292095854&quot;
## [1] &quot;step =  2  lambda =  0.486752255959971  loss:  237.975651699805&quot;
## [1] &quot;step =  1  lambda =  0.481908990090202  loss:  235.630333488101&quot;
## [1] &quot;step =  2  lambda =  0.481908990090202  loss:  238.216743743093&quot;
## [1] &quot;step =  1  lambda =  0.477113915521034  loss:  235.868803131322&quot;
## [1] &quot;step =  2  lambda =  0.477113915521034  loss:  238.424726415915&quot;
## [1] &quot;step =  1  lambda =  0.472366552741015  loss:  236.07450379791&quot;
## [1] &quot;step =  2  lambda =  0.472366552741015  loss:  238.593526741932&quot;
## [1] &quot;step =  1  lambda =  0.467666427009909  loss:  236.241423454451&quot;
## [1] &quot;step =  2  lambda =  0.467666427009909  loss:  238.71914078075&quot;
## [1] &quot;step =  1  lambda =  0.463013068311228  loss:  236.365597793815&quot;
## [1] &quot;step =  2  lambda =  0.463013068311228  loss:  238.798964525243&quot;
## [1] &quot;step =  1  lambda =  0.458406011305224  loss:  236.444448089908&quot;
## [1] &quot;step =  2  lambda =  0.458406011305224  loss:  238.818435293673&quot;
## [1] &quot;step =  1  lambda =  0.453844795282356  loss:  236.466641972001&quot;
## [1] &quot;step =  2  lambda =  0.453844795282356  loss:  238.432168381242&quot;
## [1] &quot;step =  1  lambda =  0.449328964117222  loss:  236.084465138381&quot;
## [1] &quot;step =  2  lambda =  0.449328964117222  loss:  238.204213336903&quot;
## [1] &quot;step =  1  lambda =  0.444858066222941  loss:  235.858565006803&quot;
## [1] &quot;step =  2  lambda =  0.444858066222941  loss:  238.021253546991&quot;
## [1] &quot;step =  1  lambda =  0.440431654505999  loss:  235.677214442712&quot;
## [1] &quot;step =  2  lambda =  0.440431654505999  loss:  237.843330598102&quot;
## [1] &quot;step =  1  lambda =  0.436049286321536  loss:  235.500858945236&quot;
## [1] &quot;step =  2  lambda =  0.436049286321536  loss:  237.650054373809&quot;
## [1] &quot;step =  1  lambda =  0.43171052342908  loss:  235.309312032627&quot;
## [1] &quot;step =  2  lambda =  0.43171052342908  loss:  237.423896944805&quot;
## [1] &quot;step =  1  lambda =  0.427414931948727  loss:  235.086948779704&quot;
## [1] &quot;step =  2  lambda =  0.427414931948727  loss:  237.151119269594&quot;
## [1] &quot;step =  1  lambda =  0.423162082317749  loss:  234.816667096846&quot;
## [1] &quot;step =  2  lambda =  0.423162082317749  loss:  236.846861594352&quot;
## [1] &quot;step =  1  lambda =  0.418951549247639  loss:  234.515225613468&quot;
## [1] &quot;step =  2  lambda =  0.418951549247639  loss:  236.503459277133&quot;
## [1] &quot;step =  1  lambda =  0.414782911681582  loss:  234.175035089315&quot;
## [1] &quot;step =  2  lambda =  0.414782911681582  loss:  236.12087303699&quot;
## [1] &quot;step =  1  lambda =  0.410655752752345  loss:  233.796056054038&quot;
## [1] &quot;step =  2  lambda =  0.410655752752345  loss:  235.700193265616&quot;
## [1] &quot;step =  1  lambda =  0.406569659740599  loss:  233.379367313622&quot;
## [1] &quot;step =  2  lambda =  0.406569659740599  loss:  235.242772061559&quot;
## [1] &quot;step =  1  lambda =  0.402524224033636  loss:  232.926306925872&quot;
## [1] &quot;step =  2  lambda =  0.402524224033636  loss:  234.750070741154&quot;
## [1] &quot;step =  1  lambda =  0.398519041084514  loss:  232.438321212624&quot;
## [1] &quot;step =  2  lambda =  0.398519041084514  loss:  234.223610012428&quot;
## [1] &quot;step =  1  lambda =  0.394553710371601  loss:  231.916915393566&quot;
## [1] &quot;step =  2  lambda =  0.394553710371601  loss:  233.664934905632&quot;
## [1] &quot;step =  1  lambda =  0.390627835358521  loss:  231.363618838854&quot;
## [1] &quot;step =  2  lambda =  0.390627835358521  loss:  233.075585739765&quot;
## [1] &quot;step =  1  lambda =  0.386741023454502  loss:  230.779956305773&quot;
## [1] &quot;step =  2  lambda =  0.386741023454502  loss:  232.457075542321&quot;
## [1] &quot;step =  1  lambda =  0.382892885975112  loss:  230.167425569923&quot;
## [1] &quot;step =  2  lambda =  0.382892885975112  loss:  231.810873817689&quot;
## [1] &quot;step =  1  lambda =  0.379083038103399  loss:  229.527481345648&quot;
## [1] &quot;step =  2  lambda =  0.379083038103399  loss:  231.138395821753&quot;
## [1] &quot;step =  1  lambda =  0.375311098851399  loss:  228.861524660534&quot;
## [1] &quot;step =  2  lambda =  0.375311098851399  loss:  230.440996252994&quot;
## [1] &quot;step =  1  lambda =  0.371576691022046  loss:  228.170896604339&quot;
## [1] &quot;step =  2  lambda =  0.371576691022046  loss:  229.71996629793&quot;
## [1] &quot;step =  1  lambda =  0.367879441171442  loss:  227.456875400252&quot;
## [1] &quot;step =  2  lambda =  0.367879441171442  loss:  228.976533108258&quot;
## [1] &quot;step =  1  lambda =  0.364218979571523  loss:  226.720675884633&quot;
## [1] &quot;step =  2  lambda =  0.364218979571523  loss:  228.211860958551&quot;
## [1] &quot;step =  1  lambda =  0.360594940173078  loss:  225.963450651278&quot;
## [1] &quot;step =  2  lambda =  0.360594940173078  loss:  227.427053500309&quot;
## [1] &quot;step =  1  lambda =  0.357006960569148  loss:  225.186292281652&quot;
## [1] &quot;step =  2  lambda =  0.357006960569148  loss:  226.623156674833&quot;
## [1] &quot;step =  1  lambda =  0.35345468195878  loss:  224.390236227727&quot;
## [1] &quot;step =  2  lambda =  0.35345468195878  loss:  225.801161968476&quot;
## [1] &quot;step =  1  lambda =  0.349937749111156  loss:  223.576264034077&quot;
## [1] &quot;step =  2  lambda =  0.349937749111156  loss:  224.962009789655&quot;
## [1] &quot;step =  1  lambda =  0.346455810330057  loss:  222.745306680677&quot;
## [1] &quot;step =  2  lambda =  0.346455810330057  loss:  224.106592819998&quot;
## [1] &quot;step =  1  lambda =  0.343008517418707  loss:  221.898247900264&quot;
## [1] &quot;step =  2  lambda =  0.343008517418707  loss:  223.235759246101&quot;
## [1] &quot;step =  1  lambda =  0.339595525644939  loss:  221.035927377557&quot;
## [1] &quot;step =  2  lambda =  0.339595525644939  loss:  222.350315817072&quot;
## [1] &quot;step =  1  lambda =  0.336216493706733  loss:  220.159143776103&quot;
## [1] &quot;step =  2  lambda =  0.336216493706733  loss:  221.451030700062&quot;
## [1] &quot;step =  1  lambda =  0.33287108369808  loss:  219.268657565142&quot;
## [1] &quot;step =  2  lambda =  0.33287108369808  loss:  220.538636124006&quot;
## [1] &quot;step =  1  lambda =  0.329558961075189  loss:  218.365193636847&quot;
## [1] &quot;step =  2  lambda =  0.329558961075189  loss:  219.613830813461&quot;
## [1] &quot;step =  1  lambda =  0.32627979462304  loss:  217.449443715762&quot;
## [1] &quot;step =  2  lambda =  0.32627979462304  loss:  218.677282221412&quot;
## [1] &quot;step =  1  lambda =  0.323033256422253  loss:  216.522068569248&quot;
## [1] &quot;step =  2  lambda =  0.323033256422253  loss:  217.729628573779&quot;
## [1] &quot;step =  1  lambda =  0.319819021816304  loss:  215.583700031503&quot;
## [1] &quot;step =  2  lambda =  0.319819021816304  loss:  216.771480740072&quot;
## [1] &quot;step =  1  lambda =  0.316636769379053  loss:  214.634942855481&quot;
## [1] &quot;step =  2  lambda =  0.316636769379053  loss:  215.803423945018&quot;
## [1] &quot;step =  1  lambda =  0.313486180882605  loss:  213.676376407371&quot;
## [1] &quot;step =  2  lambda =  0.313486180882605  loss:  214.826019335528&quot;
## [1] &quot;step =  1  lambda =  0.310366941265485  loss:  212.708556217865&quot;
## [1] &quot;step =  2  lambda =  0.310366941265485  loss:  213.839805416486&quot;
## [1] &quot;step =  1  lambda =  0.307278738601131  loss:  211.732015403559&quot;
## [1] &quot;step =  2  lambda =  0.307278738601131  loss:  212.845299367711&quot;
## [1] &quot;step =  1  lambda =  0.304221264066704  loss:  210.747265970711&quot;
## [1] &quot;step =  2  lambda =  0.304221264066704  loss:  211.842349020844&quot;
## [1] &quot;step =  1  lambda =  0.301194211912202  loss:  209.754177112874&quot;
## [1] &quot;step =  2  lambda =  0.301194211912202  loss:  210.831124272788&quot;
## [1] &quot;step =  1  lambda =  0.298197279429888  loss:  208.752876669299&quot;
## [1] &quot;step =  2  lambda =  0.298197279429888  loss:  209.814998603499&quot;
## [1] &quot;step =  1  lambda =  0.295230166924014  loss:  207.74672598177&quot;
## [1] &quot;step =  2  lambda =  0.295230166924014  loss:  208.804021327898&quot;
## [1] &quot;step =  1  lambda =  0.292292577680859  loss:  206.745526481252&quot;
## [1] &quot;step =  2  lambda =  0.292292577680859  loss:  207.820518034312&quot;
## [1] &quot;step =  1  lambda =  0.289384217939051  loss:  205.769956462866&quot;
## [1] &quot;step =  2  lambda =  0.289384217939051  loss:  206.997793128407&quot;
## [1] &quot;step =  1  lambda =  0.28650479686019  loss:  204.955379736889&quot;
## [1] &quot;step =  2  lambda =  0.28650479686019  loss:  206.17758672654&quot;
## [1] &quot;step =  1  lambda =  0.28365402649977  loss:  204.143218456098&quot;
## [1] &quot;step =  2  lambda =  0.28365402649977  loss:  205.311743480738&quot;
## [1] &quot;step =  1  lambda =  0.28083162177838  loss:  203.285883550625&quot;
## [1] &quot;step =  2  lambda =  0.28083162177838  loss:  204.405929817701&quot;
## [1] &quot;step =  1  lambda =  0.278037300453194  loss:  202.388981813102&quot;
## [1] &quot;step =  2  lambda =  0.278037300453194  loss:  203.467623425185&quot;
## [1] &quot;step =  1  lambda =  0.275270783089753  loss:  201.459259673669&quot;
## [1] &quot;step =  2  lambda =  0.275270783089753  loss:  202.496879039785&quot;
## [1] &quot;step =  1  lambda =  0.272531793034013  loss:  200.498083001769&quot;
## [1] &quot;step =  2  lambda =  0.272531793034013  loss:  201.500174132142&quot;
## [1] &quot;step =  1  lambda =  0.269820056384687  loss:  199.511224924367&quot;
## [1] &quot;step =  2  lambda =  0.269820056384687  loss:  200.512383598844&quot;
## [1] &quot;step =  1  lambda =  0.26713530196585  loss:  198.532516695876&quot;
## [1] &quot;step =  2  lambda =  0.26713530196585  loss:  199.473620731882&quot;
## [1] &quot;step =  1  lambda =  0.264477261299824  loss:  197.504008494934&quot;
## [1] &quot;step =  2  lambda =  0.264477261299824  loss:  198.399404957349&quot;
## [1] &quot;step =  1  lambda =  0.261845668580326  loss:  196.440415004209&quot;
## [1] &quot;step =  2  lambda =  0.261845668580326  loss:  197.315643638261&quot;
## [1] &quot;step =  1  lambda =  0.259240260645892  loss:  195.368343347774&quot;
## [1] &quot;step =  2  lambda =  0.259240260645892  loss:  196.221911049277&quot;
## [1] &quot;step =  1  lambda =  0.256660776953556  loss:  194.284765132917&quot;
## [1] &quot;step =  2  lambda =  0.256660776953556  loss:  195.124042259477&quot;
## [1] &quot;step =  1  lambda =  0.2541069595528  loss:  193.197753552141&quot;
## [1] &quot;step =  2  lambda =  0.2541069595528  loss:  194.02915341779&quot;
## [1] &quot;step =  1  lambda =  0.251578553059757  loss:  192.113691933073&quot;
## [1] &quot;step =  2  lambda =  0.251578553059757  loss:  192.937512088834&quot;
## [1] &quot;step =  1  lambda =  0.249075304631668  loss:  191.032845135256&quot;
## [1] &quot;step =  2  lambda =  0.249075304631668  loss:  191.849172805352&quot;
## [1] &quot;step =  1  lambda =  0.246596963941606  loss:  189.955267243295&quot;
## [1] &quot;step =  2  lambda =  0.246596963941606  loss:  190.764103161849&quot;
## [1] &quot;step =  1  lambda =  0.244143283153437  loss:  188.880926282309&quot;
## [1] &quot;step =  2  lambda =  0.244143283153437  loss:  189.682179710659&quot;
## [1] &quot;step =  1  lambda =  0.241714016897036  loss:  187.809700131677&quot;
## [1] &quot;step =  2  lambda =  0.241714016897036  loss:  188.603216859097&quot;
## [1] &quot;step =  1  lambda =  0.239308922243755  loss:  186.741405127436&quot;
## [1] &quot;step =  2  lambda =  0.239308922243755  loss:  187.526171006107&quot;
## [1] &quot;step =  1  lambda =  0.236927758682122  loss:  185.674625981154&quot;
## [1] &quot;step =  2  lambda =  0.236927758682122  loss:  186.452695546832&quot;
## [1] &quot;step =  1  lambda =  0.234570288093798  loss:  184.611769597559&quot;
## [1] &quot;step =  2  lambda =  0.234570288093798  loss:  185.391916404721&quot;
## [1] &quot;step =  1  lambda =  0.232236274729759  loss:  183.560330318471&quot;
## [1] &quot;step =  2  lambda =  0.232236274729759  loss:  184.334439096256&quot;
## [1] &quot;step =  1  lambda =  0.229925485186724  loss:  182.513339607178&quot;
## [1] &quot;step =  2  lambda =  0.229925485186724  loss:  183.278347603978&quot;
## [1] &quot;step =  1  lambda =  0.227637688383813  loss:  181.467721136425&quot;
## [1] &quot;step =  2  lambda =  0.227637688383813  loss:  182.222464168689&quot;
## [1] &quot;step =  1  lambda =  0.225372655539439  loss:  180.422308599645&quot;
## [1] &quot;step =  2  lambda =  0.225372655539439  loss:  181.166282640145&quot;
## [1] &quot;step =  1  lambda =  0.22313016014843  loss:  179.376600742106&quot;
## [1] &quot;step =  2  lambda =  0.22313016014843  loss:  180.109637860017&quot;
## [1] &quot;step =  1  lambda =  0.220909977959378  loss:  178.330433967924&quot;
## [1] &quot;step =  2  lambda =  0.220909977959378  loss:  179.052547508928&quot;
## [1] &quot;step =  1  lambda =  0.218711886952215  loss:  177.283825735161&quot;
## [1] &quot;step =  2  lambda =  0.218711886952215  loss:  177.995131293011&quot;
## [1] &quot;step =  1  lambda =  0.216535667316007  loss:  176.236894533619&quot;
## [1] &quot;step =  2  lambda =  0.216535667316007  loss:  176.937566041961&quot;
## [1] &quot;step =  1  lambda =  0.214381101426978  loss:  175.189815422157&quot;
## [1] &quot;step =  2  lambda =  0.214381101426978  loss:  175.880059382479&quot;
## [1] &quot;step =  1  lambda =  0.212247973826743  loss:  174.142793959336&quot;
## [1] &quot;step =  2  lambda =  0.212247973826743  loss:  174.822833869339&quot;
## [1] &quot;step =  1  lambda =  0.210136071200765  loss:  173.096050489274&quot;
## [1] &quot;step =  2  lambda =  0.210136071200765  loss:  173.766117316244&quot;
## [1] &quot;step =  1  lambda =  0.20804518235702  loss:  172.04981056663&quot;
## [1] &quot;step =  2  lambda =  0.20804518235702  loss:  172.710136907167&quot;
## [1] &quot;step =  1  lambda =  0.205975098204883  loss:  171.00429912516&quot;
## [1] &quot;step =  2  lambda =  0.205975098204883  loss:  171.655115646317&quot;
## [1] &quot;step =  1  lambda =  0.203925611734213  loss:  169.959736962076&quot;
## [1] &quot;step =  2  lambda =  0.203925611734213  loss:  170.601270264524&quot;
## [1] &quot;step =  1  lambda =  0.201896517994655  loss:  168.916338664645&quot;
## [1] &quot;step =  2  lambda =  0.201896517994655  loss:  169.548810034809&quot;
## [1] &quot;step =  1  lambda =  0.199887614075145  loss:  167.87431143713&quot;
## [1] &quot;step =  2  lambda =  0.199887614075145  loss:  168.497936155323&quot;
## [1] &quot;step =  1  lambda =  0.197898699083615  loss:  166.83385448959&quot;
## [1] &quot;step =  2  lambda =  0.197898699083615  loss:  167.448841485423&quot;
## [1] &quot;step =  1  lambda =  0.19592957412691  loss:  165.795158776396&quot;
## [1] &quot;step =  2  lambda =  0.19592957412691  loss:  166.401710500437&quot;
## [1] &quot;step =  1  lambda =  0.193980042290892  loss:  164.758406951321&quot;
## [1] &quot;step =  2  lambda =  0.193980042290892  loss:  165.356719380736&quot;
## [1] &quot;step =  1  lambda =  0.192049908620754  loss:  163.723773455642&quot;
## [1] &quot;step =  2  lambda =  0.192049908620754  loss:  164.314036182093&quot;
## [1] &quot;step =  1  lambda =  0.19013898010152  loss:  162.691424686734&quot;
## [1] &quot;step =  2  lambda =  0.19013898010152  loss:  163.273821054028&quot;
## [1] &quot;step =  1  lambda =  0.188247065638747  loss:  161.661519214213&quot;
## [1] &quot;step =  2  lambda =  0.188247065638747  loss:  162.236226485229&quot;
## [1] &quot;step =  1  lambda =  0.18637397603941  loss:  160.634208022871&quot;
## [1] &quot;step =  2  lambda =  0.18637397603941  loss:  161.201397562831&quot;
## [1] &quot;step =  1  lambda =  0.184519523992989  loss:  159.609634769363&quot;
## [1] &quot;step =  2  lambda =  0.184519523992989  loss:  160.169472237255&quot;
## [1] &quot;step =  1  lambda =  0.182683524052735  loss:  158.587936044385&quot;
## [1] &quot;step =  2  lambda =  0.182683524052735  loss:  159.140581587327&quot;
## [1] &quot;step =  1  lambda =  0.180865792617122  loss:  157.569241635146&quot;
## [1] &quot;step =  2  lambda =  0.180865792617122  loss:  158.114850082356&quot;
## [1] &quot;step =  1  lambda =  0.179066147911493  loss:  156.553674784822&quot;
## [1] &quot;step =  2  lambda =  0.179066147911493  loss:  157.092395839063&quot;
## [1] &quot;step =  1  lambda =  0.177284409969878  loss:  155.541352446928&quot;
## [1] &quot;step =  2  lambda =  0.177284409969878  loss:  156.073330872058&quot;
## [1] &quot;step =  1  lambda =  0.175520400616997  loss:  154.532385533284&quot;
## [1] &quot;step =  2  lambda =  0.175520400616997  loss:  155.05776133706&quot;
## [1] &quot;step =  1  lambda =  0.173773943450445  loss:  153.526879154816&quot;
## [1] &quot;step =  2  lambda =  0.173773943450445  loss:  154.045787766424&quot;
## [1] &quot;step =  1  lambda =  0.172044863823051  loss:  152.524932854735&quot;
## [1] &quot;step =  2  lambda =  0.172044863823051  loss:  153.037505296742&quot;
## [1] &quot;step =  1  lambda =  0.17033298882541  loss:  151.526640833865&quot;
## [1] &quot;step =  2  lambda =  0.17033298882541  loss:  152.033003888457&quot;
## [1] &quot;step =  1  lambda =  0.168638147268596  loss:  150.532092168069&quot;
## [1] &quot;step =  2  lambda =  0.168638147268596  loss:  151.032368537536&quot;
## [1] &quot;step =  1  lambda =  0.166960169667041  loss:  149.54137101782&quot;
## [1] &quot;step =  2  lambda =  0.166960169667041  loss:  150.035679479318&quot;
## [1] &quot;step =  1  lambda =  0.165298888221586  loss:  148.554556830011&quot;
## [1] &quot;step =  2  lambda =  0.165298888221586  loss:  149.04301238471&quot;
## [1] &quot;step =  1  lambda =  0.163654136802704  loss:  147.571724532202&quot;
## [1] &quot;step =  2  lambda =  0.163654136802704  loss:  148.05443854893&quot;
## [1] &quot;step =  1  lambda =  0.162025750933881  loss:  146.592944719485&quot;
## [1] &quot;step =  2  lambda =  0.162025750933881  loss:  147.070025073024&quot;
## [1] &quot;step =  1  lambda =  0.160413567775173  loss:  145.618283834195&quot;
## [1] &quot;step =  2  lambda =  0.160413567775173  loss:  146.089835038399&quot;
## [1] &quot;step =  1  lambda =  0.158817426106921  loss:  144.647804338705&quot;
## [1] &quot;step =  2  lambda =  0.158817426106921  loss:  145.113927674599&quot;
## [1] &quot;step =  1  lambda =  0.157237166313628  loss:  143.681564881542&quot;
## [1] &quot;step =  2  lambda =  0.157237166313628  loss:  144.142358520591&quot;
## [1] &quot;step =  1  lambda =  0.155672630367997  loss:  142.71962045706&quot;
## [1] &quot;step =  2  lambda =  0.155672630367997  loss:  143.175179579767&quot;
## [1] &quot;step =  1  lambda =  0.154123661815132  loss:  141.762022558911&quot;
## [1] &quot;step =  2  lambda =  0.154123661815132  loss:  142.212439468928&quot;
## [1] &quot;step =  1  lambda =  0.152590105756884  loss:  140.808818839846&quot;
## [1] &quot;step =  2  lambda =  0.152590105756884  loss:  141.24838956536&quot;
## [1] &quot;step =  1  lambda =  0.151071808836371  loss:  139.854524122365&quot;
## [1] &quot;step =  2  lambda =  0.151071808836371  loss:  140.290505811794&quot;
## [1] &quot;step =  1  lambda =  0.149568619222635  loss:  138.906124727078&quot;
## [1] &quot;step =  2  lambda =  0.149568619222635  loss:  139.337829771244&quot;
## [1] &quot;step =  1  lambda =  0.148080386595462  loss:  137.962881078543&quot;
## [1] &quot;step =  2  lambda =  0.148080386595462  loss:  138.390195883021&quot;
## [1] &quot;step =  1  lambda =  0.14660696213035  loss:  137.024629358514&quot;
## [1] &quot;step =  2  lambda =  0.14660696213035  loss:  137.447757570496&quot;
## [1] &quot;step =  1  lambda =  0.145148198483624  loss:  136.091521557542&quot;
## [1] &quot;step =  2  lambda =  0.145148198483624  loss:  136.510620690289&quot;
## [1] &quot;step =  1  lambda =  0.143703949777703  loss:  135.163662543244&quot;
## [1] &quot;step =  2  lambda =  0.143703949777703  loss:  135.578825381206&quot;
## [1] &quot;step =  1  lambda =  0.142274071586514  loss:  134.241092093665&quot;
## [1] &quot;step =  2  lambda =  0.142274071586514  loss:  134.652366582803&quot;
## [1] &quot;step =  1  lambda =  0.140858420921045  loss:  133.323805220203&quot;
## [1] &quot;step =  2  lambda =  0.140858420921045  loss:  133.73121431316&quot;
## [1] &quot;step =  1  lambda =  0.139456856215051  loss:  132.411772250351&quot;
## [1] &quot;step =  2  lambda =  0.139456856215051  loss:  132.815327498398&quot;
## [1] &quot;step =  1  lambda =  0.138069237310893  loss:  131.504952524025&quot;
## [1] &quot;step =  2  lambda =  0.138069237310893  loss:  131.904662258551&quot;
## [1] &quot;step =  1  lambda =  0.136695425445524  loss:  130.603302599692&quot;
## [1] &quot;step =  2  lambda =  0.136695425445524  loss:  130.999176440102&quot;
## [1] &quot;step =  1  lambda =  0.135335283236613  loss:  129.706780743439&quot;
## [1] &quot;step =  2  lambda =  0.135335283236613  loss:  130.098831807117&quot;
## [1] &quot;step =  1  lambda =  0.133988674668805  loss:  128.815349099232&quot;
## [1] &quot;step =  2  lambda =  0.133988674668805  loss:  129.203594839661&quot;
## [1] &quot;step =  1  lambda =  0.132655465080122  loss:  127.928974479905&quot;
## [1] &quot;step =  2  lambda =  0.132655465080122  loss:  128.313436744208&quot;
## [1] &quot;step =  1  lambda =  0.131335521148493  loss:  127.047628377729&quot;
## [1] &quot;step =  2  lambda =  0.131335521148493  loss:  127.428333051237&quot;
## [1] &quot;step =  1  lambda =  0.130028710878426  loss:  126.171286566128&quot;
## [1] &quot;step =  2  lambda =  0.130028710878426  loss:  126.548263027012&quot;
## [1] &quot;step =  1  lambda =  0.128734903587804  loss:  125.299928517336&quot;
## [1] &quot;step =  2  lambda =  0.128734903587804  loss:  125.673209032268&quot;
## [1] &quot;step =  1  lambda =  0.127453969894821  loss:  124.433536767452&quot;
## [1] &quot;step =  2  lambda =  0.127453969894821  loss:  124.80315590146&quot;
## [1] &quot;step =  1  lambda =  0.126185781705039  loss:  123.572096301824&quot;
## [1] &quot;step =  2  lambda =  0.126185781705039  loss:  123.938090379796&quot;
## [1] &quot;step =  1  lambda =  0.124930212198582  loss:  122.715593997643&quot;
## [1] &quot;step =  2  lambda =  0.124930212198582  loss:  123.078000633465&quot;
## [1] &quot;step =  1  lambda =  0.123687135817455  loss:  121.864018138999&quot;
## [1] &quot;step =  2  lambda =  0.123687135817455  loss:  122.222875835861&quot;
## [1] &quot;step =  1  lambda =  0.122456428252982  loss:  121.017358007178&quot;
## [1] &quot;step =  2  lambda =  0.122456428252982  loss:  121.372705825753&quot;
## [1] &quot;step =  1  lambda =  0.121237966433382  loss:  120.175603542212&quot;
## [1] &quot;step =  2  lambda =  0.121237966433382  loss:  120.527480830085&quot;
## [1] &quot;step =  1  lambda =  0.120031628511457  loss:  119.338745068397&quot;
## [1] &quot;step =  2  lambda =  0.120031628511457  loss:  119.687191242819&quot;
## [1] &quot;step =  1  lambda =  0.11883729385241  loss:  118.506773075327&quot;
## [1] &quot;step =  2  lambda =  0.11883729385241  loss:  118.851827451266&quot;
## [1] &quot;step =  1  lambda =  0.117654843021779  loss:  117.679678045933&quot;
## [1] &quot;step =  2  lambda =  0.117654843021779  loss:  118.021379701902&quot;
## [1] &quot;step =  1  lambda =  0.116484157773497  loss:  116.857450323623&quot;
## [1] &quot;step =  2  lambda =  0.116484157773497  loss:  117.19583799859&quot;
## [1] &quot;step =  1  lambda =  0.115325121038063  loss:  116.040080011517&quot;
## [1] &quot;step =  2  lambda =  0.115325121038063  loss:  116.375192027082&quot;
## [1] &quot;step =  1  lambda =  0.114177616910836  loss:  115.227556897691&quot;
## [1] &quot;step =  2  lambda =  0.114177616910836  loss:  115.559431100633&quot;
## [1] &quot;step =  1  lambda =  0.11304153064045  loss:  114.419870401338&quot;
## [1] &quot;step =  2  lambda =  0.11304153064045  loss:  114.748544122431&quot;
## [1] &quot;step =  1  lambda =  0.111916748617329  loss:  113.617009535566&quot;
## [1] &quot;step =  2  lambda =  0.111916748617329  loss:  113.942519561306&quot;
## [1] &quot;step =  1  lambda =  0.110803158362334  loss:  112.818962883362&quot;
## [1] &quot;step =  2  lambda =  0.110803158362334  loss:  113.14134543785&quot;
## [1] &quot;step =  1  lambda =  0.109700648515511  loss:  112.025718583844&quot;
## [1] &quot;step =  2  lambda =  0.109700648515511  loss:  112.345224716926&quot;
## [1] &quot;step =  1  lambda =  0.108609108824958  loss:  111.237579201374&quot;
## [1] &quot;step =  2  lambda =  0.108609108824958  loss:  111.554371094577&quot;
## [1] &quot;step =  1  lambda =  0.107528430135795  loss:  110.454551203885&quot;
## [1] &quot;step =  2  lambda =  0.107528430135795  loss:  110.768485983947&quot;
## [1] &quot;step =  1  lambda =  0.106458504379253  loss:  109.676442522006&quot;
## [1] &quot;step =  2  lambda =  0.106458504379253  loss:  109.986679851619&quot;
## [1] &quot;step =  1  lambda =  0.105399224561864  loss:  108.90240984081&quot;
## [1] &quot;step =  2  lambda =  0.105399224561864  loss:  109.207721909672&quot;
## [1] &quot;step =  1  lambda =  0.104350484754765  loss:  108.131162236117&quot;
## [1] &quot;step =  2  lambda =  0.104350484754765  loss:  108.432056754777&quot;
## [1] &quot;step =  1  lambda =  0.1033121800831  loss:  107.363174412742&quot;
## [1] &quot;step =  2  lambda =  0.1033121800831  loss:  107.659344827565&quot;
## [1] &quot;step =  1  lambda =  0.102284206715537  loss:  106.598056845216&quot;
## [1] &quot;step =  2  lambda =  0.102284206715537  loss:  106.890166417155&quot;
## [1] &quot;step =  1  lambda =  0.101266461853883  loss:  105.836430797073&quot;
## [1] &quot;step =  2  lambda =  0.101266461853883  loss:  106.125242066227&quot;
## [1] &quot;step =  1  lambda =  0.100258843722804  loss:  105.07907860919&quot;
## [1] &quot;step =  2  lambda =  0.100258843722804  loss:  105.364944583623&quot;
## [1] &quot;step =  1  lambda =  0.0992612515596457  loss:  104.326307264521&quot;
## [1] &quot;step =  2  lambda =  0.0992612515596457  loss:  104.609402475802&quot;
## [1] &quot;step =  1  lambda =  0.0982735856043615  loss:  103.578244047798&quot;
## [1] &quot;step =  2  lambda =  0.0982735856043615  loss:  103.85866774266&quot;
## [1] &quot;step =  1  lambda =  0.0972957470895328  loss:  102.83494047241&quot;
## [1] &quot;step =  2  lambda =  0.0972957470895328  loss:  103.112755941736&quot;
## [1] &quot;step =  1  lambda =  0.096327638230493  loss:  102.09641195839&quot;
## [1] &quot;step =  2  lambda =  0.096327638230493  loss:  102.371663360481&quot;
## [1] &quot;step =  1  lambda =  0.0953691622155497  loss:  101.362654839476&quot;
## [1] &quot;step =  2  lambda =  0.0953691622155497  loss:  101.635375816039&quot;
## [1] &quot;step =  1  lambda =  0.0944202231963024  loss:  100.633655078537&quot;
## [1] &quot;step =  2  lambda =  0.0944202231963024  loss:  100.903873631534&quot;
## [1] &quot;step =  1  lambda =  0.0934807262780585  loss:  99.9093931962974&quot;
## [1] &quot;step =  2  lambda =  0.0934807262780585  loss:  100.177134481559&quot;
## [1] &quot;step =  1  lambda =  0.0925505775103433  loss:  99.1898470897292&quot;
## [1] &quot;step =  2  lambda =  0.0925505775103433  loss:  99.4551349573751&quot;
## [1] &quot;step =  1  lambda =  0.0916296838775049  loss:  98.4749935824048&quot;
## [1] &quot;step =  2  lambda =  0.0916296838775049  loss:  98.748667623886&quot;
## [1] &quot;step =  1  lambda =  0.0907179532894126  loss:  97.7754094095794&quot;
## [1] &quot;step =  2  lambda =  0.0907179532894126  loss:  98.0403810592118&quot;
## [1] &quot;step =  1  lambda =  0.0898152945722476  loss:  97.0741335011398&quot;
## [1] &quot;step =  2  lambda =  0.0898152945722476  loss:  97.3391330741839&quot;
## [1] &quot;step =  1  lambda =  0.0889216174593863  loss:  96.3798274120973&quot;
## [1] &quot;step =  2  lambda =  0.0889216174593863  loss:  96.6434074637905&quot;
## [1] &quot;step =  1  lambda =  0.0880368325823726  loss:  95.6909893389525&quot;
## [1] &quot;step =  2  lambda =  0.0880368325823726  loss:  95.9524808815125&quot;
## [1] &quot;step =  1  lambda =  0.0871608514619813  loss:  95.0069028750255&quot;
## [1] &quot;step =  2  lambda =  0.0871608514619813  loss:  95.2659669439282&quot;
## [1] &quot;step =  1  lambda =  0.0862935864993705  loss:  94.3271853438776&quot;
## [1] &quot;step =  2  lambda =  0.0862935864993705  loss:  94.5836247639275&quot;
## [1] &quot;step =  1  lambda =  0.0854349509673212  loss:  93.6515981698587&quot;
## [1] &quot;step =  2  lambda =  0.0854349509673212  loss:  93.9077548372248&quot;
## [1] &quot;step =  1  lambda =  0.0845848590015647  loss:  92.9823944595836&quot;
## [1] &quot;step =  2  lambda =  0.0845848590015647  loss:  93.2348369905321&quot;
## [1] &quot;step =  1  lambda =  0.083743225592196  loss:  92.3161382893929&quot;
## [1] &quot;step =  2  lambda =  0.083743225592196  loss:  92.5649750659944&quot;
## [1] &quot;step =  1  lambda =  0.0829099665751727  loss:  91.6529077637373&quot;
## [1] &quot;step =  2  lambda =  0.0829099665751727  loss:  91.898406648787&quot;
## [1] &quot;step =  1  lambda =  0.0820849986238988  loss:  90.992937955239&quot;
## [1] &quot;step =  2  lambda =  0.0820849986238988  loss:  91.2353223912075&quot;
## [1] &quot;step =  1  lambda =  0.0812682392408917  loss:  90.3364175920788&quot;
## [1] &quot;step =  2  lambda =  0.0812682392408917  loss:  90.5758888785117&quot;
## [1] &quot;step =  1  lambda =  0.0804596067495325  loss:  89.6835116128759&quot;
## [1] &quot;step =  2  lambda =  0.0804596067495325  loss:  89.9202318443082&quot;
## [1] &quot;step =  1  lambda =  0.079659020285898  loss:  89.0343445169409&quot;
## [1] &quot;step =  2  lambda =  0.079659020285898  loss:  89.2684391394322&quot;
## [1] &quot;step =  1  lambda =  0.0788663997906749  loss:  88.3890032958062&quot;
## [1] &quot;step =  2  lambda =  0.0788663997906749  loss:  88.620570885614&quot;
## [1] &quot;step =  1  lambda =  0.0780816660011532  loss:  87.7475474851326&quot;
## [1] &quot;step =  2  lambda =  0.0780816660011532  loss:  87.9766683383692&quot;
## [1] &quot;step =  1  lambda =  0.0773047404432998  loss:  87.1100179399761&quot;
## [1] &quot;step =  2  lambda =  0.0773047404432998  loss:  87.3367598992098&quot;
## [1] &quot;step =  1  lambda =  0.0765355454239115  loss:  86.4764427877092&quot;
## [1] &quot;step =  2  lambda =  0.0765355454239115  loss:  86.7008649022741&quot;
## [1] &quot;step =  1  lambda =  0.0757740040228455  loss:  85.8468411773303&quot;
## [1] &quot;step =  2  lambda =  0.0757740040228455  loss:  86.0689959866997&quot;
## [1] &quot;step =  1  lambda =  0.075020040085327  loss:  85.2212256284608&quot;
## [1] &quot;step =  2  lambda =  0.075020040085327  loss:  85.4411606239982&quot;
## [1] &quot;step =  1  lambda =  0.0742735782143339  loss:  84.5996035436835&quot;
## [1] &quot;step =  2  lambda =  0.0742735782143339  loss:  84.81736214498&quot;
## [1] &quot;step =  1  lambda =  0.0735345437630571  loss:  83.9819782253662&quot;
## [1] &quot;step =  2  lambda =  0.0735345437630571  loss:  84.1976004657301&quot;
## [1] &quot;step =  1  lambda =  0.0728028628274356  loss:  83.3683495945096&quot;
## [1] &quot;step =  2  lambda =  0.0728028628274356  loss:  83.5818726272271&quot;
## [1] &quot;step =  1  lambda =  0.0720784622387661  loss:  82.7587147250749&quot;
## [1] &quot;step =  2  lambda =  0.0720784622387661  loss:  82.9701732148459&quot;
## [1] &quot;step =  1  lambda =  0.0713612695563861  loss:  82.1530682593797&quot;
## [1] &quot;step =  2  lambda =  0.0713612695563861  loss:  82.3624946966271&quot;
## [1] &quot;step =  1  lambda =  0.0706512130604296  loss:  81.5514027430593&quot;
## [1] &quot;step =  2  lambda =  0.0706512130604296  loss:  81.7588277036713&quot;
## [1] &quot;step =  1  lambda =  0.0699482217446554  loss:  80.9537089027244&quot;
## [1] &quot;step =  2  lambda =  0.0699482217446554  loss:  81.1591612671863&quot;
## [1] &quot;step =  1  lambda =  0.069252225309346  loss:  80.359975880693&quot;
## [1] &quot;step =  2  lambda =  0.069252225309346  loss:  80.563483021616&quot;
## [1] &quot;step =  1  lambda =  0.0685631541542779  loss:  79.7701914361382&quot;
## [1] &quot;step =  2  lambda =  0.0685631541542779  loss:  79.9717793803036&quot;
## [1] &quot;step =  1  lambda =  0.0678809393717615  loss:  79.1843421190397&quot;
## [1] &quot;step =  2  lambda =  0.0678809393717615  loss:  79.3840356883468&quot;
## [1] &quot;step =  1  lambda =  0.0672055127397498  loss:  78.6024134215476&quot;
## [1] &quot;step =  2  lambda =  0.0672055127397498  loss:  78.8002363561768&quot;
## [1] &quot;step =  1  lambda =  0.0665368067150169  loss:  78.0243899102609&quot;
## [1] &quot;step =  2  lambda =  0.0665368067150169  loss:  78.2203649766536&quot;
## [1] &quot;step =  1  lambda =  0.065874754426403  loss:  77.4502553421804&quot;
## [1] &quot;step =  2  lambda =  0.065874754426403  loss:  77.6444044279536&quot;
## [1] &quot;step =  1  lambda =  0.0652192896681276  loss:  76.8799927665935&quot;
## [1] &quot;step =  2  lambda =  0.0652192896681276  loss:  77.0723369641465&quot;
## [1] &quot;step =  1  lambda =  0.0645703468931685  loss:  76.3135846147694&quot;
## [1] &quot;step =  2  lambda =  0.0645703468931685  loss:  76.5041442950669&quot;
## [1] &quot;step =  1  lambda =  0.0639278612067076  loss:  75.7510127790507&quot;
## [1] &quot;step =  2  lambda =  0.0639278612067076  loss:  75.9398076568471&quot;
## [1] &quot;step =  1  lambda =  0.0632917683596407  loss:  75.1922586826994&quot;
## [1] &quot;step =  2  lambda =  0.0632917683596407  loss:  75.3793078742901&quot;
## [1] &quot;step =  1  lambda =  0.0626620047421532  loss:  74.6373033416615&quot;
## [1] &quot;step =  2  lambda =  0.0626620047421532  loss:  74.8226254160946&quot;
## [1] &quot;step =  1  lambda =  0.0620385073773583  loss:  74.086127419253&quot;
## [1] &quot;step =  2  lambda =  0.0620385073773583  loss:  74.2697404438118&quot;
## [1] &quot;step =  1  lambda =  0.0614212139150001  loss:  73.5387112746396&quot;
## [1] &quot;step =  2  lambda =  0.0614212139150001  loss:  73.7206328552938&quot;
## [1] &quot;step =  1  lambda =  0.060810062625218  loss:  72.9950350058606&quot;
## [1] &quot;step =  2  lambda =  0.060810062625218  loss:  73.1752823232951&quot;
## [1] &quot;step =  1  lambda =  0.0602049923923736  loss:  72.4550784880541&quot;
## [1] &quot;step =  2  lambda =  0.0602049923923736  loss:  72.6336683298046&quot;
## [1] &quot;step =  1  lambda =  0.0596059427089393  loss:  71.9188214074539&quot;
## [1] &quot;step =  2  lambda =  0.0596059427089393  loss:  72.0957701966092&quot;
## [1] &quot;step =  1  lambda =  0.0590128536694478  loss:  71.386243291654&quot;
## [1] &quot;step =  2  lambda =  0.0590128536694478  loss:  71.5615671125276&quot;
## [1] &quot;step =  1  lambda =  0.0584256659645008  loss:  70.8573235365764&quot;
## [1] &quot;step =  2  lambda =  0.0584256659645008  loss:  71.031038157699&quot;
## [1] &quot;step =  1  lambda =  0.0578443208748385  loss:  70.3320414305234&quot;
## [1] &quot;step =  2  lambda =  0.0578443208748385  loss:  70.504162325264&quot;
## [1] &quot;step =  1  lambda =  0.0572687602654674  loss:  69.8103761756451&quot;
## [1] &quot;step =  2  lambda =  0.0572687602654674  loss:  69.9809185407311&quot;
## [1] &quot;step =  1  lambda =  0.0566989265798469  loss:  69.2923069071175&quot;
## [1] &quot;step =  2  lambda =  0.0566989265798469  loss:  69.4612856792912&quot;
## [1] &quot;step =  1  lambda =  0.0561347628341337  loss:  68.7778127102877&quot;
## [1] &quot;step =  2  lambda =  0.0561347628341337  loss:  68.9452425813074&quot;
## [1] &quot;step =  1  lambda =  0.0555762126114831  loss:  68.2668726360109&quot;
## [1] &quot;step =  2  lambda =  0.0555762126114831  loss:  68.4327680661819&quot;
## [1] &quot;step =  1  lambda =  0.0550232200564073  loss:  67.7594657143827&quot;
## [1] &quot;step =  2  lambda =  0.0550232200564073  loss:  67.9238409447782&quot;
## [1] &quot;step =  1  lambda =  0.0544757298691899  loss:  67.255570967039&quot;
## [1] &quot;step =  2  lambda =  0.0544757298691899  loss:  67.4184400305567&quot;
## [1] &quot;step =  1  lambda =  0.053933687300356  loss:  66.7551674181832&quot;
## [1] &quot;step =  2  lambda =  0.053933687300356  loss:  66.9165441495627&quot;
## [1] &quot;step =  1  lambda =  0.0533970381451971  loss:  66.2582341044747&quot;
## [1] &quot;step =  2  lambda =  0.0533970381451971  loss:  66.4181321493887&quot;
## [1] &quot;step =  1  lambda =  0.0528657287383504  loss:  65.7647500839048&quot;
## [1] &quot;step =  2  lambda =  0.0528657287383504  loss:  65.923182907223&quot;
## [1] &quot;step =  1  lambda =  0.0523397059484324  loss:  65.2746944437656&quot;
## [1] &quot;step =  2  lambda =  0.0523397059484324  loss:  65.4316753370793&quot;
## [1] &quot;step =  1  lambda =  0.0518189171727258  loss:  64.7880463078088&quot;
## [1] &quot;step =  2  lambda =  0.0518189171727258  loss:  64.9435883962958&quot;
## [1] &quot;step =  1  lambda =  0.0513033103319191  loss:  64.3047848426805&quot;
## [1] &quot;step =  2  lambda =  0.0513033103319191  loss:  64.4589010913776&quot;
## [1] &quot;step =  1  lambda =  0.0507928338648985  loss:  63.8248892637065&quot;
## [1] &quot;step =  2  lambda =  0.0507928338648985  loss:  63.9775924832535&quot;
## [1] &quot;step =  1  lambda =  0.0502874367235919  loss:  63.3483388400974&quot;
## [1] &quot;step =  2  lambda =  0.0502874367235919  loss:  63.4996416920061&quot;
## [1] &quot;step =  1  lambda =  0.0497870683678639  loss:  62.8751128996318&quot;
## [1] &quot;step =  2  lambda =  0.0497870683678639  loss:  63.02502790113&quot;
## [1] &quot;step =  1  lambda =  0.0492916787604622  loss:  62.4051908328729&quot;
## [1] &quot;step =  2  lambda =  0.0492916787604622  loss:  62.5537303613657&quot;
## [1] &quot;step =  1  lambda =  0.048801218362013  loss:  61.938552096964&quot;
## [1] &quot;step =  2  lambda =  0.048801218362013  loss:  62.085728394153&quot;
## [1] &quot;step =  1  lambda =  0.0483156381260678  loss:  61.4751762190485&quot;
## [1] &quot;step =  2  lambda =  0.0483156381260678  loss:  61.6210013947412&quot;
## [1] &quot;step =  1  lambda =  0.0478348894941984  loss:  61.0150427993491&quot;
## [1] &quot;step =  2  lambda =  0.0478348894941984  loss:  61.1595288349917&quot;
## [1] &quot;step =  1  lambda =  0.0473589243911409  loss:  60.5581315139422&quot;
## [1] &quot;step =  2  lambda =  0.0473589243911409  loss:  60.7012902659022&quot;
## [1] &quot;step =  1  lambda =  0.0468876952199885  loss:  60.1044221172576&quot;
## [1] &quot;step =  2  lambda =  0.0468876952199885  loss:  60.2462653198803&quot;
## [1] &quot;step =  1  lambda =  0.0464211548574313  loss:  59.6538944443298&quot;
## [1] &quot;step =  2  lambda =  0.0464211548574313  loss:  59.7944337127921&quot;
## [1] &quot;step =  1  lambda =  0.0459592566490442  loss:  59.2065284128257&quot;
## [1] &quot;step =  2  lambda =  0.0459592566490442  loss:  59.3457752458054&quot;
## [1] &quot;step =  1  lambda =  0.0455019544046216  loss:  58.7623040248695&quot;
## [1] &quot;step =  2  lambda =  0.0455019544046216  loss:  58.9002698070485&quot;
## [1] &quot;step =  1  lambda =  0.0450492023935578  loss:  58.3212013686856&quot;
## [1] &quot;step =  2  lambda =  0.0450492023935578  loss:  58.457897373103&quot;
## [1] &quot;step =  1  lambda =  0.0446009553402746  loss:  57.8832006200758&quot;
## [1] &quot;step =  2  lambda =  0.0446009553402746  loss:  58.0186380103436&quot;
## [1] &quot;step =  1  lambda =  0.0441571684196929  loss:  57.4482820437462&quot;
## [1] &quot;step =  2  lambda =  0.0441571684196929  loss:  57.5824718761413&quot;
## [1] &quot;step =  1  lambda =  0.0437177972527509  loss:  57.0164259944985&quot;
## [1] &quot;step =  2  lambda =  0.0437177972527509  loss:  57.1493792199424&quot;
## [1] &quot;step =  1  lambda =  0.0432827979019659  loss:  56.5876129182974&quot;
## [1] &quot;step =  2  lambda =  0.0432827979019659  loss:  56.7193403842326&quot;
## [1] &quot;step =  1  lambda =  0.0428521268670402  loss:  56.1618233532258&quot;
## [1] &quot;step =  2  lambda =  0.0428521268670402  loss:  56.2923358053993&quot;
## [1] &quot;step =  1  lambda =  0.0424257410805114  loss:  55.7390379303383&quot;
## [1] &quot;step =  2  lambda =  0.0424257410805114  loss:  55.8683460144984&quot;
## [1] &quot;step =  1  lambda =  0.0420035979034456  loss:  55.3192373744202&quot;
## [1] &quot;step =  2  lambda =  0.0420035979034456  loss:  55.4473516379364&quot;
## [1] &quot;step =  1  lambda =  0.0415856551211732  loss:  54.9024025046623&quot;
## [1] &quot;step =  2  lambda =  0.0415856551211732  loss:  55.0293333980726&quot;
## [1] &quot;step =  1  lambda =  0.0411718709390678  loss:  54.4885142352578&quot;
## [1] &quot;step =  2  lambda =  0.0411718709390678  loss:  54.6142721137501&quot;
## [1] &quot;step =  1  lambda =  0.0407622039783662  loss:  54.0775535759272&quot;
## [1] &quot;step =  2  lambda =  0.0407622039783662  loss:  54.2021487007609&quot;
## [1] &quot;step =  1  lambda =  0.0403566132720311  loss:  53.6695016323791&quot;
## [1] &quot;step =  2  lambda =  0.0403566132720311  loss:  53.7929441722496&quot;
## [1] &quot;step =  1  lambda =  0.0399550582606539  loss:  53.2643396067093&quot;
## [1] &quot;step =  2  lambda =  0.0399550582606539  loss:  53.3866396390619&quot;
## [1] &quot;step =  1  lambda =  0.0395574987883987  loss:  52.8620487977464&quot;
## [1] &quot;step =  2  lambda =  0.0395574987883987  loss:  52.9832163100415&quot;
## [1] &quot;step =  1  lambda =  0.0391638950989871  loss:  52.4626106013449&quot;
## [1] &quot;step =  2  lambda =  0.0391638950989871  loss:  52.5826554922801&quot;
## [1] &quot;step =  1  lambda =  0.038774207831722  loss:  52.0660065106329&quot;
## [1] &quot;step =  2  lambda =  0.038774207831722  loss:  52.1849385913222&quot;
## [1] &quot;step =  1  lambda =  0.0383883980175521  loss:  51.672218116215&quot;
## [1] &quot;step =  2  lambda =  0.0383883980175521  loss:  51.7900471113312&quot;
## [1] &quot;step =  1  lambda =  0.0380064270751743  loss:  51.2812271063359&quot;
## [1] &quot;step =  2  lambda =  0.0380064270751743  loss:  51.397962655216&quot;
## [1] &quot;step =  1  lambda =  0.0376282568071762  loss:  50.8930152670066&quot;
## [1] &quot;step =  2  lambda =  0.0376282568071762  loss:  51.0086669247242&quot;
## [1] &quot;step =  1  lambda =  0.0372538493962158  loss:  50.5075644820957&quot;
## [1] &quot;step =  2  lambda =  0.0372538493962158  loss:  50.6221417205017&quot;
## [1] &quot;step =  1  lambda =  0.03688316740124  loss:  50.1248567333885&quot;
## [1] &quot;step =  2  lambda =  0.03688316740124  loss:  50.2383689421226&quot;
## [1] &quot;step =  1  lambda =  0.0365161737537404  loss:  49.7448741006167&quot;
## [1] &quot;step =  2  lambda =  0.0365161737537404  loss:  49.8573305880903&quot;
## [1] &quot;step =  1  lambda =  0.0361528317540464  loss:  49.367598761459&quot;
## [1] &quot;step =  2  lambda =  0.0361528317540464  loss:  49.4790087558128&quot;
## [1] &quot;step =  1  lambda =  0.0357931050676553  loss:  48.9930129915173&quot;
## [1] &quot;step =  2  lambda =  0.0357931050676553  loss:  49.1033856415529&quot;
## [1] &quot;step =  1  lambda =  0.0354369577215986  loss:  48.6210991642659&quot;
## [1] &quot;step =  2  lambda =  0.0354369577215986  loss:  48.7304435403551&quot;
## [1] &quot;step =  1  lambda =  0.035084354100845  loss:  48.2518397509803&quot;
## [1] &quot;step =  2  lambda =  0.035084354100845  loss:  48.360164845951&quot;
## [1] &quot;step =  1  lambda =  0.0347352589447386  loss:  47.8852173206431&quot;
## [1] &quot;step =  2  lambda =  0.0347352589447386  loss:  47.9925320506442&quot;
## [1] &quot;step =  1  lambda =  0.0343896373434727  loss:  47.5212145398292&quot;
## [1] &quot;step =  2  lambda =  0.0343896373434727  loss:  47.6275277451761&quot;
## [1] &quot;step =  1  lambda =  0.0340474547345993  loss:  47.1598141725739&quot;
## [1] &quot;step =  2  lambda =  0.0340474547345993  loss:  47.2651346185734&quot;
## [1] &quot;step =  1  lambda =  0.0337086768995724  loss:  46.8009990802208&quot;
## [1] &quot;step =  2  lambda =  0.0337086768995724  loss:  46.9053354579783&quot;
## [1] &quot;step =  1  lambda =  0.0333732699603261  loss:  46.4447522212546&quot;
## [1] &quot;step =  2  lambda =  0.0333732699603261  loss:  46.5481131484635&quot;
## [1] &quot;step =  1  lambda =  0.0330412003758869  loss:  46.0910566511165&quot;
## [1] &quot;step =  2  lambda =  0.0330412003758869  loss:  46.1934506728309&quot;
## [1] &quot;step =  1  lambda =  0.0327124349390198  loss:  45.7398955220063&quot;
## [1] &quot;step =  2  lambda =  0.0327124349390198  loss:  45.8413311113967&quot;
## [1] &quot;step =  1  lambda =  0.0323869407729071  loss:  45.3912520826685&quot;
## [1] &quot;step =  2  lambda =  0.0323869407729071  loss:  45.4917376417625&quot;
## [1] &quot;step =  1  lambda =  0.0320646853278608  loss:  45.0451096781657&quot;
## [1] &quot;step =  2  lambda =  0.0320646853278608  loss:  45.1446535385741&quot;
## [1] &quot;step =  1  lambda =  0.0317456363780679  loss:  44.7014517496401&quot;
## [1] &quot;step =  2  lambda =  0.0317456363780679  loss:  44.8000621732673&quot;
## [1] &quot;step =  1  lambda =  0.0314297620183677  loss:  44.3602618340614&quot;
## [1] &quot;step =  2  lambda =  0.0314297620183677  loss:  44.4579470138029&quot;
## [1] &quot;step =  1  lambda =  0.0311170306610609  loss:  44.0215235639642&quot;
## [1] &quot;step =  2  lambda =  0.0311170306610609  loss:  44.1182916243904&quot;
## [1] &quot;step =  1  lambda =  0.0308074110327511  loss:  43.6852206671749&quot;
## [1] &quot;step =  2  lambda =  0.0308074110327511  loss:  43.7810796652015&quot;
## [1] &quot;step =  1  lambda =  0.0305008721712175  loss:  43.3513369665275&quot;
## [1] &quot;step =  2  lambda =  0.0305008721712175  loss:  43.4462948920741&quot;
## [1] &quot;step =  1  lambda =  0.0301973834223185  loss:  43.0198563795703&quot;
## [1] &quot;step =  2  lambda =  0.0301973834223185  loss:  43.1139211562062&quot;
## [1] &quot;step =  1  lambda =  0.0298969144369263  loss:  42.6907629182635&quot;
## [1] &quot;step =  2  lambda =  0.0298969144369263  loss:  42.783942403842&quot;
## [1] &quot;step =  1  lambda =  0.029599435167892  loss:  42.3640406886673&quot;
## [1] &quot;step =  2  lambda =  0.029599435167892  loss:  42.4563426759486&quot;
## [1] &quot;step =  1  lambda =  0.0293049158670407  loss:  42.0396738906227&quot;
## [1] &quot;step =  2  lambda =  0.0293049158670407  loss:  42.1311061078862&quot;
## [1] &quot;step =  1  lambda =  0.0290133270821971  loss:  41.7176468174243&quot;
## [1] &quot;step =  2  lambda =  0.0290133270821971  loss:  41.8082169290693&quot;
## [1] &quot;step =  1  lambda =  0.0287246396542394  loss:  41.3979438554849&quot;
## [1] &quot;step =  2  lambda =  0.0287246396542394  loss:  41.4876594626218&quot;
## [1] &quot;step =  1  lambda =  0.0284388247141845  loss:  41.0805494839942&quot;
## [1] &quot;step =  2  lambda =  0.0284388247141845  loss:  41.1694181250255&quot;
## [1] &quot;step =  1  lambda =  0.0281558536803001  loss:  40.7654482745701&quot;
## [1] &quot;step =  2  lambda =  0.0281558536803001  loss:  40.8534774257615&quot;
## [1] &quot;step =  1  lambda =  0.027875698255247  loss:  40.4526248909043&quot;
## [1] &quot;step =  2  lambda =  0.027875698255247  loss:  40.5398219669461&quot;
## [1] &quot;step =  1  lambda =  0.0275983304232493  loss:  40.1420640884017&quot;
## [1] &quot;step =  2  lambda =  0.0275983304232493  loss:  40.2284364429614&quot;
## [1] &quot;step =  1  lambda =  0.0273237224472926  loss:  39.8337507138137&quot;
## [1] &quot;step =  2  lambda =  0.0273237224472926  loss:  39.9193056400798&quot;
## [1] &quot;step =  1  lambda =  0.0270518468663504  loss:  39.5276697048675&quot;
## [1] &quot;step =  2  lambda =  0.0270518468663504  loss:  39.6124144360844&quot;
## [1] &quot;step =  1  lambda =  0.0267826764926382  loss:  39.2238060898895&quot;
## [1] &quot;step =  2  lambda =  0.0267826764926382  loss:  39.3077477998839&quot;
## [1] &quot;step =  1  lambda =  0.0265161844088942  loss:  38.9221449874243&quot;
## [1] &quot;step =  2  lambda =  0.0265161844088942  loss:  39.0052907911233&quot;
## [1] &quot;step =  1  lambda =  0.026252343965688  loss:  38.6226716058489&quot;
## [1] &quot;step =  2  lambda =  0.026252343965688  loss:  38.7050285597911&quot;
## [1] &quot;step =  1  lambda =  0.0259911287787554  loss:  38.3253712429841&quot;
## [1] &quot;step =  2  lambda =  0.0259911287787554  loss:  38.4069463458216&quot;
## [1] &quot;step =  1  lambda =  0.0257325127263599  loss:  38.0302292857002&quot;
## [1] &quot;step =  2  lambda =  0.0257325127263599  loss:  38.1110294786941&quot;
## [1] &quot;step =  1  lambda =  0.025476469946681  loss:  37.737231209521&quot;
## [1] &quot;step =  2  lambda =  0.025476469946681  loss:  37.8172633770291&quot;
## [1] &quot;step =  1  lambda =  0.0252229748352272  loss:  37.4463625782227&quot;
## [1] &quot;step =  2  lambda =  0.0252229748352272  loss:  37.5256335481807&quot;
## [1] &quot;step =  1  lambda =  0.0249720020422762  loss:  37.1576090434314&quot;
## [1] &quot;step =  2  lambda =  0.0249720020422762  loss:  37.236125587826&quot;
## [1] &quot;step =  1  lambda =  0.0247235264703394  loss:  36.8709563442161&quot;
## [1] &quot;step =  2  lambda =  0.0247235264703394  loss:  36.9487251795528&quot;
## [1] &quot;step =  1  lambda =  0.0244775232716527  loss:  36.5863903066804&quot;
## [1] &quot;step =  2  lambda =  0.0244775232716527  loss:  36.6634180944438&quot;
## [1] &quot;step =  1  lambda =  0.0242339678456911  loss:  36.3038968435507&quot;
## [1] &quot;step =  2  lambda =  0.0242339678456911  loss:  36.3801901906589&quot;
## [1] &quot;step =  1  lambda =  0.0239928358367092  loss:  36.023461953763&quot;
## [1] &quot;step =  2  lambda =  0.0239928358367092  loss:  36.0990274130156&quot;
## [1] &quot;step =  1  lambda =  0.023754103131305  loss:  35.7450717220467&quot;
## [1] &quot;step =  2  lambda =  0.023754103131305  loss:  35.8199157925669&quot;
## [1] &quot;step =  1  lambda =  0.0235177458560091  loss:  35.4687123185079&quot;
## [1] &quot;step =  2  lambda =  0.0235177458560091  loss:  35.5428414461786&quot;
## [1] &quot;step =  1  lambda =  0.023283740374897  loss:  35.1943699982092&quot;
## [1] &quot;step =  2  lambda =  0.023283740374897  loss:  35.2677905761036&quot;
## [1] &quot;step =  1  lambda =  0.0230520632872256  loss:  34.9220311007497&quot;
## [1] &quot;step =  2  lambda =  0.0230520632872256  loss:  34.994749469556&quot;
## [1] &quot;step =  1  lambda =  0.022822691425093  loss:  34.6516820498424&quot;
## [1] &quot;step =  2  lambda =  0.022822691425093  loss:  34.7237044982839&quot;
## [1] &quot;step =  1  lambda =  0.0225956018511219  loss:  34.3833093528914&quot;
## [1] &quot;step =  2  lambda =  0.0225956018511219  loss:  34.4546421181403&quot;
## [1] &quot;step =  1  lambda =  0.0223707718561656  loss:  34.1168996005671&quot;
## [1] &quot;step =  2  lambda =  0.0223707718561656  loss:  34.1875488686539&quot;
## [1] &quot;step =  1  lambda =  0.0221481789570373  loss:  33.8524394663815&quot;
## [1] &quot;step =  2  lambda =  0.0221481789570373  loss:  33.922411372599&quot;
## [1] &quot;step =  1  lambda =  0.0219278008942616  loss:  33.5899157062616&quot;
## [1] &quot;step =  2  lambda =  0.0219278008942616  loss:  33.6592163355644&quot;
## [1] &quot;step =  1  lambda =  0.0217096156298486  loss:  33.3293151581237&quot;
## [1] &quot;step =  2  lambda =  0.0217096156298486  loss:  33.3979505455229&quot;
## [1] &quot;step =  1  lambda =  0.0214936013450899  loss:  33.0706247414455&quot;
## [1] &quot;step =  2  lambda =  0.0214936013450899  loss:  33.1386008723986&quot;
## [1] &quot;step =  1  lambda =  0.0212797364383772  loss:  32.8138314568395&quot;
## [1] &quot;step =  2  lambda =  0.0212797364383772  loss:  32.8811542676358&quot;
## [1] &quot;step =  1  lambda =  0.0210679995230414  loss:  32.5589223856248&quot;
## [1] &quot;step =  2  lambda =  0.0210679995230414  loss:  32.6255977637671&quot;
## [1] &quot;step =  1  lambda =  0.0208583694252147  loss:  32.3058846894002&quot;
## [1] &quot;step =  2  lambda =  0.0208583694252147  loss:  32.371918473981&quot;
## [1] &quot;step =  1  lambda =  0.0206508251817126  loss:  32.0547056096156&quot;
## [1] &quot;step =  2  lambda =  0.0206508251817126  loss:  32.1201035916904&quot;
## [1] &quot;step =  1  lambda =  0.0204453460379377  loss:  31.8053724671452&quot;
## [1] &quot;step =  2  lambda =  0.0204453460379377  loss:  31.8701403901011&quot;
## [1] &quot;step =  1  lambda =  0.0202419114458044  loss:  31.55787266186&quot;
## [1] &quot;step =  2  lambda =  0.0202419114458044  loss:  31.6220162217804&quot;
## [1] &quot;step =  1  lambda =  0.020040501061684  loss:  31.3121936722006&quot;
## [1] &quot;step =  2  lambda =  0.020040501061684  loss:  31.3757185182262&quot;
## [1] &quot;step =  1  lambda =  0.0198410947443703  loss:  31.0683230547508&quot;
## [1] &quot;step =  2  lambda =  0.0198410947443703  loss:  31.1312347894369&quot;
## [1] &quot;step =  1  lambda =  0.0196436725530653  loss:  30.8262484438117&quot;
## [1] &quot;step =  2  lambda =  0.0196436725530653  loss:  30.8885526234817&quot;
## [1] &quot;step =  1  lambda =  0.0194482147453854  loss:  30.5859575509761&quot;
## [1] &quot;step =  2  lambda =  0.0194482147453854  loss:  30.6476596860714&quot;
## [1] &quot;step =  1  lambda =  0.0192547017753869  loss:  30.3474381647035&quot;
## [1] &quot;step =  2  lambda =  0.0192547017753869  loss:  30.4085437201304&quot;
## [1] &quot;step =  1  lambda =  0.0190631142916116  loss:  30.1106781498968&quot;
## [1] &quot;step =  2  lambda =  0.0190631142916116  loss:  30.1711925453691&quot;
## [1] &quot;step =  1  lambda =  0.0188734331351515  loss:  29.8756654474784&quot;
## [1] &quot;step =  2  lambda =  0.0188734331351515  loss:  29.9355940578576&quot;
## [1] &quot;step =  1  lambda =  0.0186856393377328  loss:  29.6423880739685&quot;
## [1] &quot;step =  2  lambda =  0.0186856393377328  loss:  29.7017362296003&quot;
## [1] &quot;step =  1  lambda =  0.0184997141198192  loss:  29.4108341210635&quot;
## [1] &quot;step =  2  lambda =  0.0184997141198192  loss:  29.4696071081113&quot;
## [1] &quot;step =  1  lambda =  0.0183156388887342  loss:  29.1809917552161&quot;
## [1] &quot;step =  2  lambda =  0.0183156388887342  loss:  29.2391948159912&quot;
## [1] &quot;step =  1  lambda =  0.0181333952368011  loss:  28.9528492172162&quot;
## [1] &quot;step =  2  lambda =  0.0181333952368011  loss:  29.0104875505053&quot;
## [1] &quot;step =  1  lambda =  0.0179529649395029  loss:  28.7263948217725&quot;
## [1] &quot;step =  2  lambda =  0.0179529649395029  loss:  28.7834735831623&quot;
## [1] &quot;step =  1  lambda =  0.0177743299536594  loss:  28.5016169570968&quot;
## [1] &quot;step =  2  lambda =  0.0177743299536594  loss:  28.558141259295&quot;
## [1] &quot;step =  1  lambda =  0.0175974724156234  loss:  28.2785040844878&quot;
## [1] &quot;step =  2  lambda =  0.0175974724156234  loss:  28.3344789976423&quot;
## [1] &quot;step =  1  lambda =  0.0174223746394935  loss:  28.0570447379173&quot;
## [1] &quot;step =  2  lambda =  0.0174223746394935  loss:  28.1124752899321&quot;
## [1] &quot;step =  1  lambda =  0.0172490191153463  loss:  27.8372275236178&quot;
## [1] &quot;step =  2  lambda =  0.0172490191153463  loss:  27.8921187004661&quot;
## [1] &quot;step =  1  lambda =  0.0170773885074848  loss:  27.6190411196709&quot;
## [1] &quot;step =  2  lambda =  0.0170773885074848  loss:  27.6733978657061&quot;
## [1] &quot;step =  1  lambda =  0.0169074656527053  loss:  27.402474275598&quot;
## [1] &quot;step =  2  lambda =  0.0169074656527053  loss:  27.4563014938615&quot;
## [1] &quot;step =  1  lambda =  0.0167392335585806  loss:  27.1875158119519&quot;
## [1] &quot;step =  2  lambda =  0.0167392335585806  loss:  27.2408183644788&quot;
## [1] &quot;step =  1  lambda =  0.0165726754017613  loss:  26.9741546199101&quot;
## [1] &quot;step =  2  lambda =  0.0165726754017613  loss:  27.0269373280321&quot;
## [1] &quot;step =  1  lambda =  0.0164077745262926  loss:  26.7623796608696&quot;
## [1] &quot;step =  2  lambda =  0.0164077745262926  loss:  26.8146473055161&quot;
## [1] &quot;step =  1  lambda =  0.0162445144419499  loss:  26.5521799660441&quot;
## [1] &quot;step =  2  lambda =  0.0162445144419499  loss:  26.6039372880401&quot;
## [1] &quot;step =  1  lambda =  0.0160828788225884  loss:  26.3435446360616&quot;
## [1] &quot;step =  2  lambda =  0.0160828788225884  loss:  26.3947963364239&quot;
## [1] &quot;step =  1  lambda =  0.0159228515045117  loss:  26.1364628405648&quot;
## [1] &quot;step =  2  lambda =  0.0159228515045117  loss:  26.1872135807957&quot;
## [1] &quot;step =  1  lambda =  0.0157644164848545  loss:  25.9309238178123&quot;
## [1] &quot;step =  2  lambda =  0.0157644164848545  loss:  25.9811782201911&quot;
## [1] &quot;step =  1  lambda =  0.0156075579199828  loss:  25.7269168742826&quot;
## [1] &quot;step =  2  lambda =  0.0156075579199828  loss:  25.7766795221547&quot;
## [1] &quot;step =  1  lambda =  0.0154522601239095  loss:  25.524431384279&quot;
## [1] &quot;step =  2  lambda =  0.0154522601239095  loss:  25.5737068223432&quot;
## [1] &quot;step =  1  lambda =  0.0152985075667255  loss:  25.3234567895363&quot;
## [1] &quot;step =  2  lambda =  0.0152985075667255  loss:  25.3722495241299&quot;
## [1] &quot;step =  1  lambda =  0.015146284873047  loss:  25.1239825988303&quot;
## [1] &quot;step =  2  lambda =  0.015146284873047  loss:  25.1722970982118&quot;
## [1] &quot;step =  1  lambda =  0.0149955768204777  loss:  24.925998387588&quot;
## [1] &quot;step =  2  lambda =  0.0149955768204777  loss:  24.9738390822181&quot;
## [1] &quot;step =  1  lambda =  0.0148463683380868  loss:  24.7294937975&quot;
## [1] &quot;step =  2  lambda =  0.0148463683380868  loss:  24.7768650803207&quot;
## [1] &quot;step =  1  lambda =  0.0146986445049018  loss:  24.5344585361353&quot;
## [1] &quot;step =  2  lambda =  0.0146986445049018  loss:  24.5813647628465&quot;
## [1] &quot;step =  1  lambda =  0.0145523905484161  loss:  24.3408823765572&quot;
## [1] &quot;step =  2  lambda =  0.0145523905484161  loss:  24.3873278658919&quot;
## [1] &quot;step =  1  lambda =  0.0144075918431123  loss:  24.1487551569418&quot;
## [1] &quot;step =  2  lambda =  0.0144075918431123  loss:  24.194744190939&quot;
## [1] &quot;step =  1  lambda =  0.0142642339089993  loss:  23.9580667801978&quot;
## [1] &quot;step =  2  lambda =  0.0142642339089993  loss:  24.0036036044739&quot;
## [1] &quot;step =  1  lambda =  0.014122302410164  loss:  23.7688072135888&quot;
## [1] &quot;step =  2  lambda =  0.014122302410164  loss:  23.8138960376067&quot;
## [1] &quot;step =  1  lambda =  0.0139817831533383  loss:  23.5809664883569&quot;
## [1] &quot;step =  2  lambda =  0.0139817831533383  loss:  23.6256114856938&quot;
## [1] &quot;step =  1  lambda =  0.0138426620864795  loss:  23.3945346993489&quot;
## [1] &quot;step =  2  lambda =  0.0138426620864795  loss:  23.4387400079617&quot;
## [1] &quot;step =  1  lambda =  0.0137049252973649  loss:  23.2095020046441&quot;
## [1] &quot;step =  2  lambda =  0.0137049252973649  loss:  23.2532717271337&quot;
## [1] &quot;step =  1  lambda =  0.0135685590122009  loss:  23.0258586251839&quot;
## [1] &quot;step =  2  lambda =  0.0135685590122009  loss:  23.0691968290571&quot;
## [1] &quot;step =  1  lambda =  0.0134335495942453  loss:  22.8435948444039&quot;
## [1] &quot;step =  2  lambda =  0.0134335495942453  loss:  22.8865055623341&quot;
## [1] &quot;step =  1  lambda =  0.0132998835424438  loss:  22.6627010078674&quot;
## [1] &quot;step =  2  lambda =  0.0132998835424438  loss:  22.7051882379531&quot;
## [1] &quot;step =  1  lambda =  0.0131675474900798  loss:  22.4831675229011&quot;
## [1] &quot;step =  2  lambda =  0.0131675474900798  loss:  22.5252352289234&quot;
## [1] &quot;step =  1  lambda =  0.0130365282034377  loss:  22.3049848582329&quot;
## [1] &quot;step =  2  lambda =  0.0130365282034377  loss:  22.3466369699106&quot;
## [1] &quot;step =  1  lambda =  0.0129068125804799  loss:  22.1281435436315&quot;
## [1] &quot;step =  2  lambda =  0.0129068125804799  loss:  22.1693839568752&quot;
## [1] &quot;step =  1  lambda =  0.0127783876495358  loss:  21.9526341695481&quot;
## [1] &quot;step =  2  lambda =  0.0127783876495358  loss:  21.993466746712&quot;
## [1] &quot;step =  1  lambda =  0.0126512405680053  loss:  21.7784473867599&quot;
## [1] &quot;step =  2  lambda =  0.0126512405680053  loss:  21.8188759568928&quot;
## [1] &quot;step =  1  lambda =  0.0125253586210744  loss:  21.6055739060157&quot;
## [1] &quot;step =  2  lambda =  0.0125253586210744  loss:  21.6456022651098&quot;
## [1] &quot;step =  1  lambda =  0.0124007292204434  loss:  21.4340044976837&quot;
## [1] &quot;step =  2  lambda =  0.0124007292204434  loss:  21.473636408922&quot;
## [1] &quot;step =  1  lambda =  0.0122773399030684  loss:  21.2637299914005&quot;
## [1] &quot;step =  2  lambda =  0.0122773399030684  loss:  21.3029691854028&quot;
## [1] &quot;step =  1  lambda =  0.0121551783299149  loss:  21.0947412757232&quot;
## [1] &quot;step =  2  lambda =  0.0121551783299149  loss:  21.1335914507905&quot;
## [1] &quot;step =  1  lambda =  0.0120342322847238  loss:  20.9270292977825&quot;
## [1] &quot;step =  2  lambda =  0.0120342322847238  loss:  20.9654941201398&quot;
## [1] &quot;step =  1  lambda =  0.0119144896727896  loss:  20.7605850629383&quot;
## [1] &quot;step =  2  lambda =  0.0119144896727896  loss:  20.7986681669758&quot;
## [1] &quot;step =  1  lambda =  0.0117959385197516  loss:  20.5953996344369&quot;
## [1] &quot;step =  2  lambda =  0.0117959385197516  loss:  20.63310462295&quot;
## [1] &quot;step =  1  lambda =  0.0116785669703954  loss:  20.4314641330704&quot;
## [1] &quot;step =  2  lambda =  0.0116785669703954  loss:  20.4687945774982&quot;
## [1] &quot;step =  1  lambda =  0.0115623632874685  loss:  20.2687697368381&quot;
## [1] &quot;step =  2  lambda =  0.0115623632874685  loss:  20.3057291775003&quot;
## [1] &quot;step =  1  lambda =  0.0114473158505057  loss:  20.1073076806097&quot;
## [1] &quot;step =  2  lambda =  0.0114473158505057  loss:  20.1438996269418&quot;
## [1] &quot;step =  1  lambda =  0.0113334131546674  loss:  19.9470692557904&quot;
## [1] &quot;step =  2  lambda =  0.0113334131546674  loss:  19.9832971865779&quot;
## [1] &quot;step =  1  lambda =  0.0112206438095891  loss:  19.7880458099878&quot;
## [1] &quot;step =  2  lambda =  0.0112206438095891  loss:  19.8239131735991&quot;
## [1] &quot;step =  1  lambda =  0.0111089965382423  loss:  19.6302287466812&quot;
## [1] &quot;step =  2  lambda =  0.0111089965382423  loss:  19.6657389612985&quot;
## [1] &quot;step =  1  lambda =  0.0109984601758069  loss:  19.4736095248926&quot;
## [1] &quot;step =  2  lambda =  0.0109984601758069  loss:  19.5087659787416&quot;
## [1] &quot;step =  1  lambda =  0.0108890236685545  loss:  19.3181796588588&quot;
## [1] &quot;step =  2  lambda =  0.0108890236685545  loss:  19.3529857104375&quot;
## [1] &quot;step =  1  lambda =  0.0107806760727431  loss:  19.1639307177072&quot;
## [1] &quot;step =  2  lambda =  0.0107806760727431  loss:  19.1983896960126&quot;
## [1] &quot;step =  1  lambda =  0.0106734065535229  loss:  19.0108543251314&quot;
## [1] &quot;step =  2  lambda =  0.0106734065535229  loss:  19.0449695298854&quot;
## [1] &quot;step =  1  lambda =  0.0105672043838527  loss:  18.8589421590704&quot;
## [1] &quot;step =  2  lambda =  0.0105672043838527  loss:  18.8927168609439&quot;
## [1] &quot;step =  1  lambda =  0.0104620589434268  loss:  18.7081859513883&quot;
## [1] &quot;step =  2  lambda =  0.0104620589434268  loss:  18.7416233922243&quot;
## [1] &quot;step =  1  lambda =  0.0103579597176137  loss:  18.5585774875572&quot;
## [1] &quot;step =  2  lambda =  0.0103579597176137  loss:  18.5916808805923&quot;
## [1] &quot;step =  1  lambda =  0.010254896296404  loss:  18.4101086063408&quot;
## [1] &quot;step =  2  lambda =  0.010254896296404  loss:  18.4428811364255&quot;
## [1] &quot;step =  1  lambda =  0.0101528583733698  loss:  18.2627711994802&quot;
## [1] &quot;step =  2  lambda =  0.0101528583733698  loss:  18.295216023298&quot;
## [1] &quot;step =  1  lambda =  0.0100518357446336  loss:  18.116557211382&quot;
## [1] &quot;step =  2  lambda =  0.0100518357446336  loss:  18.1486774576667&quot;
## [1] &quot;step =  1  lambda =  0.00995181830784842  loss:  17.9714586388077&quot;
## [1] &quot;step =  2  lambda =  0.00995181830784842  loss:  18.0032574085598&quot;
## [1] &quot;step =  1  lambda =  0.00985279606118726  loss:  17.8274675305648&quot;
## [1] &quot;step =  2  lambda =  0.00985279606118726  loss:  17.8589478972668&quot;
## [1] &quot;step =  1  lambda =  0.0097547591023429  loss:  17.6845759872003&quot;
## [1] &quot;step =  2  lambda =  0.0097547591023429  loss:  17.7157409970301&quot;
## [1] &quot;step =  1  lambda =  0.00965769762753778  loss:  17.5427761606954&quot;
## [1] &quot;step =  2  lambda =  0.00965769762753778  loss:  17.5736288327388&quot;
## [1] &quot;step =  1  lambda =  0.00956160193054351  loss:  17.4020602541624&quot;
## [1] &quot;step =  2  lambda =  0.00956160193054351  loss:  17.4326035806242&quot;
## [1] &quot;step =  1  lambda =  0.00946646240171032  loss:  17.2624205215431&quot;
## [1] &quot;step =  2  lambda =  0.00946646240171032  loss:  17.2926574679569&quot;
## [1] &quot;step =  1  lambda =  0.00937226952700606  loss:  17.1238492673087&quot;
## [1] &quot;step =  2  lambda =  0.00937226952700606  loss:  17.1537827727457&quot;
## [1] &quot;step =  1  lambda =  0.00927901388706474  loss:  16.9863388461624&quot;
## [1] &quot;step =  2  lambda =  0.00927901388706474  loss:  17.0159718234384&quot;
## [1] &quot;step =  1  lambda =  0.00918668615624467  loss:  16.8498816627427&quot;
## [1] &quot;step =  2  lambda =  0.00918668615624467  loss:  16.8792169986243&quot;
## [1] &quot;step =  1  lambda =  0.00909527710169582  loss:  16.7144701713286&quot;
## [1] &quot;step =  2  lambda =  0.00909527710169582  loss:  16.7435107267383&quot;
## [1] &quot;step =  1  lambda =  0.00900477758243656  loss:  16.5800968755476&quot;
## [1] &quot;step =  2  lambda =  0.00900477758243656  loss:  16.6088454857667&quot;
## [1] &quot;step =  1  lambda =  0.00891517854843955  loss:  16.4467543280834&quot;
## [1] &quot;step =  2  lambda =  0.00891517854843955  loss:  16.4752138029548&quot;
## [1] &quot;step =  1  lambda =  0.00882647103972673  loss:  16.3144351303874&quot;
## [1] &quot;step =  2  lambda =  0.00882647103972673  loss:  16.3426082545164&quot;
## [1] &quot;step =  1  lambda =  0.00873864618547329  loss:  16.1831319323901&quot;
## [1] &quot;step =  2  lambda =  0.00873864618547329  loss:  16.2110214653446&quot;
## [1] &quot;step =  1  lambda =  0.00865169520312063  loss:  16.0528374322157&quot;
## [1] &quot;step =  2  lambda =  0.00865169520312063  loss:  16.0804461087244&quot;
## [1] &quot;step =  1  lambda =  0.00856560939749806  loss:  15.9235443758969&quot;
## [1] &quot;step =  2  lambda =  0.00856560939749806  loss:  15.9508749060468&quot;
## [1] &quot;step =  1  lambda =  0.00848038015995327  loss:  15.7952455570927&quot;
## [1] &quot;step =  2  lambda =  0.00848038015995327  loss:  15.8223006265254&quot;
## [1] &quot;step =  1  lambda =  0.00839599896749147  loss:  15.6679338168065&quot;
## [1] &quot;step =  2  lambda =  0.00839599896749147  loss:  15.6947160869131&quot;
## [1] &quot;step =  1  lambda =  0.00831245738192312  loss:  15.5416020431072&quot;
## [1] &quot;step =  2  lambda =  0.00831245738192312  loss:  15.5681141512219&quot;
## [1] &quot;step =  1  lambda =  0.00822974704902003  loss:  15.4162431708507&quot;
## [1] &quot;step =  2  lambda =  0.00822974704902003  loss:  15.4424877304434&quot;
## [1] &quot;step =  1  lambda =  0.00814785969767999  loss:  15.2918501814035&quot;
## [1] &quot;step =  2  lambda =  0.00814785969767999  loss:  15.3178297822714&quot;
## [1] &quot;step =  1  lambda =  0.00806678713909961  loss:  15.1684161023683&quot;
## [1] &quot;step =  2  lambda =  0.00806678713909961  loss:  15.1941333108257&quot;
## [1] &quot;step =  1  lambda =  0.0079865212659555  loss:  15.0459340073104&quot;
## [1] &quot;step =  2  lambda =  0.0079865212659555  loss:  15.0713913663777&quot;
## [1] &quot;step =  1  lambda =  0.00790705405159344  loss:  14.9243970154858&quot;
## [1] &quot;step =  2  lambda =  0.00790705405159344  loss:  14.9495970450775&quot;
## [1] &quot;step =  1  lambda =  0.00782837754922577  loss:  14.8037982915718&quot;
## [1] &quot;step =  2  lambda =  0.00782837754922577  loss:  14.8287434886828&quot;
## [1] &quot;step =  1  lambda =  0.00775048389113669  loss:  14.6841310453975&quot;
## [1] &quot;step =  2  lambda =  0.00775048389113669  loss:  14.7088238842889&quot;
## [1] &quot;step =  1  lambda =  0.00767336528789549  loss:  14.5653885316775&quot;
## [1] &quot;step =  2  lambda =  0.00767336528789549  loss:  14.5898314640602&quot;
## [1] &quot;step =  1  lambda =  0.00759701402757757  loss:  14.4475640497458&quot;
## [1] &quot;step =  2  lambda =  0.00759701402757757  loss:  14.4717595049642&quot;
## [1] &quot;step =  1  lambda =  0.00752142247499327  loss:  14.330650943292&quot;
## [1] &quot;step =  2  lambda =  0.00752142247499327  loss:  14.3546013285056&quot;
## [1] &quot;step =  1  lambda =  0.00744658307092434  loss:  14.2146426000986&quot;
## [1] &quot;step =  2  lambda =  0.00744658307092434  loss:  14.2383503004628&quot;
## [1] &quot;step =  1  lambda =  0.00737248833136801  loss:  14.0995324517801&quot;
## [1] &quot;step =  2  lambda =  0.00737248833136801  loss:  14.1229998306257&quot;
## [1] &quot;step =  1  lambda =  0.00729913084678858  loss:  13.9853139735233&quot;
## [1] &quot;step =  2  lambda =  0.00729913084678858  loss:  14.008543372535&quot;
## [1] &quot;step =  1  lambda =  0.00722650328137646  loss:  13.871980683829&quot;
## [1] &quot;step =  2  lambda =  0.00722650328137646  loss:  13.8949744232227&quot;
## [1] &quot;step =  1  lambda =  0.00715459837231459  loss:  13.7595261442555&quot;
## [1] &quot;step =  2  lambda =  0.00715459837231459  loss:  13.7822865229543&quot;
## [1] &quot;step =  1  lambda =  0.00708340892905212  loss:  13.6479439591633&quot;
## [1] &quot;step =  2  lambda =  0.00708340892905212  loss:  13.6704732549727&quot;
## [1] &quot;step =  1  lambda =  0.00701292783258542  loss:  13.5372277754612&quot;
## [1] &quot;step =  2  lambda =  0.00701292783258542  loss:  13.5595282452428&quot;
## [1] &quot;step =  1  lambda =  0.00694314803474611  loss:  13.4273712823541&quot;
## [1] &quot;step =  2  lambda =  0.00694314803474611  loss:  13.4494451621982&quot;
## [1] &quot;step =  1  lambda =  0.00687406255749626  loss:  13.3183682110917&quot;
## [1] &quot;step =  2  lambda =  0.00687406255749626  loss:  13.3402177164891&quot;
## [1] &quot;step =  1  lambda =  0.00680566449223054  loss:  13.2102123347189&quot;
## [1] &quot;step =  2  lambda =  0.00680566449223054  loss:  13.2318396607313&quot;
## [1] &quot;step =  1  lambda =  0.00673794699908547  loss:  13.102897467828&quot;
## [1] &quot;step =  2  lambda =  0.00673794699908547  loss:  13.1243047892574&quot;
## [1] &quot;step =  1  lambda =  0.00667090330625527  loss:  12.9964174663113&quot;
## [1] &quot;step =  2  lambda =  0.00667090330625527  loss:  13.0176069378681&quot;
## [1] &quot;step =  1  lambda =  0.00660452670931481  loss:  12.8907662271162&quot;
## [1] &quot;step =  2  lambda =  0.00660452670931481  loss:  12.9117399835862&quot;
## [1] &quot;step =  1  lambda =  0.00653881057054906  loss:  12.7859376880005&quot;
## [1] &quot;step =  2  lambda =  0.00653881057054906  loss:  12.8066978444111&quot;
## [1] &quot;step =  1  lambda =  0.0064737483182894  loss:  12.6819257932344&quot;
## [1] &quot;step =  2  lambda =  0.0064737483182894  loss:  12.7024741465566&quot;
## [1] &quot;step =  1  lambda =  0.00640933344625638  loss:  12.5787242853554&quot;
## [1] &quot;step =  2  lambda =  0.00640933344625638  loss:  12.5990623833169&quot;
## [1] &quot;step =  1  lambda =  0.00634555951290912  loss:  12.4763267232116&quot;
## [1] &quot;step =  2  lambda =  0.00634555951290912  loss:  12.4964563339562&quot;
## [1] &quot;step =  1  lambda =  0.00628242014080112  loss:  12.3747269288467&quot;
## [1] &quot;step =  2  lambda =  0.00628242014080112  loss:  12.3946500594695&quot;
## [1] &quot;step =  1  lambda =  0.00621990901594257  loss:  12.2739190222178&quot;
## [1] &quot;step =  2  lambda =  0.00621990901594257  loss:  12.2936377312719&quot;
## [1] &quot;step =  1  lambda =  0.0061580198871689  loss:  12.173897232587&quot;
## [1] &quot;step =  2  lambda =  0.0061580198871689  loss:  12.1934135722887&quot;
## [1] &quot;step =  1  lambda =  0.00609674656551564  loss:  12.0746558401861&quot;
## [1] &quot;step =  2  lambda =  0.00609674656551564  loss:  12.0939718454323&quot;
## [1] &quot;step =  1  lambda =  0.00603608292359956  loss:  11.9761891648177&quot;
## [1] &quot;step =  2  lambda =  0.00603608292359956  loss:  11.9953068515487&quot;
## [1] &quot;step =  1  lambda =  0.00597602289500594  loss:  11.8784915638291&quot;
## [1] &quot;step =  2  lambda =  0.00597602289500594  loss:  11.8974129285763&quot;
## [1] &quot;step =  1  lambda =  0.00591656047368186  loss:  11.7815574312854&quot;
## [1] &quot;step =  2  lambda =  0.00591656047368186  loss:  11.8002844508362&quot;
## [1] &quot;step =  1  lambda =  0.00585768971333562  loss:  11.6853811972685&quot;
## [1] &quot;step =  2  lambda =  0.00585768971333562  loss:  11.7039158284649&quot;
## [1] &quot;step =  1  lambda =  0.00579940472684215  loss:  11.589957327317&quot;
## [1] &quot;step =  2  lambda =  0.00579940472684215  loss:  11.6083015070219&quot;
## [1] &quot;step =  1  lambda =  0.0057416996856542  loss:  11.4952803220387&quot;
## [1] &quot;step =  2  lambda =  0.0057416996856542  loss:  11.5134359672435&quot;
## [1] &quot;step =  1  lambda =  0.0056845688192196  loss:  11.4013447168668&quot;
## [1] &quot;step =  2  lambda =  0.0056845688192196  loss:  11.4193137248955&quot;
## [1] &quot;step =  1  lambda =  0.00562800641440407  loss:  11.3081450819141&quot;
## [1] &quot;step =  2  lambda =  0.00562800641440407  loss:  11.3259293306805&quot;
## [1] &quot;step =  1  lambda =  0.00557200681492  loss:  11.215676021882&quot;
## [1] &quot;step =  2  lambda =  0.00557200681492  loss:  11.2332773701695&quot;
## [1] &quot;step =  1  lambda =  0.00551656442076077  loss:  11.1239321759921&quot;
## [1] &quot;step =  2  lambda =  0.00551656442076077  loss:  11.1413524637356&quot;
## [1] &quot;step =  1  lambda =  0.00546167368764078  loss:  11.0329082179209&quot;
## [1] &quot;step =  2  lambda =  0.00546167368764078  loss:  11.0501492664796&quot;
## [1] &quot;step =  1  lambda =  0.00540732912644096  loss:  10.9425988557262&quot;
## [1] &quot;step =  2  lambda =  0.00540732912644096  loss:  10.959662468142&quot;
## [1] &quot;step =  1  lambda =  0.00535352530265991  loss:  10.8529988317603&quot;
## [1] &quot;step =  2  lambda =  0.00535352530265991  loss:  10.8698867930003&quot;
## [1] &quot;step =  1  lambda =  0.0053002568358704  loss:  10.7641029225677&quot;
## [1] &quot;step =  2  lambda =  0.0053002568358704  loss:  10.7808169997513&quot;
## [1] &quot;step =  1  lambda =  0.00524751839918138  loss:  10.6759059387691&quot;
## [1] &quot;step =  2  lambda =  0.00524751839918138  loss:  10.6924478813822&quot;
## [1] &quot;step =  1  lambda =  0.00519530471870523  loss:  10.5884027249336&quot;
## [1] &quot;step =  2  lambda =  0.00519530471870523  loss:  10.6047742650302&quot;
## [1] &quot;step =  1  lambda =  0.00514361057303038  loss:  10.5015881594399&quot;
## [1] &quot;step =  2  lambda =  0.00514361057303038  loss:  10.5177910118351&quot;
## [1] &quot;step =  1  lambda =  0.00509243079269919  loss:  10.4154571543304&quot;
## [1] &quot;step =  2  lambda =  0.00509243079269919  loss:  10.4314930167856&quot;
## [1] &quot;step =  1  lambda =  0.00504176025969098  loss:  10.3300046551589&quot;
## [1] &quot;step =  2  lambda =  0.00504176025969098  loss:  10.3458752085608&quot;
## [1] &quot;step =  1  lambda =  0.00499159390691022  loss:  10.2452256408337&quot;
## [1] &quot;step =  2  lambda =  0.00499159390691022  loss:  10.2609325493689&quot;
## [1] &quot;step =  1  lambda =  0.00494192671767982  loss:  10.161115123458&quot;
## [1] &quot;step =  2  lambda =  0.00494192671767982  loss:  10.1766600347838&quot;
## [1] &quot;step =  1  lambda =  0.00489275372523948  loss:  10.0776681481685&quot;
## [1] &quot;step =  2  lambda =  0.00489275372523948  loss:  10.0930526935798&quot;
## [1] &quot;step =  1  lambda =  0.00484407001224897  loss:  9.99487979297129&quot;
## [1] &quot;step =  2  lambda =  0.00484407001224897  loss:  10.0101055875667&quot;
## [1] &quot;step =  1  lambda =  0.00479587071029642  loss:  9.91274516857858&quot;
## [1] &quot;step =  2  lambda =  0.00479587071029642  loss:  9.92781381142252&quot;
## [1] &quot;step =  1  lambda =  0.00474815099941148  loss:  9.83125941824377&quot;
## [1] &quot;step =  2  lambda =  0.00474815099941148  loss:  9.84617249252808&quot;
## [1] &quot;step =  1  lambda =  0.00470090610758328  loss:  9.7504177175969&quot;
## [1] &quot;step =  2  lambda =  0.00470090610758328  loss:  9.7651767908001&quot;
## [1] &quot;step =  1  lambda =  0.00465413131028327  loss:  9.67021527448004&quot;
## [1] &quot;step =  2  lambda =  0.00465413131028327  loss:  9.6848218985252&quot;
## [1] &quot;step =  1  lambda =  0.00460782192999275  loss:  9.59064732878261&quot;
## [1] &quot;step =  2  lambda =  0.00460782192999275  loss:  9.60510304019366&quot;
## [1] &quot;step =  1  lambda =  0.0045619733357351  loss:  9.51170915227702&quot;
## [1] &quot;step =  2  lambda =  0.0045619733357351  loss:  9.52601547233359&quot;
## [1] &quot;step =  1  lambda =  0.00451658094261267  loss:  9.43339604845441&quot;
## [1] &quot;step =  2  lambda =  0.00451658094261267  loss:  9.44755448334523&quot;
## [1] &quot;step =  1  lambda =  0.00447164021134833  loss:  9.35570335236068&quot;
## [1] &quot;step =  2  lambda =  0.00447164021134833  loss:  9.36971539333542&quot;
## [1] &quot;step =  1  lambda =  0.00442714664783151  loss:  9.27862643043265&quot;
## [1] &quot;step =  2  lambda =  0.00442714664783151  loss:  9.29249355395234&quot;
## [1] &quot;step =  1  lambda =  0.00438309580266878  loss:  9.20216068033437&quot;
## [1] &quot;step =  2  lambda =  0.00438309580266878  loss:  9.21588434822038&quot;
## [1] &quot;step =  1  lambda =  0.0043394832707389  loss:  9.12630153079373&quot;
## [1] &quot;step =  2  lambda =  0.0043394832707389  loss:  9.13988319037512&quot;
## [1] &quot;step =  1  lambda =  0.00429630469075234  loss:  9.05104444143911&quot;
## [1] &quot;step =  2  lambda =  0.00429630469075234  loss:  9.06448552569861&quot;
## [1] &quot;step =  1  lambda =  0.00425355574481513  loss:  8.97638490263631&quot;
## [1] &quot;step =  2  lambda =  0.00425355574481513  loss:  8.98968683035464&quot;
## [1] &quot;step =  1  lambda =  0.00421123215799704  loss:  8.90231843532547&quot;
## [1] &quot;step =  2  lambda =  0.00421123215799704  loss:  8.91548261122429&quot;
## [1] &quot;step =  1  lambda =  0.00416932969790412  loss:  8.82884059085822&quot;
## [1] &quot;step =  2  lambda =  0.00416932969790412  loss:  8.84186840574144&quot;
## [1] &quot;step =  1  lambda =  0.00412784417425544  loss:  8.75594695083501&quot;
## [1] &quot;step =  2  lambda =  0.00412784417425544  loss:  8.76883978172863&quot;
## [1] &quot;step =  1  lambda =  0.00408677143846407  loss:  8.68363312694244&quot;
## [1] &quot;step =  2  lambda =  0.00408677143846407  loss:  8.69639233723282&quot;
## [1] &quot;step =  1  lambda =  0.0040461073832222  loss:  8.61189476079078&quot;
## [1] &quot;step =  2  lambda =  0.0040461073832222  loss:  8.6245217003615&quot;
## [1] &quot;step =  1  lambda =  0.00400584794209042  loss:  8.54072752375168&quot;
## [1] &quot;step =  2  lambda =  0.00400584794209042  loss:  8.55322352911874&quot;
## [1] &quot;step =  1  lambda =  0.00396598908909106  loss:  8.47012711679591&quot;
## [1] &quot;step =  2  lambda =  0.00396598908909106  loss:  8.48249351124161&quot;
## [1] &quot;step =  1  lambda =  0.00392652683830562  loss:  8.40008927033142&quot;
## [1] &quot;step =  2  lambda =  0.00392652683830562  loss:  8.41232736403656&quot;
## [1] &quot;step =  1  lambda =  0.00388745724347613  loss:  8.33060974404128&quot;
## [1] &quot;step =  2  lambda =  0.00388745724347613  loss:  8.3427208342161&quot;
## [1] &quot;step =  1  lambda =  0.00384877639761054  loss:  8.26168432672213&quot;
## [1] &quot;step =  2  lambda =  0.00384877639761054  loss:  8.27366969773557&quot;
## [1] &quot;step =  1  lambda =  0.00381048043259204  loss:  8.19330883612252&quot;
## [1] &quot;step =  2  lambda =  0.00381048043259204  loss:  8.20516975963023&quot;
## [1] &quot;step =  1  lambda =  0.00377256551879221  loss:  8.12547911878162&quot;
## [1] &quot;step =  2  lambda =  0.00377256551879221  loss:  8.13721685385244&quot;
## [1] &quot;step =  1  lambda =  0.00373502786468807  loss:  8.05819104986809&quot;
## [1] &quot;step =  2  lambda =  0.00373502786468807  loss:  8.06980684310911&quot;
## [1] &quot;step =  1  lambda =  0.00369786371648293  loss:  7.99144053301909&quot;
## [1] &quot;step =  2  lambda =  0.00369786371648293  loss:  8.00293561869941&quot;
## [1] &quot;step =  1  lambda =  0.00366106935773101  loss:  7.92522350017971&quot;
## [1] &quot;step =  2  lambda =  0.00366106935773101  loss:  7.93659910035272&quot;
## [1] &quot;step =  1  lambda =  0.00362464110896576  loss:  7.85953591144246&quot;
## [1] &quot;step =  2  lambda =  0.00362464110896576  loss:  7.87079323606685&quot;
## [1] &quot;step =  1  lambda =  0.00358857532733195  loss:  7.79437375488715&quot;
## [1] &quot;step =  2  lambda =  0.00358857532733195  loss:  7.80551400194649&quot;
## [1] &quot;step =  1  lambda =  0.00355286840622136  loss:  7.72973304642099&quot;
## [1] &quot;step =  2  lambda =  0.00355286840622136  loss:  7.74075740204213&quot;
## [1] &quot;step =  1  lambda =  0.00351751677491213  loss:  7.66560982961903&quot;
## [1] &quot;step =  2  lambda =  0.00351751677491213  loss:  7.67651946818906&quot;
## [1] &quot;step =  1  lambda =  0.00348251689821166  loss:  7.60200017556492&quot;
## [1] &quot;step =  2  lambda =  0.00348251689821166  loss:  7.61279625984691&quot;
## [1] &quot;step =  1  lambda =  0.00344786527610313  loss:  7.53890018269194&quot;
## [1] &quot;step =  2  lambda =  0.00344786527610313  loss:  7.54958386393944&quot;
## [1] &quot;step =  1  lambda =  0.00341355844339543  loss:  7.4763059766244&quot;
## [1] &quot;step =  2  lambda =  0.00341355844339543  loss:  7.48687839469466&quot;
## [1] &quot;step =  1  lambda =  0.00337959296937672  loss:  7.41421371001945&quot;
## [1] &quot;step =  2  lambda =  0.00337959296937672  loss:  7.42467599348542&quot;
## [1] &quot;step =  1  lambda =  0.00334596545747127  loss:  7.35261956240919&quot;
## [1] &quot;step =  2  lambda =  0.00334596545747127  loss:  7.36297282867032&quot;
## [1] &quot;step =  1  lambda =  0.00331267254489989  loss:  7.29151974004318&quot;
## [1] &quot;step =  2  lambda =  0.00331267254489989  loss:  7.30176509543505&quot;
## [1] &quot;step =  1  lambda =  0.00327971090234357  loss:  7.23091047573142&quot;
## [1] &quot;step =  2  lambda =  0.00327971090234357  loss:  7.24104901563414&quot;
## [1] &quot;step =  1  lambda =  0.00324707723361059  loss:  7.17078802868761&quot;
## [1] &quot;step =  2  lambda =  0.00324707723361059  loss:  7.18082083763315&quot;
## [1] &quot;step =  1  lambda =  0.00321476827530687  loss:  7.11114868437298&quot;
## [1] &quot;step =  2  lambda =  0.00321476827530687  loss:  7.12107683615127&quot;
## [1] &quot;step =  1  lambda =  0.00318278079650967  loss:  7.05198875434048&quot;
## [1] &quot;step =  2  lambda =  0.00318278079650967  loss:  7.06181331210444&quot;
## [1] &quot;step =  1  lambda =  0.00315111159844444  loss:  6.99330457607939&quot;
## [1] &quot;step =  2  lambda =  0.00315111159844444  loss:  7.00302659244888&quot;
## [1] &quot;step =  1  lambda =  0.00311975751416499  loss:  6.93509251286048&quot;
## [1] &quot;step =  2  lambda =  0.00311975751416499  loss:  6.9447130300251&quot;
## [1] &quot;step =  1  lambda =  0.00308871540823677  loss:  6.87734895358155&quot;
## [1] &quot;step =  2  lambda =  0.00308871540823677  loss:  6.8868690034024&quot;
## [1] &quot;step =  1  lambda =  0.00305798217642331  loss:  6.82007031261357&quot;
## [1] &quot;step =  2  lambda =  0.00305798217642331  loss:  6.82949091672397&quot;
## [1] &quot;step =  1  lambda =  0.00302755474537582  loss:  6.76325302964715&quot;
## [1] &quot;step =  2  lambda =  0.00302755474537582  loss:  6.77257519955229&quot;
## [1] &quot;step =  1  lambda =  0.00299743007232583  loss:  6.7068935695397&quot;
## [1] &quot;step =  2  lambda =  0.00299743007232583  loss:  6.71611830671525&quot;
## [1] &quot;step =  1  lambda =  0.00296760514478094  loss:  6.65098842216291&quot;
## [1] &quot;step =  2  lambda =  0.00296760514478094  loss:  6.66011671815268&quot;
## [1] &quot;step =  1  lambda =  0.00293807698022355  loss:  6.59553410225098&quot;
## [1] &quot;step =  2  lambda =  0.00293807698022355  loss:  6.60456693876349&quot;
## [1] &quot;step =  1  lambda =  0.00290884262581258  loss:  6.54052714924913&quot;
## [1] &quot;step =  2  lambda =  0.00290884262581258  loss:  6.5494654982533&quot;
## [1] &quot;step =  1  lambda =  0.00287989915808824  loss:  6.48596412716285&quot;
## [1] &quot;step =  2  lambda =  0.00287989915808824  loss:  6.49480895098262&quot;
## [1] &quot;step =  1  lambda =  0.00285124368267963  loss:  6.43184162440757&quot;
## [1] &quot;step =  2  lambda =  0.00285124368267963  loss:  6.44059387581562&quot;
## [1] &quot;step =  1  lambda =  0.00282287333401534  loss:  6.37815625365899&quot;
## [1] &quot;step =  2  lambda =  0.00282287333401534  loss:  6.38681687596951&quot;
## [1] &quot;step =  1  lambda =  0.00279478527503684  loss:  6.32490465170386&quot;
## [1] &quot;step =  2  lambda =  0.00279478527503684  loss:  6.33347457886437&quot;
## [1] &quot;step =  1  lambda =  0.00276697669691485  loss:  6.27208347929143&quot;
## [1] &quot;step =  2  lambda =  0.00276697669691485  loss:  6.28056363597371&quot;
## [1] &quot;step =  1  lambda =  0.00273944481876837  loss:  6.21968942098538&quot;
## [1] &quot;step =  2  lambda =  0.00273944481876837  loss:  6.22808072267545&quot;
## [1] &quot;step =  1  lambda =  0.00271218688738664  loss:  6.16771918501647&quot;
## [1] &quot;step =  2  lambda =  0.00271218688738664  loss:  6.17602253810369&quot;
## [1] &quot;step =  1  lambda =  0.00268520017695382  loss:  6.11616950313557&quot;
## [1] &quot;step =  2  lambda =  0.00268520017695382  loss:  6.1243858050009&quot;
## [1] &quot;step =  1  lambda =  0.00265848198877637  loss:  6.0650371304675&quot;
## [1] &quot;step =  2  lambda =  0.00265848198877637  loss:  6.07316726957081&quot;
## [1] &quot;step =  1  lambda =  0.0026320296510132  loss:  6.01431884536534&quot;
## [1] &quot;step =  2  lambda =  0.0026320296510132  loss:  6.02236370133191&quot;
## [1] &quot;step =  1  lambda =  0.0026058405184085  loss:  5.96401144926532&quot;
## [1] &quot;step =  2  lambda =  0.0026058405184085  loss:  5.97197189297148&quot;
## [1] &quot;step =  1  lambda =  0.00257991197202718  loss:  5.91411176654246&quot;
## [1] &quot;step =  2  lambda =  0.00257991197202718  loss:  5.92198866020039&quot;
## [1] &quot;step =  1  lambda =  0.002554241418993  loss:  5.86461664436665&quot;
## [1] &quot;step =  2  lambda =  0.002554241418993  loss:  5.87241084160835&quot;
## [1] &quot;step =  1  lambda =  0.00252882629222926  loss:  5.81552295255949&quot;
## [1] &quot;step =  2  lambda =  0.00252882629222926  loss:  5.82323529851989&quot;
## [1] &quot;step =  1  lambda =  0.0025036640502021  loss:  5.76682758345161&quot;
## [1] &quot;step =  2  lambda =  0.0025036640502021  loss:  5.77445891485097&quot;
## [1] &quot;step =  1  lambda =  0.00247875217666636  loss:  5.71852745174079&quot;
## [1] &quot;step =  2  lambda =  0.00247875217666636  loss:  5.72607859696617&quot;
## [1] &quot;step =  1  lambda =  0.00245408818041392  loss:  5.6706194943505&quot;
## [1] &quot;step =  2  lambda =  0.00245408818041392  loss:  5.67809127353655&quot;
## [1] &quot;step =  1  lambda =  0.0024296695950246  loss:  5.62310067028924&quot;
## [1] &quot;step =  2  lambda =  0.0024296695950246  loss:  5.63049389539813&quot;
## [1] &quot;step =  1  lambda =  0.00240549397861951  loss:  5.5759679605104&quot;
## [1] &quot;step =  2  lambda =  0.00240549397861951  loss:  5.58328343541105&quot;
## [1] &quot;step =  1  lambda =  0.00238155891361687  loss:  5.52921836777286&quot;
## [1] &quot;step =  2  lambda =  0.00238155891361687  loss:  5.53645688831932&quot;
## [1] &quot;step =  1  lambda =  0.00235786200649023  loss:  5.48284891650212&quot;
## [1] &quot;step =  2  lambda =  0.00235786200649023  loss:  5.49001127061126&quot;
## [1] &quot;step =  1  lambda =  0.00233440088752913  loss:  5.4368566526521&quot;
## [1] &quot;step =  2  lambda =  0.00233440088752913  loss:  5.44394362038057&quot;
## [1] &quot;step =  1  lambda =  0.00231117321060213  loss:  5.39123864356766&quot;
## [1] &quot;step =  2  lambda =  0.00231117321060213  loss:  5.39825099718802&quot;
## [1] &quot;step =  1  lambda =  0.00228817665292217  loss:  5.34599197784763&quot;
## [1] &quot;step =  2  lambda =  0.00228817665292217  loss:  5.35293048192385&quot;
## [1] &quot;step =  1  lambda =  0.00226540891481432  loss:  5.30111376520861&quot;
## [1] &quot;step =  2  lambda =  0.00226540891481432  loss:  5.30797917667078&quot;
## [1] &quot;step =  1  lambda =  0.0022428677194858  loss:  5.25660113634933&quot;
## [1] &quot;step =  2  lambda =  0.0022428677194858  loss:  5.26339420456769&quot;
## [1] &quot;step =  1  lambda =  0.0022205508127983  loss:  5.21245124281569&quot;
## [1] &quot;step =  2  lambda =  0.0022205508127983  loss:  5.21917270967395&quot;
## [1] &quot;step =  1  lambda =  0.00219845596304253  loss:  5.16866125686644&quot;
## [1] &quot;step =  2  lambda =  0.00219845596304253  loss:  5.17531185683441&quot;
## [1] &quot;step =  1  lambda =  0.00217658096071513  loss:  5.12522837133955&quot;
## [1] &quot;step =  2  lambda =  0.00217658096071513  loss:  5.13180883154504&quot;
## [1] &quot;step =  1  lambda =  0.00215492361829761  loss:  5.08214979951915&quot;
## [1] &quot;step =  2  lambda =  0.00215492361829761  loss:  5.08866083981921&quot;
## [1] &quot;step =  1  lambda =  0.00213348177003771  loss:  5.0394227750032&quot;
## [1] &quot;step =  2  lambda =  0.00213348177003771  loss:  5.04586510805472&quot;
## [1] &quot;step =  1  lambda =  0.00211225327173271  loss:  4.99704455157176&quot;
## [1] &quot;step =  2  lambda =  0.00211225327173271  loss:  5.00341888290133&quot;
## [1] &quot;step =  1  lambda =  0.00209123600051511  loss:  4.95501240305593&quot;
## [1] &quot;step =  2  lambda =  0.00209123600051511  loss:  4.9613194311291&quot;
## [1] &quot;step =  1  lambda =  0.00207042785464026  loss:  4.91332362320745&quot;
## [1] &quot;step =  2  lambda =  0.00207042785464026  loss:  4.91956403949728&quot;
## [1] &quot;step =  1  lambda =  0.00204982675327624  loss:  4.87197552556895&quot;
## [1] &quot;step =  2  lambda =  0.00204982675327624  loss:  4.87815001462397&quot;
## [1] &quot;step =  1  lambda =  0.00202943063629574  loss:  4.83096544334481&quot;
## [1] &quot;step =  2  lambda =  0.00202943063629574  loss:  4.8370746828563&quot;
## [1] &quot;step =  1  lambda =  0.00200923746407006  loss:  4.79029072927277&quot;
## [1] &quot;step =  2  lambda =  0.00200923746407006  loss:  4.79633539014138&quot;
## [1] &quot;step =  1  lambda =  0.00198924521726516  loss:  4.74994875549609&quot;
## [1] &quot;step =  2  lambda =  0.00198924521726516  loss:  4.7559295018979&quot;
## [1] &quot;step =  1  lambda =  0.0019694518966397  loss:  4.70993691343639&quot;
## [1] &quot;step =  2  lambda =  0.0019694518966397  loss:  4.71585440288829&quot;
## [1] &quot;step =  1  lambda =  0.00194985552284512  loss:  4.6702526136672&quot;
## [1] &quot;step =  2  lambda =  0.00194985552284512  loss:  4.67610749709167&quot;
## [1] &quot;step =  1  lambda =  0.00193045413622771  loss:  4.63089328578808&quot;
## [1] &quot;step =  2  lambda =  0.00193045413622771  loss:  4.63668620757738&quot;
## [1] &quot;step =  1  lambda =  0.00191124579663264  loss:  4.59185637829945&quot;
## [1] &quot;step =  2  lambda =  0.00191124579663264  loss:  4.59758797637915&quot;
## [1] &quot;step =  1  lambda =  0.00189222858320994  loss:  4.55313935847802&quot;
## [1] &quot;step =  2  lambda =  0.00189222858320994  loss:  4.55881026437&quot;
## [1] &quot;step =  1  lambda =  0.00187340059422243  loss:  4.51473971225295&quot;
## [1] &quot;step =  2  lambda =  0.00187340059422243  loss:  4.52035055113775&quot;
## [1] &quot;step =  1  lambda =  0.0018547599468555  loss:  4.47665494408253&quot;
## [1] &quot;step =  2  lambda =  0.0018547599468555  loss:  4.48220633486114&quot;
## [1] &quot;step =  1  lambda =  0.00183630477702891  loss:  4.43888257683163&quot;
## [1] &quot;step =  2  lambda =  0.00183630477702891  loss:  4.44437513218671&quot;
## [1] &quot;step =  1  lambda =  0.00181803323921027  loss:  4.40142015164977&quot;
## [1] &quot;step =  2  lambda =  0.00181803323921027  loss:  4.40685447810627&quot;
## [1] &quot;step =  1  lambda =  0.00179994350623059  loss:  4.36426522784976&quot;
## [1] &quot;step =  2  lambda =  0.00179994350623059  loss:  4.36964192583497&quot;
## [1] &quot;step =  1  lambda =  0.00178203376910149  loss:  4.32741538278708&quot;
## [1] &quot;step =  2  lambda =  0.00178203376910149  loss:  4.33273504669011&quot;
## [1] &quot;step =  1  lambda =  0.00176430223683434  loss:  4.29086821173985&quot;
## [1] &quot;step =  2  lambda =  0.00176430223683434  loss:  4.2961314299706&quot;
## [1] &quot;step =  1  lambda =  0.00174674713626112  loss:  4.25462132778946&quot;
## [1] &quot;step =  2  lambda =  0.00174674713626112  loss:  4.25982868283698&quot;
## [1] &quot;step =  1  lambda =  0.00172936671185716  loss:  4.21867236170185&quot;
## [1] &quot;step =  2  lambda =  0.00172936671185716  loss:  4.22382443019215&quot;
## [1] &quot;step =  1  lambda =  0.00171215922556552  loss:  4.18301896180941&quot;
## [1] &quot;step =  2  lambda =  0.00171215922556552  loss:  4.18811631456276&quot;
## [1] &quot;step =  1  lambda =  0.00169512295662325  loss:  4.1476587938935&quot;
## [1] &quot;step =  2  lambda =  0.00169512295662325  loss:  4.15270199598118&quot;
## [1] &quot;step =  1  lambda =  0.00167825620138925  loss:  4.11258954106769&quot;
## [1] &quot;step =  2  lambda =  0.00167825620138925  loss:  4.1175791518682&quot;
## [1] &quot;step =  1  lambda =  0.00166155727317393  loss:  4.07780890366155&quot;
## [1] &quot;step =  2  lambda =  0.00166155727317393  loss:  4.08274547691627&quot;
## [1] &quot;step =  1  lambda =  0.00164502450207057  loss:  4.04331459910509&quot;
## [1] &quot;step =  2  lambda =  0.00164502450207057  loss:  4.04819868297348&quot;
## [1] &quot;step =  1  lambda =  0.00162865623478828  loss:  4.00910436181387&quot;
## [1] &quot;step =  2  lambda =  0.00162865623478828  loss:  4.01393649892807&quot;
## [1] &quot;step =  1  lambda =  0.00161245083448668  loss:  3.97517594307473&quot;
## [1] &quot;step =  2  lambda =  0.00161245083448668  loss:  3.97995667059371&quot;
## [1] &quot;step =  1  lambda =  0.00159640668061225  loss:  3.94152711093212&quot;
## [1] &quot;step =  2  lambda =  0.00159640668061225  loss:  3.94625696059526&quot;
## [1] &quot;step =  1  lambda =  0.00158052216873622  loss:  3.90815565007507&quot;
## [1] &quot;step =  2  lambda =  0.00158052216873622  loss:  3.91283514825532&quot;
## [1] &quot;step =  1  lambda =  0.00156479571039417  loss:  3.87505936172483&quot;
## [1] &quot;step =  2  lambda =  0.00156479571039417  loss:  3.87968902948128&quot;
## [1] &quot;step =  1  lambda =  0.00154922573292716  loss:  3.84223606352306&quot;
## [1] &quot;step =  2  lambda =  0.00154922573292716  loss:  3.84681641665307&quot;
## [1] &quot;step =  1  lambda =  0.00153381067932446  loss:  3.8096835894207&quot;
## [1] &quot;step =  2  lambda =  0.00153381067932446  loss:  3.81421513851151&quot;
## [1] &quot;step =  1  lambda =  0.00151854900806788  loss:  3.77739978956746&quot;
## [1] &quot;step =  2  lambda =  0.00151854900806788  loss:  3.78188304004734&quot;
## [1] &quot;step =  1  lambda =  0.00150343919297757  loss:  3.74538253020188&quot;
## [1] &quot;step =  2  lambda =  0.00150343919297757  loss:  3.74981798239077&quot;
## [1] &quot;step =  1  lambda =  0.00148847972305943  loss:  3.71362969354206&quot;
## [1] &quot;step =  2  lambda =  0.00148847972305943  loss:  3.71801784270175&quot;
## [1] &quot;step =  1  lambda =  0.001473669102354  loss:  3.68213917767699&quot;
## [1] &quot;step =  2  lambda =  0.001473669102354  loss:  3.68648051406082&quot;
## [1] &quot;step =  1  lambda =  0.00145900584978686  loss:  3.65090889645847&quot;
## [1] &quot;step =  2  lambda =  0.00145900584978686  loss:  3.65520390536059&quot;
## [1] &quot;step =  1  lambda =  0.00144448849902054  loss:  3.61993677939369&quot;
## [1] &quot;step =  2  lambda =  0.00144448849902054  loss:  3.62418594119776&quot;
## [1] &quot;step =  1  lambda =  0.00143011559830787  loss:  3.58922077153839&quot;
## [1] &quot;step =  2  lambda =  0.00143011559830787  loss:  3.59342456176595&quot;
## [1] &quot;step =  1  lambda =  0.0014158857103468  loss:  3.55875883339059&quot;
## [1] &quot;step =  2  lambda =  0.0014158857103468  loss:  3.56291772274888&quot;
## [1] &quot;step =  1  lambda =  0.00140179741213667  loss:  3.52854894078502&quot;
## [1] &quot;step =  2  lambda =  0.00140179741213667  loss:  3.53266339521439&quot;
## [1] &quot;step =  1  lambda =  0.00138784929483593  loss:  3.49858908478808&quot;
## [1] &quot;step =  2  lambda =  0.00138784929483593  loss:  3.50265956550891&quot;
## [1] &quot;step =  1  lambda =  0.00137403996362121  loss:  3.46887727159341&quot;
## [1] &quot;step =  2  lambda =  0.00137403996362121  loss:  3.47290423515266&quot;
## [1] &quot;step =  1  lambda =  0.00136036803754789  loss:  3.43941152241809&quot;
## [1] &quot;step =  2  lambda =  0.00136036803754789  loss:  3.44339542073532&quot;
## [1] &quot;step =  1  lambda =  0.00134683214941197  loss:  3.41018987339943&quot;
## [1] &quot;step =  2  lambda =  0.00134683214941197  loss:  3.41413115381244&quot;
## [1] &quot;step =  1  lambda =  0.00133343094561336  loss:  3.3812103754923&quot;
## [1] &quot;step =  2  lambda =  0.00133343094561336  loss:  3.38510948080234&quot;
## [1] &quot;step =  1  lambda =  0.0013201630860205  loss:  3.35247109436718&quot;
## [1] &quot;step =  2  lambda =  0.0013201630860205  loss:  3.35632846288371&quot;
## [1] &quot;step =  1  lambda =  0.00130702724383639  loss:  3.32397011030864&quot;
## [1] &quot;step =  2  lambda =  0.00130702724383639  loss:  3.32778617589366&quot;
## [1] &quot;step =  1  lambda =  0.00129402210546585  loss:  3.29570551811456&quot;
## [1] &quot;step =  2  lambda =  0.00129402210546585  loss:  3.29948071022653&quot;
## [1] &quot;step =  1  lambda =  0.00128114637038421  loss:  3.26767542699584&quot;
## [1] &quot;step =  2  lambda =  0.00128114637038421  loss:  3.27141017073319&quot;
## [1] &quot;step =  1  lambda =  0.00126839875100724  loss:  3.2398779604767&quot;
## [1] &quot;step =  2  lambda =  0.00126839875100724  loss:  3.24357267662094&quot;
## [1] &quot;step =  1  lambda =  0.00125577797256237  loss:  3.21231125629565&quot;
## [1] &quot;step =  2  lambda =  0.00125577797256237  loss:  3.215966361354&quot;
## [1] &quot;step =  1  lambda =  0.00124328277296124  loss:  3.18497346630693&quot;
## [1] &quot;step =  2  lambda =  0.00124328277296124  loss:  3.18858937255464&quot;
## [1] &quot;step =  1  lambda =  0.00123091190267348  loss:  3.15786275638261&quot;
## [1] &quot;step =  2  lambda =  0.00123091190267348  loss:  3.1614398719048&quot;
## [1] &quot;step =  1  lambda =  0.00121866412460175  loss:  3.1309773063152&quot;
## [1] &quot;step =  2  lambda =  0.00121866412460175  loss:  3.13451603504836&quot;
## [1] &quot;step =  1  lambda =  0.00120653821395804  loss:  3.10431530972092&quot;
## [1] &quot;step =  2  lambda =  0.00120653821395804  loss:  3.10781605149398&quot;
## [1] &quot;step =  1  lambda =  0.00119453295814118  loss:  3.07787497394345&quot;
## [1] &quot;step =  2  lambda =  0.00119453295814118  loss:  3.08133812451844&quot;
## [1] &quot;step =  1  lambda =  0.00118264715661557  loss:  3.0516545199583&quot;
## [1] &quot;step =  2  lambda =  0.00118264715661557  loss:  3.0550804710707&quot;
## [1] &quot;step =  1  lambda =  0.00117087962079117  loss:  3.02565218227777&quot;
## [1] &quot;step =  2  lambda =  0.00117087962079117  loss:  3.02904132167638&quot;
## [1] &quot;step =  1  lambda =  0.00115922917390459  loss:  2.99986620885641&quot;
## [1] &quot;step =  2  lambda =  0.00115922917390459  loss:  3.00321892034291&quot;
## [1] &quot;step =  1  lambda =  0.00114769465090143  loss:  2.97429486099711&quot;
## [1] &quot;step =  2  lambda =  0.00114769465090143  loss:  2.97761152446517&quot;
## [1] &quot;step =  1  lambda =  0.00113627489831977  loss:  2.94893641325768&quot;
## [1] &quot;step =  2  lambda =  0.00113627489831977  loss:  2.9522174047318&quot;
## [1] &quot;step =  1  lambda =  0.00112496877417484  loss:  2.92378915335807&quot;
## [1] &quot;step =  2  lambda =  0.00112496877417484  loss:  2.92703484503196&quot;
## [1] &quot;step =  1  lambda =  0.0011137751478448  loss:  2.89885138208811&quot;
## [1] &quot;step =  2  lambda =  0.0011137751478448  loss:  2.90206214236273&quot;
## [1] &quot;step =  1  lambda =  0.0011026928999577  loss:  2.87412141321574&quot;
## [1] &quot;step =  2  lambda =  0.0011026928999577  loss:  2.877297606737&quot;
## [1] &quot;step =  1  lambda =  0.00109172092227951  loss:  2.84959757339593&quot;
## [1] &quot;step =  2  lambda =  0.00109172092227951  loss:  2.85273956109198&quot;
## [1] &quot;step =  1  lambda =  0.00108085811760332  loss:  2.82527820208003&quot;
## [1] &quot;step =  2  lambda =  0.00108085811760332  loss:  2.82838634119823&quot;
## [1] &quot;step =  1  lambda =  0.0010701033996396  loss:  2.80116165142572&quot;
## [1] &quot;step =  2  lambda =  0.0010701033996396  loss:  2.80423629556925&quot;
## [1] &quot;step =  1  lambda =  0.00105945569290761  loss:  2.77724628620747&quot;
## [1] &quot;step =  2  lambda =  0.00105945569290761  loss:  2.7802877853716&quot;
## [1] &quot;step =  1  lambda =  0.00104891393262779  loss:  2.75353048372763&quot;
## [1] &quot;step =  2  lambda =  0.00104891393262779  loss:  2.75653918433558&quot;
## [1] &quot;step =  1  lambda =  0.00103847706461533  loss:  2.73001263372792&quot;
## [1] &quot;step =  2  lambda =  0.00103847706461533  loss:  2.73298887866648&quot;
## [1] &quot;step =  1  lambda =  0.00102814404517473  loss:  2.70669113830162&quot;
## [1] &quot;step =  2  lambda =  0.00102814404517473  loss:  2.70963526695631&quot;
## [1] &quot;step =  1  lambda =  0.00101791384099544  loss:  2.68356441180613&quot;
## [1] &quot;step =  2  lambda =  0.00101791384099544  loss:  2.68647676009614&quot;
## [1] &quot;step =  1  lambda =  0.00100778542904851  loss:  2.66063088077622&quot;
## [1] &quot;step =  2  lambda =  0.00100778542904851  loss:  2.66351178118892&quot;
## [1] &quot;step =  1  lambda =  0.000997757796484312  loss:  2.63788898383769&quot;
## [1] &quot;step =  2  lambda =  0.000997757796484312  loss:  2.64073876546285&quot;
## [1] &quot;step =  1  lambda =  0.000987829940531229  loss:  2.61533717162164&quot;
## [1] &quot;step =  2  lambda =  0.000987829940531229  loss:  2.61815616018531&quot;
## [1] &quot;step =  1  lambda =  0.000978000868395395  loss:  2.59297390667921&quot;
## [1] &quot;step =  2  lambda =  0.000978000868395395  loss:  2.59576242457728&quot;
## [1] &quot;step =  1  lambda =  0.000968269597161403  loss:  2.57079766339687&quot;
## [1] &quot;step =  2  lambda =  0.000968269597161403  loss:  2.57355602972832&quot;
## [1] &quot;step =  1  lambda =  0.000958635153694021  loss:  2.54880692791227&quot;
## [1] &quot;step =  2  lambda =  0.000958635153694021  loss:  2.55153545851204&quot;
## [1] &quot;step =  1  lambda =  0.000949096574540873  loss:  2.5270001980305&quot;
## [1] &quot;step =  2  lambda =  0.000949096574540873  loss:  2.52969920550215&quot;
## [1] &quot;step =  1  lambda =  0.000939652905836096  loss:  2.50537598314103&quot;
## [1] &quot;step =  2  lambda =  0.000939652905836096  loss:  2.50804577688896&quot;
## [1] &quot;step =  1  lambda =  0.000930303203204949  loss:  2.48393280413499&quot;
## [1] &quot;step =  2  lambda =  0.000930303203204949  loss:  2.48657369039644&quot;
## [1] &quot;step =  1  lambda =  0.000921046531669378  loss:  2.46266919332309&quot;
## [1] &quot;step =  2  lambda =  0.000921046531669378  loss:  2.4652814751998&quot;
## [1] &quot;step =  1  lambda =  0.000911881965554516  loss:  2.44158369435402&quot;
## [1] &quot;step =  2  lambda =  0.000911881965554516  loss:  2.44416767184356&quot;
## [1] &quot;step =  1  lambda =  0.000902808588396114  loss:  2.42067486213329&quot;
## [1] &quot;step =  2  lambda =  0.000902808588396114  loss:  2.42323083216015&quot;
## [1] &quot;step =  1  lambda =  0.000893825492848894  loss:  2.39994126274271&quot;
## [1] &quot;step =  2  lambda =  0.000893825492848894  loss:  2.40246951918901&quot;
## [1] &quot;step =  1  lambda =  0.000884931780595815  loss:  2.37938147336022&quot;
## [1] &quot;step =  2  lambda =  0.000884931780595815  loss:  2.3818823070962&quot;
## [1] &quot;step =  1  lambda =  0.000876126562258242  loss:  2.35899408218036&quot;
## [1] &quot;step =  2  lambda =  0.000876126562258242  loss:  2.36146778109449&quot;
## [1] &quot;step =  1  lambda =  0.000867408957307003  loss:  2.33877768833513&quot;
## [1] &quot;step =  2  lambda =  0.000867408957307003  loss:  2.34122453736402&quot;
## [1] &quot;step =  1  lambda =  0.000858778093974336  loss:  2.31873090181544&quot;
## [1] &quot;step =  2  lambda =  0.000858778093974336  loss:  2.32115118297337&quot;
## [1] &quot;step =  1  lambda =  0.000850233109166719  loss:  2.29885234339294&quot;
## [1] &quot;step =  2  lambda =  0.000850233109166719  loss:  2.3012463358012&quot;
## [1] &quot;step =  1  lambda =  0.000841773148378549  loss:  2.27914064454249&quot;
## [1] &quot;step =  2  lambda =  0.000841773148378549  loss:  2.28150862445833&quot;
## [1] &quot;step =  1  lambda =  0.000833397365606696  loss:  2.25959444736496&quot;
## [1] &quot;step =  2  lambda =  0.000833397365606696  loss:  2.26193668821036&quot;
## [1] &quot;step =  1  lambda =  0.000825104923265905  loss:  2.24021240451068&quot;
## [1] &quot;step =  2  lambda =  0.000825104923265905  loss:  2.24252917690074&quot;
## [1] &quot;step =  1  lambda =  0.000816894992105029  loss:  2.22099317910321&quot;
## [1] &quot;step =  2  lambda =  0.000816894992105029  loss:  2.22328475087437&quot;
## [1] &quot;step =  1  lambda =  0.000808766751124111  loss:  2.20193544466374&quot;
## [1] &quot;step =  2  lambda =  0.000808766751124111  loss:  2.20420208090163&quot;
## [1] &quot;step =  1  lambda =  0.000800719387492281  loss:  2.18303788503588&quot;
## [1] &quot;step =  2  lambda =  0.000800719387492281  loss:  2.18527984810295&quot;
## [1] &quot;step =  1  lambda =  0.000792752096466468  loss:  2.16429919431095&quot;
## [1] &quot;step =  2  lambda =  0.000792752096466468  loss:  2.16651674387383&quot;
## [1] &quot;step =  1  lambda =  0.000784864081310932  loss:  2.14571807675375&quot;
## [1] &quot;step =  2  lambda =  0.000784864081310932  loss:  2.14791146981032&quot;
## [1] &quot;step =  1  lambda =  0.000777054553217582  loss:  2.12729324672881&quot;
## [1] &quot;step =  2  lambda =  0.000777054553217582  loss:  2.12946273763505&quot;
## [1] &quot;step =  1  lambda =  0.000769322731227101  loss:  2.10902342862713&quot;
## [1] &quot;step =  2  lambda =  0.000769322731227101  loss:  2.11116926912362&quot;
## [1] &quot;step =  1  lambda =  0.000761667842150847  loss:  2.0909073567933&quot;
## [1] &quot;step =  2  lambda =  0.000761667842150847  loss:  2.0930297960316&quot;
## [1] &quot;step =  1  lambda =  0.000754089120493534  loss:  2.07294377545322&quot;
## [1] &quot;step =  2  lambda =  0.000754089120493534  loss:  2.07504306002183&quot;
## [1] &quot;step =  1  lambda =  0.00074658580837668  loss:  2.05513143864219&quot;
## [1] &quot;step =  2  lambda =  0.00074658580837668  loss:  2.0572078125924&quot;
## [1] &quot;step =  1  lambda =  0.00073915715546282  loss:  2.03746911013346&quot;
## [1] &quot;step =  2  lambda =  0.00073915715546282  loss:  2.03952281500484&quot;
## [1] &quot;step =  1  lambda =  0.000731802418880473  loss:  2.01995556336733&quot;
## [1] &quot;step =  2  lambda =  0.000731802418880473  loss:  2.02198683821305&quot;
## [1] &quot;step =  1  lambda =  0.000724520863149851  loss:  2.00258958138057&quot;
## [1] &quot;step =  2  lambda =  0.000724520863149851  loss:  2.00459866279244&quot;
## [1] &quot;step =  1  lambda =  0.000717311760109313  loss:  1.98536995673644&quot;
## [1] &quot;step =  2  lambda =  0.000717311760109313  loss:  1.98735707886967&quot;
## [1] &quot;step =  1  lambda =  0.000710174388842549  loss:  1.96829549145508&quot;
## [1] &quot;step =  2  lambda =  0.000710174388842549  loss:  1.97026088605285&quot;
## [1] &quot;step =  1  lambda =  0.000703108035606483  loss:  1.9513649969443&quot;
## [1] &quot;step =  2  lambda =  0.000703108035606483  loss:  1.95330889336208&quot;
## [1] &quot;step =  1  lambda =  0.000696111993759903  loss:  1.934577293931&quot;
## [1] &quot;step =  2  lambda =  0.000696111993759903  loss:  1.93649991916061&quot;
## [1] &quot;step =  1  lambda =  0.000689185563692794  loss:  1.9179312123928&quot;
## [1] &quot;step =  2  lambda =  0.000689185563692794  loss:  1.91983279108626&quot;
## [1] &quot;step =  1  lambda =  0.000682328052756377  loss:  1.90142559149034&quot;
## [1] &quot;step =  2  lambda =  0.000682328052756377  loss:  1.90330634598345&quot;
## [1] &quot;step =  1  lambda =  0.000675538775193844  loss:  1.88505927949984&quot;
## [1] &quot;step =  2  lambda =  0.000675538775193844  loss:  1.88691942983556&quot;
## [1] &quot;step =  1  lambda =  0.000668817052071782  loss:  1.86883113374622&quot;
## [1] &quot;step =  2  lambda =  0.000668817052071782  loss:  1.87067089769782&quot;
## [1] &quot;step =  1  lambda =  0.000662162211212276  loss:  1.85274002053659&quot;
## [1] &quot;step =  2  lambda =  0.000662162211212276  loss:  1.85455961363055&quot;
## [1] &quot;step =  1  lambda =  0.000655573587125696  loss:  1.83678481509421&quot;
## [1] &quot;step =  2  lambda =  0.000655573587125696  loss:  1.83858445063294&quot;
## [1] &quot;step =  1  lambda =  0.000649050520944141  loss:  1.82096440149288&quot;
## [1] &quot;step =  2  lambda =  0.000649050520944141  loss:  1.82274429057713&quot;
## [1] &quot;step =  1  lambda =  0.000642592360355558  loss:  1.80527767259173&quot;
## [1] &quot;step =  2  lambda =  0.000642592360355558  loss:  1.80703802414288&quot;
## [1] &quot;step =  1  lambda =  0.000636198459538506  loss:  1.78972352997049&quot;
## [1] &quot;step =  2  lambda =  0.000636198459538506  loss:  1.79146455075256&quot;
## [1] &quot;step =  1  lambda =  0.000629868179097574  loss:  1.77430088386512&quot;
## [1] &quot;step =  2  lambda =  0.000629868179097574  loss:  1.77602277850658&quot;
## [1] &quot;step =  1  lambda =  0.000623600885999444  loss:  1.75900865310394&quot;
## [1] &quot;step =  2  lambda =  0.000623600885999444  loss:  1.76071162411931&quot;
## [1] &quot;step =  1  lambda =  0.000617395953509583  loss:  1.74384576504412&quot;
## [1] &quot;step =  2  lambda =  0.000617395953509583  loss:  1.74553001285535&quot;
## [1] &quot;step =  1  lambda =  0.000611252761129572  loss:  1.72881115550862&quot;
## [1] &quot;step =  2  lambda =  0.000611252761129572  loss:  1.73047687846625&quot;
## [1] &quot;step =  1  lambda =  0.000605170694535053  loss:  1.71390376872355&quot;
## [1] &quot;step =  2  lambda =  0.000605170694535053  loss:  1.71555116312767&quot;
## [1] &quot;step =  1  lambda =  0.000599149145514298  loss:  1.6991225572559&quot;
## [1] &quot;step =  2  lambda =  0.000599149145514298  loss:  1.70075181737693&quot;
## [1] &quot;step =  1  lambda =  0.000593187511907387  loss:  1.68446648195177&quot;
## [1] &quot;step =  2  lambda =  0.000593187511907387  loss:  1.68607780005099&quot;
## [1] &quot;step =  1  lambda =  0.000587285197545991  loss:  1.66993451187491&quot;
## [1] &quot;step =  2  lambda =  0.000587285197545991  loss:  1.67152807822481&quot;
## [1] &quot;step =  1  lambda =  0.000581441612193756  loss:  1.65552562424575&quot;
## [1] &quot;step =  2  lambda =  0.000581441612193756  loss:  1.65710162715018&quot;
## [1] &quot;step =  1  lambda =  0.000575656171487276  loss:  1.64123880438081&quot;
## [1] &quot;step =  2  lambda =  0.000575656171487276  loss:  1.64279743019493&quot;
## [1] &quot;step =  1  lambda =  0.00056992829687766  loss:  1.62707304563248&quot;
## [1] &quot;step =  2  lambda =  0.00056992829687766  loss:  1.62861447878249&quot;
## [1] &quot;step =  1  lambda =  0.000564257415572674  loss:  1.61302734932924&quot;
## [1] &quot;step =  2  lambda =  0.000564257415572674  loss:  1.61455177233196&quot;
## [1] &quot;step =  1  lambda =  0.000558642960479461  loss:  1.59910072471629&quot;
## [1] &quot;step =  2  lambda =  0.000558642960479461  loss:  1.60060831819852&quot;
## [1] &quot;step =  1  lambda =  0.000553084370147834  loss:  1.58529218889652&quot;
## [1] &quot;step =  2  lambda =  0.000553084370147834  loss:  1.58678313161421&quot;
## [1] &quot;step =  1  lambda =  0.000547581088714126  loss:  1.57160076677198&quot;
## [1] &quot;step =  2  lambda =  0.000547581088714126  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000542132565845609  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000536738256685455  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000531397621798253  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000526110127116064  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000520875243885012  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000515692448612414  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000510561223014422  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0005054810539642  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000500451433440611  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00049547185847741  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000490541831112951  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000485660858340389  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00048082845205838  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000476044129022269  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000471307410795765  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000466617823703098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000461974898781651  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000457378171735063  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000452827182886797  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000448321477134178  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000443860603902874  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000439444117101845  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000435071575078732  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000430742540575688  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000426456580685654  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00042221326680907  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000418012174611013  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000413852883978762  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000409734978979787  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000405658047820157  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000401621682803358  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000397625480289526  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000393669040655078  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000389751968252755  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000385873871372051  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000382034362200047  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000378233056782626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000374469574986078  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000370743540459088  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000367054580595098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000363402326495048  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000359786412930483  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000356206478307034  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000352662164628256  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000349153117459826  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000345678985894105  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000342239422515039  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000338834083363426  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000335462627902512  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000332124718983941  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00032882002281404  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000325548208920438  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000322308950119019  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000319101922481203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000315926805301555  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000312783281065711  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000309671035418626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000306589757133144  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000303539138078867  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000300518873191348  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000297528660441581  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0002945682008058  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000291637198235574  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000288735359628203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000285862394797409  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000283018016444314  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000280201940128712  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000277413884240626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000274653569972143  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000271920721289535  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000269215064905658  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000266536330252618  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000263884249454718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000261258557301668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000258658991222064  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000256085291257132  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00025353720003473  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000251014462743614  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000248516827107952  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000246044043362099  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000243595864225619  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000241172044878559  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000238772342936964  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000236396518428641  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000234044333769158  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00023171555373809  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000229409945455492  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000227127278358615  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000224867324178848  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000222629856918889  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000220414652830147  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000218221490390368  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000216050150281479  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000213900415367661  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000211772070673631  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000209664903363145  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000207578702717718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000205513260115544  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000203468369010644  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000201443824912203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000199439425364123  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000197454969924779  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000195490260146975  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000193545099558094  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000191619293640457  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000189712649811868  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000187824977406354  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000185956087655102  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000184105793667579  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000182273910412845  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000180460254701048  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000178664645165106  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000176886902242567  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000175126848157658  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000173384306903506  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00017165910422453  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000169951067599028  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000168260026221911  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000166585810987634  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000164928254473277  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000163287190921808  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000161662456225505  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000160053887909543  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000158461325115751  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000156884608586522  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00015532358064889  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000153778085198759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000152247967685297  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000150733075095477  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000149233255938777  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000147748360232034  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000146278239484437  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000144822746682688  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000143381736276293  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000141955064163011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000140542587674442  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000139144165561759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000137759657981586  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000136388926482011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000135031833988743  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0001336882447914  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000132358024529944  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000131041040181239  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000129737160045754  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000128446253734388  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000127168192155434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012590284750167  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000124650093237575  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012340980408668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000122181856019035  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012096612623881  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000119762493172015  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000118570836454339  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000117391036919118  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000116222976585415  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000115066538646224  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000113921607456786  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000112788068523029  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000111665808490115  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000110554715131105  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000109454677335737  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000108365585099315  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000107287329511708  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000106219802746459  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000105162898050001  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000104116509730984  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000103080533149705  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000102054864707641  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000101039401837093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00010003404299093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.9038687632427e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.80532362252201e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.70775902233471e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.61116520613947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.51553251447417e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.42085138398996e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.32711234649488e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.23430602800707e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.14242314781733e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.05145451756109e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.96139104029952e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.87222370960982e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.78394360868463e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.69654190944029e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.61000987163404e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.52433884198997e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.43952025333374e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.3555456237358e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.27240655566322e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1900947351399e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1086019309152e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.02791999364078e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.94804085505568e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.86895652717947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.79065910151345e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.71314074824984e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.63639371548869e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.56041032846277e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.48518298877006e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.4107041736139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.33696643505071e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.26396239924518e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.1916847657329e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.12012630669027e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.04927986621178e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.97913835959434e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.90969477262882e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.84094216089865e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.77287364908539e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.70548243028111e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.63876176530778e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.5727049820433e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.50730547475429e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.44255670343554e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.37845219315595e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.31498553341106e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.25215037748203e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.18994044180088e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.12834950532221e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.06737140890104e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.00700005467694e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.94722940546415e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.88805348414794e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.82946637308688e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.77146221352103e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.71403520498611e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.65717960473339e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.60088972715548e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.54515994321769e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.48998467989523e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.43535841961575e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.38127569970771e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.32773111185406e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.27471930155139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.22223496757448e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.1702728614462e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.11882778691264e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.06789459942348e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.01746820561753e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.96754356281337e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.91811567850513e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.86917960986318e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.82073046323988e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.7727633936802e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.72527360443719e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.67825634649237e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.63170691808076e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.58562066422073e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.53999297624849e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0  loss:  NaN&quot;
## [1] &quot;fold:  2&quot;
## [1] &quot;step =  1  lambda =  22026.4657948067  loss:  68.5967849341713&quot;
## [1] &quot;step =  2  lambda =  22026.4657948067  loss:  68.2814771274233&quot;
## [1] &quot;step =  3  lambda =  22026.4657948067  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  21807.2987982302  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  21590.3125497062  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  21375.4853504291  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  21162.7957175002  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  20952.2223817786  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  20743.7442857556  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  20537.3405814475  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  20332.9906283122  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  20130.6739911839  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  19930.3704382303  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  19732.0599389292  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  19535.7226620655  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  19341.3389737478  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  19148.8894354453  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  18958.3548020439  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  18769.7160199212  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  18582.9542250422  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  18398.0507410714  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  18214.9870775064  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  18033.7449278285  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  17854.3061676715  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  17676.65285301  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  17500.7672183642  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  17326.6316750244  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  17154.228809291  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  16983.5413807338  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  16814.5523204676  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  16647.2447294456  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  16481.6018767693  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  16317.6071980154  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  16155.2442935794  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  15994.4969270355  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  15835.3490235131  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  15677.7846680892  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  15521.788104197  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  15367.34373205  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  15214.4361070824  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  15063.0499384043  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  14913.1700872726  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  14764.7815655773  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  14617.8695343426  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  14472.4193022429  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  14328.4163241338  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  14185.8461995975  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  14044.6946715028  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  13904.9476245792  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  13766.5910840055  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  13629.6112140124  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  13493.9943164988  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  13359.7268296619  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  13226.7953266411  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  13095.1865141752  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12964.8872312735  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12835.8844478991  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12708.165263666  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12581.7169065495  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12456.5267316084  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12332.582219721  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12209.8709763327  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  12088.380730217  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11968.099332248  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11849.0147541856  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11731.1150874729  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11614.3885420449  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11498.8234451498  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11384.4082401816  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11271.1314855245  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11158.9818534085  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  11047.9481287771  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10938.0192081652  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10829.1840985891  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10721.4319164473  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10614.7518864317  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10509.1333404504  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10404.5657165607  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10301.0385579133  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10198.5415117058  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  10097.0643281483  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9996.59685943788  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9897.12905874391  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9798.65097920349  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9701.15277292652  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9604.6246900112  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9509.05707756873  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9414.44037875829  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9320.76513183108  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9228.02196918439  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9136.2016164247  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  9045.29489144014  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8955.29270348252  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8866.186052258  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8777.96602702724  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8690.62380571415  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8604.15065402384  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8518.53792456912  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8433.77705600563  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8349.85957217593  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8266.77708126167  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8184.52127494457  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8103.08392757538  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  8022.45689535158  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7942.63211550269  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7863.60160548423  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7785.35746217936  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7707.89186110851  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7631.19705564706  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7555.2653762505  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7480.08922968767  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7405.66109828121  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7331.97353915601  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7259.01918349469  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7186.79073580093  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7115.28097316979  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  7044.48274456536  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6974.38897010583  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6904.9926403553  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6836.286815623  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6768.26462526917  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6700.91926701811  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6634.24400627789  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6568.23217546683  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6502.87717334688  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6438.17246436333  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6374.1115779914  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6310.68810808902  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6247.8957122564  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6185.72811120158  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6124.17908811267  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6063.24248803609  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  6002.91221726102  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5943.18224271013  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5884.04659133616  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5825.49934952474  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5767.53466250285  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5710.14673375352  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5653.32982443602  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5597.07825281208  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5541.3863936777  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5486.2486778005  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5431.65959136299  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5377.61367541099  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5324.10552530792  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5271.12979019412  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5218.68117245197  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5166.754427176  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5115.34436164836  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5064.44583481972  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  5014.05375679492  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4964.16308832421  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4914.76884029913  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4865.86607325376  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4817.44989687059  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4769.51546949167  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4722.05799763432  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4675.07273551178  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4628.55498455872  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4582.50009296124  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4536.90345519183  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4491.76051154869  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4447.06674769987  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4402.8176942317  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4359.00892620198  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4315.63606269742  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4272.69476639549  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4230.1807431308  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4188.08974146558  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4146.4175522646  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4105.16000827419  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4064.31298370559  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  4023.87239382231  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3983.83419453164  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3944.19438198031  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3904.948992154  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3866.09410048106  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3827.62582143991  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3789.54030817061  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3751.83375209008  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3714.50238251129  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3677.5424662662  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3640.95030733235  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3604.72224646338  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3568.854660823  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3533.34396362276  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3498.18660376333  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3463.37906547946  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3428.91786798828  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3394.79956514135  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3361.02074507995  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3327.5780298939  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3294.46807528385  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3261.68757022671  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3229.23323664469  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3197.10182907735  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3165.29013435719  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3133.79497128823  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3102.61319032788  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3071.7416732721  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3041.17733294343  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  3010.91711288239  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2980.95798704173  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2951.29695948392  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2921.93106408148  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2892.85736422039  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2864.07295250646  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2835.57495047451  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2807.3605083006  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2779.42680451698  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2751.77104573003  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2724.39046634078  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2697.28232826851  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2670.44392067681  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2643.87255970255  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2617.5655881875  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2591.52037541257  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2565.7343168348  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2540.20483382683  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2514.92937341909  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2489.90540804446  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2465.13043528557  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2440.6019776245  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2416.31758219502  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2392.27482053738  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2368.47128835535  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2344.9046052759  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2321.57241461106  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2298.47238312234  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2275.60220078732  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2252.95958056872  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2230.54225818566  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2208.34799188721  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2186.37456222824  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2164.61977184748  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2143.08144524776  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2121.75742857847  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2100.64558942018  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2079.74381657137  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2059.05001983734  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2038.56212982119  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  2018.27809771681  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1998.19589510412  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1978.3135137461  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1958.62896538806  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1939.14028155876  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1919.84551337356  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1900.74273133958  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1881.83002516269  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1863.10550355652  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1844.56729405329  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1826.21354281661  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1808.04241445606  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1790.05209184367  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1772.24077593218  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1754.60668557515  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1737.14805734885  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1719.86314537592  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1702.75022115076  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1685.80757336666  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1669.03350774476  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1652.42634686448  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1635.98442999593  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1619.7061129337  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1603.58976783251  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1587.63378304445  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1571.83656295772  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1556.19652783716  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1540.71211366621  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1525.38177199056  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1510.20396976326  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1495.17718919145  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1480.29992758455  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1465.57069720398  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1450.98802511446  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1436.5504530366  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1422.25653720118  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1408.1048482047  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1394.09397086646  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1380.22250408705  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1366.48906070825  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1352.89226737426  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1339.43076439442  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1326.10320560722  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1312.90825824566  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1299.84460280403  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1286.91093290588  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1274.10595517346  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1261.4283890983  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1248.87696691325  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1236.45043346563  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1224.14754609174  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1211.96707449258  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1199.90780061084  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1187.9685185091  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1176.14803424917  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1164.4451657728  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1152.85874278339  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1141.38760662897  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1130.03061018637  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1118.78661774649  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1107.65450490071  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1096.63315842846  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1085.72147618592  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1074.91836699577  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1064.22275053809  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1053.63355724232  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1043.1497281803  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1032.7702149604  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1022.49397962264  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1012.31999453492  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  1002.24724229025  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  992.274715605024  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  982.401417218259  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  972.626359791883  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  962.948565812015  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  953.367067491183  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  943.88090667158  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  934.48913472921  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  925.19081247906  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  915.98501008115  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  906.870806947571  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  897.847291650418  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  888.913561830636  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  880.068724107804  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  871.311893990772  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  862.642195789238  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  854.058762526152  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  845.560735851038  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  837.147265954143  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  828.817511481469  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  820.570639450629  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  812.405825167543  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  804.322252143983  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  796.319112015905  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  788.395604462634  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  780.550937126804  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  772.784325535149  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  765.094993020038  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  757.482170641809  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  749.945097111883  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  742.483018716622  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  735.095189241974  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  727.780869898828  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  720.539329249161  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  713.369843132868  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  706.271694595365  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  699.244173815886  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  692.286578036491  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  685.39821149181  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  678.578385339442  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  671.826417591095  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  665.141633044362  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  658.523363215222  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  651.970946271172  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  645.483726965061  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  639.061056569553  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  632.702292812253  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  626.40679981149  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  620.173948012713  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  614.003114125553  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  607.893681061474  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  601.845037872081  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  595.856579688017  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  589.927707658468  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  584.057828891295  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  578.246356393726  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  572.492709013672  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  566.796311381596  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  561.156593852992  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  555.572992451403  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  550.044948812038  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  544.571910125929  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  539.153329084642  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  533.788663825562  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  528.477377877687  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  523.218940108002  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  518.012824668342  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  512.85851094283  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  507.755483495794  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  502.703232020238  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  497.701251286808  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  492.749041093256  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  487.846106214441  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  482.991956352786  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  478.186106089262  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  473.428074834835  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  468.717386782416  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  464.053570859277  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  459.436160679934  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  454.864694499525  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  450.338715167621  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  445.857770082518  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  441.421411145971  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  437.029194718392  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  432.680681574476  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  428.375436859286  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  424.113030044765  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  419.893034886674  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  415.715029381986  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  411.578595726666  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  407.483320273902  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  403.428793492735  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  399.41460992711  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  395.440368155324  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  391.505670749889  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  387.610124237784  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  383.753339061112  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  379.934929538142  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  376.154513824739  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  372.411713876182  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  368.706155409357  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  365.037467865329  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  361.405284372286  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  357.809241708853  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  354.248980267766  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  350.724144019914  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  347.234380478734  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  343.779340664966  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  340.358679071749  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  336.972053630071  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  333.619125674568  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  330.299559909649  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  327.013024375971  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  323.759190417243  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  320.537732647356  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  317.34832891785  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  314.190660285694  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  311.064410981393  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  307.969268377411  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  304.904922956909  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  301.87106828279  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  298.867400967061  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  295.893620640484  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  292.94942992255  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  290.034534391735  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  287.148642556054  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  284.291465823921  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  281.46271847528  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  278.66211763304  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  275.889383234783  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  273.144238004757  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  270.426407426153  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  267.735619713647  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  265.071605786227  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  262.434099240279  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  259.822836322951  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  257.237555905775  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  254.677999458555  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  252.143911023513  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  249.635037189694  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  247.151127067624  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  244.69193226422  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  242.257206857954  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  239.846707374255  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  237.460192761167  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  235.097424365239  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  232.758165907662  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  230.442183460642  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  228.149245424004  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  225.879122502033  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  223.631587680546  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  221.406416204187  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  219.203385553955  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  217.022275424948  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  214.862867704336  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  212.724946449547  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  210.608297866674  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  208.512710289096  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  206.437974156308  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  204.383881992968  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  202.350228388148  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  200.336809974792  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  198.343425409381  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  196.369875351799  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  194.415962445393  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  192.481491297246  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  190.56626845863  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  188.670102405666  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  186.792803520168  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  184.934184070684  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  183.094058193718  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  181.272241875151  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  179.468552931832  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  177.682810993364  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  175.914837484065  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  174.164455605111  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  172.431490316854  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  170.715768321323  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  169.017118044887  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  167.335369621104  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  165.67035487373  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  164.021907299902  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  162.389862053489  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  160.774055928607  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  159.174327343297  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  157.590516323367  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  156.022464486395  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  154.470015025891  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  152.933012695615  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  151.411303794053  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  149.904736149047  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  148.413159102577  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  146.936423495695  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  145.47438165361  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  144.02688737092  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  142.593795896989  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  141.174963921477  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  139.770249560003  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  138.379512339961  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  137.002613186469  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  135.639414408465  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  134.289779684936  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  132.953574051283  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  131.63066388583  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  130.320916896459  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  129.024202107378  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  127.740389846029  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  126.469351730115  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  125.210960654765  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  123.965090779824  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  122.731617517265  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  121.510417518735  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  120.301368663216  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  119.104350044814  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  117.919241960671  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  116.74592589899  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  115.584284527188  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  114.434201680159  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  113.29556234866  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  112.168252667809  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  111.052159905699  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  109.947172452124  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  108.853179807416  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  107.7700725714  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  106.697742432451  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  105.63608215666  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  104.584985577114  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  103.544347583281  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  102.514064110494  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  101.494032129546  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  100.484149636389  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  99.4843156419338  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  98.4944301619463  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  97.514394207054  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  96.5441097728447  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  95.5834798300663  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  94.6324083149241  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  93.6908001194741  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  92.7585610821118  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  91.8355979781567  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  90.9218185105295  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  90.0171313005218  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  89.1214458786587  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  88.2346726756515  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  87.3567230134411  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  86.4875090963295  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  85.6269440022007  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  84.774941673828  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  83.9314169102688  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  83.0962853583438  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  82.2694635042017  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  81.4508686649681  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  80.6404189804771  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  79.8380334050846  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  79.0436316995645  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  78.2571344230842  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  77.4784629252608  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  76.7075393382956  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  75.9442865691873  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  75.1886282920231  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  74.4404889403455  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  73.6997936995958  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  72.9664684996329  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  72.2404400073254  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  71.5216356192192  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  70.8099834542765  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  70.1054123466879  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  69.4078518387552  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  68.7172321738465  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  68.0334842894197  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  67.3565398101166  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  66.6863310409252  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  66.0227909604099  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  65.3658532140099  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  64.715452107403  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  64.0715225999366  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  63.4340002981233  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  62.8028214492017  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  62.1779229347609  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  61.5592422644286  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  60.9467175696222  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  60.340287597362  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  59.7398917041452  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  59.1454698498823  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  58.5569625918924  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  57.9743110789593  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  57.3974570454462  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  56.8263428054691  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  56.2609112471279  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  55.7011058267956  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  55.1468705634638  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  54.5981500331442  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  54.0548893633266  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  53.5170342274912  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  52.9845308396762  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  52.4573259490991  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  51.9353668348315  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  51.4186013005269  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  50.9069776692014  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  50.4004447780655  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  49.8989519734079  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  49.4024491055302  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  48.9108865237319  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  48.4242150713452  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  47.9423860808193  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  47.4653513688535  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  46.9930632315793  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  46.5254744397892  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  46.0625382342145  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  45.6042083208487  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  45.1504388663187  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  44.7011844933009  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  44.2564002759834  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  43.816041735574  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  43.3800648358516  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  42.948425978763  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  42.5210820000628  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  42.0979901649969  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  41.6791081640293  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  41.2643941086108  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  40.8538065269903  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  40.4473043600674  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  40.0448469572867  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  39.6463940725726  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  39.2519058603045  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  38.8613428713325  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  38.4746660490321  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  38.091836725399  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  37.7128166171818  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  37.3375678220537  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  36.9660528148225  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  36.598234443678  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  36.2340759264765  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  35.8735408470628  loss:  68.2807585934771&quot;
## [1] &quot;step =  1  lambda =  35.5165931516285  loss:  76.708792599884&quot;
## [1] &quot;step =  2  lambda =  35.5165931516285  loss:  75.0931643736927&quot;
## [1] &quot;step =  3  lambda =  35.5165931516285  loss:  75.2727380991012&quot;
## [1] &quot;step =  1  lambda =  35.1631971451066  loss:  86.3592493895581&quot;
## [1] &quot;step =  2  lambda =  35.1631971451066  loss:  85.5044926668043&quot;
## [1] &quot;step =  3  lambda =  35.1631971451066  loss:  84.845151355332&quot;
## [1] &quot;step =  4  lambda =  35.1631971451066  loss:  84.9766078389182&quot;
## [1] &quot;step =  1  lambda =  34.813317487602  loss:  95.8974630203056&quot;
## [1] &quot;step =  2  lambda =  34.813317487602  loss:  94.958527922211&quot;
## [1] &quot;step =  3  lambda =  34.813317487602  loss:  94.4989121220407&quot;
## [1] &quot;step =  4  lambda =  34.813317487602  loss:  94.5791980606526&quot;
## [1] &quot;step =  1  lambda =  34.4669191908574  loss:  105.309947061648&quot;
## [1] &quot;step =  2  lambda =  34.4669191908574  loss:  104.350527535767&quot;
## [1] &quot;step =  3  lambda =  34.4669191908574  loss:  104.042442590386&quot;
## [1] &quot;step =  1  lambda =  34.1239676147544  loss:  114.598605593978&quot;
## [1] &quot;step =  2  lambda =  34.1239676147544  loss:  113.635575746707&quot;
## [1] &quot;step =  3  lambda =  34.1239676147544  loss:  113.489981857123&quot;
## [1] &quot;step =  1  lambda =  33.7844284638495  loss:  123.785503343304&quot;
## [1] &quot;step =  2  lambda =  33.7844284638495  loss:  122.937378107217&quot;
## [1] &quot;step =  3  lambda =  33.7844284638495  loss:  122.834083888445&quot;
## [1] &quot;step =  1  lambda =  33.4482677839449  loss:  132.868390636953&quot;
## [1] &quot;step =  2  lambda =  33.4482677839449  loss:  132.158504708096&quot;
## [1] &quot;step =  3  lambda =  33.4482677839449  loss:  132.095857176517&quot;
## [1] &quot;step =  1  lambda =  33.1154519586923  loss:  141.856938456739&quot;
## [1] &quot;step =  2  lambda =  33.1154519586923  loss:  141.316676194744&quot;
## [1] &quot;step =  3  lambda =  33.1154519586923  loss:  141.280576661701&quot;
## [1] &quot;step =  1  lambda =  32.7859477062319  loss:  150.758660785431&quot;
## [1] &quot;step =  2  lambda =  32.7859477062319  loss:  150.413613246993&quot;
## [1] &quot;step =  3  lambda =  32.7859477062319  loss:  150.395322530066&quot;
## [1] &quot;step =  1  lambda =  32.4597220758638  loss:  159.58172622536&quot;
## [1] &quot;step =  2  lambda =  32.4597220758638  loss:  159.452953399819&quot;
## [1] &quot;step =  3  lambda =  32.4597220758638  loss:  159.446977801745&quot;
## [1] &quot;step =  1  lambda =  32.1367424447532  loss:  168.334520762315&quot;
## [1] &quot;step =  2  lambda =  32.1367424447532  loss:  168.43834005886&quot;
## [1] &quot;step =  1  lambda =  31.8169765146677  loss:  177.032993041645&quot;
## [1] &quot;step =  2  lambda =  31.8169765146677  loss:  177.366254124246&quot;
## [1] &quot;step =  1  lambda =  31.500392308748  loss:  185.675187576313&quot;
## [1] &quot;step =  2  lambda =  31.500392308748  loss:  186.23748015795&quot;
## [1] &quot;step =  1  lambda =  31.1869581683094  loss:  194.272010449848&quot;
## [1] &quot;step =  2  lambda =  31.1869581683094  loss:  195.054932672703&quot;
## [1] &quot;step =  1  lambda =  30.876642749677  loss:  202.832077094216&quot;
## [1] &quot;step =  2  lambda =  30.876642749677  loss:  203.820279151741&quot;
## [1] &quot;step =  1  lambda =  30.5694150210502  loss:  211.363933940809&quot;
## [1] &quot;step =  2  lambda =  30.5694150210502  loss:  212.53441803315&quot;
## [1] &quot;step =  1  lambda =  30.2652442594001  loss:  219.876327822101&quot;
## [1] &quot;step =  2  lambda =  30.2652442594001  loss:  221.197748944165&quot;
## [1] &quot;step =  1  lambda =  29.964100047397  loss:  228.448768803819&quot;
## [1] &quot;step =  2  lambda =  29.964100047397  loss:  229.778944562231&quot;
## [1] &quot;step =  1  lambda =  29.6659522703689  loss:  237.18224814266&quot;
## [1] &quot;step =  2  lambda =  29.6659522703689  loss:  238.216930977584&quot;
## [1] &quot;step =  1  lambda =  29.3707711132895  loss:  245.975636484028&quot;
## [1] &quot;step =  2  lambda =  29.3707711132895  loss:  246.522162855888&quot;
## [1] &quot;step =  1  lambda =  29.0785270577971  loss:  254.82434932206&quot;
## [1] &quot;step =  2  lambda =  29.0785270577971  loss:  255.110126931538&quot;
## [1] &quot;step =  1  lambda =  28.7891908792427  loss:  263.734746106321&quot;
## [1] &quot;step =  2  lambda =  28.7891908792427  loss:  264.154011054412&quot;
## [1] &quot;step =  1  lambda =  28.5027336437673  loss:  272.719239296201&quot;
## [1] &quot;step =  2  lambda =  28.5027336437673  loss:  273.26196969558&quot;
## [1] &quot;step =  1  lambda =  28.2191267054086  loss:  281.794637598004&quot;
## [1] &quot;step =  2  lambda =  28.2191267054086  loss:  282.445776729616&quot;
## [1] &quot;step =  1  lambda =  27.9383417032365  loss:  290.981434943872&quot;
## [1] &quot;step =  2  lambda =  27.9383417032365  loss:  291.717540285754&quot;
## [1] &quot;step =  1  lambda =  27.6603505585168  loss:  300.303666698452&quot;
## [1] &quot;step =  2  lambda =  27.6603505585168  loss:  301.089623310659&quot;
## [1] &quot;step =  1  lambda =  27.3851254719032  loss:  309.814884760433&quot;
## [1] &quot;step =  2  lambda =  27.3851254719032  loss:  310.707249607741&quot;
## [1] &quot;step =  1  lambda =  27.1126389206579  loss:  319.53930548252&quot;
## [1] &quot;step =  2  lambda =  27.1126389206579  loss:  320.53285465765&quot;
## [1] &quot;step =  1  lambda =  26.8428636558986  loss:  329.50725667679&quot;
## [1] &quot;step =  2  lambda =  26.8428636558986  loss:  330.579175026985&quot;
## [1] &quot;step =  1  lambda =  26.575772699874  loss:  339.762746332158&quot;
## [1] &quot;step =  2  lambda =  26.575772699874  loss:  340.883257770312&quot;
## [1] &quot;step =  1  lambda =  26.3113393432659  loss:  350.358620193049&quot;
## [1] &quot;step =  2  lambda =  26.3113393432659  loss:  351.549916274582&quot;
## [1] &quot;step =  1  lambda =  26.0495371425183  loss:  361.358260016859&quot;
## [1] &quot;step =  2  lambda =  26.0495371425183  loss:  362.841198821267&quot;
## [1] &quot;step =  1  lambda =  25.7903399171931  loss:  372.838211870135&quot;
## [1] &quot;step =  2  lambda =  25.7903399171931  loss:  374.692236839089&quot;
## [1] &quot;step =  1  lambda =  25.5337217473515  loss:  384.891710602782&quot;
## [1] &quot;step =  2  lambda =  25.5337217473515  loss:  387.218881621411&quot;
## [1] &quot;step =  1  lambda =  25.2796569709629  loss:  397.633397799292&quot;
## [1] &quot;step =  2  lambda =  25.2796569709629  loss:  400.565389390041&quot;
## [1] &quot;step =  1  lambda =  25.0281201813378  loss:  411.205684517717&quot;
## [1] &quot;step =  2  lambda =  25.0281201813378  loss:  414.913294263807&quot;
## [1] &quot;step =  1  lambda =  24.7790862245877  loss:  425.78741972303&quot;
## [1] &quot;step =  2  lambda =  24.7790862245877  loss:  430.493619245299&quot;
## [1] &quot;step =  1  lambda =  24.5325301971094  loss:  441.605840043637&quot;
## [1] &quot;step =  2  lambda =  24.5325301971094  loss:  447.603894621794&quot;
## [1] &quot;step =  1  lambda =  24.2884274430945  loss:  458.953261659649&quot;
## [1] &quot;step =  2  lambda =  24.2884274430945  loss:  466.632216712174&quot;
## [1] &quot;step =  1  lambda =  24.0467535520645  loss:  478.210739739389&quot;
## [1] &quot;step =  2  lambda =  24.0467535520645  loss:  488.091810202068&quot;
## [1] &quot;step =  1  lambda =  23.8074843564287  loss:  499.882148803218&quot;
## [1] &quot;step =  2  lambda =  23.8074843564287  loss:  512.671567296763&quot;
## [1] &quot;step =  1  lambda =  23.5705959290681  loss:  524.644129008288&quot;
## [1] &quot;step =  2  lambda =  23.5705959290681  loss:  541.311291759016&quot;
## [1] &quot;step =  1  lambda =  23.3360645809427  loss:  553.420533659552&quot;
## [1] &quot;step =  2  lambda =  23.3360645809427  loss:  575.315339389319&quot;
## [1] &quot;step =  1  lambda =  23.1038668587222  loss:  587.494813405989&quot;
## [1] &quot;step =  2  lambda =  23.1038668587222  loss:  616.52460829311&quot;
## [1] &quot;step =  1  lambda =  22.8739795424408  loss:  628.679730375174&quot;
## [1] &quot;step =  2  lambda =  22.8739795424408  loss:  667.570202994105&quot;
## [1] &quot;step =  1  lambda =  22.6463796431754  loss:  680.666541740616&quot;
## [1] &quot;step =  2  lambda =  22.6463796431754  loss:  741.359273166642&quot;
## [1] &quot;step =  1  lambda =  22.4210444007463  loss:  759.704910586165&quot;
## [1] &quot;step =  2  lambda =  22.4210444007463  loss:  866.713522570431&quot;
## [1] &quot;step =  1  lambda =  22.1979512814416  loss:  888.090448269089&quot;
## [1] &quot;step =  1  lambda =  21.9770779757634  loss:  914.38019339309&quot;
## [1] &quot;step =  1  lambda =  21.7584023961971  loss:  939.693700456508&quot;
## [1] &quot;step =  1  lambda =  21.5419026750024  loss:  970.219347068708&quot;
## [1] &quot;step =  1  lambda =  21.3275571620269  loss:  1000.00905348306&quot;
## [1] &quot;step =  1  lambda =  21.1153444225406  loss:  1033.56287259043&quot;
## [1] &quot;step =  1  lambda =  20.9052432350928  loss:  1069.62065156462&quot;
## [1] &quot;step =  1  lambda =  20.6972325893895  loss:  1107.31822264476&quot;
## [1] &quot;step =  1  lambda =  20.4912916841929  loss:  1143.59582616298&quot;
## [1] &quot;step =  1  lambda =  20.2873999252409  loss:  1178.45127401995&quot;
## [1] &quot;step =  1  lambda =  20.0855369231877  loss:  1211.88419572074&quot;
## [1] &quot;step =  1  lambda =  19.8856824915647  loss:  1243.89601416444&quot;
## [1] &quot;step =  1  lambda =  19.6878166447624  loss:  1274.49001126517&quot;
## [1] &quot;step =  1  lambda =  19.4919195960311  loss:  1303.67145816859&quot;
## [1] &quot;step =  1  lambda =  19.2979717555028  loss:  1332.95704149279&quot;
## [1] &quot;step =  1  lambda =  19.1059537282317  loss:  1363.14090009706&quot;
## [1] &quot;step =  1  lambda =  18.915846312255  loss:  1391.99134336112&quot;
## [1] &quot;step =  1  lambda =  18.7276304966729  loss:  1424.42948452067&quot;
## [1] &quot;step =  1  lambda =  18.5412874597469  loss:  1454.84273054222&quot;
## [1] &quot;step =  1  lambda =  18.3567985670179  loss:  1483.28771735015&quot;
## [1] &quot;step =  1  lambda =  18.1741453694431  loss:  1512.13971450176&quot;
## [1] &quot;step =  1  lambda =  17.9933096015503  loss:  1540.11728281337&quot;
## [1] &quot;step =  1  lambda =  17.8142731796122  loss:  1565.82138423593&quot;
## [1] &quot;step =  1  lambda =  17.6370181998373  loss:  1589.35934074571&quot;
## [1] &quot;step =  1  lambda =  17.46152693658  loss:  1610.83674043983&quot;
## [1] &quot;step =  1  lambda =  17.2877818405676  loss:  1630.3575134211&quot;
## [1] &quot;step =  1  lambda =  17.1157655371459  loss:  1650.61234668885&quot;
## [1] &quot;step =  1  lambda =  16.945460824541  loss:  1669.969417893&quot;
## [1] &quot;step =  1  lambda =  16.7768506721399  loss:  1687.51313365929&quot;
## [1] &quot;step =  1  lambda =  16.6099182187867  loss:  1703.34782441852&quot;
## [1] &quot;step =  1  lambda =  16.4446467710971  loss:  1717.57453673456&quot;
## [1] &quot;step =  1  lambda =  16.2810198017884  loss:  1730.2905680691&quot;
## [1] &quot;step =  1  lambda =  16.1190209480276  loss:  1741.5891149949&quot;
## [1] &quot;step =  1  lambda =  15.958634009794  loss:  1751.55902970778&quot;
## [1] &quot;step =  1  lambda =  15.7998429482604  loss:  1760.28467444341&quot;
## [1] &quot;step =  1  lambda =  15.6426318841882  loss:  1767.8458608319&quot;
## [1] &quot;step =  1  lambda =  15.4869850963399  loss:  1774.91635511843&quot;
## [1] &quot;step =  1  lambda =  15.3328870199072  loss:  1781.14866860775&quot;
## [1] &quot;step =  1  lambda =  15.1803222449539  loss:  1786.38857376883&quot;
## [1] &quot;step =  1  lambda =  15.0292755148754  loss:  1790.70172003775&quot;
## [1] &quot;step =  1  lambda =  14.8797317248728  loss:  1794.1492380764&quot;
## [1] &quot;step =  1  lambda =  14.7316759204426  loss:  1796.78799548817&quot;
## [1] &quot;step =  1  lambda =  14.5850932958808  loss:  1798.6708570196&quot;
## [1] &quot;step =  1  lambda =  14.4399691928029  loss:  1799.84694384568&quot;
## [1] &quot;step =  1  lambda =  14.2962890986776  loss:  1800.36188775953&quot;
## [1] &quot;step =  1  lambda =  14.1540386453758  loss:  1800.25807713809&quot;
## [1] &quot;step =  1  lambda =  14.0132036077336  loss:  1799.57489244108&quot;
## [1] &quot;step =  1  lambda =  13.8737699021299  loss:  1799.98695439527&quot;
## [1] &quot;step =  1  lambda =  13.7357235850779  loss:  1800.06594707632&quot;
## [1] &quot;step =  1  lambda =  13.5990508518309  loss:  1799.68848294457&quot;
## [1] &quot;step =  1  lambda =  13.4637380350017  loss:  1798.87923012673&quot;
## [1] &quot;step =  1  lambda =  13.3297716031958  loss:  1797.66107766193&quot;
## [1] &quot;step =  1  lambda =  13.1971381596584  loss:  1796.05530547186&quot;
## [1] &quot;step =  1  lambda =  13.0658244409346  loss:  1794.08173750094&quot;
## [1] &quot;step =  1  lambda =  12.9358173155431  loss:  1791.75887906294&quot;
## [1] &quot;step =  1  lambda =  12.807103782663  loss:  1789.10403956897&quot;
## [1] &quot;step =  1  lambda =  12.6796709708339  loss:  1786.44127632564&quot;
## [1] &quot;step =  1  lambda =  12.5535061366682  loss:  1783.76443689977&quot;
## [1] &quot;step =  1  lambda =  12.4285966635775  loss:  1780.81982624437&quot;
## [1] &quot;step =  1  lambda =  12.3049300605104  loss:  1777.6179925535&quot;
## [1] &quot;step =  1  lambda =  12.1824939607035  loss:  1774.16897692265&quot;
## [1] &quot;step =  1  lambda =  12.0612761204447  loss:  1770.48235291303&quot;
## [1] &quot;step =  1  lambda =  11.9412644178491  loss:  1766.56726182449&quot;
## [1] &quot;step =  1  lambda =  11.8224468516464  loss:  1762.43244418965&quot;
## [1] &quot;step =  1  lambda =  11.7048115399809  loss:  1758.08626795154&quot;
## [1] &quot;step =  1  lambda =  11.5883467192234  loss:  1753.536753737&quot;
## [1] &quot;step =  1  lambda =  11.4730407427948  loss:  1748.79159759124&quot;
## [1] &quot;step =  1  lambda =  11.3588820800015  loss:  1743.85819149479&quot;
## [1] &quot;step =  1  lambda =  11.2458593148818  loss:  1738.74364194364&quot;
## [1] &quot;step =  1  lambda =  11.1339611450653  loss:  1733.45478683637&quot;
## [1] &quot;step =  1  lambda =  11.0231763806416  loss:  1727.99821087927&quot;
## [1] &quot;step =  1  lambda =  10.913493943042  loss:  1722.38025969107&quot;
## [1] &quot;step =  1  lambda =  10.8049028639313  loss:  1716.60705276304&quot;
## [1] &quot;step =  1  lambda =  10.6973922841111  loss:  1710.68449540796&quot;
## [1] &quot;step =  1  lambda =  10.5909514524338  loss:  1704.61828981178&quot;
## [1] &quot;step =  1  lambda =  10.4855697247276  loss:  1698.41394528535&quot;
## [1] &quot;step =  1  lambda =  10.3812365627318  loss:  1692.07678779907&quot;
## [1] &quot;step =  1  lambda =  10.2779415330434  loss:  1685.61196887154&quot;
## [1] &quot;step =  1  lambda =  10.1756743060733  loss:  1679.02447387282&quot;
## [1] &quot;step =  1  lambda =  10.0744246550136  loss:  1672.31912979453&quot;
## [1] &quot;step =  1  lambda =  9.97418245481473  loss:  1663.76261567691&quot;
## [1] &quot;step =  1  lambda =  9.87493768117319  loss:  1653.72835307597&quot;
## [1] &quot;step =  1  lambda =  9.77668040952892  loss:  1643.69362133654&quot;
## [1] &quot;step =  1  lambda =  9.67940081407284  loss:  1633.67170959643&quot;
## [1] &quot;step =  1  lambda =  9.58308916676438  loss:  1623.99164494014&quot;
## [1] &quot;step =  1  lambda =  9.48773583635853  loss:  1614.93364391569&quot;
## [1] &quot;step =  1  lambda =  9.39333128744278  loss:  1605.95842251908&quot;
## [1] &quot;step =  1  lambda =  9.29986607948359  loss:  1596.99870716414&quot;
## [1] &quot;step =  1  lambda =  9.20733086588226  loss:  1588.0190463557&quot;
## [1] &quot;step =  1  lambda =  9.11571639304031  loss:  1579.02141755603&quot;
## [1] &quot;step =  1  lambda =  9.02501349943413  loss:  1569.90527891929&quot;
## [1] &quot;step =  1  lambda =  8.93521311469874  loss:  1559.56449376428&quot;
## [1] &quot;step =  1  lambda =  8.84630625872088  loss:  1549.24595763592&quot;
## [1] &quot;step =  1  lambda =  8.75828404074083  loss:  1538.9506908591&quot;
## [1] &quot;step =  1  lambda =  8.67113765846346  loss:  1528.67964226658&quot;
## [1] &quot;step =  1  lambda =  8.5848583971779  loss:  1518.43369685051&quot;
## [1] &quot;step =  1  lambda =  8.49943762888613  loss:  1508.21368276909&quot;
## [1] &quot;step =  1  lambda =  8.41486681144014  loss:  1498.02037772818&quot;
## [1] &quot;step =  1  lambda =  8.3311374876877  loss:  1487.85451476199&quot;
## [1] &quot;step =  1  lambda =  8.24824128462666  loss:  1477.71678744033&quot;
## [1] &quot;step =  1  lambda =  8.16616991256765  loss:  1467.60785453277&quot;
## [1] &quot;step =  1  lambda =  8.08491516430506  loss:  1457.52834416166&quot;
## [1] &quot;step =  1  lambda =  8.00446891429635  loss:  1447.47885747775&quot;
## [1] &quot;step =  1  lambda =  7.92482311784949  loss:  1437.45997189229&quot;
## [1] &quot;step =  1  lambda =  7.84596981031845  loss:  1427.47224390016&quot;
## [1] &quot;step =  1  lambda =  7.76790110630678  loss:  1417.51621152792&quot;
## [1] &quot;step =  1  lambda =  7.69060919887901  loss:  1407.59239644013&quot;
## [1] &quot;step =  1  lambda =  7.61408635877998  loss:  1397.70130573632&quot;
## [1] &quot;step =  1  lambda =  7.53832493366192  loss:  1387.84343346951&quot;
## [1] &quot;step =  1  lambda =  7.46331734731919  loss:  1378.01926191578&quot;
## [1] &quot;step =  1  lambda =  7.38905609893065  loss:  1368.22926262272&quot;
## [1] &quot;step =  1  lambda =  7.31553376230957  loss:  1358.47389726281&quot;
## [1] &quot;step =  1  lambda =  7.24274298516102  loss:  1348.75361831583&quot;
## [1] &quot;step =  1  lambda =  7.17067648834662  loss:  1339.06886960266&quot;
## [1] &quot;step =  1  lambda =  7.09932706515664  loss:  1329.42008669088&quot;
## [1] &quot;step =  1  lambda =  7.0286875805893  loss:  1319.80769719069&quot;
## [1] &quot;step =  1  lambda =  6.95875097063727  loss:  1309.91063834517&quot;
## [1] &quot;step =  1  lambda =  6.88951024158129  loss:  1299.75111695369&quot;
## [1] &quot;step =  1  lambda =  6.82095846929075  loss:  1289.64702367412&quot;
## [1] &quot;step =  1  lambda =  6.75308879853129  loss:  1279.59840165374&quot;
## [1] &quot;step =  1  lambda =  6.68589444227927  loss:  1269.60529262602&quot;
## [1] &quot;step =  1  lambda =  6.61936868104308  loss:  1259.66773648984&quot;
## [1] &quot;step =  1  lambda =  6.55350486219115  loss:  1249.78577091055&quot;
## [1] &quot;step =  1  lambda =  6.48829639928672  loss:  1239.95943094567&quot;
## [1] &quot;step =  1  lambda =  6.42373677142913  loss:  1230.18874869714&quot;
## [1] &quot;step =  1  lambda =  6.35981952260183  loss:  1220.47375299096&quot;
## [1] &quot;step =  1  lambda =  6.29653826102666  loss:  1210.81446908491&quot;
## [1] &quot;step =  1  lambda =  6.23388665852472  loss:  1201.210918404&quot;
## [1] &quot;step =  1  lambda =  6.17185844988355  loss:  1191.66311830333&quot;
## [1] &quot;step =  1  lambda =  6.11044743223061  loss:  1182.17108185734&quot;
## [1] &quot;step =  1  lambda =  6.04964746441295  loss:  1172.73481767461&quot;
## [1] &quot;step =  1  lambda =  5.98945246638312  loss:  1163.35432973678&quot;
## [1] &quot;step =  1  lambda =  5.92985641859114  loss:  1154.02961726043&quot;
## [1] &quot;step =  1  lambda =  5.8708533613826  loss:  1144.7606745803&quot;
## [1] &quot;step =  1  lambda =  5.81243739440259  loss:  1135.54749105259&quot;
## [1] &quot;step =  1  lambda =  5.75460267600573  loss:  1126.3596783154&quot;
## [1] &quot;step =  1  lambda =  5.69734342267199  loss:  1116.84015893644&quot;
## [1] &quot;step =  1  lambda =  5.64065390842832  loss:  1107.38782628019&quot;
## [1] &quot;step =  1  lambda =  5.58452846427606  loss:  1098.00239534851&quot;
## [1] &quot;step =  1  lambda =  5.52896147762401  loss:  1088.68358088344&quot;
## [1] &quot;step =  1  lambda =  5.47394739172721  loss:  1079.43109740257&quot;
## [1] &quot;step =  1  lambda =  5.4194807051312  loss:  1070.24465922198&quot;
## [1] &quot;step =  1  lambda =  5.36555597112197  loss:  1061.11619051777&quot;
## [1] &quot;step =  1  lambda =  5.31216779718117  loss:  1052.04697374633&quot;
## [1] &quot;step =  1  lambda =  5.2593108444469  loss:  1043.04295925863&quot;
## [1] &quot;step =  1  lambda =  5.20697982717985  loss:  1034.10386058909&quot;
## [1] &quot;step =  1  lambda =  5.15516951223468  loss:  1025.22939215297&quot;
## [1] &quot;step =  1  lambda =  5.10387471853673  loss:  1016.41926911607&quot;
## [1] &quot;step =  1  lambda =  5.05309031656387  loss:  1007.67320726676&quot;
## [1] &quot;step =  1  lambda =  5.00281122783359  loss:  998.990922891187&quot;
## [1] &quot;step =  1  lambda =  4.95303242439511  loss:  990.372132652489&quot;
## [1] &quot;step =  1  lambda =  4.90374892832662  loss:  981.816553474613&quot;
## [1] &quot;step =  1  lambda =  4.85495581123743  loss:  973.323902431216&quot;
## [1] &quot;step =  1  lambda =  4.80664819377518  loss:  964.893896640085&quot;
## [1] &quot;step =  1  lambda =  4.75882124513786  loss:  956.526253163377&quot;
## [1] &quot;step =  1  lambda =  4.71147018259074  loss:  948.166410372733&quot;
## [1] &quot;step =  1  lambda =  4.66459027098813  loss:  939.793134197258&quot;
## [1] &quot;step =  1  lambda =  4.61817682229978  loss:  931.485326953523&quot;
## [1] &quot;step =  1  lambda =  4.57222519514216  loss:  923.242621631347&quot;
## [1] &quot;step =  1  lambda =  4.52673079431425  loss:  915.064651713922&quot;
## [1] &quot;step =  1  lambda =  4.48168907033806  loss:  906.951051134148&quot;
## [1] &quot;step =  1  lambda =  4.43709551900367  loss:  898.901454236305&quot;
## [1] &quot;step =  1  lambda =  4.39294568091876  loss:  890.915495742827&quot;
## [1] &quot;step =  1  lambda =  4.34923514106274  loss:  882.992810725931&quot;
## [1] &quot;step =  1  lambda =  4.30595952834521  loss:  875.133034583821&quot;
## [1] &quot;step =  1  lambda =  4.26311451516882  loss:  867.335803021247&quot;
## [1] &quot;step =  1  lambda =  4.22069581699655  loss:  859.600752034138&quot;
## [1] &quot;step =  1  lambda =  4.17869919192325  loss:  851.927517898099&quot;
## [1] &quot;step =  1  lambda =  4.13712044025139  loss:  844.315737160493&quot;
## [1] &quot;step =  1  lambda =  4.09595540407118  loss:  836.76504663591&quot;
## [1] &quot;step =  1  lambda =  4.05519996684468  loss:  829.275083404782&quot;
## [1] &quot;step =  1  lambda =  4.0148500529942  loss:  821.845484814919&quot;
## [1] &quot;step =  1  lambda =  3.97490162749475  loss:  814.475888485769&quot;
## [1] &quot;step =  1  lambda =  3.93535069547048  loss:  807.165932315187&quot;
## [1] &quot;step =  1  lambda =  3.89619330179521  loss:  799.91525448852&quot;
## [1] &quot;step =  1  lambda =  3.85742553069697  loss:  792.723493489841&quot;
## [1] &quot;step =  1  lambda =  3.81904350536634  loss:  785.590288115107&quot;
## [1] &quot;step =  1  lambda =  3.78104338756878  loss:  778.515277487133&quot;
## [1] &quot;step =  1  lambda =  3.74342137726086  loss:  771.498101072168&quot;
## [1] &quot;step =  1  lambda =  3.7061737122102  loss:  764.538398697945&quot;
## [1] &quot;step =  1  lambda =  3.66929666761925  loss:  757.635810573063&quot;
## [1] &quot;step =  1  lambda =  3.63278655575281  loss:  750.789977307546&quot;
## [1] &quot;step =  1  lambda =  3.59663972556928  loss:  743.9464503279&quot;
## [1] &quot;step =  1  lambda =  3.56085256235552  loss:  736.949665723689&quot;
## [1] &quot;step =  1  lambda =  3.52542148736538  loss:  730.016375754138&quot;
## [1] &quot;step =  1  lambda =  3.49034295746184  loss:  723.146043706249&quot;
## [1] &quot;step =  1  lambda =  3.45561346476268  loss:  716.338137023218&quot;
## [1] &quot;step =  1  lambda =  3.42122953628968  loss:  709.592127268559&quot;
## [1] &quot;step =  1  lambda =  3.38718773362134  loss:  702.907490090942&quot;
## [1] &quot;step =  1  lambda =  3.35348465254903  loss:  696.28370518973&quot;
## [1] &quot;step =  1  lambda =  3.32011692273655  loss:  689.720256281173&quot;
## [1] &quot;step =  1  lambda =  3.28708120738312  loss:  683.216631065236&quot;
## [1] &quot;step =  1  lambda =  3.25437420288967  loss:  676.772321193048&quot;
## [1] &quot;step =  1  lambda =  3.2219926385285  loss:  670.386822234916&quot;
## [1] &quot;step =  1  lambda =  3.18993327611618  loss:  664.059633648917&quot;
## [1] &quot;step =  1  lambda =  3.15819290968977  loss:  657.790258750029&quot;
## [1] &quot;step =  1  lambda =  3.12676836518616  loss:  651.578204679769&quot;
## [1] &quot;step =  1  lambda =  3.09565650012471  loss:  645.422982376343&quot;
## [1] &quot;step =  1  lambda =  3.06485420329301  loss:  639.32410654527&quot;
## [1] &quot;step =  1  lambda =  3.03435839443567  loss:  633.281095630467&quot;
## [1] &quot;step =  1  lambda =  3.00416602394643  loss:  627.293471785788&quot;
## [1] &quot;step =  1  lambda =  2.97427407256306  loss:  621.360760846974&quot;
## [1] &quot;step =  1  lambda =  2.94467955106552  loss:  615.482492304037&quot;
## [1] &quot;step =  1  lambda =  2.915379499977  loss:  609.658199274022&quot;
## [1] &quot;step =  1  lambda =  2.88637098926796  loss:  603.887418474175&quot;
## [1] &quot;step =  1  lambda =  2.85765111806317  loss:  598.169690195458&quot;
## [1] &quot;step =  1  lambda =  2.82921701435156  loss:  592.504558276441&quot;
## [1] &quot;step =  1  lambda =  2.80106583469908  loss:  586.891570077528&quot;
## [1] &quot;step =  1  lambda =  2.7731947639643  loss:  581.330276455522&quot;
## [1] &quot;step =  1  lambda =  2.74560101501692  loss:  575.820231738507&quot;
## [1] &quot;step =  1  lambda =  2.71828182845905  loss:  570.360993701049&quot;
## [1] &quot;step =  1  lambda =  2.69123447234926  loss:  564.952123539686&quot;
## [1] &quot;step =  1  lambda =  2.66445624192942  loss:  559.593185848726&quot;
## [1] &quot;step =  1  lambda =  2.63794445935415  loss:  554.28374859631&quot;
## [1] &quot;step =  1  lambda =  2.61169647342312  loss:  549.023383100754&quot;
## [1] &quot;step =  1  lambda =  2.58570965931585  loss:  543.81166400716&quot;
## [1] &quot;step =  1  lambda =  2.55998141832927  loss:  538.648169264274&quot;
## [1] &quot;step =  1  lambda =  2.53450917761785  loss:  533.532480101596&quot;
## [1] &quot;step =  1  lambda =  2.5092903899363  loss:  528.464181006724&quot;
## [1] &quot;step =  1  lambda =  2.48432253338482  loss:  523.442859702939&quot;
## [1] &quot;step =  1  lambda =  2.45960311115695  loss:  518.468107127007&quot;
## [1] &quot;step =  1  lambda =  2.43512965128988  loss:  513.539517407202&quot;
## [1] &quot;step =  1  lambda =  2.41089970641721  loss:  508.656687841541&quot;
## [1] &quot;step =  1  lambda =  2.38691085352428  loss:  503.819218876222&quot;
## [1] &quot;step =  1  lambda =  2.36316069370579  loss:  499.026714084263&quot;
## [1] &quot;step =  1  lambda =  2.33964685192599  loss:  494.278780144337&quot;
## [1] &quot;step =  1  lambda =  2.31636697678109  loss:  489.556741727635&quot;
## [1] &quot;step =  1  lambda =  2.29331874026418  loss:  484.856036498208&quot;
## [1] &quot;step =  1  lambda =  2.27049983753241  loss:  480.20009942147&quot;
## [1] &quot;step =  1  lambda =  2.24790798667647  loss:  475.588512406408&quot;
## [1] &quot;step =  1  lambda =  2.22554092849247  loss:  471.02086133174&quot;
## [1] &quot;step =  1  lambda =  2.20339642625594  loss:  466.496736000476&quot;
## [1] &quot;step =  1  lambda =  2.1814722654982  loss:  462.015730095305&quot;
## [1] &quot;step =  1  lambda =  2.15976625378491  loss:  457.577441134785&quot;
## [1] &quot;step =  1  lambda =  2.13827622049682  loss:  453.181470430306&quot;
## [1] &quot;step =  1  lambda =  2.11700001661267  loss:  448.82742304382&quot;
## [1] &quot;step =  1  lambda =  2.09593551449437  loss:  444.514907746307&quot;
## [1] &quot;step =  1  lambda =  2.07508060767412  loss:  440.243536976964&quot;
## [1] &quot;step =  1  lambda =  2.05443321064389  loss:  436.012926803093&quot;
## [1] &quot;step =  1  lambda =  2.03399125864675  loss:  431.822696880681&quot;
## [1] &quot;step =  1  lambda =  2.01375270747048  loss:  427.672470415635&quot;
## [1] &quot;step =  1  lambda =  1.99371553324308  loss:  423.56187412568&quot;
## [1] &quot;step =  1  lambda =  1.97387773223045  loss:  419.490538202883&quot;
## [1] &quot;step =  1  lambda =  1.95423732063594  loss:  415.458096276791&quot;
## [1] &quot;step =  1  lambda =  1.93479233440203  loss:  411.464185378179&quot;
## [1] &quot;step =  1  lambda =  1.9155408290139  loss:  407.508445903376&quot;
## [1] &quot;step =  1  lambda =  1.89648087930495  loss:  403.590521579169&quot;
## [1] &quot;step =  1  lambda =  1.87761057926434  loss:  399.710059428255&quot;
## [1] &quot;step =  1  lambda =  1.85892804184634  loss:  395.866709735249&quot;
## [1] &quot;step =  1  lambda =  1.84043139878164  loss:  392.060126013204&quot;
## [1] &quot;step =  1  lambda =  1.82211880039051  loss:  388.289964970666&quot;
## [1] &quot;step =  1  lambda =  1.80398841539786  loss:  384.555886479213&quot;
## [1] &quot;step =  1  lambda =  1.78603843075007  loss:  380.8575535415&quot;
## [1] &quot;step =  1  lambda =  1.76826705143374  loss:  377.194632259771&quot;
## [1] &quot;step =  1  lambda =  1.7506725002961  loss:  373.566791804848&quot;
## [1] &quot;step =  1  lambda =  1.7332530178674  loss:  369.973704385566&quot;
## [1] &quot;step =  1  lambda =  1.71600686218486  loss:  366.415045218658&quot;
## [1] &quot;step =  1  lambda =  1.69893230861855  loss:  362.890492499067&quot;
## [1] &quot;step =  1  lambda =  1.68202764969889  loss:  359.39972737069&quot;
## [1] &quot;step =  1  lambda =  1.66529119494589  loss:  355.942433897528&quot;
## [1] &quot;step =  1  lambda =  1.64872127070013  loss:  352.518299035235&quot;
## [1] &quot;step =  1  lambda =  1.63231621995538  loss:  349.127012603074&quot;
## [1] &quot;step =  1  lambda =  1.61607440219289  loss:  345.76826725624&quot;
## [1] &quot;step =  1  lambda =  1.59999419321736  loss:  342.44175845857&quot;
## [1] &quot;step =  1  lambda =  1.58407398499448  loss:  339.147184455614&quot;
## [1] &quot;step =  1  lambda =  1.56831218549017  loss:  335.884246248063&quot;
## [1] &quot;step =  1  lambda =  1.55270721851134  loss:  332.652647565529&quot;
## [1] &quot;step =  1  lambda =  1.53725752354828  loss:  329.452094840667&quot;
## [1] &quot;step =  1  lambda =  1.52196155561863  loss:  326.282297183621&quot;
## [1] &quot;step =  1  lambda =  1.50681778511285  loss:  323.142966356811&quot;
## [1] &quot;step =  1  lambda =  1.49182469764127  loss:  320.033816750027&quot;
## [1] &quot;step =  1  lambda =  1.47698079388264  loss:  316.954565355839&quot;
## [1] &quot;step =  1  lambda =  1.46228458943423  loss:  313.904931745312&quot;
## [1] &quot;step =  1  lambda =  1.44773461466333  loss:  310.884638044016&quot;
## [1] &quot;step =  1  lambda =  1.43332941456034  loss:  307.89340890833&quot;
## [1] &quot;step =  1  lambda =  1.41906754859326  loss:  304.930971502036&quot;
## [1] &quot;step =  1  lambda =  1.40494759056359  loss:  301.997055473177&quot;
## [1] &quot;step =  1  lambda =  1.39096812846378  loss:  299.091392931206&quot;
## [1] &quot;step =  1  lambda =  1.37712776433596  loss:  296.213718424392&quot;
## [1] &quot;step =  1  lambda =  1.36342511413218  loss:  293.363768917489&quot;
## [1] &quot;step =  1  lambda =  1.349858807576  loss:  290.541283769666&quot;
## [1] &quot;step =  1  lambda =  1.33642748802547  loss:  287.746004712682&quot;
## [1] &quot;step =  1  lambda =  1.32312981233744  loss:  284.977675829312&quot;
## [1] &quot;step =  1  lambda =  1.30996445073325  loss:  282.236043532015&quot;
## [1] &quot;step =  1  lambda =  1.29693008666577  loss:  279.520856541831&quot;
## [1] &quot;step =  1  lambda =  1.28402541668774  loss:  276.83186586752&quot;
## [1] &quot;step =  1  lambda =  1.2712491503214  loss:  274.168824784919&quot;
## [1] &quot;step =  1  lambda =  1.25860000992948  loss:  271.53148881653&quot;
## [1] &quot;step =  1  lambda =  1.24607673058738  loss:  268.919615711322&quot;
## [1] &quot;step =  1  lambda =  1.23367805995674  loss:  266.332965424748&quot;
## [1] &quot;step =  1  lambda =  1.22140275816017  loss:  263.771300098975&quot;
## [1] &quot;step =  1  lambda =  1.20924959765725  loss:  261.234362646778&quot;
## [1] &quot;step =  1  lambda =  1.19721736312181  loss:  258.720119022703&quot;
## [1] &quot;step =  1  lambda =  1.18530485132037  loss:  256.230213154101&quot;
## [1] &quot;step =  1  lambda =  1.17351087099181  loss:  253.764414560073&quot;
## [1] &quot;step =  1  lambda =  1.16183424272828  loss:  251.3224948727&quot;
## [1] &quot;step =  1  lambda =  1.15027379885723  loss:  248.904227818139&quot;
## [1] &quot;step =  1  lambda =  1.13882838332462  loss:  246.509389197905&quot;
## [1] &quot;step =  1  lambda =  1.12749685157938  loss:  244.137756870337&quot;
## [1] &quot;step =  1  lambda =  1.11627807045887  loss:  241.789110732243&quot;
## [1] &quot;step =  1  lambda =  1.10517091807565  loss:  239.463232700726&quot;
## [1] &quot;step =  1  lambda =  1.09417428370521  loss:  237.159906695182&quot;
## [1] &quot;step =  1  lambda =  1.08328706767496  loss:  234.878918619475&quot;
## [1] &quot;step =  1  lambda =  1.07250818125422  loss:  232.620056344275&quot;
## [1] &quot;step =  1  lambda =  1.06183654654536  loss:  230.38310968957&quot;
## [1] &quot;step =  1  lambda =  1.05127109637602  loss:  228.167870407343&quot;
## [1] &quot;step =  1  lambda =  1.04081077419239  loss:  225.974132164408&quot;
## [1] &quot;step =  1  lambda =  1.03045453395352  loss:  223.801690525409&quot;
## [1] &quot;step =  1  lambda =  1.02020134002676  loss:  221.650342935976&quot;
## [1] &quot;step =  1  lambda =  1.01005016708417  loss:  219.519888706043&quot;
## [1] &quot;step =  1  lambda =  1  loss:  217.410128993311&quot;
## [1] &quot;step =  1  lambda =  0.990049833749168  loss:  215.320866786866&quot;
## [1] &quot;step =  1  lambda =  0.980198673306756  loss:  213.251906890956&quot;
## [1] &quot;step =  1  lambda =  0.970445533548509  loss:  211.2030559089&quot;
## [1] &quot;step =  1  lambda =  0.960789439152324  loss:  209.174122227159&quot;
## [1] &quot;step =  1  lambda =  0.951229424500715  loss:  207.164915999537&quot;
## [1] &quot;step =  1  lambda =  0.941764533584248  loss:  205.175249131539&quot;
## [1] &quot;step =  1  lambda =  0.932393819905948  loss:  203.204935264857&quot;
## [1] &quot;step =  1  lambda =  0.923116346386636  loss:  201.253789761999&quot;
## [1] &quot;step =  1  lambda =  0.913931185271228  loss:  199.320660611239&quot;
## [1] &quot;step =  1  lambda =  0.90483741803596  loss:  197.404728299534&quot;
## [1] &quot;step =  1  lambda =  0.895834135296529  loss:  195.50750510891&quot;
## [1] &quot;step =  1  lambda =  0.886920436717158  loss:  193.628810853599&quot;
## [1] &quot;step =  1  lambda =  0.878095430920562  loss:  191.768467053497&quot;
## [1] &quot;step =  1  lambda =  0.869358235398805  loss:  189.926296918002&quot;
## [1] &quot;step =  1  lambda =  0.860707976425057  loss:  188.102125330025&quot;
## [1] &quot;step =  1  lambda =  0.852143788966211  loss:  186.295778830171&quot;
## [1] &quot;step =  1  lambda =  0.843664816596384  loss:  184.50708560109&quot;
## [1] &quot;step =  1  lambda =  0.835270211411272  loss:  182.735875451981&quot;
## [1] &quot;step =  1  lambda =  0.826959133943363  loss:  180.981979803268&quot;
## [1] &quot;step =  1  lambda =  0.818730753077982  loss:  179.24523167143&quot;
## [1] &quot;step =  1  lambda =  0.810584245970188  loss:  177.525465653984&quot;
## [1] &quot;step =  1  lambda =  0.802518797962478  loss:  175.822517914631&quot;
## [1] &quot;step =  1  lambda =  0.794533602503334  loss:  174.136226168544&quot;
## [1] &quot;step =  1  lambda =  0.786627861066553  loss:  172.466429667811&quot;
## [1] &quot;step =  1  lambda =  0.778800783071405  loss:  170.812969187026&quot;
## [1] &quot;step =  1  lambda =  0.771051585803566  loss:  169.17568700902&quot;
## [1] &quot;step =  1  lambda =  0.763379494336853  loss:  167.554426910742&quot;
## [1] &quot;step =  1  lambda =  0.755783741455726  loss:  165.949034149277&quot;
## [1] &quot;step =  1  lambda =  0.748263567578566  loss:  164.359355448004&quot;
## [1] &quot;step =  1  lambda =  0.740818220681719  loss:  162.785238982892&quot;
## [1] &quot;step =  1  lambda =  0.733446956224289  loss:  161.226534368932&quot;
## [1] &quot;step =  1  lambda =  0.726149037073691  loss:  159.683092646701&quot;
## [1] &quot;step =  1  lambda =  0.718923733431926  loss:  158.154766269059&quot;
## [1] &quot;step =  1  lambda =  0.71177032276261  loss:  156.641409087976&quot;
## [1] &quot;step =  1  lambda =  0.704688089718714  loss:  155.14287634149&quot;
## [1] &quot;step =  1  lambda =  0.697676326071031  loss:  153.659024640786&quot;
## [1] &quot;step =  2  lambda =  0.697676326071031  loss:  171.301222039934&quot;
## [1] &quot;step =  1  lambda =  0.690734330637355  loss:  169.647860810204&quot;
## [1] &quot;step =  2  lambda =  0.690734330637355  loss:  178.970697528262&quot;
## [1] &quot;step =  1  lambda =  0.683861409212356  loss:  177.230696316816&quot;
## [1] &quot;step =  2  lambda =  0.683861409212356  loss:  183.169531032701&quot;
## [1] &quot;step =  1  lambda =  0.677056874498164  loss:  181.383741000998&quot;
## [1] &quot;step =  2  lambda =  0.677056874498164  loss:  186.075100722181&quot;
## [1] &quot;step =  1  lambda =  0.670320046035639  loss:  184.258949759442&quot;
## [1] &quot;step =  2  lambda =  0.670320046035639  loss:  190.240955489946&quot;
## [1] &quot;step =  1  lambda =  0.663650250136319  loss:  188.374502991303&quot;
## [1] &quot;step =  2  lambda =  0.663650250136319  loss:  194.714441073256&quot;
## [1] &quot;step =  1  lambda =  0.657046819815057  loss:  192.808396680118&quot;
## [1] &quot;step =  2  lambda =  0.657046819815057  loss:  199.419125920533&quot;
## [1] &quot;step =  1  lambda =  0.650509094723317  loss:  197.464110390672&quot;
## [1] &quot;step =  2  lambda =  0.650509094723317  loss:  204.574754637958&quot;
## [1] &quot;step =  1  lambda =  0.644036421083142  loss:  202.566897004911&quot;
## [1] &quot;step =  2  lambda =  0.644036421083142  loss:  209.42089803412&quot;
## [1] &quot;step =  1  lambda =  0.637628151621774  loss:  207.364165042285&quot;
## [1] &quot;step =  2  lambda =  0.637628151621774  loss:  213.905220013097&quot;
## [1] &quot;step =  1  lambda =  0.631283645506927  loss:  211.803287148139&quot;
## [1] &quot;step =  2  lambda =  0.631283645506927  loss:  217.915989457238&quot;
## [1] &quot;step =  1  lambda =  0.6250022682827  loss:  215.779219134328&quot;
## [1] &quot;step =  2  lambda =  0.6250022682827  loss:  221.579779478107&quot;
## [1] &quot;step =  1  lambda =  0.618783391806141  loss:  219.406155678549&quot;
## [1] &quot;step =  2  lambda =  0.618783391806141  loss:  225.068133261825&quot;
## [1] &quot;step =  1  lambda =  0.612626394184416  loss:  222.859397605072&quot;
## [1] &quot;step =  2  lambda =  0.612626394184416  loss:  228.409058398557&quot;
## [1] &quot;step =  1  lambda =  0.606530659712633  loss:  226.166684314327&quot;
## [1] &quot;step =  2  lambda =  0.606530659712633  loss:  231.597047489626&quot;
## [1] &quot;step =  1  lambda =  0.600495578812266  loss:  229.322559834471&quot;
## [1] &quot;step =  2  lambda =  0.600495578812266  loss:  234.632345187161&quot;
## [1] &quot;step =  1  lambda =  0.594520547970195  loss:  232.327266066357&quot;
## [1] &quot;step =  2  lambda =  0.594520547970195  loss:  237.517341198037&quot;
## [1] &quot;step =  1  lambda =  0.588604969678356  loss:  235.183168857753&quot;
## [1] &quot;step =  2  lambda =  0.588604969678356  loss:  240.254958094167&quot;
## [1] &quot;step =  1  lambda =  0.58274825237399  loss:  237.893161541393&quot;
## [1] &quot;step =  2  lambda =  0.58274825237399  loss:  242.848308682089&quot;
## [1] &quot;step =  1  lambda =  0.576949810380487  loss:  240.460325793473&quot;
## [1] &quot;step =  2  lambda =  0.576949810380487  loss:  245.30065843842&quot;
## [1] &quot;step =  1  lambda =  0.571209063848815  loss:  242.887894536859&quot;
## [1] &quot;step =  2  lambda =  0.571209063848815  loss:  247.615466661522&quot;
## [1] &quot;step =  1  lambda =  0.565525438699537  loss:  245.179292731024&quot;
## [1] &quot;step =  2  lambda =  0.565525438699537  loss:  249.796431143491&quot;
## [1] &quot;step =  1  lambda =  0.559898366565402  loss:  247.338181602195&quot;
## [1] &quot;step =  2  lambda =  0.559898366565402  loss:  251.847507229185&quot;
## [1] &quot;step =  1  lambda =  0.554327284734507  loss:  249.368477485914&quot;
## [1] &quot;step =  2  lambda =  0.554327284734507  loss:  253.768344387021&quot;
## [1] &quot;step =  1  lambda =  0.548811636094027  loss:  251.263758601475&quot;
## [1] &quot;step =  2  lambda =  0.548811636094027  loss:  255.535619822673&quot;
## [1] &quot;step =  1  lambda =  0.5433508690745  loss:  253.013102548623&quot;
## [1] &quot;step =  2  lambda =  0.5433508690745  loss:  257.14737908598&quot;
## [1] &quot;step =  1  lambda =  0.537944437594675  loss:  254.610548775016&quot;
## [1] &quot;step =  2  lambda =  0.537944437594675  loss:  258.576520092494&quot;
## [1] &quot;step =  1  lambda =  0.532591801006898  loss:  256.02507752671&quot;
## [1] &quot;step =  2  lambda =  0.532591801006898  loss:  259.954397091808&quot;
## [1] &quot;step =  1  lambda =  0.527292424043048  loss:  257.388847019388&quot;
## [1] &quot;step =  2  lambda =  0.527292424043048  loss:  261.221751570739&quot;
## [1] &quot;step =  1  lambda =  0.522045776761016  loss:  258.643205220448&quot;
## [1] &quot;step =  2  lambda =  0.522045776761016  loss:  262.373673949813&quot;
## [1] &quot;step =  1  lambda =  0.516851334491699  loss:  259.783290857379&quot;
## [1] &quot;step =  2  lambda =  0.516851334491699  loss:  263.41412045297&quot;
## [1] &quot;step =  1  lambda =  0.511708577786543  loss:  260.813021255766&quot;
## [1] &quot;step =  2  lambda =  0.511708577786543  loss:  264.348299900332&quot;
## [1] &quot;step =  1  lambda =  0.50661699236559  loss:  261.737553687378&quot;
## [1] &quot;step =  2  lambda =  0.50661699236559  loss:  265.181680484541&quot;
## [1] &quot;step =  1  lambda =  0.501576069066056  loss:  262.562301983804&quot;
## [1] &quot;step =  2  lambda =  0.501576069066056  loss:  265.919652858258&quot;
## [1] &quot;step =  1  lambda =  0.49658530379141  loss:  263.292603036755&quot;
## [1] &quot;step =  2  lambda =  0.49658530379141  loss:  266.567385521927&quot;
## [1] &quot;step =  1  lambda =  0.491644197460966  loss:  263.933573684834&quot;
## [1] &quot;step =  2  lambda =  0.491644197460966  loss:  267.129774229327&quot;
## [1] &quot;step =  1  lambda =  0.486752255959971  loss:  264.490060667318&quot;
## [1] &quot;step =  2  lambda =  0.486752255959971  loss:  267.611433251643&quot;
## [1] &quot;step =  1  lambda =  0.481908990090202  loss:  264.966632004888&quot;
## [1] &quot;step =  2  lambda =  0.481908990090202  loss:  268.078013431478&quot;
## [1] &quot;step =  1  lambda =  0.477113915521034  loss:  265.429919066422&quot;
## [1] &quot;step =  2  lambda =  0.477113915521034  loss:  268.466668458042&quot;
## [1] &quot;step =  1  lambda =  0.472366552741015  loss:  265.814413782945&quot;
## [1] &quot;step =  2  lambda =  0.472366552741015  loss:  268.759725249076&quot;
## [1] &quot;step =  1  lambda =  0.467666427009909  loss:  266.104281253268&quot;
## [1] &quot;step =  2  lambda =  0.467666427009909  loss:  268.992073802722&quot;
## [1] &quot;step =  1  lambda =  0.463013068311228  loss:  266.334050141968&quot;
## [1] &quot;step =  2  lambda =  0.463013068311228  loss:  269.169205511103&quot;
## [1] &quot;step =  1  lambda =  0.458406011305224  loss:  266.509158744479&quot;
## [1] &quot;step =  2  lambda =  0.458406011305224  loss:  269.292784226799&quot;
## [1] &quot;step =  1  lambda =  0.453844795282356  loss:  266.631254441446&quot;
## [1] &quot;step =  2  lambda =  0.453844795282356  loss:  269.364551418894&quot;
## [1] &quot;step =  1  lambda =  0.449328964117222  loss:  266.70206122831&quot;
## [1] &quot;step =  2  lambda =  0.449328964117222  loss:  269.386435456925&quot;
## [1] &quot;step =  1  lambda =  0.444858066222941  loss:  266.723488081514&quot;
## [1] &quot;step =  2  lambda =  0.444858066222941  loss:  269.360437383174&quot;
## [1] &quot;step =  1  lambda =  0.440431654505999  loss:  266.697515911782&quot;
## [1] &quot;step =  2  lambda =  0.440431654505999  loss:  269.288563649038&quot;
## [1] &quot;step =  1  lambda =  0.436049286321536  loss:  266.626130985357&quot;
## [1] &quot;step =  2  lambda =  0.436049286321536  loss:  269.172790791214&quot;
## [1] &quot;step =  1  lambda =  0.43171052342908  loss:  266.511289956404&quot;
## [1] &quot;step =  2  lambda =  0.43171052342908  loss:  269.015046682842&quot;
## [1] &quot;step =  1  lambda =  0.427414931948727  loss:  266.354901305099&quot;
## [1] &quot;step =  2  lambda =  0.427414931948727  loss:  268.817200808433&quot;
## [1] &quot;step =  1  lambda =  0.423162082317749  loss:  266.1588157083&quot;
## [1] &quot;step =  2  lambda =  0.423162082317749  loss:  268.581059597112&quot;
## [1] &quot;step =  1  lambda =  0.418951549247639  loss:  265.924821418025&quot;
## [1] &quot;step =  2  lambda =  0.418951549247639  loss:  268.308364591008&quot;
## [1] &quot;step =  1  lambda =  0.414782911681582  loss:  265.654642447126&quot;
## [1] &quot;step =  2  lambda =  0.414782911681582  loss:  268.000792172974&quot;
## [1] &quot;step =  1  lambda =  0.410655752752345  loss:  265.349938299178&quot;
## [1] &quot;step =  2  lambda =  0.410655752752345  loss:  267.659954121273&quot;
## [1] &quot;step =  1  lambda =  0.406569659740599  loss:  265.012304517469&quot;
## [1] &quot;step =  2  lambda =  0.406569659740599  loss:  267.030411953588&quot;
## [1] &quot;step =  1  lambda =  0.402524224033636  loss:  264.389611215868&quot;
## [1] &quot;step =  2  lambda =  0.402524224033636  loss:  266.518436344895&quot;
## [1] &quot;step =  1  lambda =  0.398519041084514  loss:  263.882864306867&quot;
## [1] &quot;step =  2  lambda =  0.398519041084514  loss:  265.996501491214&quot;
## [1] &quot;step =  1  lambda =  0.394553710371601  loss:  263.365074804703&quot;
## [1] &quot;step =  2  lambda =  0.394553710371601  loss:  265.447595962661&quot;
## [1] &quot;step =  1  lambda =  0.390627835358521  loss:  262.821281749779&quot;
## [1] &quot;step =  2  lambda =  0.390627835358521  loss:  264.874056087402&quot;
## [1] &quot;step =  1  lambda =  0.386741023454502  loss:  262.253281523921&quot;
## [1] &quot;step =  2  lambda =  0.386741023454502  loss:  264.277701654394&quot;
## [1] &quot;step =  1  lambda =  0.382892885975112  loss:  261.662697575392&quot;
## [1] &quot;step =  2  lambda =  0.382892885975112  loss:  263.660033959307&quot;
## [1] &quot;step =  1  lambda =  0.379083038103399  loss:  261.0510161099&quot;
## [1] &quot;step =  2  lambda =  0.379083038103399  loss:  263.022359538299&quot;
## [1] &quot;step =  1  lambda =  0.375311098851399  loss:  260.419530506889&quot;
## [1] &quot;step =  2  lambda =  0.375311098851399  loss:  262.365830228132&quot;
## [1] &quot;step =  1  lambda =  0.371576691022046  loss:  259.769380985328&quot;
## [1] &quot;step =  2  lambda =  0.371576691022046  loss:  261.691481396365&quot;
## [1] &quot;step =  1  lambda =  0.367879441171442  loss:  259.101592456992&quot;
## [1] &quot;step =  2  lambda =  0.367879441171442  loss:  261.000258189355&quot;
## [1] &quot;step =  1  lambda =  0.364218979571523  loss:  258.417100515413&quot;
## [1] &quot;step =  2  lambda =  0.364218979571523  loss:  260.293032990736&quot;
## [1] &quot;step =  1  lambda =  0.360594940173078  loss:  257.716768721707&quot;
## [1] &quot;step =  2  lambda =  0.360594940173078  loss:  259.570617112758&quot;
## [1] &quot;step =  1  lambda =  0.357006960569148  loss:  257.001400179992&quot;
## [1] &quot;step =  2  lambda =  0.357006960569148  loss:  258.833768787499&quot;
## [1] &quot;step =  1  lambda =  0.35345468195878  loss:  256.271745449128&quot;
## [1] &quot;step =  2  lambda =  0.35345468195878  loss:  258.083198819479&quot;
## [1] &quot;step =  1  lambda =  0.349937749111156  loss:  255.528508138947&quot;
## [1] &quot;step =  2  lambda =  0.349937749111156  loss:  257.319574775136&quot;
## [1] &quot;step =  1  lambda =  0.346455810330057  loss:  254.772349057797&quot;
## [1] &quot;step =  2  lambda =  0.346455810330057  loss:  256.543524260661&quot;
## [1] &quot;step =  1  lambda =  0.343008517418707  loss:  254.003889457501&quot;
## [1] &quot;step =  2  lambda =  0.343008517418707  loss:  255.755637630117&quot;
## [1] &quot;step =  1  lambda =  0.339595525644939  loss:  253.22371371426&quot;
## [1] &quot;step =  2  lambda =  0.339595525644939  loss:  254.956470333075&quot;
## [1] &quot;step =  1  lambda =  0.336216493706733  loss:  252.43237165266&quot;
## [1] &quot;step =  2  lambda =  0.336216493706733  loss:  254.146545028231&quot;
## [1] &quot;step =  1  lambda =  0.33287108369808  loss:  251.630380638016&quot;
## [1] &quot;step =  2  lambda =  0.33287108369808  loss:  253.326353538436&quot;
## [1] &quot;step =  1  lambda =  0.329558961075189  loss:  250.8182275117&quot;
## [1] &quot;step =  2  lambda =  0.329558961075189  loss:  252.496358691349&quot;
## [1] &quot;step =  1  lambda =  0.32627979462304  loss:  249.99637041326&quot;
## [1] &quot;step =  2  lambda =  0.32627979462304  loss:  251.656996071008&quot;
## [1] &quot;step =  1  lambda =  0.323033256422253  loss:  249.165240514335&quot;
## [1] &quot;step =  2  lambda =  0.323033256422253  loss:  250.808675694273&quot;
## [1] &quot;step =  1  lambda =  0.319819021816304  loss:  248.325243678208&quot;
## [1] &quot;step =  2  lambda =  0.319819021816304  loss:  249.951783619382&quot;
## [1] &quot;step =  1  lambda =  0.316636769379053  loss:  247.476762052156&quot;
## [1] &quot;step =  2  lambda =  0.316636769379053  loss:  249.086683490036&quot;
## [1] &quot;step =  1  lambda =  0.313486180882605  loss:  246.620155595965&quot;
## [1] &quot;step =  2  lambda =  0.313486180882605  loss:  248.213718016279&quot;
## [1] &quot;step =  1  lambda =  0.310366941265485  loss:  245.755763547886&quot;
## [1] &quot;step =  2  lambda =  0.310366941265485  loss:  247.333210392386&quot;
## [1] &quot;step =  1  lambda =  0.307278738601131  loss:  244.883905828226&quot;
## [1] &quot;step =  2  lambda =  0.307278738601131  loss:  246.445465651524&quot;
## [1] &quot;step =  1  lambda =  0.304221264066704  loss:  244.004884380351&quot;
## [1] &quot;step =  2  lambda =  0.304221264066704  loss:  245.5507719569&quot;
## [1] &quot;step =  1  lambda =  0.301194211912202  loss:  243.118984448803&quot;
## [1] &quot;step =  2  lambda =  0.301194211912202  loss:  244.649401829221&quot;
## [1] &quot;step =  1  lambda =  0.298197279429888  loss:  242.22647579438&quot;
## [1] &quot;step =  2  lambda =  0.298197279429888  loss:  243.741613310577&quot;
## [1] &quot;step =  1  lambda =  0.295230166924014  loss:  241.327613846271&quot;
## [1] &quot;step =  2  lambda =  0.295230166924014  loss:  242.827651065129&quot;
## [1] &quot;step =  1  lambda =  0.292292577680859  loss:  240.422640791637&quot;
## [1] &quot;step =  2  lambda =  0.292292577680859  loss:  241.907747417335&quot;
## [1] &quot;step =  1  lambda =  0.289384217939051  loss:  239.511786603348&quot;
## [1] &quot;step =  2  lambda =  0.289384217939051  loss:  240.9821233287&quot;
## [1] &quot;step =  1  lambda =  0.28650479686019  loss:  238.595270006863&quot;
## [1] &quot;step =  2  lambda =  0.28650479686019  loss:  240.050989314352&quot;
## [1] &quot;step =  1  lambda =  0.28365402649977  loss:  237.673299387556&quot;
## [1] &quot;step =  2  lambda =  0.28365402649977  loss:  239.114546300981&quot;
## [1] &quot;step =  1  lambda =  0.28083162177838  loss:  236.746073639975&quot;
## [1] &quot;step =  2  lambda =  0.28083162177838  loss:  238.17298642788&quot;
## [1] &quot;step =  1  lambda =  0.278037300453194  loss:  235.813782960781&quot;
## [1] &quot;step =  2  lambda =  0.278037300453194  loss:  237.226493793011&quot;
## [1] &quot;step =  1  lambda =  0.275270783089753  loss:  234.876609587277&quot;
## [1] &quot;step =  2  lambda =  0.275270783089753  loss:  236.275245146164&quot;
## [1] &quot;step =  1  lambda =  0.272531793034013  loss:  233.934728483538&quot;
## [1] &quot;step =  2  lambda =  0.272531793034013  loss:  235.319410531365&quot;
## [1] &quot;step =  1  lambda =  0.269820056384687  loss:  232.988307976331&quot;
## [1] &quot;step =  2  lambda =  0.269820056384687  loss:  234.359153880809&quot;
## [1] &quot;step =  1  lambda =  0.26713530196585  loss:  232.037510343021&quot;
## [1] &quot;step =  2  lambda =  0.26713530196585  loss:  233.394633562602&quot;
## [1] &quot;step =  1  lambda =  0.264477261299824  loss:  231.082492353764&quot;
## [1] &quot;step =  2  lambda =  0.264477261299824  loss:  232.426002884643&quot;
## [1] &quot;step =  1  lambda =  0.261845668580326  loss:  230.123405770281&quot;
## [1] &quot;step =  2  lambda =  0.261845668580326  loss:  231.453410556991&quot;
## [1] &quot;step =  1  lambda =  0.259240260645892  loss:  229.160397803539&quot;
## [1] &quot;step =  2  lambda =  0.259240260645892  loss:  230.477001115039&quot;
## [1] &quot;step =  1  lambda =  0.256660776953556  loss:  228.193611532617&quot;
## [1] &quot;step =  2  lambda =  0.256660776953556  loss:  229.507670170222&quot;
## [1] &quot;step =  1  lambda =  0.2541069595528  loss:  227.233686888584&quot;
## [1] &quot;step =  2  lambda =  0.2541069595528  loss:  228.507396028285&quot;
## [1] &quot;step =  1  lambda =  0.251578553059757  loss:  226.243278413626&quot;
## [1] &quot;step =  2  lambda =  0.251578553059757  loss:  227.491457106066&quot;
## [1] &quot;step =  1  lambda =  0.249075304631668  loss:  225.237360755209&quot;
## [1] &quot;step =  2  lambda =  0.249075304631668  loss:  226.471382105916&quot;
## [1] &quot;step =  1  lambda =  0.246596963941606  loss:  224.227348579802&quot;
## [1] &quot;step =  2  lambda =  0.246596963941606  loss:  225.449135240589&quot;
## [1] &quot;step =  1  lambda =  0.244143283153437  loss:  223.215186687174&quot;
## [1] &quot;step =  2  lambda =  0.244143283153437  loss:  224.425091747184&quot;
## [1] &quot;step =  1  lambda =  0.241714016897036  loss:  222.201246610966&quot;
## [1] &quot;step =  2  lambda =  0.241714016897036  loss:  223.399369735487&quot;
## [1] &quot;step =  1  lambda =  0.239308922243755  loss:  221.185645303066&quot;
## [1] &quot;step =  2  lambda =  0.239308922243755  loss:  222.372034207983&quot;
## [1] &quot;step =  1  lambda =  0.236927758682122  loss:  220.168447133&quot;
## [1] &quot;step =  2  lambda =  0.236927758682122  loss:  221.342754073198&quot;
## [1] &quot;step =  1  lambda =  0.234570288093798  loss:  219.148835458502&quot;
## [1] &quot;step =  2  lambda =  0.234570288093798  loss:  220.311328465468&quot;
## [1] &quot;step =  1  lambda =  0.232236274729759  loss:  218.127597695875&quot;
## [1] &quot;step =  2  lambda =  0.232236274729759  loss:  219.279371132992&quot;
## [1] &quot;step =  1  lambda =  0.229925485186724  loss:  217.105834071163&quot;
## [1] &quot;step =  2  lambda =  0.229925485186724  loss:  218.246837551645&quot;
## [1] &quot;step =  1  lambda =  0.227637688383813  loss:  216.083500460106&quot;
## [1] &quot;step =  2  lambda =  0.227637688383813  loss:  217.213542928427&quot;
## [1] &quot;step =  1  lambda =  0.225372655539439  loss:  215.060413890257&quot;
## [1] &quot;step =  2  lambda =  0.225372655539439  loss:  216.179333385935&quot;
## [1] &quot;step =  1  lambda =  0.22313016014843  loss:  214.036422010468&quot;
## [1] &quot;step =  2  lambda =  0.22313016014843  loss:  215.144100518253&quot;
## [1] &quot;step =  1  lambda =  0.220909977959378  loss:  213.011417494212&quot;
## [1] &quot;step =  2  lambda =  0.220909977959378  loss:  214.107772025802&quot;
## [1] &quot;step =  1  lambda =  0.218711886952215  loss:  211.985328763867&quot;
## [1] &quot;step =  2  lambda =  0.218711886952215  loss:  213.070302646619&quot;
## [1] &quot;step =  1  lambda =  0.216535667316007  loss:  210.958111010663&quot;
## [1] &quot;step =  2  lambda =  0.216535667316007  loss:  212.031668171404&quot;
## [1] &quot;step =  1  lambda =  0.214381101426978  loss:  209.929740268722&quot;
## [1] &quot;step =  2  lambda =  0.214381101426978  loss:  210.991861721865&quot;
## [1] &quot;step =  1  lambda =  0.212247973826743  loss:  208.900209730244&quot;
## [1] &quot;step =  2  lambda =  0.212247973826743  loss:  209.950891337813&quot;
## [1] &quot;step =  1  lambda =  0.210136071200765  loss:  207.869527356593&quot;
## [1] &quot;step =  2  lambda =  0.210136071200765  loss:  208.908778261918&quot;
## [1] &quot;step =  1  lambda =  0.20804518235702  loss:  206.837714180119&quot;
## [1] &quot;step =  2  lambda =  0.20804518235702  loss:  207.865555586799&quot;
## [1] &quot;step =  1  lambda =  0.205975098204883  loss:  205.80480296474&quot;
## [1] &quot;step =  2  lambda =  0.205975098204883  loss:  206.821267093808&quot;
## [1] &quot;step =  1  lambda =  0.203925611734213  loss:  204.770837056303&quot;
## [1] &quot;step =  2  lambda =  0.203925611734213  loss:  205.77596620142&quot;
## [1] &quot;step =  1  lambda =  0.201896517994655  loss:  203.73586934147&quot;
## [1] &quot;step =  2  lambda =  0.201896517994655  loss:  204.729714986113&quot;
## [1] &quot;step =  1  lambda =  0.199887614075145  loss:  202.699961278363&quot;
## [1] &quot;step =  2  lambda =  0.199887614075145  loss:  203.682583260321&quot;
## [1] &quot;step =  1  lambda =  0.197898699083615  loss:  201.66318198373&quot;
## [1] &quot;step =  2  lambda =  0.197898699083615  loss:  202.634647701943&quot;
## [1] &quot;step =  1  lambda =  0.19592957412691  loss:  200.625607371139&quot;
## [1] &quot;step =  2  lambda =  0.19592957412691  loss:  201.585991033887&quot;
## [1] &quot;step =  1  lambda =  0.193980042290892  loss:  199.587319338722&quot;
## [1] &quot;step =  2  lambda =  0.193980042290892  loss:  200.536701253479&quot;
## [1] &quot;step =  1  lambda =  0.192049908620754  loss:  198.548405006285&quot;
## [1] &quot;step =  2  lambda =  0.192049908620754  loss:  199.486870911707&quot;
## [1] &quot;step =  1  lambda =  0.19013898010152  loss:  197.508956001745&quot;
## [1] &quot;step =  2  lambda =  0.19013898010152  loss:  198.436596441911&quot;
## [1] &quot;step =  1  lambda =  0.188247065638747  loss:  196.469067796541&quot;
## [1] &quot;step =  2  lambda =  0.188247065638747  loss:  197.385977537142&quot;
## [1] &quot;step =  1  lambda =  0.18637397603941  loss:  195.428839089212&quot;
## [1] &quot;step =  2  lambda =  0.18637397603941  loss:  196.335116574956&quot;
## [1] &quot;step =  1  lambda =  0.184519523992989  loss:  194.388371235938&quot;
## [1] &quot;step =  2  lambda =  0.184519523992989  loss:  195.284118088058&quot;
## [1] &quot;step =  1  lambda =  0.182683524052735  loss:  193.34776772648&quot;
## [1] &quot;step =  2  lambda =  0.182683524052735  loss:  194.233088278967&quot;
## [1] &quot;step =  1  lambda =  0.180865792617122  loss:  192.307081700374&quot;
## [1] &quot;step =  2  lambda =  0.180865792617122  loss:  193.182222321979&quot;
## [1] &quot;step =  1  lambda =  0.179066147911493  loss:  191.266516385378&quot;
## [1] &quot;step =  2  lambda =  0.179066147911493  loss:  192.131506196613&quot;
## [1] &quot;step =  1  lambda =  0.177284409969878  loss:  190.226187881288&quot;
## [1] &quot;step =  2  lambda =  0.177284409969878  loss:  191.081114144587&quot;
## [1] &quot;step =  1  lambda =  0.175520400616997  loss:  189.186188933273&quot;
## [1] &quot;step =  2  lambda =  0.175520400616997  loss:  190.031150314899&quot;
## [1] &quot;step =  1  lambda =  0.173773943450445  loss:  188.146614419187&quot;
## [1] &quot;step =  2  lambda =  0.173773943450445  loss:  188.981713610906&quot;
## [1] &quot;step =  1  lambda =  0.172044863823051  loss:  187.107562253703&quot;
## [1] &quot;step =  2  lambda =  0.172044863823051  loss:  187.932904898448&quot;
## [1] &quot;step =  1  lambda =  0.17033298882541  loss:  186.069132294574&quot;
## [1] &quot;step =  2  lambda =  0.17033298882541  loss:  186.884826197603&quot;
## [1] &quot;step =  1  lambda =  0.168638147268596  loss:  185.031425542431&quot;
## [1] &quot;step =  2  lambda =  0.168638147268596  loss:  185.837580066125&quot;
## [1] &quot;step =  1  lambda =  0.166960169667041  loss:  183.99454353035&quot;
## [1] &quot;step =  2  lambda =  0.166960169667041  loss:  184.791269094263&quot;
## [1] &quot;step =  1  lambda =  0.165298888221586  loss:  182.958587823699&quot;
## [1] &quot;step =  2  lambda =  0.165298888221586  loss:  183.74599548681&quot;
## [1] &quot;step =  1  lambda =  0.163654136802704  loss:  181.92365960635&quot;
## [1] &quot;step =  2  lambda =  0.163654136802704  loss:  182.701860717837&quot;
## [1] &quot;step =  1  lambda =  0.162025750933881  loss:  180.889859338837&quot;
## [1] &quot;step =  2  lambda =  0.162025750933881  loss:  181.658965245762&quot;
## [1] &quot;step =  1  lambda =  0.160413567775173  loss:  179.857286476259&quot;
## [1] &quot;step =  2  lambda =  0.160413567775173  loss:  180.617408278563&quot;
## [1] &quot;step =  1  lambda =  0.158817426106921  loss:  178.826039235824&quot;
## [1] &quot;step =  2  lambda =  0.158817426106921  loss:  179.577287580986&quot;
## [1] &quot;step =  1  lambda =  0.157237166313628  loss:  177.796214405967&quot;
## [1] &quot;step =  2  lambda =  0.157237166313628  loss:  178.538699317143&quot;
## [1] &quot;step =  1  lambda =  0.155672630367997  loss:  176.767907190513&quot;
## [1] &quot;step =  2  lambda =  0.155672630367997  loss:  177.501737923122&quot;
## [1] &quot;step =  1  lambda =  0.154123661815132  loss:  175.741211082543&quot;
## [1] &quot;step =  2  lambda =  0.154123661815132  loss:  176.4664960051&quot;
## [1] &quot;step =  1  lambda =  0.152590105756884  loss:  174.716217763522&quot;
## [1] &quot;step =  2  lambda =  0.152590105756884  loss:  175.43306425919&quot;
## [1] &quot;step =  1  lambda =  0.151071808836371  loss:  173.693017023932&quot;
## [1] &quot;step =  2  lambda =  0.151071808836371  loss:  174.401531409791&quot;
## [1] &quot;step =  1  lambda =  0.149568619222635  loss:  172.671696702233&quot;
## [1] &quot;step =  2  lambda =  0.149568619222635  loss:  173.371984163709&quot;
## [1] &quot;step =  1  lambda =  0.148080386595462  loss:  171.652342639439&quot;
## [1] &quot;step =  2  lambda =  0.148080386595462  loss:  172.344507177691&quot;
## [1] &quot;step =  1  lambda =  0.14660696213035  loss:  170.635038646967&quot;
## [1] &quot;step =  2  lambda =  0.14660696213035  loss:  171.31918303738&quot;
## [1] &quot;step =  1  lambda =  0.145148198483624  loss:  169.619866485796&quot;
## [1] &quot;step =  2  lambda =  0.145148198483624  loss:  170.296092245951&quot;
## [1] &quot;step =  1  lambda =  0.143703949777703  loss:  168.606905855218&quot;
## [1] &quot;step =  2  lambda =  0.143703949777703  loss:  169.275313220976&quot;
## [1] &quot;step =  1  lambda =  0.142274071586514  loss:  167.596234389727&quot;
## [1] &quot;step =  2  lambda =  0.142274071586514  loss:  168.256922298255&quot;
## [1] &quot;step =  1  lambda =  0.140858420921045  loss:  166.587927662806&quot;
## [1] &quot;step =  2  lambda =  0.140858420921045  loss:  167.240993741516&quot;
## [1] &quot;step =  1  lambda =  0.139456856215051  loss:  165.582059196533&quot;
## [1] &quot;step =  2  lambda =  0.139456856215051  loss:  166.227599757084&quot;
## [1] &quot;step =  1  lambda =  0.138069237310893  loss:  164.5787004761&quot;
## [1] &quot;step =  2  lambda =  0.138069237310893  loss:  165.216810512706&quot;
## [1] &quot;step =  1  lambda =  0.136695425445524  loss:  163.577920968444&quot;
## [1] &quot;step =  2  lambda =  0.136695425445524  loss:  164.20869415986&quot;
## [1] &quot;step =  1  lambda =  0.135335283236613  loss:  162.579788144333&quot;
## [1] &quot;step =  2  lambda =  0.135335283236613  loss:  163.203316858951&quot;
## [1] &quot;step =  1  lambda =  0.133988674668805  loss:  161.584367503312&quot;
## [1] &quot;step =  2  lambda =  0.133988674668805  loss:  162.200742806896&quot;
## [1] &quot;step =  1  lambda =  0.132655465080122  loss:  160.591722601009&quot;
## [1] &quot;step =  2  lambda =  0.132655465080122  loss:  161.201034266668&quot;
## [1] &quot;step =  1  lambda =  0.131335521148493  loss:  159.601915078382&quot;
## [1] &quot;step =  2  lambda =  0.131335521148493  loss:  160.204251598395&quot;
## [1] &quot;step =  1  lambda =  0.130028710878426  loss:  158.615004692511&quot;
## [1] &quot;step =  2  lambda =  0.130028710878426  loss:  159.210453291732&quot;
## [1] &quot;step =  1  lambda =  0.128734903587804  loss:  157.631049348638&quot;
## [1] &quot;step =  2  lambda =  0.128734903587804  loss:  158.219641341913&quot;
## [1] &quot;step =  1  lambda =  0.127453969894821  loss:  156.650105133163&quot;
## [1] &quot;step =  2  lambda =  0.127453969894821  loss:  157.231617954788&quot;
## [1] &quot;step =  1  lambda =  0.126185781705039  loss:  155.671902377338&quot;
## [1] &quot;step =  2  lambda =  0.126185781705039  loss:  156.24719060016&quot;
## [1] &quot;step =  1  lambda =  0.124930212198582  loss:  154.697224357577&quot;
## [1] &quot;step =  2  lambda =  0.124930212198582  loss:  155.26610586333&quot;
## [1] &quot;step =  1  lambda =  0.123687135817455  loss:  153.725856101265&quot;
## [1] &quot;step =  2  lambda =  0.123687135817455  loss:  154.288294247379&quot;
## [1] &quot;step =  1  lambda =  0.122456428252982  loss:  152.757728786669&quot;
## [1] &quot;step =  2  lambda =  0.122456428252982  loss:  153.313783521128&quot;
## [1] &quot;step =  1  lambda =  0.121237966433382  loss:  151.792869901045&quot;
## [1] &quot;step =  2  lambda =  0.121237966433382  loss:  152.342619232651&quot;
## [1] &quot;step =  1  lambda =  0.120031628511457  loss:  150.831324536106&quot;
## [1] &quot;step =  2  lambda =  0.120031628511457  loss:  151.374848038063&quot;
## [1] &quot;step =  1  lambda =  0.11883729385241  loss:  149.873138880999&quot;
## [1] &quot;step =  2  lambda =  0.11883729385241  loss:  150.410514931092&quot;
## [1] &quot;step =  1  lambda =  0.117654843021779  loss:  148.918357479162&quot;
## [1] &quot;step =  2  lambda =  0.117654843021779  loss:  149.449663030123&quot;
## [1] &quot;step =  1  lambda =  0.116484157773497  loss:  147.967023017438&quot;
## [1] &quot;step =  2  lambda =  0.116484157773497  loss:  148.492333720446&quot;
## [1] &quot;step =  1  lambda =  0.115325121038063  loss:  147.019176466875&quot;
## [1] &quot;step =  2  lambda =  0.115325121038063  loss:  147.538566788541&quot;
## [1] &quot;step =  1  lambda =  0.114177616910836  loss:  146.074857215657&quot;
## [1] &quot;step =  2  lambda =  0.114177616910836  loss:  146.588400513932&quot;
## [1] &quot;step =  1  lambda =  0.11304153064045  loss:  145.134103160028&quot;
## [1] &quot;step =  2  lambda =  0.11304153064045  loss:  145.641871731169&quot;
## [1] &quot;step =  1  lambda =  0.111916748617329  loss:  144.196950765651&quot;
## [1] &quot;step =  2  lambda =  0.111916748617329  loss:  144.699015874765&quot;
## [1] &quot;step =  1  lambda =  0.110803158362334  loss:  143.263435112091&quot;
## [1] &quot;step =  2  lambda =  0.110803158362334  loss:  143.759867015145&quot;
## [1] &quot;step =  1  lambda =  0.109700648515511  loss:  142.333589928411&quot;
## [1] &quot;step =  2  lambda =  0.109700648515511  loss:  142.82445789008&quot;
## [1] &quot;step =  1  lambda =  0.108609108824958  loss:  141.407447624278&quot;
## [1] &quot;step =  2  lambda =  0.108609108824958  loss:  141.8928199339&quot;
## [1] &quot;step =  1  lambda =  0.107528430135795  loss:  140.485039318899&quot;
## [1] &quot;step =  2  lambda =  0.107528430135795  loss:  140.964983305633&quot;
## [1] &quot;step =  1  lambda =  0.106458504379253  loss:  139.566394868873&quot;
## [1] &quot;step =  2  lambda =  0.106458504379253  loss:  140.040976916588&quot;
## [1] &quot;step =  1  lambda =  0.105399224561864  loss:  138.6515428955&quot;
## [1] &quot;step =  2  lambda =  0.105399224561864  loss:  139.120828457566&quot;
## [1] &quot;step =  1  lambda =  0.104350484754765  loss:  137.740510811722&quot;
## [1] &quot;step =  2  lambda =  0.104350484754765  loss:  138.204564425779&quot;
## [1] &quot;step =  1  lambda =  0.1033121800831  loss:  136.833324848775&quot;
## [1] &quot;step =  2  lambda =  0.1033121800831  loss:  137.292210151447&quot;
## [1] &quot;step =  1  lambda =  0.102284206715537  loss:  135.93001008252&quot;
## [1] &quot;step =  2  lambda =  0.102284206715537  loss:  136.383789824058&quot;
## [1] &quot;step =  1  lambda =  0.101266461853883  loss:  135.030590459443&quot;
## [1] &quot;step =  2  lambda =  0.101266461853883  loss:  135.479326518243&quot;
## [1] &quot;step =  1  lambda =  0.100258843722804  loss:  134.135088822273&quot;
## [1] &quot;step =  2  lambda =  0.100258843722804  loss:  134.578842219248&quot;
## [1] &quot;step =  1  lambda =  0.0992612515596457  loss:  133.243526935199&quot;
## [1] &quot;step =  2  lambda =  0.0992612515596457  loss:  133.682357847964&quot;
## [1] &quot;step =  1  lambda =  0.0982735856043615  loss:  132.355925508651&quot;
## [1] &quot;step =  2  lambda =  0.0982735856043615  loss:  132.789893285509&quot;
## [1] &quot;step =  1  lambda =  0.0972957470895328  loss:  131.472304223639&quot;
## [1] &quot;step =  2  lambda =  0.0972957470895328  loss:  131.901467397342&quot;
## [1] &quot;step =  1  lambda =  0.096327638230493  loss:  130.592681755626&quot;
## [1] &quot;step =  2  lambda =  0.096327638230493  loss:  131.017098056902&quot;
## [1] &quot;step =  1  lambda =  0.0953691622155497  loss:  129.717075797933&quot;
## [1] &quot;step =  2  lambda =  0.0953691622155497  loss:  130.136802168775&quot;
## [1] &quot;step =  1  lambda =  0.0944202231963024  loss:  128.845503084672&quot;
## [1] &quot;step =  2  lambda =  0.0944202231963024  loss:  129.260595691374&quot;
## [1] &quot;step =  1  lambda =  0.0934807262780585  loss:  127.977979413205&quot;
## [1] &quot;step =  2  lambda =  0.0934807262780585  loss:  128.388493659151&quot;
## [1] &quot;step =  1  lambda =  0.0925505775103433  loss:  127.114519666133&quot;
## [1] &quot;step =  2  lambda =  0.0925505775103433  loss:  127.520510204331&quot;
## [1] &quot;step =  1  lambda =  0.0916296838775049  loss:  126.255137832812&quot;
## [1] &quot;step =  2  lambda =  0.0916296838775049  loss:  126.656658578176&quot;
## [1] &quot;step =  1  lambda =  0.0907179532894126  loss:  125.399847030409&quot;
## [1] &quot;step =  2  lambda =  0.0907179532894126  loss:  125.796951171796&quot;
## [1] &quot;step =  1  lambda =  0.0898152945722476  loss:  124.548659524503&quot;
## [1] &quot;step =  2  lambda =  0.0898152945722476  loss:  124.941399536489&quot;
## [1] &quot;step =  1  lambda =  0.0889216174593863  loss:  123.701586749226&quot;
## [1] &quot;step =  2  lambda =  0.0889216174593863  loss:  124.090014403652&quot;
## [1] &quot;step =  1  lambda =  0.0880368325823726  loss:  122.858639326971&quot;
## [1] &quot;step =  2  lambda =  0.0880368325823726  loss:  123.242805704239&quot;
## [1] &quot;step =  1  lambda =  0.0871608514619813  loss:  122.019827087659&quot;
## [1] &quot;step =  2  lambda =  0.0871608514619813  loss:  122.399782587793&quot;
## [1] &quot;step =  1  lambda =  0.0862935864993705  loss:  121.185159087585&quot;
## [1] &quot;step =  2  lambda =  0.0862935864993705  loss:  121.560953441058&quot;
## [1] &quot;step =  1  lambda =  0.0854349509673212  loss:  120.354643627839&quot;
## [1] &quot;step =  2  lambda =  0.0854349509673212  loss:  120.726325906178&quot;
## [1] &quot;step =  1  lambda =  0.0845848590015647  loss:  119.528288272329&quot;
## [1] &quot;step =  2  lambda =  0.0845848590015647  loss:  119.895906898491&quot;
## [1] &quot;step =  1  lambda =  0.083743225592196  loss:  118.706099865395&quot;
## [1] &quot;step =  2  lambda =  0.083743225592196  loss:  119.069702623934&quot;
## [1] &quot;step =  1  lambda =  0.0829099665751727  loss:  117.888084549039&quot;
## [1] &quot;step =  2  lambda =  0.0829099665751727  loss:  118.247718596056&quot;
## [1] &quot;step =  1  lambda =  0.0820849986238988  loss:  117.074247779776&quot;
## [1] &quot;step =  2  lambda =  0.0820849986238988  loss:  117.429959652664&quot;
## [1] &quot;step =  1  lambda =  0.0812682392408917  loss:  116.264594345109&quot;
## [1] &quot;step =  2  lambda =  0.0812682392408917  loss:  116.616429972101&quot;
## [1] &quot;step =  1  lambda =  0.0804596067495325  loss:  115.459128379644&quot;
## [1] &quot;step =  2  lambda =  0.0804596067495325  loss:  115.807133089165&quot;
## [1] &quot;step =  1  lambda =  0.079659020285898  loss:  114.657853380853&quot;
## [1] &quot;step =  2  lambda =  0.079659020285898  loss:  115.002071910679&quot;
## [1] &quot;step =  1  lambda =  0.0788663997906749  loss:  113.860772224493&quot;
## [1] &quot;step =  2  lambda =  0.0788663997906749  loss:  114.201248730734&quot;
## [1] &quot;step =  1  lambda =  0.0780816660011532  loss:  113.067887179687&quot;
## [1] &quot;step =  2  lambda =  0.0780816660011532  loss:  113.404665245582&quot;
## [1] &quot;step =  1  lambda =  0.0773047404432998  loss:  112.279199923681&quot;
## [1] &quot;step =  2  lambda =  0.0773047404432998  loss:  112.612322568228&quot;
## [1] &quot;step =  1  lambda =  0.0765355454239115  loss:  111.494711556282&quot;
## [1] &quot;step =  2  lambda =  0.0765355454239115  loss:  111.824221242692&quot;
## [1] &quot;step =  1  lambda =  0.0757740040228455  loss:  110.714422613984&quot;
## [1] &quot;step =  2  lambda =  0.0757740040228455  loss:  111.040361257977&quot;
## [1] &quot;step =  1  lambda =  0.075020040085327  loss:  109.938333083792&quot;
## [1] &quot;step =  2  lambda =  0.075020040085327  loss:  110.260742061733&quot;
## [1] &quot;step =  1  lambda =  0.0742735782143339  loss:  109.166442416752&quot;
## [1] &quot;step =  2  lambda =  0.0742735782143339  loss:  109.48536257363&quot;
## [1] &quot;step =  1  lambda =  0.0735345437630571  loss:  108.398749541195&quot;
## [1] &quot;step =  2  lambda =  0.0735345437630571  loss:  108.714221198454&quot;
## [1] &quot;step =  1  lambda =  0.0728028628274356  loss:  107.635252875699&quot;
## [1] &quot;step =  2  lambda =  0.0728028628274356  loss:  107.947315838922&quot;
## [1] &quot;step =  1  lambda =  0.0720784622387661  loss:  106.875950341774&quot;
## [1] &quot;step =  2  lambda =  0.0720784622387661  loss:  107.184643908231&quot;
## [1] &quot;step =  1  lambda =  0.0713612695563861  loss:  106.120839376292&quot;
## [1] &quot;step =  2  lambda =  0.0713612695563861  loss:  106.426202342342&quot;
## [1] &quot;step =  1  lambda =  0.0706512130604296  loss:  105.369916943644&quot;
## [1] &quot;step =  2  lambda =  0.0706512130604296  loss:  105.67198761201&quot;
## [1] &quot;step =  1  lambda =  0.0699482217446554  loss:  104.623179547654&quot;
## [1] &quot;step =  2  lambda =  0.0699482217446554  loss:  104.921995734563&quot;
## [1] &quot;step =  1  lambda =  0.069252225309346  loss:  103.880623243242&quot;
## [1] &quot;step =  2  lambda =  0.069252225309346  loss:  104.175594816912&quot;
## [1] &quot;step =  1  lambda =  0.0685631541542779  loss:  103.141668930523&quot;
## [1] &quot;step =  2  lambda =  0.0685631541542779  loss:  103.433771924288&quot;
## [1] &quot;step =  1  lambda =  0.0678809393717615  loss:  102.407199833254&quot;
## [1] &quot;step =  2  lambda =  0.0678809393717615  loss:  102.696988996307&quot;
## [1] &quot;step =  1  lambda =  0.0672055127397498  loss:  101.677720901533&quot;
## [1] &quot;step =  2  lambda =  0.0672055127397498  loss:  101.964620909849&quot;
## [1] &quot;step =  1  lambda =  0.0665368067150169  loss:  100.952613160101&quot;
## [1] &quot;step =  2  lambda =  0.0665368067150169  loss:  101.236525827337&quot;
## [1] &quot;step =  1  lambda =  0.065874754426403  loss:  100.231736172782&quot;
## [1] &quot;step =  2  lambda =  0.065874754426403  loss:  100.512652605201&quot;
## [1] &quot;step =  1  lambda =  0.0652192896681276  loss:  99.5150393000513&quot;
## [1] &quot;step =  2  lambda =  0.0652192896681276  loss:  99.792973734982&quot;
## [1] &quot;step =  1  lambda =  0.0645703468931685  loss:  98.802495303037&quot;
## [1] &quot;step =  2  lambda =  0.0645703468931685  loss:  99.0774714991835&quot;
## [1] &quot;step =  1  lambda =  0.0639278612067076  loss:  98.0940866371143&quot;
## [1] &quot;step =  2  lambda =  0.0639278612067076  loss:  98.3661329194942&quot;
## [1] &quot;step =  1  lambda =  0.0632917683596407  loss:  97.3898004502976&quot;
## [1] &quot;step =  2  lambda =  0.0632917683596407  loss:  97.6589472052946&quot;
## [1] &quot;step =  1  lambda =  0.0626620047421532  loss:  96.6896260569475&quot;
## [1] &quot;step =  2  lambda =  0.0626620047421532  loss:  96.9559044271935&quot;
## [1] &quot;step =  1  lambda =  0.0620385073773583  loss:  95.9935536243374&quot;
## [1] &quot;step =  2  lambda =  0.0620385073773583  loss:  96.2569948672664&quot;
## [1] &quot;step =  1  lambda =  0.0614212139150001  loss:  95.3015735292387&quot;
## [1] &quot;step =  2  lambda =  0.0614212139150001  loss:  95.5622087293759&quot;
## [1] &quot;step =  1  lambda =  0.060810062625218  loss:  94.6136760710454&quot;
## [1] &quot;step =  2  lambda =  0.060810062625218  loss:  94.8715360287385&quot;
## [1] &quot;step =  1  lambda =  0.0602049923923736  loss:  93.9298513623977&quot;
## [1] &quot;step =  2  lambda =  0.0602049923923736  loss:  94.1849665630951&quot;
## [1] &quot;step =  1  lambda =  0.0596059427089393  loss:  93.2500893006161&quot;
## [1] &quot;step =  2  lambda =  0.0596059427089393  loss:  93.5024899159458&quot;
## [1] &quot;step =  1  lambda =  0.0590128536694478  loss:  92.574379570896&quot;
## [1] &quot;step =  2  lambda =  0.0590128536694478  loss:  92.8240954686576&quot;
## [1] &quot;step =  1  lambda =  0.0584256659645008  loss:  91.9027116582933&quot;
## [1] &quot;step =  2  lambda =  0.0584256659645008  loss:  92.1497724119066&quot;
## [1] &quot;step =  1  lambda =  0.0578443208748385  loss:  91.235074859056&quot;
## [1] &quot;step =  2  lambda =  0.0578443208748385  loss:  91.4795097535252&quot;
## [1] &quot;step =  1  lambda =  0.0572687602654674  loss:  90.5714582883977&quot;
## [1] &quot;step =  2  lambda =  0.0572687602654674  loss:  90.8132963226879&quot;
## [1] &quot;step =  1  lambda =  0.0566989265798469  loss:  89.9118508846456&quot;
## [1] &quot;step =  2  lambda =  0.0566989265798469  loss:  90.1511207713722&quot;
## [1] &quot;step =  1  lambda =  0.0561347628341337  loss:  89.2562414106927&quot;
## [1] &quot;step =  2  lambda =  0.0561347628341337  loss:  89.4929715741857&quot;
## [1] &quot;step =  1  lambda =  0.0555762126114831  loss:  88.6046184538281&quot;
## [1] &quot;step =  2  lambda =  0.0555762126114831  loss:  88.8388370274617&quot;
## [1] &quot;step =  1  lambda =  0.0550232200564073  loss:  87.9569704248465&quot;
## [1] &quot;step =  2  lambda =  0.0550232200564073  loss:  88.1887052482708&quot;
## [1] &quot;step =  1  lambda =  0.0544757298691899  loss:  87.3132855570709&quot;
## [1] &quot;step =  2  lambda =  0.0544757298691899  loss:  87.5425641737477&quot;
## [1] &quot;step =  1  lambda =  0.053933687300356  loss:  86.6735519056877&quot;
## [1] &quot;step =  2  lambda =  0.053933687300356  loss:  86.9004015609485&quot;
## [1] &quot;step =  1  lambda =  0.0533970381451971  loss:  86.0377573476073&quot;
## [1] &quot;step =  2  lambda =  0.0533970381451971  loss:  86.2622049873283&quot;
## [1] &quot;step =  1  lambda =  0.0528657287383504  loss:  85.4058895819373&quot;
## [1] &quot;step =  2  lambda =  0.0528657287383504  loss:  85.6279618518433&quot;
## [1] &quot;step =  1  lambda =  0.0523397059484324  loss:  84.7779361310743&quot;
## [1] &quot;step =  2  lambda =  0.0523397059484324  loss:  84.9976593766396&quot;
## [1] &quot;step =  1  lambda =  0.0518189171727258  loss:  84.1538843423773&quot;
## [1] &quot;step =  2  lambda =  0.0518189171727258  loss:  84.371284609268&quot;
## [1] &quot;step =  1  lambda =  0.0513033103319191  loss:  83.5337213903591&quot;
## [1] &quot;step =  2  lambda =  0.0513033103319191  loss:  83.7488457965319&quot;
## [1] &quot;step =  1  lambda =  0.0507928338648985  loss:  82.9174707854895&quot;
## [1] &quot;step =  2  lambda =  0.0507928338648985  loss:  83.13033000973&quot;
## [1] &quot;step =  1  lambda =  0.0502874367235919  loss:  82.3050887471456&quot;
## [1] &quot;step =  2  lambda =  0.0502874367235919  loss:  82.5156783856337&quot;
## [1] &quot;step =  1  lambda =  0.0497870683678639  loss:  81.6965326246153&quot;
## [1] &quot;step =  2  lambda =  0.0497870683678639  loss:  81.9048787612373&quot;
## [1] &quot;step =  1  lambda =  0.0492916787604622  loss:  81.0917903740844&quot;
## [1] &quot;step =  2  lambda =  0.0492916787604622  loss:  81.2979215233053&quot;
## [1] &quot;step =  1  lambda =  0.048801218362013  loss:  80.4908524770805&quot;
## [1] &quot;step =  2  lambda =  0.048801218362013  loss:  80.6947956879995&quot;
## [1] &quot;step =  1  lambda =  0.0483156381260678  loss:  79.8937080581478&quot;
## [1] &quot;step =  2  lambda =  0.0483156381260678  loss:  80.0954890687285&quot;
## [1] &quot;step =  1  lambda =  0.0478348894941984  loss:  79.3003450509424&quot;
## [1] &quot;step =  2  lambda =  0.0478348894941984  loss:  79.4999886715734&quot;
## [1] &quot;step =  1  lambda =  0.0473589243911409  loss:  78.7107505897283&quot;
## [1] &quot;step =  2  lambda =  0.0473589243911409  loss:  78.9082809450717&quot;
## [1] &quot;step =  1  lambda =  0.0468876952199885  loss:  78.1249112566869&quot;
## [1] &quot;step =  2  lambda =  0.0468876952199885  loss:  78.3203519308089&quot;
## [1] &quot;step =  1  lambda =  0.0464211548574313  loss:  77.5428132310187&quot;
## [1] &quot;step =  2  lambda =  0.0464211548574313  loss:  77.7361873576961&quot;
## [1] &quot;step =  1  lambda =  0.0459592566490442  loss:  76.964442382295&quot;
## [1] &quot;step =  2  lambda =  0.0459592566490442  loss:  77.1557727027373&quot;
## [1] &quot;step =  1  lambda =  0.0455019544046216  loss:  76.3897843306295&quot;
## [1] &quot;step =  2  lambda =  0.0455019544046216  loss:  76.5790932313402&quot;
## [1] &quot;step =  1  lambda =  0.0450492023935578  loss:  75.8188244865985&quot;
## [1] &quot;step =  2  lambda =  0.0450492023935578  loss:  76.0061340251543&quot;
## [1] &quot;step =  1  lambda =  0.0446009553402746  loss:  75.2515480788098&quot;
## [1] &quot;step =  2  lambda =  0.0446009553402746  loss:  75.4368800023369&quot;
## [1] &quot;step =  1  lambda =  0.0441571684196929  loss:  74.6879401739745&quot;
## [1] &quot;step =  2  lambda =  0.0441571684196929  loss:  74.8713159332067&quot;
## [1] &quot;step =  1  lambda =  0.0437177972527509  loss:  74.127985692411&quot;
## [1] &quot;step =  2  lambda =  0.0437177972527509  loss:  74.309426453036&quot;
## [1] &quot;step =  1  lambda =  0.0432827979019659  loss:  73.5716694207151&quot;
## [1] &quot;step =  2  lambda =  0.0432827979019659  loss:  73.7511960730019&quot;
## [1] &quot;step =  1  lambda =  0.0428521268670402  loss:  73.0189760226071&quot;
## [1] &quot;step =  2  lambda =  0.0428521268670402  loss:  73.196609189886&quot;
## [1] &quot;step =  1  lambda =  0.0424257410805114  loss:  72.4698900485383&quot;
## [1] &quot;step =  2  lambda =  0.0424257410805114  loss:  72.6456500948618&quot;
## [1] &quot;step =  1  lambda =  0.0420035979034456  loss:  71.9243959443953&quot;
## [1] &quot;step =  2  lambda =  0.0420035979034456  loss:  72.0983029815702&quot;
## [1] &quot;step =  1  lambda =  0.0415856551211732  loss:  71.3824780594966&quot;
## [1] &quot;step =  2  lambda =  0.0415856551211732  loss:  71.5545519536042&quot;
## [1] &quot;step =  1  lambda =  0.0411718709390678  loss:  70.8441206540066&quot;
## [1] &quot;step =  2  lambda =  0.0411718709390678  loss:  71.0143810314813&quot;
## [1] &quot;step =  1  lambda =  0.0407622039783662  loss:  70.3093079058402&quot;
## [1] &quot;step =  2  lambda =  0.0407622039783662  loss:  70.4777741591604&quot;
## [1] &quot;step =  1  lambda =  0.0403566132720311  loss:  69.7780239171165&quot;
## [1] &quot;step =  2  lambda =  0.0403566132720311  loss:  69.9447152101433&quot;
## [1] &quot;step =  1  lambda =  0.0399550582606539  loss:  69.2502527202014&quot;
## [1] &quot;step =  2  lambda =  0.0399550582606539  loss:  69.4151879931988&quot;
## [1] &quot;step =  1  lambda =  0.0395574987883987  loss:  68.725978283376&quot;
## [1] &quot;step =  2  lambda =  0.0395574987883987  loss:  68.8891762577368&quot;
## [1] &quot;step =  1  lambda =  0.0391638950989871  loss:  68.2051845161582&quot;
## [1] &quot;step =  2  lambda =  0.0391638950989871  loss:  68.3666636988619&quot;
## [1] &quot;step =  1  lambda =  0.038774207831722  loss:  67.6878552743075&quot;
## [1] &quot;step =  2  lambda =  0.038774207831722  loss:  67.8476339621297&quot;
## [1] &quot;step =  1  lambda =  0.0383883980175521  loss:  67.1739743645341&quot;
## [1] &quot;step =  2  lambda =  0.0383883980175521  loss:  67.3320706480282&quot;
## [1] &quot;step =  1  lambda =  0.0380064270751743  loss:  66.663525548937&quot;
## [1] &quot;step =  2  lambda =  0.0380064270751743  loss:  66.8199573162053&quot;
## [1] &quot;step =  1  lambda =  0.0376282568071762  loss:  66.1564925491896&quot;
## [1] &quot;step =  2  lambda =  0.0376282568071762  loss:  66.3112774894607&quot;
## [1] &quot;step =  1  lambda =  0.0372538493962158  loss:  65.6528590504932&quot;
## [1] &quot;step =  2  lambda =  0.0372538493962158  loss:  65.8060146575205&quot;
## [1] &quot;step =  1  lambda =  0.03688316740124  loss:  65.1526087053131&quot;
## [1] &quot;step =  2  lambda =  0.03688316740124  loss:  65.3041522806083&quot;
## [1] &quot;step =  1  lambda =  0.0365161737537404  loss:  64.655726016805&quot;
## [1] &quot;step =  2  lambda =  0.0365161737537404  loss:  64.8056954554279&quot;
## [1] &quot;step =  1  lambda =  0.0361528317540464  loss:  64.1622207647471&quot;
## [1] &quot;step =  2  lambda =  0.0361528317540464  loss:  64.3105939320714&quot;
## [1] &quot;step =  1  lambda =  0.0357931050676553  loss:  63.6720309550627&quot;
## [1] &quot;step =  2  lambda =  0.0357931050676553  loss:  63.8188229101705&quot;
## [1] &quot;step =  1  lambda =  0.0354369577215986  loss:  63.1851386621896&quot;
## [1] &quot;step =  2  lambda =  0.0354369577215986  loss:  63.3303672923623&quot;
## [1] &quot;step =  1  lambda =  0.035084354100845  loss:  62.7015289376404&quot;
## [1] &quot;step =  2  lambda =  0.035084354100845  loss:  62.8452117056501&quot;
## [1] &quot;step =  1  lambda =  0.0347352589447386  loss:  62.2211865599011&quot;
## [1] &quot;step =  2  lambda =  0.0347352589447386  loss:  62.3633406237387&quot;
## [1] &quot;step =  1  lambda =  0.0343896373434727  loss:  61.744096155697&quot;
## [1] &quot;step =  2  lambda =  0.0343896373434727  loss:  61.8847383987811&quot;
## [1] &quot;step =  1  lambda =  0.0340474547345993  loss:  61.2702422314373&quot;
## [1] &quot;step =  2  lambda =  0.0340474547345993  loss:  61.4093892633222&quot;
## [1] &quot;step =  1  lambda =  0.0337086768995724  loss:  60.7996091751254&quot;
## [1] &quot;step =  2  lambda =  0.0337086768995724  loss:  60.9372773528384&quot;
## [1] &quot;step =  1  lambda =  0.0333732699603261  loss:  60.3321812786681&quot;
## [1] &quot;step =  2  lambda =  0.0333732699603261  loss:  60.4683867345531&quot;
## [1] &quot;step =  1  lambda =  0.0330412003758869  loss:  59.8679427664011&quot;
## [1] &quot;step =  2  lambda =  0.0330412003758869  loss:  60.0027014307747&quot;
## [1] &quot;step =  1  lambda =  0.0327124349390198  loss:  59.4068778181964&quot;
## [1] &quot;step =  2  lambda =  0.0327124349390198  loss:  59.5402054343255&quot;
## [1] &quot;step =  1  lambda =  0.0323869407729071  loss:  58.9489705847382&quot;
## [1] &quot;step =  2  lambda =  0.0323869407729071  loss:  59.0808827176274&quot;
## [1] &quot;step =  1  lambda =  0.0320646853278608  loss:  58.4942051965205&quot;
## [1] &quot;step =  2  lambda =  0.0320646853278608  loss:  58.6247172376893&quot;
## [1] &quot;step =  1  lambda =  0.0317456363780679  loss:  58.0425657687856&quot;
## [1] &quot;step =  2  lambda =  0.0317456363780679  loss:  58.1716929387924&quot;
## [1] &quot;step =  1  lambda =  0.0314297620183677  loss:  57.5940364041838&quot;
## [1] &quot;step =  2  lambda =  0.0314297620183677  loss:  57.7217937540567&quot;
## [1] &quot;step =  1  lambda =  0.0311170306610609  loss:  57.1486011943248&quot;
## [1] &quot;step =  2  lambda =  0.0311170306610609  loss:  57.2750036065814&quot;
## [1] &quot;step =  1  lambda =  0.0308074110327511  loss:  56.7062442209073&quot;
## [1] &quot;step =  2  lambda =  0.0308074110327511  loss:  56.8313064105236&quot;
## [1] &quot;step =  1  lambda =  0.0305008721712175  loss:  56.2669495567873&quot;
## [1] &quot;step =  2  lambda =  0.0305008721712175  loss:  56.3906860722818&quot;
## [1] &quot;step =  1  lambda =  0.0301973834223185  loss:  55.8307012671498&quot;
## [1] &quot;step =  2  lambda =  0.0301973834223185  loss:  55.9531264918422&quot;
## [1] &quot;step =  1  lambda =  0.0298969144369263  loss:  55.397483410842&quot;
## [1] &quot;step =  2  lambda =  0.0298969144369263  loss:  55.5186115642896&quot;
## [1] &quot;step =  1  lambda =  0.029599435167892  loss:  54.9672800418694&quot;
## [1] &quot;step =  2  lambda =  0.029599435167892  loss:  55.087125181461&quot;
## [1] &quot;step =  1  lambda =  0.0293049158670407  loss:  54.5400752110324&quot;
## [1] &quot;step =  2  lambda =  0.0293049158670407  loss:  54.6586512337106&quot;
## [1] &quot;step =  1  lambda =  0.0290133270821971  loss:  54.1158529676746&quot;
## [1] &quot;step =  2  lambda =  0.0290133270821971  loss:  54.2331736117547&quot;
## [1] &quot;step =  1  lambda =  0.0287246396542394  loss:  53.6945973615086&quot;
## [1] &quot;step =  2  lambda =  0.0287246396542394  loss:  53.8106762085697&quot;
## [1] &quot;step =  1  lambda =  0.0284388247141845  loss:  53.2762924444955&quot;
## [1] &quot;step =  2  lambda =  0.0284388247141845  loss:  53.391142921319&quot;
## [1] &quot;step =  1  lambda =  0.0281558536803001  loss:  52.8609222727524&quot;
## [1] &quot;step =  2  lambda =  0.0281558536803001  loss:  52.9745576532895&quot;
## [1] &quot;step =  1  lambda =  0.027875698255247  loss:  52.4484709084702&quot;
## [1] &quot;step =  2  lambda =  0.027875698255247  loss:  52.5609043158237&quot;
## [1] &quot;step =  1  lambda =  0.0275983304232493  loss:  52.0389224218255&quot;
## [1] &quot;step =  2  lambda =  0.0275983304232493  loss:  52.1501668302334&quot;
## [1] &quot;step =  1  lambda =  0.0273237224472926  loss:  51.6322608928763&quot;
## [1] &quot;step =  2  lambda =  0.0273237224472926  loss:  51.7423291296883&quot;
## [1] &quot;step =  1  lambda =  0.0270518468663504  loss:  51.2284704134308&quot;
## [1] &quot;step =  2  lambda =  0.0270518468663504  loss:  51.33737516107&quot;
## [1] &quot;step =  1  lambda =  0.0267826764926382  loss:  50.8275350888835&quot;
## [1] &quot;step =  2  lambda =  0.0267826764926382  loss:  50.935288886788&quot;
## [1] &quot;step =  1  lambda =  0.0265161844088942  loss:  50.4294390400133&quot;
## [1] &quot;step =  2  lambda =  0.0265161844088942  loss:  50.5360542865523&quot;
## [1] &quot;step =  1  lambda =  0.026252343965688  loss:  50.034166404738&quot;
## [1] &quot;step =  2  lambda =  0.026252343965688  loss:  50.1396553591014&quot;
## [1] &quot;step =  1  lambda =  0.0259911287787554  loss:  49.641701339825&quot;
## [1] &quot;step =  2  lambda =  0.0259911287787554  loss:  49.7460761238815&quot;
## [1] &quot;step =  1  lambda =  0.0257325127263599  loss:  49.2520280225543&quot;
## [1] &quot;step =  2  lambda =  0.0257325127263599  loss:  49.3553006226783&quot;
## [1] &quot;step =  1  lambda =  0.025476469946681  loss:  48.8651306523336&quot;
## [1] &quot;step =  2  lambda =  0.025476469946681  loss:  48.9673129211987&quot;
## [1] &quot;step =  1  lambda =  0.0252229748352272  loss:  48.4809934522644&quot;
## [1] &quot;step =  2  lambda =  0.0252229748352272  loss:  48.5820971106031&quot;
## [1] &quot;step =  1  lambda =  0.0249720020422762  loss:  48.0996006706588&quot;
## [1] &quot;step =  2  lambda =  0.0249720020422762  loss:  48.1996373089879&quot;
## [1] &quot;step =  1  lambda =  0.0247235264703394  loss:  47.7209365825078&quot;
## [1] &quot;step =  2  lambda =  0.0247235264703394  loss:  47.8199176628201&quot;
## [1] &quot;step =  1  lambda =  0.0244775232716527  loss:  47.3449854909008&quot;
## [1] &quot;step =  2  lambda =  0.0244775232716527  loss:  47.4429223483225&quot;
## [1] &quot;step =  1  lambda =  0.0242339678456911  loss:  46.9717317283975&quot;
## [1] &quot;step =  2  lambda =  0.0242339678456911  loss:  47.0686355728118&quot;
## [1] &quot;step =  1  lambda =  0.0239928358367092  loss:  46.6011596583524&quot;
## [1] &quot;step =  2  lambda =  0.0239928358367092  loss:  46.6970415759905&quot;
## [1] &quot;step =  1  lambda =  0.023754103131305  loss:  46.233253676194&quot;
## [1] &quot;step =  2  lambda =  0.023754103131305  loss:  46.3281246311935&quot;
## [1] &quot;step =  1  lambda =  0.0235177458560091  loss:  45.867998210659&quot;
## [1] &quot;step =  2  lambda =  0.0235177458560091  loss:  45.9618690465905&quot;
## [1] &quot;step =  1  lambda =  0.023283740374897  loss:  45.505377724982&quot;
## [1] &quot;step =  2  lambda =  0.023283740374897  loss:  45.5982591663448&quot;
## [1] &quot;step =  1  lambda =  0.0230520632872256  loss:  45.145376718044&quot;
## [1] &quot;step =  2  lambda =  0.0230520632872256  loss:  45.2372793717321&quot;
## [1] &quot;step =  1  lambda =  0.022822691425093  loss:  44.7879797254785&quot;
## [1] &quot;step =  2  lambda =  0.022822691425093  loss:  44.8789140822177&quot;
## [1] &quot;step =  1  lambda =  0.0225956018511219  loss:  44.4331713207393&quot;
## [1] &quot;step =  2  lambda =  0.0225956018511219  loss:  44.5231477564949&quot;
## [1] &quot;step =  1  lambda =  0.0223707718561656  loss:  44.0809361161275&quot;
## [1] &quot;step =  2  lambda =  0.0223707718561656  loss:  44.1699648934862&quot;
## [1] &quot;step =  1  lambda =  0.0221481789570373  loss:  43.7312587637833&quot;
## [1] &quot;step =  2  lambda =  0.0221481789570373  loss:  43.819350033308&quot;
## [1] &quot;step =  1  lambda =  0.0219278008942616  loss:  43.3841239566407&quot;
## [1] &quot;step =  2  lambda =  0.0219278008942616  loss:  43.4712877581996&quot;
## [1] &quot;step =  1  lambda =  0.0217096156298486  loss:  43.0395164293475&quot;
## [1] &quot;step =  2  lambda =  0.0217096156298486  loss:  43.1257626934196&quot;
## [1] &quot;step =  1  lambda =  0.0214936013450899  loss:  42.6974209591522&quot;
## [1] &quot;step =  2  lambda =  0.0214936013450899  loss:  42.7827595081078&quot;
## [1] &quot;step =  1  lambda =  0.0212797364383772  loss:  42.357822366758&quot;
## [1] &quot;step =  2  lambda =  0.0212797364383772  loss:  42.4422629161179&quot;
## [1] &quot;step =  1  lambda =  0.0210679995230414  loss:  42.0207055171463&quot;
## [1] &quot;step =  2  lambda =  0.0210679995230414  loss:  42.1042576768176&quot;
## [1] &quot;step =  1  lambda =  0.0208583694252147  loss:  41.6860553203697&quot;
## [1] &quot;step =  2  lambda =  0.0208583694252147  loss:  41.7687285958616&quot;
## [1] &quot;step =  1  lambda =  0.0206508251817126  loss:  41.3538567323164&quot;
## [1] &quot;step =  2  lambda =  0.0206508251817126  loss:  41.4356605259347&quot;
## [1] &quot;step =  1  lambda =  0.0204453460379377  loss:  41.0240947554464&quot;
## [1] &quot;step =  2  lambda =  0.0204453460379377  loss:  41.1050383674689&quot;
## [1] &quot;step =  1  lambda =  0.0202419114458044  loss:  40.6967544395012&quot;
## [1] &quot;step =  2  lambda =  0.0202419114458044  loss:  40.7768470693341&quot;
## [1] &quot;step =  1  lambda =  0.020040501061684  loss:  40.3718208821872&quot;
## [1] &quot;step =  2  lambda =  0.020040501061684  loss:  40.4510716295032&quot;
## [1] &quot;step =  1  lambda =  0.0198410947443703  loss:  40.0492792298351&quot;
## [1] &quot;step =  2  lambda =  0.0198410947443703  loss:  40.1276970956938&quot;
## [1] &quot;step =  1  lambda =  0.0196436725530653  loss:  39.7291146780338&quot;
## [1] &quot;step =  2  lambda =  0.0196436725530653  loss:  39.8067085659851&quot;
## [1] &quot;step =  1  lambda =  0.0194482147453854  loss:  39.4113124722424&quot;
## [1] &quot;step =  2  lambda =  0.0194482147453854  loss:  39.488091189414&quot;
## [1] &quot;step =  1  lambda =  0.0192547017753869  loss:  39.0958579083793&quot;
## [1] &quot;step =  2  lambda =  0.0192547017753869  loss:  39.1718301665475&quot;
## [1] &quot;step =  1  lambda =  0.0190631142916116  loss:  38.7827363333897&quot;
## [1] &quot;step =  2  lambda =  0.0190631142916116  loss:  38.8579107500357&quot;
## [1] &quot;step =  1  lambda =  0.0188734331351515  loss:  38.471933145792&quot;
## [1] &quot;step =  2  lambda =  0.0188734331351515  loss:  38.5463182451434&quot;
## [1] &quot;step =  1  lambda =  0.0186856393377328  loss:  38.163433796205&quot;
## [1] &quot;step =  2  lambda =  0.0186856393377328  loss:  38.2370380102621&quot;
## [1] &quot;step =  1  lambda =  0.0184997141198192  loss:  37.8572237878541&quot;
## [1] &quot;step =  2  lambda =  0.0184997141198192  loss:  37.9300554574035&quot;
## [1] &quot;step =  1  lambda =  0.0183156388887342  loss:  37.5532886770602&quot;
## [1] &quot;step =  2  lambda =  0.0183156388887342  loss:  37.6253560526742&quot;
## [1] &quot;step =  1  lambda =  0.0181333952368011  loss:  37.2516140737096&quot;
## [1] &quot;step =  2  lambda =  0.0181333952368011  loss:  37.3229253167329&quot;
## [1] &quot;step =  1  lambda =  0.0179529649395029  loss:  36.952185641706&quot;
## [1] &quot;step =  2  lambda =  0.0179529649395029  loss:  37.0227488252295&quot;
## [1] &quot;step =  1  lambda =  0.0177743299536594  loss:  36.6549890994059&quot;
## [1] &quot;step =  2  lambda =  0.0177743299536594  loss:  36.7248122092285&quot;
## [1] &quot;step =  1  lambda =  0.0175974724156234  loss:  36.3600102200376&quot;
## [1] &quot;step =  2  lambda =  0.0175974724156234  loss:  36.4291011556158&quot;
## [1] &quot;step =  1  lambda =  0.0174223746394935  loss:  36.0672348321032&quot;
## [1] &quot;step =  2  lambda =  0.0174223746394935  loss:  36.1356014074892&quot;
## [1] &quot;step =  1  lambda =  0.0172490191153463  loss:  35.776648819766&quot;
## [1] &quot;step =  2  lambda =  0.0172490191153463  loss:  35.8442987645345&quot;
## [1] &quot;step =  1  lambda =  0.0170773885074848  loss:  35.4882381232221&quot;
## [1] &quot;step =  2  lambda =  0.0170773885074848  loss:  35.5551790833859&quot;
## [1] &quot;step =  1  lambda =  0.0169074656527053  loss:  35.2019887390579&quot;
## [1] &quot;step =  2  lambda =  0.0169074656527053  loss:  35.2682282779725&quot;
## [1] &quot;step =  1  lambda =  0.0167392335585806  loss:  34.9178867205924&quot;
## [1] &quot;step =  2  lambda =  0.0167392335585806  loss:  34.9834323198506&quot;
## [1] &quot;step =  1  lambda =  0.0165726754017613  loss:  34.6359181782062&quot;
## [1] &quot;step =  2  lambda =  0.0165726754017613  loss:  34.7007772385221&quot;
## [1] &quot;step =  1  lambda =  0.0164077745262926  loss:  34.3560692796571&quot;
## [1] &quot;step =  2  lambda =  0.0164077745262926  loss:  34.4202491217397&quot;
## [1] &quot;step =  1  lambda =  0.0162445144419499  loss:  34.0783262503818&quot;
## [1] &quot;step =  2  lambda =  0.0162445144419499  loss:  34.1418341157995&quot;
## [1] &quot;step =  1  lambda =  0.0160828788225884  loss:  33.8026753737856&quot;
## [1] &quot;step =  2  lambda =  0.0160828788225884  loss:  33.86551842582&quot;
## [1] &quot;step =  1  lambda =  0.0159228515045117  loss:  33.5291029915186&quot;
## [1] &quot;step =  2  lambda =  0.0159228515045117  loss:  33.5912883160099&quot;
## [1] &quot;step =  1  lambda =  0.0157644164848545  loss:  33.2575955037409&quot;
## [1] &quot;step =  2  lambda =  0.0157644164848545  loss:  33.3191301099228&quot;
## [1] &quot;step =  1  lambda =  0.0156075579199828  loss:  32.9881393693743&quot;
## [1] &quot;step =  2  lambda =  0.0156075579199828  loss:  33.0490301907011&quot;
## [1] &quot;step =  1  lambda =  0.0154522601239095  loss:  32.7207211063442&quot;
## [1] &quot;step =  2  lambda =  0.0154522601239095  loss:  32.7809750013077&quot;
## [1] &quot;step =  1  lambda =  0.0152985075667255  loss:  32.4553272918084&quot;
## [1] &quot;step =  2  lambda =  0.0152985075667255  loss:  32.5149510447465&quot;
## [1] &quot;step =  1  lambda =  0.015146284873047  loss:  32.1919445623758&quot;
## [1] &quot;step =  2  lambda =  0.015146284873047  loss:  32.2509448842725&quot;
## [1] &quot;step =  1  lambda =  0.0149955768204777  loss:  31.9305596143141&quot;
## [1] &quot;step =  2  lambda =  0.0149955768204777  loss:  31.9889431435907&quot;
## [1] &quot;step =  1  lambda =  0.0148463683380868  loss:  31.6711592037468&quot;
## [1] &quot;step =  2  lambda =  0.0148463683380868  loss:  31.7289325070448&quot;
## [1] &quot;step =  1  lambda =  0.0146986445049018  loss:  31.4137301468398&quot;
## [1] &quot;step =  2  lambda =  0.0146986445049018  loss:  31.4708997197953&quot;
## [1] &quot;step =  1  lambda =  0.0145523905484161  loss:  31.1582593199775&quot;
## [1] &quot;step =  2  lambda =  0.0145523905484161  loss:  31.2148315879877&quot;
## [1] &quot;step =  1  lambda =  0.0144075918431123  loss:  30.9047336599297&quot;
## [1] &quot;step =  2  lambda =  0.0144075918431123  loss:  30.9607149789111&quot;
## [1] &quot;step =  1  lambda =  0.0142642339089993  loss:  30.6531401640085&quot;
## [1] &quot;step =  2  lambda =  0.0142642339089993  loss:  30.708536821147&quot;
## [1] &quot;step =  1  lambda =  0.014122302410164  loss:  30.4034658902151&quot;
## [1] &quot;step =  2  lambda =  0.014122302410164  loss:  30.4582841047087&quot;
## [1] &quot;step =  1  lambda =  0.0139817831533383  loss:  30.1556979573781&quot;
## [1] &quot;step =  2  lambda =  0.0139817831533383  loss:  30.2099438811712&quot;
## [1] &quot;step =  1  lambda =  0.0138426620864795  loss:  29.9098235452819&quot;
## [1] &quot;step =  2  lambda =  0.0138426620864795  loss:  29.9635032637926&quot;
## [1] &quot;step =  1  lambda =  0.0137049252973649  loss:  29.6658298947873&quot;
## [1] &quot;step =  2  lambda =  0.0137049252973649  loss:  29.7189494276264&quot;
## [1] &quot;step =  1  lambda =  0.0135685590122009  loss:  29.4237043079415&quot;
## [1] &quot;step =  2  lambda =  0.0135685590122009  loss:  29.4762696096245&quot;
## [1] &quot;step =  1  lambda =  0.0134335495942453  loss:  29.1834341480815&quot;
## [1] &quot;step =  2  lambda =  0.0134335495942453  loss:  29.2354511087331&quot;
## [1] &quot;step =  1  lambda =  0.0132998835424438  loss:  28.9450068399277&quot;
## [1] &quot;step =  2  lambda =  0.0132998835424438  loss:  28.9964812859787&quot;
## [1] &quot;step =  1  lambda =  0.0131675474900798  loss:  28.7084098696697&quot;
## [1] &quot;step =  2  lambda =  0.0131675474900798  loss:  28.7593475645468&quot;
## [1] &quot;step =  1  lambda =  0.0130365282034377  loss:  28.4736307850439&quot;
## [1] &quot;step =  2  lambda =  0.0130365282034377  loss:  28.5240374298525&quot;
## [1] &quot;step =  1  lambda =  0.0129068125804799  loss:  28.2406571954034&quot;
## [1] &quot;step =  2  lambda =  0.0129068125804799  loss:  28.2905384296032&quot;
## [1] &quot;step =  1  lambda =  0.0127783876495358  loss:  28.0094767717799&quot;
## [1] &quot;step =  2  lambda =  0.0127783876495358  loss:  28.0588381738532&quot;
## [1] &quot;step =  1  lambda =  0.0126512405680053  loss:  27.7800772469379&quot;
## [1] &quot;step =  2  lambda =  0.0126512405680053  loss:  27.828924335051&quot;
## [1] &quot;step =  1  lambda =  0.0125253586210744  loss:  27.5524464154215&quot;
## [1] &quot;step =  2  lambda =  0.0125253586210744  loss:  27.6007846480798&quot;
## [1] &quot;step =  1  lambda =  0.0124007292204434  loss:  27.3265721335938&quot;
## [1] &quot;step =  2  lambda =  0.0124007292204434  loss:  27.3744069102894&quot;
## [1] &quot;step =  1  lambda =  0.0122773399030684  loss:  27.1024423196695&quot;
## [1] &quot;step =  2  lambda =  0.0122773399030684  loss:  27.149778981522&quot;
## [1] &quot;step =  1  lambda =  0.0121551783299149  loss:  26.8800449537394&quot;
## [1] &quot;step =  2  lambda =  0.0121551783299149  loss:  26.9268887841306&quot;
## [1] &quot;step =  1  lambda =  0.0120342322847238  loss:  26.6593680777892&quot;
## [1] &quot;step =  2  lambda =  0.0120342322847238  loss:  26.7057243029909&quot;
## [1] &quot;step =  1  lambda =  0.0119144896727896  loss:  26.4403997957108&quot;
## [1] &quot;step =  2  lambda =  0.0119144896727896  loss:  26.4862735855055&quot;
## [1] &quot;step =  1  lambda =  0.0117959385197516  loss:  26.2231282733068&quot;
## [1] &quot;step =  2  lambda =  0.0117959385197516  loss:  26.2685247416029&quot;
## [1] &quot;step =  1  lambda =  0.0116785669703954  loss:  26.0075417382889&quot;
## [1] &quot;step =  2  lambda =  0.0116785669703954  loss:  26.0524659437287&quot;
## [1] &quot;step =  1  lambda =  0.0115623632874685  loss:  25.7936284802698&quot;
## [1] &quot;step =  2  lambda =  0.0115623632874685  loss:  25.8380854268313&quot;
## [1] &quot;step =  1  lambda =  0.0114473158505057  loss:  25.5813768507484&quot;
## [1] &quot;step =  2  lambda =  0.0114473158505057  loss:  25.6253714883409&quot;
## [1] &quot;step =  1  lambda =  0.0113334131546674  loss:  25.3707752630894&quot;
## [1] &quot;step =  2  lambda =  0.0113334131546674  loss:  25.4143124881427&quot;
## [1] &quot;step =  1  lambda =  0.0112206438095891  loss:  25.1618121924965&quot;
## [1] &quot;step =  2  lambda =  0.0112206438095891  loss:  25.204896848544&quot;
## [1] &quot;step =  1  lambda =  0.0111089965382423  loss:  24.9544761759795&quot;
## [1] &quot;step =  2  lambda =  0.0111089965382423  loss:  24.9971130542353&quot;
## [1] &quot;step =  1  lambda =  0.0109984601758069  loss:  24.7487558123162&quot;
## [1] &quot;step =  2  lambda =  0.0109984601758069  loss:  24.7909496522455&quot;
## [1] &quot;step =  1  lambda =  0.0108890236685545  loss:  24.5446397620081&quot;
## [1] &quot;step =  2  lambda =  0.0108890236685545  loss:  24.5863952518922&quot;
## [1] &quot;step =  1  lambda =  0.0107806760727431  loss:  24.3421167472303&quot;
## [1] &quot;step =  2  lambda =  0.0107806760727431  loss:  24.3834385247255&quot;
## [1] &quot;step =  1  lambda =  0.0106734065535229  loss:  24.1411755517768&quot;
## [1] &quot;step =  2  lambda =  0.0106734065535229  loss:  24.1820682044671&quot;
## [1] &quot;step =  1  lambda =  0.0105672043838527  loss:  23.9418050209994&quot;
## [1] &quot;step =  2  lambda =  0.0105672043838527  loss:  23.9822730869437&quot;
## [1] &quot;step =  1  lambda =  0.0104620589434268  loss:  23.7439940617423&quot;
## [1] &quot;step =  2  lambda =  0.0104620589434268  loss:  23.7840420300152&quot;
## [1] &quot;step =  1  lambda =  0.0103579597176137  loss:  23.5477316422703&quot;
## [1] &quot;step =  2  lambda =  0.0103579597176137  loss:  23.5873639534981&quot;
## [1] &quot;step =  1  lambda =  0.010254896296404  loss:  23.3530067921937&quot;
## [1] &quot;step =  2  lambda =  0.010254896296404  loss:  23.3922278390837&quot;
## [1] &quot;step =  1  lambda =  0.0101528583733698  loss:  23.1598086023865&quot;
## [1] &quot;step =  2  lambda =  0.0101528583733698  loss:  23.1986227302511&quot;
## [1] &quot;step =  1  lambda =  0.0100518357446336  loss:  22.9681262249009&quot;
## [1] &quot;step =  2  lambda =  0.0100518357446336  loss:  23.0065377321763&quot;
## [1] &quot;step =  1  lambda =  0.00995181830784842  loss:  22.7779488728766&quot;
## [1] &quot;step =  2  lambda =  0.00995181830784842  loss:  22.8159620116353&quot;
## [1] &quot;step =  1  lambda =  0.00985279606118726  loss:  22.5892658204455&quot;
## [1] &quot;step =  2  lambda =  0.00985279606118726  loss:  22.6268847969042&quot;
## [1] &quot;step =  1  lambda =  0.0097547591023429  loss:  22.4020664026318&quot;
## [1] &quot;step =  2  lambda =  0.0097547591023429  loss:  22.4392953776533&quot;
## [1] &quot;step =  1  lambda =  0.00965769762753778  loss:  22.2163400152482&quot;
## [1] &quot;step =  2  lambda =  0.00965769762753778  loss:  22.2531831048375&quot;
## [1] &quot;step =  1  lambda =  0.00956160193054351  loss:  22.0320761147868&quot;
## [1] &quot;step =  2  lambda =  0.00956160193054351  loss:  22.0685373905827&quot;
## [1] &quot;step =  1  lambda =  0.00946646240171032  loss:  21.8492642183064&quot;
## [1] &quot;step =  2  lambda =  0.00946646240171032  loss:  21.8853477080675&quot;
## [1] &quot;step =  1  lambda =  0.00937226952700606  loss:  21.6678939033155&quot;
## [1] &quot;step =  2  lambda =  0.00937226952700606  loss:  21.7036035914009&quot;
## [1] &quot;step =  1  lambda =  0.00927901388706474  loss:  21.4879548076514&quot;
## [1] &quot;step =  2  lambda =  0.00927901388706474  loss:  21.5232946354959&quot;
## [1] &quot;step =  1  lambda =  0.00918668615624467  loss:  21.3094366293546&quot;
## [1] &quot;step =  2  lambda =  0.00918668615624467  loss:  21.3444104959395&quot;
## [1] &quot;step =  1  lambda =  0.00909527710169582  loss:  21.1323291265405&quot;
## [1] &quot;step =  2  lambda =  0.00909527710169582  loss:  21.1669408888584&quot;
## [1] &quot;step =  1  lambda =  0.00900477758243656  loss:  20.956622117266&quot;
## [1] &quot;step =  2  lambda =  0.00900477758243656  loss:  20.9908755907815&quot;
## [1] &quot;step =  1  lambda =  0.00891517854843955  loss:  20.7823054793936&quot;
## [1] &quot;step =  2  lambda =  0.00891517854843955  loss:  20.8162044384981&quot;
## [1] &quot;step =  1  lambda =  0.00882647103972673  loss:  20.6093691504508&quot;
## [1] &quot;step =  2  lambda =  0.00882647103972673  loss:  20.6429173289131&quot;
## [1] &quot;step =  1  lambda =  0.00873864618547329  loss:  20.4378031274865&quot;
## [1] &quot;step =  2  lambda =  0.00873864618547329  loss:  20.4710042188982&quot;
## [1] &quot;step =  1  lambda =  0.00865169520312063  loss:  20.2675974669241&quot;
## [1] &quot;step =  2  lambda =  0.00865169520312063  loss:  20.3004551251399&quot;
## [1] &quot;step =  1  lambda =  0.00856560939749806  loss:  20.098742284411&quot;
## [1] &quot;step =  2  lambda =  0.00856560939749806  loss:  20.1312601239846&quot;
## [1] &quot;step =  1  lambda =  0.00848038015995327  loss:  19.9312277546643&quot;
## [1] &quot;step =  2  lambda =  0.00848038015995327  loss:  19.9634093512795&quot;
## [1] &quot;step =  1  lambda =  0.00839599896749147  loss:  19.7650441113144&quot;
## [1] &quot;step =  2  lambda =  0.00839599896749147  loss:  19.7968930022111&quot;
## [1] &quot;step =  1  lambda =  0.00831245738192312  loss:  19.6001816467444&quot;
## [1] &quot;step =  2  lambda =  0.00831245738192312  loss:  19.6317013311406&quot;
## [1] &quot;step =  1  lambda =  0.00822974704902003  loss:  19.4366307119272&quot;
## [1] &quot;step =  2  lambda =  0.00822974704902003  loss:  19.4678246514354&quot;
## [1] &quot;step =  1  lambda =  0.00814785969767999  loss:  19.2743817162588&quot;
## [1] &quot;step =  2  lambda =  0.00814785969767999  loss:  19.3052533352992&quot;
## [1] &quot;step =  1  lambda =  0.00806678713909961  loss:  19.1134251273896&quot;
## [1] &quot;step =  2  lambda =  0.00806678713909961  loss:  19.1439778135975&quot;
## [1] &quot;step =  1  lambda =  0.0079865212659555  loss:  18.9537514710524&quot;
## [1] &quot;step =  2  lambda =  0.0079865212659555  loss:  18.9839885756819&quot;
## [1] &quot;step =  1  lambda =  0.00790705405159344  loss:  18.7953513308876&quot;
## [1] &quot;step =  2  lambda =  0.00790705405159344  loss:  18.8252761692104&quot;
## [1] &quot;step =  1  lambda =  0.00782837754922577  loss:  18.6382153482656&quot;
## [1] &quot;step =  2  lambda =  0.00782837754922577  loss:  18.6678311999659&quot;
## [1] &quot;step =  1  lambda =  0.00775048389113669  loss:  18.482334222107&quot;
## [1] &quot;step =  2  lambda =  0.00775048389113669  loss:  18.5116443316715&quot;
## [1] &quot;step =  1  lambda =  0.00767336528789549  loss:  18.3276987087002&quot;
## [1] &quot;step =  2  lambda =  0.00767336528789549  loss:  18.3567062858044&quot;
## [1] &quot;step =  1  lambda =  0.00759701402757757  loss:  18.1742996215159&quot;
## [1] &quot;step =  2  lambda =  0.00759701402757757  loss:  18.2030078414057&quot;
## [1] &quot;step =  1  lambda =  0.00752142247499327  loss:  18.0221278310201&quot;
## [1] &quot;step =  2  lambda =  0.00752142247499327  loss:  18.0505398348892&quot;
## [1] &quot;step =  1  lambda =  0.00744658307092434  loss:  17.871174264484&quot;
## [1] &quot;step =  2  lambda =  0.00744658307092434  loss:  17.8992931598475&quot;
## [1] &quot;step =  1  lambda =  0.00737248833136801  loss:  17.721429905792&quot;
## [1] &quot;step =  2  lambda =  0.00737248833136801  loss:  17.7492587668557&quot;
## [1] &quot;step =  1  lambda =  0.00729913084678858  loss:  17.5728857952478&quot;
## [1] &quot;step =  2  lambda =  0.00729913084678858  loss:  17.6004276632731&quot;
## [1] &quot;step =  1  lambda =  0.00722650328137646  loss:  17.4255330293774&quot;
## [1] &quot;step =  2  lambda =  0.00722650328137646  loss:  17.4527909130427&quot;
## [1] &quot;step =  1  lambda =  0.00715459837231459  loss:  17.2793627607309&quot;
## [1] &quot;step =  2  lambda =  0.00715459837231459  loss:  17.3063396364887&quot;
## [1] &quot;step =  1  lambda =  0.00708340892905212  loss:  17.1343661976823&quot;
## [1] &quot;step =  2  lambda =  0.00708340892905212  loss:  17.1610650101122&quot;
## [1] &quot;step =  1  lambda =  0.00701292783258542  loss:  16.9905346042265&quot;
## [1] &quot;step =  2  lambda =  0.00701292783258542  loss:  17.0169582663847&quot;
## [1] &quot;step =  1  lambda =  0.00694314803474611  loss:  16.8478592997754&quot;
## [1] &quot;step =  2  lambda =  0.00694314803474611  loss:  16.8740106935401&quot;
## [1] &quot;step =  1  lambda =  0.00687406255749626  loss:  16.7063316589514&quot;
## [1] &quot;step =  2  lambda =  0.00687406255749626  loss:  16.7322136353644&quot;
## [1] &quot;step =  1  lambda =  0.00680566449223054  loss:  16.5659431113797&quot;
## [1] &quot;step =  2  lambda =  0.00680566449223054  loss:  16.5915584909842&quot;
## [1] &quot;step =  1  lambda =  0.00673794699908547  loss:  16.4266851414785&quot;
## [1] &quot;step =  2  lambda =  0.00673794699908547  loss:  16.452036714653&quot;
## [1] &quot;step =  1  lambda =  0.00667090330625527  loss:  16.2885492882479&quot;
## [1] &quot;step =  2  lambda =  0.00667090330625527  loss:  16.3136398155368&quot;
## [1] &quot;step =  1  lambda =  0.00660452670931481  loss:  16.1515271450565&quot;
## [1] &quot;step =  2  lambda =  0.00660452670931481  loss:  16.1763593574966&quot;
## [1] &quot;step =  1  lambda =  0.00653881057054906  loss:  16.0156103594275&quot;
## [1] &quot;step =  2  lambda =  0.00653881057054906  loss:  16.0401869588713&quot;
## [1] &quot;step =  1  lambda =  0.0064737483182894  loss:  15.8807906328225&quot;
## [1] &quot;step =  2  lambda =  0.0064737483182894  loss:  15.9051142922573&quot;
## [1] &quot;step =  1  lambda =  0.00640933344625638  loss:  15.7470597204239&quot;
## [1] &quot;step =  2  lambda =  0.00640933344625638  loss:  15.7711330842881&quot;
## [1] &quot;step =  1  lambda =  0.00634555951290912  loss:  15.6144094309165&quot;
## [1] &quot;step =  2  lambda =  0.00634555951290912  loss:  15.6382351154119&quot;
## [1] &quot;step =  1  lambda =  0.00628242014080112  loss:  15.4828316262672&quot;
## [1] &quot;step =  2  lambda =  0.00628242014080112  loss:  15.5064122196676&quot;
## [1] &quot;step =  1  lambda =  0.00621990901594257  loss:  15.3523182215037&quot;
## [1] &quot;step =  2  lambda =  0.00621990901594257  loss:  15.3756562844609&quot;
## [1] &quot;step =  1  lambda =  0.0061580198871689  loss:  15.2228611844916&quot;
## [1] &quot;step =  2  lambda =  0.0061580198871689  loss:  15.2459592503371&quot;
## [1] &quot;step =  1  lambda =  0.00609674656551564  loss:  15.0944525357109&quot;
## [1] &quot;step =  2  lambda =  0.00609674656551564  loss:  15.1173131107551&quot;
## [1] &quot;step =  1  lambda =  0.00603608292359956  loss:  14.9670843480311&quot;
## [1] &quot;step =  2  lambda =  0.00603608292359956  loss:  14.9897099118582&quot;
## [1] &quot;step =  1  lambda =  0.00597602289500594  loss:  14.840748746485&quot;
## [1] &quot;step =  2  lambda =  0.00597602289500594  loss:  14.8631417522454&quot;
## [1] &quot;step =  1  lambda =  0.00591656047368186  loss:  14.7154379080415&quot;
## [1] &quot;step =  2  lambda =  0.00591656047368186  loss:  14.7376007827406&quot;
## [1] &quot;step =  1  lambda =  0.00585768971333562  loss:  14.5911440613781&quot;
## [1] &quot;step =  2  lambda =  0.00585768971333562  loss:  14.613079206162&quot;
## [1] &quot;step =  1  lambda =  0.00579940472684215  loss:  14.4678594866512&quot;
## [1] &quot;step =  2  lambda =  0.00579940472684215  loss:  14.4895692770892&quot;
## [1] &quot;step =  1  lambda =  0.0057416996856542  loss:  14.3455765152669&quot;
## [1] &quot;step =  2  lambda =  0.0057416996856542  loss:  14.3670633016305&quot;
## [1] &quot;step =  1  lambda =  0.0056845688192196  loss:  14.2242875296495&quot;
## [1] &quot;step =  2  lambda =  0.0056845688192196  loss:  14.2455536371892&quot;
## [1] &quot;step =  1  lambda =  0.00562800641440407  loss:  14.1039849630108&quot;
## [1] &quot;step =  2  lambda =  0.00562800641440407  loss:  14.1250326922286&quot;
## [1] &quot;step =  1  lambda =  0.00557200681492  loss:  13.9846612991168&quot;
## [1] &quot;step =  2  lambda =  0.00557200681492  loss:  14.0054929260367&quot;
## [1] &quot;step =  1  lambda =  0.00551656442076077  loss:  13.8663090720555&quot;
## [1] &quot;step =  2  lambda =  0.00551656442076077  loss:  13.8869268484904&quot;
## [1] &quot;step =  1  lambda =  0.00546167368764078  loss:  13.7489208660025&quot;
## [1] &quot;step =  2  lambda =  0.00546167368764078  loss:  13.7693270198184&quot;
## [1] &quot;step =  1  lambda =  0.00540732912644096  loss:  13.632489314987&quot;
## [1] &quot;step =  2  lambda =  0.00540732912644096  loss:  13.652686050364&quot;
## [1] &quot;step =  1  lambda =  0.00535352530265991  loss:  13.5170071026565&quot;
## [1] &quot;step =  2  lambda =  0.00535352530265991  loss:  13.5369966003471&quot;
## [1] &quot;step =  1  lambda =  0.0053002568358704  loss:  13.4024669620413&quot;
## [1] &quot;step =  2  lambda =  0.0053002568358704  loss:  13.4222513796258&quot;
## [1] &quot;step =  1  lambda =  0.00524751839918138  loss:  13.2888616753185&quot;
## [1] &quot;step =  2  lambda =  0.00524751839918138  loss:  13.3084431474575&quot;
## [1] &quot;step =  1  lambda =  0.00519530471870523  loss:  13.1761840735752&quot;
## [1] &quot;step =  2  lambda =  0.00519530471870523  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00514361057303038  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00509243079269919  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00504176025969098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00499159390691022  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00494192671767982  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00489275372523948  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00484407001224897  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00479587071029642  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00474815099941148  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00470090610758328  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00465413131028327  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00460782192999275  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0045619733357351  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00451658094261267  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00447164021134833  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00442714664783151  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00438309580266878  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0043394832707389  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00429630469075234  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00425355574481513  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00421123215799704  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00416932969790412  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00412784417425544  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00408677143846407  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0040461073832222  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00400584794209042  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00396598908909106  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00392652683830562  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00388745724347613  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00384877639761054  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00381048043259204  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00377256551879221  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00373502786468807  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00369786371648293  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00366106935773101  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00362464110896576  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00358857532733195  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00355286840622136  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00351751677491213  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00348251689821166  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00344786527610313  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00341355844339543  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00337959296937672  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00334596545747127  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00331267254489989  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00327971090234357  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00324707723361059  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00321476827530687  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00318278079650967  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00315111159844444  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00311975751416499  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00308871540823677  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00305798217642331  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00302755474537582  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00299743007232583  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00296760514478094  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00293807698022355  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00290884262581258  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00287989915808824  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00285124368267963  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00282287333401534  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00279478527503684  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00276697669691485  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00273944481876837  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00271218688738664  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00268520017695382  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00265848198877637  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0026320296510132  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0026058405184085  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00257991197202718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.002554241418993  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00252882629222926  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0025036640502021  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00247875217666636  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00245408818041392  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0024296695950246  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00240549397861951  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00238155891361687  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00235786200649023  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00233440088752913  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00231117321060213  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00228817665292217  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00226540891481432  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0022428677194858  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0022205508127983  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00219845596304253  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00217658096071513  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00215492361829761  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00213348177003771  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00211225327173271  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00209123600051511  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00207042785464026  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00204982675327624  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00202943063629574  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00200923746407006  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00198924521726516  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0019694518966397  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00194985552284512  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00193045413622771  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00191124579663264  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00189222858320994  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00187340059422243  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0018547599468555  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00183630477702891  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00181803323921027  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00179994350623059  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00178203376910149  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00176430223683434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00174674713626112  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00172936671185716  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00171215922556552  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00169512295662325  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00167825620138925  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00166155727317393  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00164502450207057  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00162865623478828  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00161245083448668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00159640668061225  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00158052216873622  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00156479571039417  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00154922573292716  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00153381067932446  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00151854900806788  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00150343919297757  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00148847972305943  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.001473669102354  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00145900584978686  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00144448849902054  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00143011559830787  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0014158857103468  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00140179741213667  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00138784929483593  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00137403996362121  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00136036803754789  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00134683214941197  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00133343094561336  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0013201630860205  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00130702724383639  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00129402210546585  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00128114637038421  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00126839875100724  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00125577797256237  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00124328277296124  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00123091190267348  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00121866412460175  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00120653821395804  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00119453295814118  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00118264715661557  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00117087962079117  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00115922917390459  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00114769465090143  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00113627489831977  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00112496877417484  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0011137751478448  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0011026928999577  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00109172092227951  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00108085811760332  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0010701033996396  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00105945569290761  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00104891393262779  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00103847706461533  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00102814404517473  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00101791384099544  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00100778542904851  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000997757796484312  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000987829940531229  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000978000868395395  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000968269597161403  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000958635153694021  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000949096574540873  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000939652905836096  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000930303203204949  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000921046531669378  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000911881965554516  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000902808588396114  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000893825492848894  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000884931780595815  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000876126562258242  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000867408957307003  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000858778093974336  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000850233109166719  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000841773148378549  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000833397365606696  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000825104923265905  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000816894992105029  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000808766751124111  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000800719387492281  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000792752096466468  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000784864081310932  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000777054553217582  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000769322731227101  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000761667842150847  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000754089120493534  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00074658580837668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00073915715546282  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000731802418880473  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000724520863149851  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000717311760109313  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000710174388842549  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000703108035606483  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000696111993759903  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000689185563692794  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000682328052756377  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000675538775193844  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000668817052071782  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000662162211212276  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000655573587125696  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000649050520944141  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000642592360355558  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000636198459538506  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000629868179097574  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000623600885999444  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000617395953509583  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000611252761129572  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000605170694535053  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000599149145514298  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000593187511907387  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000587285197545991  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000581441612193756  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000575656171487276  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00056992829687766  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000564257415572674  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000558642960479461  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000553084370147834  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000547581088714126  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000542132565845609  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000536738256685455  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000531397621798253  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000526110127116064  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000520875243885012  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000515692448612414  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000510561223014422  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0005054810539642  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000500451433440611  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00049547185847741  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000490541831112951  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000485660858340389  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00048082845205838  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000476044129022269  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000471307410795765  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000466617823703098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000461974898781651  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000457378171735063  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000452827182886797  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000448321477134178  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000443860603902874  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000439444117101845  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000435071575078732  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000430742540575688  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000426456580685654  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00042221326680907  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000418012174611013  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000413852883978762  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000409734978979787  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000405658047820157  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000401621682803358  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000397625480289526  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000393669040655078  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000389751968252755  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000385873871372051  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000382034362200047  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000378233056782626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000374469574986078  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000370743540459088  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000367054580595098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000363402326495048  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000359786412930483  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000356206478307034  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000352662164628256  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000349153117459826  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000345678985894105  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000342239422515039  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000338834083363426  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000335462627902512  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000332124718983941  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00032882002281404  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000325548208920438  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000322308950119019  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000319101922481203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000315926805301555  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000312783281065711  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000309671035418626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000306589757133144  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000303539138078867  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000300518873191348  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000297528660441581  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0002945682008058  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000291637198235574  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000288735359628203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000285862394797409  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000283018016444314  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000280201940128712  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000277413884240626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000274653569972143  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000271920721289535  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000269215064905658  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000266536330252618  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000263884249454718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000261258557301668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000258658991222064  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000256085291257132  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00025353720003473  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000251014462743614  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000248516827107952  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000246044043362099  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000243595864225619  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000241172044878559  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000238772342936964  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000236396518428641  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000234044333769158  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00023171555373809  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000229409945455492  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000227127278358615  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000224867324178848  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000222629856918889  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000220414652830147  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000218221490390368  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000216050150281479  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000213900415367661  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000211772070673631  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000209664903363145  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000207578702717718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000205513260115544  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000203468369010644  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000201443824912203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000199439425364123  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000197454969924779  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000195490260146975  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000193545099558094  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000191619293640457  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000189712649811868  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000187824977406354  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000185956087655102  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000184105793667579  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000182273910412845  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000180460254701048  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000178664645165106  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000176886902242567  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000175126848157658  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000173384306903506  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00017165910422453  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000169951067599028  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000168260026221911  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000166585810987634  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000164928254473277  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000163287190921808  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000161662456225505  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000160053887909543  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000158461325115751  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000156884608586522  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00015532358064889  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000153778085198759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000152247967685297  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000150733075095477  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000149233255938777  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000147748360232034  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000146278239484437  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000144822746682688  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000143381736276293  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000141955064163011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000140542587674442  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000139144165561759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000137759657981586  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000136388926482011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000135031833988743  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0001336882447914  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000132358024529944  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000131041040181239  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000129737160045754  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000128446253734388  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000127168192155434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012590284750167  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000124650093237575  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012340980408668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000122181856019035  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012096612623881  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000119762493172015  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000118570836454339  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000117391036919118  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000116222976585415  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000115066538646224  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000113921607456786  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000112788068523029  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000111665808490115  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000110554715131105  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000109454677335737  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000108365585099315  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000107287329511708  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000106219802746459  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000105162898050001  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000104116509730984  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000103080533149705  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000102054864707641  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000101039401837093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00010003404299093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.9038687632427e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.80532362252201e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.70775902233471e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.61116520613947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.51553251447417e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.42085138398996e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.32711234649488e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.23430602800707e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.14242314781733e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.05145451756109e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.96139104029952e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.87222370960982e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.78394360868463e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.69654190944029e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.61000987163404e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.52433884198997e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.43952025333374e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.3555456237358e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.27240655566322e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1900947351399e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1086019309152e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.02791999364078e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.94804085505568e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.86895652717947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.79065910151345e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.71314074824984e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.63639371548869e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.56041032846277e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.48518298877006e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.4107041736139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.33696643505071e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.26396239924518e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.1916847657329e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.12012630669027e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.04927986621178e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.97913835959434e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.90969477262882e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.84094216089865e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.77287364908539e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.70548243028111e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.63876176530778e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.5727049820433e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.50730547475429e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.44255670343554e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.37845219315595e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.31498553341106e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.25215037748203e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.18994044180088e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.12834950532221e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.06737140890104e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.00700005467694e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.94722940546415e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.88805348414794e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.82946637308688e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.77146221352103e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.71403520498611e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.65717960473339e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.60088972715548e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.54515994321769e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.48998467989523e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.43535841961575e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.38127569970771e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.32773111185406e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.27471930155139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.22223496757448e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.1702728614462e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.11882778691264e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.06789459942348e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.01746820561753e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.96754356281337e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.91811567850513e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.86917960986318e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.82073046323988e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.7727633936802e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.72527360443719e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.67825634649237e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.63170691808076e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.58562066422073e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.53999297624849e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0  loss:  NaN&quot;
## [1] &quot;fold:  3&quot;
## [1] &quot;step =  1  lambda =  22026.4657948067  loss:  71.1659522160834&quot;
## [1] &quot;step =  2  lambda =  22026.4657948067  loss:  70.9876371715585&quot;
## [1] &quot;step =  3  lambda =  22026.4657948067  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  21807.2987982302  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  21590.3125497062  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  21375.4853504291  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  21162.7957175002  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  20952.2223817786  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  20743.7442857556  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  20537.3405814475  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  20332.9906283122  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  20130.6739911839  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  19930.3704382303  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  19732.0599389292  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  19535.7226620655  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  19341.3389737478  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  19148.8894354453  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  18958.3548020439  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  18769.7160199212  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  18582.9542250422  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  18398.0507410714  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  18214.9870775064  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  18033.7449278285  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  17854.3061676715  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  17676.65285301  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  17500.7672183642  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  17326.6316750244  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  17154.228809291  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  16983.5413807338  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  16814.5523204676  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  16647.2447294456  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  16481.6018767693  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  16317.6071980154  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  16155.2442935794  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  15994.4969270355  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  15835.3490235131  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  15677.7846680892  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  15521.788104197  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  15367.34373205  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  15214.4361070824  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  15063.0499384043  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  14913.1700872726  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  14764.7815655773  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  14617.8695343426  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  14472.4193022429  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  14328.4163241338  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  14185.8461995975  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  14044.6946715028  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  13904.9476245792  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  13766.5910840055  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  13629.6112140124  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  13493.9943164988  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  13359.7268296619  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  13226.7953266411  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  13095.1865141752  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12964.8872312735  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12835.8844478991  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12708.165263666  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12581.7169065495  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12456.5267316084  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12332.582219721  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12209.8709763327  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  12088.380730217  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11968.099332248  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11849.0147541856  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11731.1150874729  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11614.3885420449  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11498.8234451498  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11384.4082401816  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11271.1314855245  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11158.9818534085  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  11047.9481287771  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10938.0192081652  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10829.1840985891  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10721.4319164473  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10614.7518864317  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10509.1333404504  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10404.5657165607  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10301.0385579133  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10198.5415117058  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  10097.0643281483  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9996.59685943788  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9897.12905874391  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9798.65097920349  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9701.15277292652  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9604.6246900112  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9509.05707756873  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9414.44037875829  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9320.76513183108  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9228.02196918439  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9136.2016164247  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  9045.29489144014  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8955.29270348252  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8866.186052258  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8777.96602702724  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8690.62380571415  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8604.15065402384  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8518.53792456912  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8433.77705600563  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8349.85957217593  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8266.77708126167  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8184.52127494457  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8103.08392757538  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  8022.45689535158  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7942.63211550269  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7863.60160548423  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7785.35746217936  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7707.89186110851  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7631.19705564706  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7555.2653762505  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7480.08922968767  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7405.66109828121  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7331.97353915601  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7259.01918349469  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7186.79073580093  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7115.28097316979  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  7044.48274456536  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6974.38897010583  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6904.9926403553  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6836.286815623  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6768.26462526917  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6700.91926701811  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6634.24400627789  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6568.23217546683  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6502.87717334688  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6438.17246436333  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6374.1115779914  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6310.68810808902  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6247.8957122564  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6185.72811120158  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6124.17908811267  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6063.24248803609  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  6002.91221726102  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5943.18224271013  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5884.04659133616  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5825.49934952474  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5767.53466250285  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5710.14673375352  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5653.32982443602  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5597.07825281208  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5541.3863936777  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5486.2486778005  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5431.65959136299  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5377.61367541099  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5324.10552530792  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5271.12979019412  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5218.68117245197  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5166.754427176  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5115.34436164836  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5064.44583481972  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  5014.05375679492  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4964.16308832421  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4914.76884029913  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4865.86607325376  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4817.44989687059  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4769.51546949167  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4722.05799763432  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4675.07273551178  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4628.55498455872  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4582.50009296124  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4536.90345519183  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4491.76051154869  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4447.06674769987  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4402.8176942317  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4359.00892620198  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4315.63606269742  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4272.69476639549  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4230.1807431308  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4188.08974146558  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4146.4175522646  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4105.16000827419  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4064.31298370559  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  4023.87239382231  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3983.83419453164  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3944.19438198031  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3904.948992154  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3866.09410048106  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3827.62582143991  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3789.54030817061  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3751.83375209008  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3714.50238251129  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3677.5424662662  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3640.95030733235  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3604.72224646338  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3568.854660823  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3533.34396362276  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3498.18660376333  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3463.37906547946  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3428.91786798828  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3394.79956514135  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3361.02074507995  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3327.5780298939  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3294.46807528385  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3261.68757022671  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3229.23323664469  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3197.10182907735  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3165.29013435719  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3133.79497128823  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3102.61319032788  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3071.7416732721  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3041.17733294343  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  3010.91711288239  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2980.95798704173  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2951.29695948392  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2921.93106408148  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2892.85736422039  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2864.07295250646  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2835.57495047451  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2807.3605083006  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2779.42680451698  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2751.77104573003  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2724.39046634078  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2697.28232826851  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2670.44392067681  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2643.87255970255  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2617.5655881875  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2591.52037541257  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2565.7343168348  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2540.20483382683  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2514.92937341909  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2489.90540804446  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2465.13043528557  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2440.6019776245  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2416.31758219502  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2392.27482053738  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2368.47128835535  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2344.9046052759  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2321.57241461106  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2298.47238312234  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2275.60220078732  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2252.95958056872  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2230.54225818566  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2208.34799188721  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2186.37456222824  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2164.61977184748  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2143.08144524776  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2121.75742857847  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2100.64558942018  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2079.74381657137  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2059.05001983734  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2038.56212982119  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  2018.27809771681  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1998.19589510412  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1978.3135137461  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1958.62896538806  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1939.14028155876  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1919.84551337356  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1900.74273133958  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1881.83002516269  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1863.10550355652  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1844.56729405329  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1826.21354281661  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1808.04241445606  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1790.05209184367  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1772.24077593218  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1754.60668557515  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1737.14805734885  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1719.86314537592  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1702.75022115076  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1685.80757336666  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1669.03350774476  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1652.42634686448  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1635.98442999593  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1619.7061129337  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1603.58976783251  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1587.63378304445  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1571.83656295772  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1556.19652783716  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1540.71211366621  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1525.38177199056  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1510.20396976326  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1495.17718919145  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1480.29992758455  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1465.57069720398  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1450.98802511446  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1436.5504530366  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1422.25653720118  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1408.1048482047  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1394.09397086646  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1380.22250408705  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1366.48906070825  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1352.89226737426  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1339.43076439442  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1326.10320560722  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1312.90825824566  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1299.84460280403  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1286.91093290588  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1274.10595517346  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1261.4283890983  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1248.87696691325  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1236.45043346563  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1224.14754609174  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1211.96707449258  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1199.90780061084  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1187.9685185091  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1176.14803424917  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1164.4451657728  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1152.85874278339  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1141.38760662897  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1130.03061018637  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1118.78661774649  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1107.65450490071  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1096.63315842846  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1085.72147618592  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1074.91836699577  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1064.22275053809  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1053.63355724232  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1043.1497281803  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1032.7702149604  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1022.49397962264  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1012.31999453492  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  1002.24724229025  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  992.274715605024  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  982.401417218259  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  972.626359791883  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  962.948565812015  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  953.367067491183  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  943.88090667158  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  934.48913472921  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  925.19081247906  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  915.98501008115  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  906.870806947571  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  897.847291650418  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  888.913561830636  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  880.068724107804  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  871.311893990772  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  862.642195789238  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  854.058762526152  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  845.560735851038  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  837.147265954143  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  828.817511481469  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  820.570639450629  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  812.405825167543  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  804.322252143983  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  796.319112015905  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  788.395604462634  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  780.550937126804  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  772.784325535149  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  765.094993020038  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  757.482170641809  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  749.945097111883  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  742.483018716622  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  735.095189241974  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  727.780869898828  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  720.539329249161  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  713.369843132868  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  706.271694595365  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  699.244173815886  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  692.286578036491  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  685.39821149181  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  678.578385339442  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  671.826417591095  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  665.141633044362  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  658.523363215222  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  651.970946271172  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  645.483726965061  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  639.061056569553  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  632.702292812253  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  626.40679981149  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  620.173948012713  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  614.003114125553  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  607.893681061474  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  601.845037872081  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  595.856579688017  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  589.927707658468  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  584.057828891295  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  578.246356393726  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  572.492709013672  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  566.796311381596  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  561.156593852992  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  555.572992451403  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  550.044948812038  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  544.571910125929  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  539.153329084642  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  533.788663825562  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  528.477377877687  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  523.218940108002  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  518.012824668342  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  512.85851094283  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  507.755483495794  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  502.703232020238  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  497.701251286808  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  492.749041093256  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  487.846106214441  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  482.991956352786  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  478.186106089262  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  473.428074834835  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  468.717386782416  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  464.053570859277  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  459.436160679934  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  454.864694499525  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  450.338715167621  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  445.857770082518  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  441.421411145971  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  437.029194718392  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  432.680681574476  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  428.375436859286  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  424.113030044765  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  419.893034886674  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  415.715029381986  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  411.578595726666  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  407.483320273902  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  403.428793492735  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  399.41460992711  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  395.440368155324  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  391.505670749889  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  387.610124237784  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  383.753339061112  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  379.934929538142  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  376.154513824739  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  372.411713876182  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  368.706155409357  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  365.037467865329  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  361.405284372286  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  357.809241708853  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  354.248980267766  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  350.724144019914  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  347.234380478734  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  343.779340664966  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  340.358679071749  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  336.972053630071  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  333.619125674568  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  330.299559909649  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  327.013024375971  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  323.759190417243  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  320.537732647356  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  317.34832891785  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  314.190660285694  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  311.064410981393  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  307.969268377411  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  304.904922956909  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  301.87106828279  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  298.867400967061  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  295.893620640484  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  292.94942992255  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  290.034534391735  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  287.148642556054  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  284.291465823921  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  281.46271847528  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  278.66211763304  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  275.889383234783  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  273.144238004757  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  270.426407426153  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  267.735619713647  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  265.071605786227  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  262.434099240279  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  259.822836322951  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  257.237555905775  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  254.677999458555  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  252.143911023513  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  249.635037189694  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  247.151127067624  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  244.69193226422  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  242.257206857954  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  239.846707374255  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  237.460192761167  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  235.097424365239  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  232.758165907662  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  230.442183460642  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  228.149245424004  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  225.879122502033  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  223.631587680546  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  221.406416204187  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  219.203385553955  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  217.022275424948  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  214.862867704336  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  212.724946449547  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  210.608297866674  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  208.512710289096  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  206.437974156308  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  204.383881992968  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  202.350228388148  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  200.336809974792  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  198.343425409381  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  196.369875351799  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  194.415962445393  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  192.481491297246  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  190.56626845863  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  188.670102405666  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  186.792803520168  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  184.934184070684  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  183.094058193718  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  181.272241875151  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  179.468552931832  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  177.682810993364  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  175.914837484065  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  174.164455605111  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  172.431490316854  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  170.715768321323  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  169.017118044887  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  167.335369621104  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  165.67035487373  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  164.021907299902  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  162.389862053489  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  160.774055928607  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  159.174327343297  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  157.590516323367  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  156.022464486395  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  154.470015025891  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  152.933012695615  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  151.411303794053  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  149.904736149047  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  148.413159102577  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  146.936423495695  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  145.47438165361  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  144.02688737092  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  142.593795896989  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  141.174963921477  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  139.770249560003  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  138.379512339961  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  137.002613186469  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  135.639414408465  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  134.289779684936  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  132.953574051283  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  131.63066388583  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  130.320916896459  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  129.024202107378  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  127.740389846029  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  126.469351730115  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  125.210960654765  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  123.965090779824  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  122.731617517265  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  121.510417518735  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  120.301368663216  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  119.104350044814  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  117.919241960671  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  116.74592589899  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  115.584284527188  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  114.434201680159  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  113.29556234866  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  112.168252667809  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  111.052159905699  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  109.947172452124  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  108.853179807416  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  107.7700725714  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  106.697742432451  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  105.63608215666  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  104.584985577114  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  103.544347583281  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  102.514064110494  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  101.494032129546  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  100.484149636389  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  99.4843156419338  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  98.4944301619463  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  97.514394207054  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  96.5441097728447  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  95.5834798300663  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  94.6324083149241  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  93.6908001194741  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  92.7585610821118  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  91.8355979781567  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  90.9218185105295  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  90.0171313005218  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  89.1214458786587  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  88.2346726756515  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  87.3567230134411  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  86.4875090963295  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  85.6269440022007  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  84.774941673828  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  83.9314169102688  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  83.0962853583438  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  82.2694635042017  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  81.4508686649681  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  80.6404189804771  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  79.8380334050846  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  79.0436316995645  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  78.2571344230842  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  77.4784629252608  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  76.7075393382956  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  75.9442865691873  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  75.1886282920231  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  74.4404889403455  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  73.6997936995958  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  72.9664684996329  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  72.2404400073254  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  71.5216356192192  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  70.8099834542765  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  70.1054123466879  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  69.4078518387552  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  68.7172321738465  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  68.0334842894197  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  67.3565398101166  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  66.6863310409252  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  66.0227909604099  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  65.3658532140099  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  64.715452107403  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  64.0715225999366  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  63.4340002981233  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  62.8028214492017  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  62.1779229347609  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  61.5592422644286  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  60.9467175696222  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  60.340287597362  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  59.7398917041452  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  59.1454698498823  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  58.5569625918924  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  57.9743110789593  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  57.3974570454462  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  56.8263428054691  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  56.2609112471279  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  55.7011058267956  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  55.1468705634638  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  54.5981500331442  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  54.0548893633266  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  53.5170342274912  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  52.9845308396762  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  52.4573259490991  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  51.9353668348315  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  51.4186013005269  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  50.9069776692014  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  50.4004447780655  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  49.8989519734079  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  49.4024491055302  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  48.9108865237319  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  48.4242150713452  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  47.9423860808193  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  47.4653513688535  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  46.9930632315793  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  46.5254744397892  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  46.0625382342145  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  45.6042083208487  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  45.1504388663187  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  44.7011844933009  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  44.2564002759834  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  43.816041735574  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  43.3800648358516  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  42.948425978763  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  42.5210820000628  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  42.0979901649969  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  41.6791081640293  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  41.2643941086108  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  40.8538065269903  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  40.4473043600674  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  40.0448469572867  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  39.6463940725726  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  39.2519058603045  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  38.8613428713325  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  38.4746660490321  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  38.091836725399  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  37.7128166171818  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  37.3375678220537  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  36.9660528148225  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  36.598234443678  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  36.2340759264765  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  35.8735408470628  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  35.5165931516285  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  35.1631971451066  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  34.813317487602  loss:  70.9874152668519&quot;
## [1] &quot;step =  1  lambda =  34.4669191908574  loss:  81.610707039381&quot;
## [1] &quot;step =  2  lambda =  34.4669191908574  loss:  79.7333254386004&quot;
## [1] &quot;step =  3  lambda =  34.4669191908574  loss:  80.042566267585&quot;
## [1] &quot;step =  1  lambda =  34.1239676147544  loss:  91.0673071596323&quot;
## [1] &quot;step =  2  lambda =  34.1239676147544  loss:  89.3354955200391&quot;
## [1] &quot;step =  3  lambda =  34.1239676147544  loss:  89.5748554336269&quot;
## [1] &quot;step =  1  lambda =  33.7844284638495  loss:  100.270191644238&quot;
## [1] &quot;step =  2  lambda =  33.7844284638495  loss:  98.8751674292819&quot;
## [1] &quot;step =  3  lambda =  33.7844284638495  loss:  99.03129010057&quot;
## [1] &quot;step =  1  lambda =  33.4482677839449  loss:  109.402704781043&quot;
## [1] &quot;step =  2  lambda =  33.4482677839449  loss:  108.330049512746&quot;
## [1] &quot;step =  3  lambda =  33.4482677839449  loss:  108.421893404087&quot;
## [1] &quot;step =  1  lambda =  33.1154519586923  loss:  118.474678591962&quot;
## [1] &quot;step =  2  lambda =  33.1154519586923  loss:  117.706610101808&quot;
## [1] &quot;step =  3  lambda =  33.1154519586923  loss:  117.752188466586&quot;
## [1] &quot;step =  1  lambda =  32.7859477062319  loss:  127.491495974612&quot;
## [1] &quot;step =  2  lambda =  32.7859477062319  loss:  127.012117899219&quot;
## [1] &quot;step =  3  lambda =  32.7859477062319  loss:  127.027980359525&quot;
## [1] &quot;step =  1  lambda =  32.4597220758638  loss:  136.458820700372&quot;
## [1] &quot;step =  2  lambda =  32.4597220758638  loss:  136.254033777432&quot;
## [1] &quot;step =  1  lambda =  32.1367424447532  loss:  145.381290963369&quot;
## [1] &quot;step =  2  lambda =  32.1367424447532  loss:  145.440021814279&quot;
## [1] &quot;step =  1  lambda =  31.8169765146677  loss:  154.268395125688&quot;
## [1] &quot;step =  2  lambda =  31.8169765146677  loss:  154.578064722419&quot;
## [1] &quot;step =  1  lambda =  31.500392308748  loss:  163.112267845294&quot;
## [1] &quot;step =  2  lambda =  31.500392308748  loss:  163.675529489042&quot;
## [1] &quot;step =  1  lambda =  31.1869581683094  loss:  171.920124600772&quot;
## [1] &quot;step =  2  lambda =  31.1869581683094  loss:  172.740047112761&quot;
## [1] &quot;step =  1  lambda =  30.876642749677  loss:  180.699443974721&quot;
## [1] &quot;step =  2  lambda =  30.876642749677  loss:  181.779641460777&quot;
## [1] &quot;step =  1  lambda =  30.5694150210502  loss:  189.458094988053&quot;
## [1] &quot;step =  2  lambda =  30.5694150210502  loss:  190.802773497788&quot;
## [1] &quot;step =  1  lambda =  30.2652442594001  loss:  198.204380842948&quot;
## [1] &quot;step =  2  lambda =  30.2652442594001  loss:  199.818389081902&quot;
## [1] &quot;step =  1  lambda =  29.964100047397  loss:  206.947086196387&quot;
## [1] &quot;step =  2  lambda =  29.964100047397  loss:  208.835972196828&quot;
## [1] &quot;step =  1  lambda =  29.6659522703689  loss:  215.695529814366&quot;
## [1] &quot;step =  2  lambda =  29.6659522703689  loss:  217.865604366863&quot;
## [1] &quot;step =  1  lambda =  29.3707711132895  loss:  224.459623345546&quot;
## [1] &quot;step =  2  lambda =  29.3707711132895  loss:  226.918031043203&quot;
## [1] &quot;step =  1  lambda =  29.0785270577971  loss:  233.249936994641&quot;
## [1] &quot;step =  2  lambda =  29.0785270577971  loss:  236.004735842742&quot;
## [1] &quot;step =  1  lambda =  28.7891908792427  loss:  242.077772967439&quot;
## [1] &quot;step =  2  lambda =  28.7891908792427  loss:  245.138023622521&quot;
## [1] &quot;step =  1  lambda =  28.5027336437673  loss:  250.955247660338&quot;
## [1] &quot;step =  2  lambda =  28.5027336437673  loss:  254.331113480366&quot;
## [1] &quot;step =  1  lambda =  28.2191267054086  loss:  259.949808747719&quot;
## [1] &quot;step =  2  lambda =  28.2191267054086  loss:  263.573093052569&quot;
## [1] &quot;step =  1  lambda =  27.9383417032365  loss:  269.511079689332&quot;
## [1] &quot;step =  2  lambda =  27.9383417032365  loss:  272.664924049479&quot;
## [1] &quot;step =  1  lambda =  27.6603505585168  loss:  279.315112676788&quot;
## [1] &quot;step =  2  lambda =  27.6603505585168  loss:  281.612617701406&quot;
## [1] &quot;step =  1  lambda =  27.3851254719032  loss:  289.39920982031&quot;
## [1] &quot;step =  2  lambda =  27.3851254719032  loss:  291.495294019039&quot;
## [1] &quot;step =  1  lambda =  27.1126389206579  loss:  299.828235716947&quot;
## [1] &quot;step =  2  lambda =  27.1126389206579  loss:  302.101155802504&quot;
## [1] &quot;step =  1  lambda =  26.8428636558986  loss:  310.593924155825&quot;
## [1] &quot;step =  2  lambda =  26.8428636558986  loss:  312.931643307308&quot;
## [1] &quot;step =  1  lambda =  26.575772699874  loss:  321.749450589418&quot;
## [1] &quot;step =  2  lambda =  26.575772699874  loss:  324.021414941382&quot;
## [1] &quot;step =  1  lambda =  26.3113393432659  loss:  333.365631379443&quot;
## [1] &quot;step =  2  lambda =  26.3113393432659  loss:  335.953571953332&quot;
## [1] &quot;step =  1  lambda =  26.0495371425183  loss:  345.531204138&quot;
## [1] &quot;step =  2  lambda =  26.0495371425183  loss:  348.720283757218&quot;
## [1] &quot;step =  1  lambda =  25.7903399171931  loss:  358.357259038135&quot;
## [1] &quot;step =  2  lambda =  25.7903399171931  loss:  362.284167886868&quot;
## [1] &quot;step =  1  lambda =  25.5337217473515  loss:  371.983946165315&quot;
## [1] &quot;step =  2  lambda =  25.5337217473515  loss:  376.825991520705&quot;
## [1] &quot;step =  1  lambda =  25.2796569709629  loss:  386.590051200529&quot;
## [1] &quot;step =  2  lambda =  25.2796569709629  loss:  392.579613829737&quot;
## [1] &quot;step =  1  lambda =  25.0281201813378  loss:  402.406622359106&quot;
## [1] &quot;step =  2  lambda =  25.0281201813378  loss:  409.851855525735&quot;
## [1] &quot;step =  1  lambda =  24.7790862245877  loss:  419.736472647614&quot;
## [1] &quot;step =  2  lambda =  24.7790862245877  loss:  429.051147952187&quot;
## [1] &quot;step =  1  lambda =  24.5325301971094  loss:  438.982297810647&quot;
## [1] &quot;step =  2  lambda =  24.5325301971094  loss:  450.729078258552&quot;
## [1] &quot;step =  1  lambda =  24.2884274430945  loss:  460.687464345032&quot;
## [1] &quot;step =  2  lambda =  24.2884274430945  loss:  475.640756778574&quot;
## [1] &quot;step =  1  lambda =  24.0467535520645  loss:  485.595322443317&quot;
## [1] &quot;step =  2  lambda =  24.0467535520645  loss:  504.8321512725&quot;
## [1] &quot;step =  1  lambda =  23.8074843564287  loss:  514.735124713725&quot;
## [1] &quot;step =  2  lambda =  23.8074843564287  loss:  539.764601948958&quot;
## [1] &quot;step =  1  lambda =  23.5705959290681  loss:  549.544739165798&quot;
## [1] &quot;step =  2  lambda =  23.5705959290681  loss:  582.486771580614&quot;
## [1] &quot;step =  1  lambda =  23.3360645809427  loss:  592.040456879737&quot;
## [1] &quot;step =  2  lambda =  23.3360645809427  loss:  635.856908610654&quot;
## [1] &quot;step =  1  lambda =  23.1038668587222  loss:  652.631007790496&quot;
## [1] &quot;step =  2  lambda =  23.1038668587222  loss:  729.969277394116&quot;
## [1] &quot;step =  1  lambda =  22.8739795424408  loss:  751.032596352391&quot;
## [1] &quot;step =  2  lambda =  22.8739795424408  loss:  892.696779210558&quot;
## [1] &quot;step =  1  lambda =  22.6463796431754  loss:  912.41357328692&quot;
## [1] &quot;step =  1  lambda =  22.4210444007463  loss:  931.401723633227&quot;
## [1] &quot;step =  1  lambda =  22.1979512814416  loss:  949.668325905152&quot;
## [1] &quot;step =  1  lambda =  21.9770779757634  loss:  967.222033425632&quot;
## [1] &quot;step =  1  lambda =  21.7584023961971  loss:  984.072769368658&quot;
## [1] &quot;step =  1  lambda =  21.5419026750024  loss:  1001.99001999843&quot;
## [1] &quot;step =  1  lambda =  21.3275571620269  loss:  1023.74389359474&quot;
## [1] &quot;step =  1  lambda =  21.1153444225406  loss:  1044.64969562315&quot;
## [1] &quot;step =  1  lambda =  20.9052432350928  loss:  1065.25671443035&quot;
## [1] &quot;step =  1  lambda =  20.6972325893895  loss:  1094.56051614093&quot;
## [1] &quot;step =  1  lambda =  20.4912916841929  loss:  1146.10174140282&quot;
## [1] &quot;step =  1  lambda =  20.2873999252409  loss:  1195.38996863029&quot;
## [1] &quot;step =  1  lambda =  20.0855369231877  loss:  1240.93756219572&quot;
## [1] &quot;step =  1  lambda =  19.8856824915647  loss:  1283.10509048066&quot;
## [1] &quot;step =  1  lambda =  19.6878166447624  loss:  1322.17091931806&quot;
## [1] &quot;step =  1  lambda =  19.4919195960311  loss:  1359.07799416428&quot;
## [1] &quot;step =  1  lambda =  19.2979717555028  loss:  1398.30864212617&quot;
## [1] &quot;step =  1  lambda =  19.1059537282317  loss:  1434.30445712059&quot;
## [1] &quot;step =  1  lambda =  18.915846312255  loss:  1476.34572528538&quot;
## [1] &quot;step =  1  lambda =  18.7276304966729  loss:  1520.98384403126&quot;
## [1] &quot;step =  1  lambda =  18.5412874597469  loss:  1558.1733895548&quot;
## [1] &quot;step =  1  lambda =  18.3567985670179  loss:  1593.96994999021&quot;
## [1] &quot;step =  1  lambda =  18.1741453694431  loss:  1626.23293459109&quot;
## [1] &quot;step =  1  lambda =  17.9933096015503  loss:  1654.5009111411&quot;
## [1] &quot;step =  1  lambda =  17.8142731796122  loss:  1679.45306278548&quot;
## [1] &quot;step =  1  lambda =  17.6370181998373  loss:  1701.59397229148&quot;
## [1] &quot;step =  1  lambda =  17.46152693658  loss:  1721.30725345902&quot;
## [1] &quot;step =  1  lambda =  17.2877818405676  loss:  1738.89156883879&quot;
## [1] &quot;step =  1  lambda =  17.1157655371459  loss:  1754.58489009243&quot;
## [1] &quot;step =  1  lambda =  16.945460824541  loss:  1768.5809936118&quot;
## [1] &quot;step =  1  lambda =  16.7768506721399  loss:  1781.0408216348&quot;
## [1] &quot;step =  1  lambda =  16.6099182187867  loss:  1792.10042266618&quot;
## [1] &quot;step =  1  lambda =  16.4446467710971  loss:  1801.87658856619&quot;
## [1] &quot;step =  1  lambda =  16.2810198017884  loss:  1810.47092163258&quot;
## [1] &quot;step =  1  lambda =  16.1190209480276  loss:  1817.97281763528&quot;
## [1] &quot;step =  1  lambda =  15.958634009794  loss:  1824.46169043834&quot;
## [1] &quot;step =  1  lambda =  15.7998429482604  loss:  1830.00865896865&quot;
## [1] &quot;step =  1  lambda =  15.6426318841882  loss:  1834.67784795145&quot;
## [1] &quot;step =  1  lambda =  15.4869850963399  loss:  1839.34643389191&quot;
## [1] &quot;step =  1  lambda =  15.3328870199072  loss:  1843.53815339097&quot;
## [1] &quot;step =  1  lambda =  15.1803222449539  loss:  1846.97829123013&quot;
## [1] &quot;step =  1  lambda =  15.0292755148754  loss:  1849.73866584681&quot;
## [1] &quot;step =  1  lambda =  14.8797317248728  loss:  1851.92275755009&quot;
## [1] &quot;step =  1  lambda =  14.7316759204426  loss:  1854.31706642928&quot;
## [1] &quot;step =  1  lambda =  14.5850932958808  loss:  1857.28877365178&quot;
## [1] &quot;step =  1  lambda =  14.4399691928029  loss:  1859.86306743646&quot;
## [1] &quot;step =  1  lambda =  14.2962890986776  loss:  1861.93446340478&quot;
## [1] &quot;step =  1  lambda =  14.1540386453758  loss:  1863.5249536471&quot;
## [1] &quot;step =  1  lambda =  14.0132036077336  loss:  1864.65513050255&quot;
## [1] &quot;step =  1  lambda =  13.8737699021299  loss:  1865.34434861632&quot;
## [1] &quot;step =  1  lambda =  13.7357235850779  loss:  1865.61086194185&quot;
## [1] &quot;step =  1  lambda =  13.5990508518309  loss:  1865.47193897574&quot;
## [1] &quot;step =  1  lambda =  13.4637380350017  loss:  1864.94395935746&quot;
## [1] &quot;step =  1  lambda =  13.3297716031958  loss:  1864.04249474555&quot;
## [1] &quot;step =  1  lambda =  13.1971381596584  loss:  1862.7823766155&quot;
## [1] &quot;step =  1  lambda =  13.0658244409346  loss:  1861.17775332899&quot;
## [1] &quot;step =  1  lambda =  12.9358173155431  loss:  1859.24213851824&quot;
## [1] &quot;step =  1  lambda =  12.807103782663  loss:  1856.98845252673&quot;
## [1] &quot;step =  1  lambda =  12.6796709708339  loss:  1854.4290583609&quot;
## [1] &quot;step =  1  lambda =  12.5535061366682  loss:  1851.57579334377&quot;
## [1] &quot;step =  1  lambda =  12.4285966635775  loss:  1848.43999742676&quot;
## [1] &quot;step =  1  lambda =  12.3049300605104  loss:  1845.03253891159&quot;
## [1] &quot;step =  1  lambda =  12.1824939607035  loss:  1841.36383816099&quot;
## [1] &quot;step =  1  lambda =  12.0612761204447  loss:  1837.99773334809&quot;
## [1] &quot;step =  1  lambda =  11.9412644178491  loss:  1834.51477628112&quot;
## [1] &quot;step =  1  lambda =  11.8224468516464  loss:  1830.79859531874&quot;
## [1] &quot;step =  1  lambda =  11.7048115399809  loss:  1827.94712369924&quot;
## [1] &quot;step =  1  lambda =  11.5883467192234  loss:  1824.9695494748&quot;
## [1] &quot;step =  1  lambda =  11.4730407427948  loss:  1821.74298307666&quot;
## [1] &quot;step =  1  lambda =  11.3588820800015  loss:  1818.27517656487&quot;
## [1] &quot;step =  1  lambda =  11.2458593148818  loss:  1814.57368426272&quot;
## [1] &quot;step =  1  lambda =  11.1339611450653  loss:  1810.6458671938&quot;
## [1] &quot;step =  1  lambda =  11.0231763806416  loss:  1806.49889771303&quot;
## [1] &quot;step =  1  lambda =  10.913493943042  loss:  1802.13976428625&quot;
## [1] &quot;step =  1  lambda =  10.8049028639313  loss:  1797.57527637313&quot;
## [1] &quot;step =  1  lambda =  10.6973922841111  loss:  1792.81206936982&quot;
## [1] &quot;step =  1  lambda =  10.5909514524338  loss:  1787.85660957124&quot;
## [1] &quot;step =  1  lambda =  10.4855697247276  loss:  1782.71519911641&quot;
## [1] &quot;step =  1  lambda =  10.3812365627318  loss:  1777.39398088478&quot;
## [1] &quot;step =  1  lambda =  10.2779415330434  loss:  1771.89894331582&quot;
## [1] &quot;step =  1  lambda =  10.1756743060733  loss:  1766.23592512859&quot;
## [1] &quot;step =  1  lambda =  10.0744246550136  loss:  1760.41061992211&quot;
## [1] &quot;step =  1  lambda =  9.97418245481473  loss:  1754.42858064164&quot;
## [1] &quot;step =  1  lambda =  9.87493768117319  loss:  1748.29522389912&quot;
## [1] &quot;step =  1  lambda =  9.77668040952892  loss:  1742.01583413983&quot;
## [1] &quot;step =  1  lambda =  9.67940081407284  loss:  1735.59556764975&quot;
## [1] &quot;step =  1  lambda =  9.58308916676438  loss:  1729.03945640098&quot;
## [1] &quot;step =  1  lambda =  9.48773583635853  loss:  1722.3524117345&quot;
## [1] &quot;step =  1  lambda =  9.39333128744278  loss:  1715.53922788166&quot;
## [1] &quot;step =  1  lambda =  9.29986607948359  loss:  1708.60458532687&quot;
## [1] &quot;step =  1  lambda =  9.20733086588226  loss:  1701.55305401583&quot;
## [1] &quot;step =  1  lambda =  9.11571639304031  loss:  1694.38909641383&quot;
## [1] &quot;step =  1  lambda =  9.02501349943413  loss:  1687.1170704202&quot;
## [1] &quot;step =  1  lambda =  8.93521311469874  loss:  1679.74123214491&quot;
## [1] &quot;step =  1  lambda =  8.84630625872088  loss:  1672.26573855407&quot;
## [1] &quot;step =  1  lambda =  8.75828404074083  loss:  1664.70776491987&quot;
## [1] &quot;step =  1  lambda =  8.67113765846346  loss:  1657.12437205993&quot;
## [1] &quot;step =  1  lambda =  8.5848583971779  loss:  1649.44988731577&quot;
## [1] &quot;step =  1  lambda =  8.49943762888613  loss:  1641.68821974177&quot;
## [1] &quot;step =  1  lambda =  8.41486681144014  loss:  1633.84318659162&quot;
## [1] &quot;step =  1  lambda =  8.3311374876877  loss:  1625.91851540987&quot;
## [1] &quot;step =  1  lambda =  8.24824128462666  loss:  1617.9178460419&quot;
## [1] &quot;step =  1  lambda =  8.16616991256765  loss:  1609.84473256774&quot;
## [1] &quot;step =  1  lambda =  8.08491516430506  loss:  1601.70264516484&quot;
## [1] &quot;step =  1  lambda =  8.00446891429635  loss:  1593.49497190481&quot;
## [1] &quot;step =  1  lambda =  7.92482311784949  loss:  1585.22502048845&quot;
## [1] &quot;step =  1  lambda =  7.84596981031845  loss:  1576.89601992365&quot;
## [1] &quot;step =  1  lambda =  7.76790110630678  loss:  1566.99936006889&quot;
## [1] &quot;step =  1  lambda =  7.69060919887901  loss:  1556.42195472355&quot;
## [1] &quot;step =  1  lambda =  7.61408635877998  loss:  1545.86685777467&quot;
## [1] &quot;step =  1  lambda =  7.53832493366192  loss:  1535.33480639521&quot;
## [1] &quot;step =  1  lambda =  7.46331734731919  loss:  1524.54023714481&quot;
## [1] &quot;step =  1  lambda =  7.38905609893065  loss:  1513.61234793443&quot;
## [1] &quot;step =  1  lambda =  7.31553376230957  loss:  1502.71219474292&quot;
## [1] &quot;step =  1  lambda =  7.24274298516102  loss:  1491.84120872074&quot;
## [1] &quot;step =  1  lambda =  7.17067648834662  loss:  1481.00077715518&quot;
## [1] &quot;step =  1  lambda =  7.09932706515664  loss:  1470.19224332974&quot;
## [1] &quot;step =  1  lambda =  7.0286875805893  loss:  1459.41690647734&quot;
## [1] &quot;step =  1  lambda =  6.95875097063727  loss:  1448.67602182465&quot;
## [1] &quot;step =  1  lambda =  6.88951024158129  loss:  1437.97080072357&quot;
## [1] &quot;step =  1  lambda =  6.82095846929075  loss:  1427.30241086603&quot;
## [1] &quot;step =  1  lambda =  6.75308879853129  loss:  1416.67197657745&quot;
## [1] &quot;step =  1  lambda =  6.68589444227927  loss:  1406.08057918425&quot;
## [1] &quot;step =  1  lambda =  6.61936868104308  loss:  1395.52925745032&quot;
## [1] &quot;step =  1  lambda =  6.55350486219115  loss:  1385.0190080777&quot;
## [1] &quot;step =  1  lambda =  6.48829639928672  loss:  1374.55078626606&quot;
## [1] &quot;step =  1  lambda =  6.42373677142913  loss:  1364.12550632616&quot;
## [1] &quot;step =  1  lambda =  6.35981952260183  loss:  1353.74404234226&quot;
## [1] &quot;step =  1  lambda =  6.29653826102666  loss:  1343.42515945127&quot;
## [1] &quot;step =  1  lambda =  6.23388665852472  loss:  1333.17462828531&quot;
## [1] &quot;step =  1  lambda =  6.17185844988355  loss:  1322.96773864062&quot;
## [1] &quot;step =  1  lambda =  6.11044743223061  loss:  1312.80529926865&quot;
## [1] &quot;step =  1  lambda =  6.04964746441295  loss:  1302.6880825216&quot;
## [1] &quot;step =  1  lambda =  5.98945246638312  loss:  1292.42872377832&quot;
## [1] &quot;step =  1  lambda =  5.92985641859114  loss:  1282.00580251927&quot;
## [1] &quot;step =  1  lambda =  5.8708533613826  loss:  1271.6473585556&quot;
## [1] &quot;step =  1  lambda =  5.81243739440259  loss:  1261.35349611025&quot;
## [1] &quot;step =  1  lambda =  5.75460267600573  loss:  1251.12429225465&quot;
## [1] &quot;step =  1  lambda =  5.69734342267199  loss:  1240.95979830267&quot;
## [1] &quot;step =  1  lambda =  5.64065390842832  loss:  1230.86004117982&quot;
## [1] &quot;step =  1  lambda =  5.58452846427606  loss:  1220.82502476434&quot;
## [1] &quot;step =  1  lambda =  5.52896147762401  loss:  1210.85473119737&quot;
## [1] &quot;step =  1  lambda =  5.47394739172721  loss:  1200.94912215939&quot;
## [1] &quot;step =  1  lambda =  5.4194807051312  loss:  1190.70049814816&quot;
## [1] &quot;step =  1  lambda =  5.36555597112197  loss:  1180.51804091355&quot;
## [1] &quot;step =  1  lambda =  5.31216779718117  loss:  1170.40492765263&quot;
## [1] &quot;step =  1  lambda =  5.2593108444469  loss:  1160.36120388802&quot;
## [1] &quot;step =  1  lambda =  5.20697982717985  loss:  1150.38689376981&quot;
## [1] &quot;step =  1  lambda =  5.15516951223468  loss:  1140.48200094031&quot;
## [1] &quot;step =  1  lambda =  5.10387471853673  loss:  1130.64650937603&quot;
## [1] &quot;step =  1  lambda =  5.05309031656387  loss:  1120.88038420648&quot;
## [1] &quot;step =  1  lambda =  5.00281122783359  loss:  1111.18357250943&quot;
## [1] &quot;step =  1  lambda =  4.95303242439511  loss:  1101.55600408257&quot;
## [1] &quot;step =  1  lambda =  4.90374892832662  loss:  1091.99759219145&quot;
## [1] &quot;step =  1  lambda =  4.85495581123743  loss:  1082.50823429386&quot;
## [1] &quot;step =  1  lambda =  4.80664819377518  loss:  1073.08781274078&quot;
## [1] &quot;step =  1  lambda =  4.75882124513786  loss:  1063.7361954542&quot;
## [1] &quot;step =  1  lambda =  4.71147018259074  loss:  1054.45323658209&quot;
## [1] &quot;step =  1  lambda =  4.66459027098813  loss:  1045.2387771309&quot;
## [1] &quot;step =  1  lambda =  4.61817682229978  loss:  1036.09264557604&quot;
## [1] &quot;step =  1  lambda =  4.57222519514216  loss:  1027.01465845075&quot;
## [1] &quot;step =  1  lambda =  4.52673079431425  loss:  1018.0046209139&quot;
## [1] &quot;step =  1  lambda =  4.48168907033806  loss:  1009.06232729716&quot;
## [1] &quot;step =  1  lambda =  4.43709551900367  loss:  1000.18756163217&quot;
## [1] &quot;step =  1  lambda =  4.39294568091876  loss:  991.380098158108&quot;
## [1] &quot;step =  1  lambda =  4.34923514106274  loss:  982.639701810333&quot;
## [1] &quot;step =  1  lambda =  4.30595952834521  loss:  973.769401287743&quot;
## [1] &quot;step =  1  lambda =  4.26311451516882  loss:  964.762799381745&quot;
## [1] &quot;step =  1  lambda =  4.22069581699655  loss:  955.834436495943&quot;
## [1] &quot;step =  1  lambda =  4.17869919192325  loss:  946.983768853613&quot;
## [1] &quot;step =  1  lambda =  4.13712044025139  loss:  938.210251307481&quot;
## [1] &quot;step =  1  lambda =  4.09595540407118  loss:  929.513337619167&quot;
## [1] &quot;step =  1  lambda =  4.05519996684468  loss:  920.892480724685&quot;
## [1] &quot;step =  1  lambda =  4.0148500529942  loss:  912.34713298661&quot;
## [1] &quot;step =  1  lambda =  3.97490162749475  loss:  903.876746433415&quot;
## [1] &quot;step =  1  lambda =  3.93535069547048  loss:  895.480772986527&quot;
## [1] &quot;step =  1  lambda =  3.89619330179521  loss:  887.158664675594&quot;
## [1] &quot;step =  1  lambda =  3.85742553069697  loss:  878.909873842475&quot;
## [1] &quot;step =  1  lambda =  3.81904350536634  loss:  870.733853334394&quot;
## [1] &quot;step =  1  lambda =  3.78104338756878  loss:  862.630056686752&quot;
## [1] &quot;step =  1  lambda =  3.74342137726086  loss:  854.597938296009&quot;
## [1] &quot;step =  1  lambda =  3.7061737122102  loss:  846.636953583073&quot;
## [1] &quot;step =  1  lambda =  3.66929666761925  loss:  838.746559147612&quot;
## [1] &quot;step =  1  lambda =  3.63278655575281  loss:  830.926212913666&quot;
## [1] &quot;step =  1  lambda =  3.59663972556928  loss:  823.175374266953&quot;
## [1] &quot;step =  1  lambda =  3.56085256235552  loss:  815.49350418424&quot;
## [1] &quot;step =  1  lambda =  3.52542148736538  loss:  807.880065355105&quot;
## [1] &quot;step =  1  lambda =  3.49034295746184  loss:  800.334522296461&quot;
## [1] &quot;step =  1  lambda =  3.45561346476268  loss:  792.856341460147&quot;
## [1] &quot;step =  1  lambda =  3.42122953628968  loss:  785.44499133391&quot;
## [1] &quot;step =  1  lambda =  3.38718773362134  loss:  778.099942536073&quot;
## [1] &quot;step =  1  lambda =  3.35348465254903  loss:  770.82066790418&quot;
## [1] &quot;step =  1  lambda =  3.32011692273655  loss:  763.606642577902&quot;
## [1] &quot;step =  1  lambda =  3.28708120738312  loss:  756.457344076469&quot;
## [1] &quot;step =  1  lambda =  3.25437420288967  loss:  749.372252370881&quot;
## [1] &quot;step =  1  lambda =  3.2219926385285  loss:  742.350849951148&quot;
## [1] &quot;step =  1  lambda =  3.18993327611618  loss:  735.392621888798&quot;
## [1] &quot;step =  1  lambda =  3.15819290968977  loss:  728.497055894881&quot;
## [1] &quot;step =  1  lambda =  3.12676836518616  loss:  721.663642373679&quot;
## [1] &quot;step =  1  lambda =  3.09565650012471  loss:  714.891874472349&quot;
## [1] &quot;step =  1  lambda =  3.06485420329301  loss:  708.181248126671&quot;
## [1] &quot;step =  1  lambda =  3.03435839443567  loss:  701.531262103129&quot;
## [1] &quot;step =  1  lambda =  3.00416602394643  loss:  694.941418037483&quot;
## [1] &quot;step =  1  lambda =  2.97427407256306  loss:  688.411220470015&quot;
## [1] &quot;step =  1  lambda =  2.94467955106552  loss:  681.940176877623&quot;
## [1] &quot;step =  1  lambda =  2.915379499977  loss:  675.527797702928&quot;
## [1] &quot;step =  1  lambda =  2.88637098926796  loss:  669.173596380533&quot;
## [1] &quot;step =  1  lambda =  2.85765111806317  loss:  662.877089360601&quot;
## [1] &quot;step =  1  lambda =  2.82921701435156  loss:  656.637796129882&quot;
## [1] &quot;step =  1  lambda =  2.80106583469908  loss:  650.455239230328&quot;
## [1] &quot;step =  1  lambda =  2.7731947639643  loss:  644.328944275431&quot;
## [1] &quot;step =  1  lambda =  2.74560101501692  loss:  638.25843996439&quot;
## [1] &quot;step =  1  lambda =  2.71828182845905  loss:  632.243258094252&quot;
## [1] &quot;step =  1  lambda =  2.69123447234926  loss:  626.282933570112&quot;
## [1] &quot;step =  1  lambda =  2.66445624192942  loss:  620.377004413504&quot;
## [1] &quot;step =  1  lambda =  2.63794445935415  loss:  614.525011769064&quot;
## [1] &quot;step =  1  lambda =  2.61169647342312  loss:  608.701261348998&quot;
## [1] &quot;step =  1  lambda =  2.58570965931585  loss:  602.916102858925&quot;
## [1] &quot;step =  1  lambda =  2.55998141832927  loss:  597.184565747284&quot;
## [1] &quot;step =  1  lambda =  2.53450917761785  loss:  591.506186203226&quot;
## [1] &quot;step =  1  lambda =  2.5092903899363  loss:  585.880503634775&quot;
## [1] &quot;step =  1  lambda =  2.48432253338482  loss:  580.307060674642&quot;
## [1] &quot;step =  1  lambda =  2.45960311115695  loss:  574.785403184673&quot;
## [1] &quot;step =  1  lambda =  2.43512965128988  loss:  569.315080258986&quot;
## [1] &quot;step =  1  lambda =  2.41089970641721  loss:  563.895644225861&quot;
## [1] &quot;step =  1  lambda =  2.38691085352428  loss:  558.52665064842&quot;
## [1] &quot;step =  1  lambda =  2.36316069370579  loss:  553.207658324164&quot;
## [1] &quot;step =  1  lambda =  2.33964685192599  loss:  547.938229283404&quot;
## [1] &quot;step =  1  lambda =  2.31636697678109  loss:  542.717928786632&quot;
## [1] &quot;step =  1  lambda =  2.29331874026418  loss:  537.5463253209&quot;
## [1] &quot;step =  1  lambda =  2.27049983753241  loss:  532.422990595225&quot;
## [1] &quot;step =  1  lambda =  2.24790798667647  loss:  527.347499535094&quot;
## [1] &quot;step =  1  lambda =  2.22554092849247  loss:  522.319430276084&quot;
## [1] &quot;step =  1  lambda =  2.20339642625594  loss:  517.338364156666&quot;
## [1] &quot;step =  1  lambda =  2.1814722654982  loss:  512.403885710215&quot;
## [1] &quot;step =  1  lambda =  2.15976625378491  loss:  507.515582656275&quot;
## [1] &quot;step =  1  lambda =  2.13827622049682  loss:  502.673045891118&quot;
## [1] &quot;step =  1  lambda =  2.11700001661267  loss:  497.875869477634&quot;
## [1] &quot;step =  1  lambda =  2.09593551449437  loss:  493.123650634579&quot;
## [1] &quot;step =  1  lambda =  2.07508060767412  loss:  488.41598972524&quot;
## [1] &quot;step =  1  lambda =  2.05443321064389  loss:  483.752490245518&quot;
## [1] &quot;step =  1  lambda =  2.03399125864675  loss:  479.132758811493&quot;
## [1] &quot;step =  1  lambda =  2.01375270747048  loss:  474.556405146478&quot;
## [1] &quot;step =  1  lambda =  1.99371553324308  loss:  470.023042067611&quot;
## [1] &quot;step =  1  lambda =  1.97387773223045  loss:  465.532285472003&quot;
## [1] &quot;step =  1  lambda =  1.95423732063594  loss:  461.083754322467&quot;
## [1] &quot;step =  1  lambda =  1.93479233440203  loss:  456.677070632873&quot;
## [1] &quot;step =  1  lambda =  1.9155408290139  loss:  452.31185945313&quot;
## [1] &quot;step =  1  lambda =  1.89648087930495  loss:  447.98774885384&quot;
## [1] &quot;step =  1  lambda =  1.87761057926434  loss:  443.704369910637&quot;
## [1] &quot;step =  1  lambda =  1.85892804184634  loss:  439.461356688238&quot;
## [1] &quot;step =  1  lambda =  1.84043139878164  loss:  435.258346224224&quot;
## [1] &quot;step =  1  lambda =  1.82211880039051  loss:  431.094978512576&quot;
## [1] &quot;step =  1  lambda =  1.80398841539786  loss:  426.970896486977&quot;
## [1] &quot;step =  1  lambda =  1.78603843075007  loss:  422.885746003918&quot;
## [1] &quot;step =  1  lambda =  1.76826705143374  loss:  418.839175825599&quot;
## [1] &quot;step =  1  lambda =  1.7506725002961  loss:  414.830837602666&quot;
## [1] &quot;step =  1  lambda =  1.7332530178674  loss:  410.860385856786&quot;
## [1] &quot;step =  1  lambda =  1.71600686218486  loss:  406.92747796308&quot;
## [1] &quot;step =  1  lambda =  1.69893230861855  loss:  403.031774132431&quot;
## [1] &quot;step =  1  lambda =  1.68202764969889  loss:  399.172937393679&quot;
## [1] &quot;step =  1  lambda =  1.66529119494589  loss:  395.350633575709&quot;
## [1] &quot;step =  1  lambda =  1.64872127070013  loss:  391.564531289461&quot;
## [1] &quot;step =  1  lambda =  1.63231621995538  loss:  387.814301909859&quot;
## [1] &quot;step =  1  lambda =  1.61607440219289  loss:  384.099619557674&quot;
## [1] &quot;step =  1  lambda =  1.59999419321736  loss:  380.42016108134&quot;
## [1] &quot;step =  1  lambda =  1.58407398499448  loss:  376.775606038718&quot;
## [1] &quot;step =  1  lambda =  1.56831218549017  loss:  373.165636678833&quot;
## [1] &quot;step =  1  lambda =  1.55270721851134  loss:  369.589937923581&quot;
## [1] &quot;step =  1  lambda =  1.53725752354828  loss:  366.048197349426&quot;
## [1] &quot;step =  1  lambda =  1.52196155561863  loss:  362.546615174456&quot;
## [1] &quot;step =  1  lambda =  1.50681778511285  loss:  359.111958278465&quot;
## [1] &quot;step =  1  lambda =  1.49182469764127  loss:  355.709146549649&quot;
## [1] &quot;step =  1  lambda =  1.47698079388264  loss:  352.337905060319&quot;
## [1] &quot;step =  1  lambda =  1.46228458943423  loss:  348.997960892011&quot;
## [1] &quot;step =  1  lambda =  1.44773461466333  loss:  345.689043127804&quot;
## [1] &quot;step =  1  lambda =  1.43332941456034  loss:  342.410882844478&quot;
## [1] &quot;step =  1  lambda =  1.41906754859326  loss:  339.16321310453&quot;
## [1] &quot;step =  1  lambda =  1.40494759056359  loss:  335.945768948046&quot;
## [1] &quot;step =  1  lambda =  1.39096812846378  loss:  332.758287384449&quot;
## [1] &quot;step =  1  lambda =  1.37712776433596  loss:  329.600507384114&quot;
## [1] &quot;step =  1  lambda =  1.36342511413218  loss:  326.472169869871&quot;
## [1] &quot;step =  1  lambda =  1.349858807576  loss:  323.373017708395&quot;
## [1] &quot;step =  1  lambda =  1.33642748802547  loss:  320.302795701493&quot;
## [1] &quot;step =  1  lambda =  1.32312981233744  loss:  317.261250577283&quot;
## [1] &quot;step =  1  lambda =  1.30996445073325  loss:  314.248130981294&quot;
## [1] &quot;step =  1  lambda =  1.29693008666577  loss:  311.263187467458&quot;
## [1] &quot;step =  1  lambda =  1.28402541668774  loss:  308.306172489037&quot;
## [1] &quot;step =  1  lambda =  1.2712491503214  loss:  305.376840389456&quot;
## [1] &quot;step =  1  lambda =  1.25860000992948  loss:  302.474947393072&quot;
## [1] &quot;step =  1  lambda =  1.24607673058738  loss:  299.600251595871&quot;
## [1] &quot;step =  1  lambda =  1.23367805995674  loss:  296.752512956094&quot;
## [1] &quot;step =  1  lambda =  1.22140275816017  loss:  293.931493284808&quot;
## [1] &quot;step =  1  lambda =  1.20924959765725  loss:  291.136956236416&quot;
## [1] &quot;step =  1  lambda =  1.19721736312181  loss:  288.368667299119&quot;
## [1] &quot;step =  1  lambda =  1.18530485132037  loss:  285.626393785316&quot;
## [1] &quot;step =  1  lambda =  1.17351087099181  loss:  282.909904821968&quot;
## [1] &quot;step =  1  lambda =  1.16183424272828  loss:  280.218971340915&quot;
## [1] &quot;step =  1  lambda =  1.15027379885723  loss:  277.553366069155&quot;
## [1] &quot;step =  1  lambda =  1.13882838332462  loss:  274.912863519078&quot;
## [1] &quot;step =  1  lambda =  1.12749685157938  loss:  272.297239978677&quot;
## [1] &quot;step =  1  lambda =  1.11627807045887  loss:  269.706273501717&quot;
## [1] &quot;step =  1  lambda =  1.10517091807565  loss:  267.139743897886&quot;
## [1] &quot;step =  1  lambda =  1.09417428370521  loss:  264.597432722906&quot;
## [1] &quot;step =  1  lambda =  1.08328706767496  loss:  262.079123268635&quot;
## [1] &quot;step =  1  lambda =  1.07250818125422  loss:  259.58460055314&quot;
## [1] &quot;step =  1  lambda =  1.06183654654536  loss:  257.113651310747&quot;
## [1] &quot;step =  1  lambda =  1.05127109637602  loss:  254.666063982089&quot;
## [1] &quot;step =  1  lambda =  1.04081077419239  loss:  252.241628704118&quot;
## [1] &quot;step =  1  lambda =  1.03045453395352  loss:  249.840137300128&quot;
## [1] &quot;step =  1  lambda =  1.02020134002676  loss:  247.461383269749&quot;
## [1] &quot;step =  1  lambda =  1.01005016708417  loss:  245.105161778943&quot;
## [1] &quot;step =  1  lambda =  1  loss:  242.771269649987&quot;
## [1] &quot;step =  1  lambda =  0.990049833749168  loss:  240.459505351459&quot;
## [1] &quot;step =  1  lambda =  0.980198673306756  loss:  238.169668988209&quot;
## [1] &quot;step =  1  lambda =  0.970445533548509  loss:  235.901562291346&quot;
## [1] &quot;step =  1  lambda =  0.960789439152324  loss:  233.654988608203&quot;
## [1] &quot;step =  1  lambda =  0.951229424500715  loss:  231.429752892327&quot;
## [1] &quot;step =  1  lambda =  0.941764533584248  loss:  229.225661693453&quot;
## [1] &quot;step =  1  lambda =  0.932393819905948  loss:  227.042523147498&quot;
## [1] &quot;step =  1  lambda =  0.923116346386636  loss:  224.880146966546&quot;
## [1] &quot;step =  1  lambda =  0.913931185271228  loss:  222.738344428856&quot;
## [1] &quot;step =  1  lambda =  0.90483741803596  loss:  220.61692836887&quot;
## [1] &quot;step =  1  lambda =  0.895834135296529  loss:  218.515713167231&quot;
## [1] &quot;step =  1  lambda =  0.886920436717158  loss:  216.434514740817&quot;
## [1] &quot;step =  1  lambda =  0.878095430920562  loss:  214.373150532783&quot;
## [1] &quot;step =  1  lambda =  0.869358235398805  loss:  212.331439502621&quot;
## [1] &quot;step =  1  lambda =  0.860707976425057  loss:  210.309202116235&quot;
## [1] &quot;step =  1  lambda =  0.852143788966211  loss:  208.306260336026&quot;
## [1] &quot;step =  1  lambda =  0.843664816596384  loss:  206.322437611002&quot;
## [1] &quot;step =  1  lambda =  0.835270211411272  loss:  204.356935072837&quot;
## [1] &quot;step =  1  lambda =  0.826959133943363  loss:  202.406754434099&quot;
## [1] &quot;step =  1  lambda =  0.818730753077982  loss:  200.475163579608&quot;
## [1] &quot;step =  1  lambda =  0.810584245970188  loss:  198.562128523443&quot;
## [1] &quot;step =  1  lambda =  0.802518797962478  loss:  196.66747690074&quot;
## [1] &quot;step =  1  lambda =  0.794533602503334  loss:  194.791037844301&quot;
## [1] &quot;step =  1  lambda =  0.786627861066553  loss:  192.932641973258&quot;
## [1] &quot;step =  1  lambda =  0.778800783071405  loss:  191.092121381808&quot;
## [1] &quot;step =  1  lambda =  0.771051585803566  loss:  189.269309628009&quot;
## [1] &quot;step =  1  lambda =  0.763379494336853  loss:  187.464041722631&quot;
## [1] &quot;step =  1  lambda =  0.755783741455726  loss:  185.676154118083&quot;
## [1] &quot;step =  1  lambda =  0.748263567578566  loss:  183.905484697385&quot;
## [1] &quot;step =  1  lambda =  0.740818220681719  loss:  182.151872763213&quot;
## [1] &quot;step =  1  lambda =  0.733446956224289  loss:  180.415159027005&quot;
## [1] &quot;step =  1  lambda =  0.726149037073691  loss:  178.695185598123&quot;
## [1] &quot;step =  1  lambda =  0.718923733431926  loss:  176.991795973081&quot;
## [1] &quot;step =  1  lambda =  0.71177032276261  loss:  175.30483502483&quot;
## [1] &quot;step =  1  lambda =  0.704688089718714  loss:  173.634148992109&quot;
## [1] &quot;step =  1  lambda =  0.697676326071031  loss:  171.979585468855&quot;
## [1] &quot;step =  1  lambda =  0.690734330637355  loss:  170.340993393668&quot;
## [1] &quot;step =  1  lambda =  0.683861409212356  loss:  168.71822303935&quot;
## [1] &quot;step =  1  lambda =  0.677056874498164  loss:  167.111126002488&quot;
## [1] &quot;step =  1  lambda =  0.670320046035639  loss:  165.519555193111&quot;
## [1] &quot;step =  1  lambda =  0.663650250136319  loss:  163.9433648244&quot;
## [1] &quot;step =  1  lambda =  0.657046819815057  loss:  162.382410402462&quot;
## [1] &quot;step =  2  lambda =  0.657046819815057  loss:  192.354487788937&quot;
## [1] &quot;step =  1  lambda =  0.650509094723317  loss:  190.49622450719&quot;
## [1] &quot;step =  1  lambda =  0.644036421083142  loss:  188.656049449604&quot;
## [1] &quot;step =  1  lambda =  0.637628151621774  loss:  186.833789920973&quot;
## [1] &quot;step =  1  lambda =  0.631283645506927  loss:  185.029274819077&quot;
## [1] &quot;step =  1  lambda =  0.6250022682827  loss:  183.242334620788&quot;
## [1] &quot;step =  1  lambda =  0.618783391806141  loss:  181.472801368286&quot;
## [1] &quot;step =  2  lambda =  0.618783391806141  loss:  204.584018145404&quot;
## [1] &quot;step =  1  lambda =  0.612626394184416  loss:  202.593814718086&quot;
## [1] &quot;step =  1  lambda =  0.606530659712633  loss:  200.623100190161&quot;
## [1] &quot;step =  1  lambda =  0.600495578812266  loss:  198.671686609078&quot;
## [1] &quot;step =  2  lambda =  0.600495578812266  loss:  215.3547917967&quot;
## [1] &quot;step =  1  lambda =  0.594520547970195  loss:  213.261363240291&quot;
## [1] &quot;step =  2  lambda =  0.594520547970195  loss:  225.969254808691&quot;
## [1] &quot;step =  1  lambda =  0.588604969678356  loss:  223.763251335598&quot;
## [1] &quot;step =  2  lambda =  0.588604969678356  loss:  233.725266111161&quot;
## [1] &quot;step =  1  lambda =  0.58274825237399  loss:  231.439712210887&quot;
## [1] &quot;step =  2  lambda =  0.58274825237399  loss:  239.915996996671&quot;
## [1] &quot;step =  1  lambda =  0.576949810380487  loss:  237.565463346212&quot;
## [1] &quot;step =  2  lambda =  0.576949810380487  loss:  245.3731066412&quot;
## [1] &quot;step =  1  lambda =  0.571209063848815  loss:  242.966959296033&quot;
## [1] &quot;step =  2  lambda =  0.571209063848815  loss:  250.035711271516&quot;
## [1] &quot;step =  1  lambda =  0.565525438699537  loss:  247.579222605667&quot;
## [1] &quot;step =  2  lambda =  0.565525438699537  loss:  254.8985456833&quot;
## [1] &quot;step =  1  lambda =  0.559898366565402  loss:  252.39063205235&quot;
## [1] &quot;step =  2  lambda =  0.559898366565402  loss:  259.137143831307&quot;
## [1] &quot;step =  1  lambda =  0.554327284734507  loss:  256.586933020626&quot;
## [1] &quot;step =  2  lambda =  0.554327284734507  loss:  262.706364529466&quot;
## [1] &quot;step =  1  lambda =  0.548811636094027  loss:  260.122005930542&quot;
## [1] &quot;step =  2  lambda =  0.548811636094027  loss:  265.658760069929&quot;
## [1] &quot;step =  1  lambda =  0.5433508690745  loss:  263.043820892727&quot;
## [1] &quot;step =  2  lambda =  0.5433508690745  loss:  268.400012280615&quot;
## [1] &quot;step =  1  lambda =  0.537944437594675  loss:  265.757684941799&quot;
## [1] &quot;step =  2  lambda =  0.537944437594675  loss:  271.013984620634&quot;
## [1] &quot;step =  1  lambda =  0.532591801006898  loss:  268.345509542527&quot;
## [1] &quot;step =  2  lambda =  0.532591801006898  loss:  273.242886100161&quot;
## [1] &quot;step =  1  lambda =  0.527292424043048  loss:  270.560536569573&quot;
## [1] &quot;step =  2  lambda =  0.527292424043048  loss:  275.167598816616&quot;
## [1] &quot;step =  1  lambda =  0.522045776761016  loss:  272.460572552772&quot;
## [1] &quot;step =  2  lambda =  0.522045776761016  loss:  277.001084117745&quot;
## [1] &quot;step =  1  lambda =  0.516851334491699  loss:  274.275558619107&quot;
## [1] &quot;step =  2  lambda =  0.516851334491699  loss:  278.604135529207&quot;
## [1] &quot;step =  1  lambda =  0.511708577786543  loss:  275.86243282375&quot;
## [1] &quot;step =  2  lambda =  0.511708577786543  loss:  280.081168588344&quot;
## [1] &quot;step =  1  lambda =  0.50661699236559  loss:  277.324527744323&quot;
## [1] &quot;step =  2  lambda =  0.50661699236559  loss:  281.446997964133&quot;
## [1] &quot;step =  1  lambda =  0.501576069066056  loss:  278.676518423806&quot;
## [1] &quot;step =  2  lambda =  0.501576069066056  loss:  282.703810282259&quot;
## [1] &quot;step =  1  lambda =  0.49658530379141  loss:  279.920575889558&quot;
## [1] &quot;step =  2  lambda =  0.49658530379141  loss:  283.853486083808&quot;
## [1] &quot;step =  1  lambda =  0.491644197460966  loss:  281.058565256461&quot;
## [1] &quot;step =  2  lambda =  0.491644197460966  loss:  285.189710210158&quot;
## [1] &quot;step =  1  lambda =  0.486752255959971  loss:  282.378906742111&quot;
## [1] &quot;step =  2  lambda =  0.486752255959971  loss:  286.481390210649&quot;
## [1] &quot;step =  1  lambda =  0.481908990090202  loss:  283.657067254723&quot;
## [1] &quot;step =  2  lambda =  0.481908990090202  loss:  287.671369553863&quot;
## [1] &quot;step =  1  lambda =  0.477113915521034  loss:  284.835013634752&quot;
## [1] &quot;step =  2  lambda =  0.477113915521034  loss:  288.713434246228&quot;
## [1] &quot;step =  1  lambda =  0.472366552741015  loss:  285.866519819146&quot;
## [1] &quot;step =  2  lambda =  0.472366552741015  loss:  289.611559279919&quot;
## [1] &quot;step =  1  lambda =  0.467666427009909  loss:  286.753248845891&quot;
## [1] &quot;step =  2  lambda =  0.467666427009909  loss:  290.359521822325&quot;
## [1] &quot;step =  1  lambda =  0.463013068311228  loss:  287.493622154965&quot;
## [1] &quot;step =  2  lambda =  0.463013068311228  loss:  290.977838540565&quot;
## [1] &quot;step =  1  lambda =  0.458406011305224  loss:  288.105651445233&quot;
## [1] &quot;step =  2  lambda =  0.458406011305224  loss:  291.483353011844&quot;
## [1] &quot;step =  1  lambda =  0.453844795282356  loss:  288.606011020191&quot;
## [1] &quot;step =  2  lambda =  0.453844795282356  loss:  291.890016548629&quot;
## [1] &quot;step =  1  lambda =  0.449328964117222  loss:  289.008511514296&quot;
## [1] &quot;step =  2  lambda =  0.449328964117222  loss:  292.209325276527&quot;
## [1] &quot;step =  1  lambda =  0.444858066222941  loss:  289.32453296991&quot;
## [1] &quot;step =  2  lambda =  0.444858066222941  loss:  292.450717226641&quot;
## [1] &quot;step =  1  lambda =  0.440431654505999  loss:  289.563418097049&quot;
## [1] &quot;step =  2  lambda =  0.440431654505999  loss:  292.62194310234&quot;
## [1] &quot;step =  1  lambda =  0.436049286321536  loss:  289.732839364208&quot;
## [1] &quot;step =  2  lambda =  0.436049286321536  loss:  292.729391421347&quot;
## [1] &quot;step =  1  lambda =  0.43171052342908  loss:  289.839120871886&quot;
## [1] &quot;step =  2  lambda =  0.43171052342908  loss:  292.778361478458&quot;
## [1] &quot;step =  1  lambda =  0.427414931948727  loss:  289.887508550344&quot;
## [1] &quot;step =  2  lambda =  0.427414931948727  loss:  292.77328612804&quot;
## [1] &quot;step =  1  lambda =  0.423162082317749  loss:  289.882390676171&quot;
## [1] &quot;step =  2  lambda =  0.423162082317749  loss:  292.717910281877&quot;
## [1] &quot;step =  1  lambda =  0.418951549247639  loss:  289.827474551096&quot;
## [1] &quot;step =  2  lambda =  0.418951549247639  loss:  292.615432183742&quot;
## [1] &quot;step =  1  lambda =  0.414782911681582  loss:  289.725926336176&quot;
## [1] &quot;step =  2  lambda =  0.414782911681582  loss:  292.468614337908&quot;
## [1] &quot;step =  1  lambda =  0.410655752752345  loss:  289.580480849957&quot;
## [1] &quot;step =  2  lambda =  0.410655752752345  loss:  292.279870208103&quot;
## [1] &quot;step =  1  lambda =  0.406569659740599  loss:  289.393527385149&quot;
## [1] &quot;step =  2  lambda =  0.406569659740599  loss:  292.051331860861&quot;
## [1] &quot;step =  1  lambda =  0.402524224033636  loss:  289.167176664921&quot;
## [1] &quot;step =  2  lambda =  0.402524224033636  loss:  291.784902792562&quot;
## [1] &quot;step =  1  lambda =  0.398519041084514  loss:  288.903313134668&quot;
## [1] &quot;step =  2  lambda =  0.398519041084514  loss:  291.482299335934&quot;
## [1] &quot;step =  1  lambda =  0.394553710371601  loss:  288.6036359501&quot;
## [1] &quot;step =  2  lambda =  0.394553710371601  loss:  291.145083318792&quot;
## [1] &quot;step =  1  lambda =  0.390627835358521  loss:  288.269691306951&quot;
## [1] &quot;step =  2  lambda =  0.390627835358521  loss:  290.774688047818&quot;
## [1] &quot;step =  1  lambda =  0.386741023454502  loss:  287.902898163818&quot;
## [1] &quot;step =  2  lambda =  0.386741023454502  loss:  290.372439203131&quot;
## [1] &quot;step =  1  lambda =  0.382892885975112  loss:  287.504568927544&quot;
## [1] &quot;step =  2  lambda =  0.382892885975112  loss:  289.939571839901&quot;
## [1] &quot;step =  1  lambda =  0.379083038103399  loss:  287.075926285133&quot;
## [1] &quot;step =  2  lambda =  0.379083038103399  loss:  289.477244384981&quot;
## [1] &quot;step =  1  lambda =  0.375311098851399  loss:  286.618117061015&quot;
## [1] &quot;step =  2  lambda =  0.375311098851399  loss:  288.98655027389&quot;
## [1] &quot;step =  1  lambda =  0.371576691022046  loss:  286.132223738365&quot;
## [1] &quot;step =  2  lambda =  0.371576691022046  loss:  288.468527683024&quot;
## [1] &quot;step =  1  lambda =  0.367879441171442  loss:  285.619274094637&quot;
## [1] &quot;step =  2  lambda =  0.367879441171442  loss:  287.924167662446&quot;
## [1] &quot;step =  1  lambda =  0.364218979571523  loss:  285.080249253489&quot;
## [1] &quot;step =  2  lambda =  0.364218979571523  loss:  287.354420857432&quot;
## [1] &quot;step =  1  lambda =  0.360594940173078  loss:  284.516090339284&quot;
## [1] &quot;step =  2  lambda =  0.360594940173078  loss:  286.760202915924&quot;
## [1] &quot;step =  1  lambda =  0.357006960569148  loss:  283.927703830257&quot;
## [1] &quot;step =  2  lambda =  0.357006960569148  loss:  286.142398610365&quot;
## [1] &quot;step =  1  lambda =  0.35345468195878  loss:  283.315965638466&quot;
## [1] &quot;step =  2  lambda =  0.35345468195878  loss:  285.501864654513&quot;
## [1] &quot;step =  1  lambda =  0.349937749111156  loss:  282.681723897274&quot;
## [1] &quot;step =  2  lambda =  0.349937749111156  loss:  284.839431169102&quot;
## [1] &quot;step =  1  lambda =  0.346455810330057  loss:  282.025800410677&quot;
## [1] &quot;step =  2  lambda =  0.346455810330057  loss:  284.155901746522&quot;
## [1] &quot;step =  1  lambda =  0.343008517418707  loss:  281.348990715188&quot;
## [1] &quot;step =  2  lambda =  0.343008517418707  loss:  283.45205208658&quot;
## [1] &quot;step =  1  lambda =  0.339595525644939  loss:  280.652062726705&quot;
## [1] &quot;step =  2  lambda =  0.339595525644939  loss:  282.710522600691&quot;
## [1] &quot;step =  1  lambda =  0.336216493706733  loss:  279.918058670885&quot;
## [1] &quot;step =  2  lambda =  0.336216493706733  loss:  281.934644246268&quot;
## [1] &quot;step =  1  lambda =  0.33287108369808  loss:  279.149815486561&quot;
## [1] &quot;step =  2  lambda =  0.33287108369808  loss:  281.129107315561&quot;
## [1] &quot;step =  1  lambda =  0.329558961075189  loss:  278.352208578631&quot;
## [1] &quot;step =  2  lambda =  0.329558961075189  loss:  280.298927433674&quot;
## [1] &quot;step =  1  lambda =  0.32627979462304  loss:  277.530203682299&quot;
## [1] &quot;step =  2  lambda =  0.32627979462304  loss:  279.447878781092&quot;
## [1] &quot;step =  1  lambda =  0.323033256422253  loss:  276.68753729318&quot;
## [1] &quot;step =  2  lambda =  0.323033256422253  loss:  278.57870881656&quot;
## [1] &quot;step =  1  lambda =  0.319819021816304  loss:  275.826929299364&quot;
## [1] &quot;step =  2  lambda =  0.319819021816304  loss:  277.693473916514&quot;
## [1] &quot;step =  1  lambda =  0.316636769379053  loss:  274.950415387535&quot;
## [1] &quot;step =  2  lambda =  0.316636769379053  loss:  276.793724851405&quot;
## [1] &quot;step =  1  lambda =  0.313486180882605  loss:  274.059530672849&quot;
## [1] &quot;step =  2  lambda =  0.313486180882605  loss:  275.880615041375&quot;
## [1] &quot;step =  1  lambda =  0.310366941265485  loss:  273.15541687296&quot;
## [1] &quot;step =  2  lambda =  0.310366941265485  loss:  274.954975337332&quot;
## [1] &quot;step =  1  lambda =  0.307278738601131  loss:  272.238896348916&quot;
## [1] &quot;step =  2  lambda =  0.307278738601131  loss:  274.017372689118&quot;
## [1] &quot;step =  1  lambda =  0.304221264066704  loss:  271.310530198462&quot;
## [1] &quot;step =  2  lambda =  0.304221264066704  loss:  273.06815979418&quot;
## [1] &quot;step =  1  lambda =  0.301194211912202  loss:  270.370667420933&quot;
## [1] &quot;step =  2  lambda =  0.301194211912202  loss:  272.107518913292&quot;
## [1] &quot;step =  1  lambda =  0.298197279429888  loss:  269.419488306391&quot;
## [1] &quot;step =  2  lambda =  0.298197279429888  loss:  271.135501311001&quot;
## [1] &quot;step =  1  lambda =  0.295230166924014  loss:  268.457043490865&quot;
## [1] &quot;step =  2  lambda =  0.295230166924014  loss:  270.152062870666&quot;
## [1] &quot;step =  1  lambda =  0.292292577680859  loss:  267.483289221502&quot;
## [1] &quot;step =  2  lambda =  0.292292577680859  loss:  269.157095937431&quot;
## [1] &quot;step =  1  lambda =  0.289384217939051  loss:  266.498118884271&quot;
## [1] &quot;step =  2  lambda =  0.289384217939051  loss:  268.150457217633&quot;
## [1] &quot;step =  1  lambda =  0.28650479686019  loss:  265.501390624665&quot;
## [1] &quot;step =  2  lambda =  0.28650479686019  loss:  267.131991532748&quot;
## [1] &quot;step =  1  lambda =  0.28365402649977  loss:  264.492950861961&quot;
## [1] &quot;step =  2  lambda =  0.28365402649977  loss:  266.101551322406&quot;
## [1] &quot;step =  1  lambda =  0.28083162177838  loss:  263.472653593233&quot;
## [1] &quot;step =  2  lambda =  0.28083162177838  loss:  264.976735709718&quot;
## [1] &quot;step =  1  lambda =  0.278037300453194  loss:  262.359433611399&quot;
## [1] &quot;step =  2  lambda =  0.278037300453194  loss:  263.870392785193&quot;
## [1] &quot;step =  1  lambda =  0.275270783089753  loss:  261.263986074846&quot;
## [1] &quot;step =  2  lambda =  0.275270783089753  loss:  262.764053472787&quot;
## [1] &quot;step =  1  lambda =  0.272531793034013  loss:  260.168537847882&quot;
## [1] &quot;step =  2  lambda =  0.272531793034013  loss:  261.651390142074&quot;
## [1] &quot;step =  1  lambda =  0.269820056384687  loss:  259.066825679781&quot;
## [1] &quot;step =  2  lambda =  0.269820056384687  loss:  260.536092725662&quot;
## [1] &quot;step =  1  lambda =  0.26713530196585  loss:  257.962340986188&quot;
## [1] &quot;step =  2  lambda =  0.26713530196585  loss:  259.408557818499&quot;
## [1] &quot;step =  1  lambda =  0.264477261299824  loss:  256.8459746122&quot;
## [1] &quot;step =  2  lambda =  0.264477261299824  loss:  258.262140123466&quot;
## [1] &quot;step =  1  lambda =  0.261845668580326  loss:  255.710847198532&quot;
## [1] &quot;step =  2  lambda =  0.261845668580326  loss:  257.106836299559&quot;
## [1] &quot;step =  1  lambda =  0.259240260645892  loss:  254.566920370966&quot;
## [1] &quot;step =  2  lambda =  0.259240260645892  loss:  255.943238310091&quot;
## [1] &quot;step =  1  lambda =  0.256660776953556  loss:  253.414780633495&quot;
## [1] &quot;step =  2  lambda =  0.256660776953556  loss:  254.771450260066&quot;
## [1] &quot;step =  1  lambda =  0.2541069595528  loss:  252.254531357252&quot;
## [1] &quot;step =  2  lambda =  0.2541069595528  loss:  253.591446283937&quot;
## [1] &quot;step =  1  lambda =  0.251578553059757  loss:  251.086147194362&quot;
## [1] &quot;step =  2  lambda =  0.251578553059757  loss:  252.403204880225&quot;
## [1] &quot;step =  1  lambda =  0.249075304631668  loss:  249.90960708391&quot;
## [1] &quot;step =  2  lambda =  0.249075304631668  loss:  251.209031723467&quot;
## [1] &quot;step =  1  lambda =  0.246596963941606  loss:  248.728094531686&quot;
## [1] &quot;step =  2  lambda =  0.246596963941606  loss:  250.009700665385&quot;
## [1] &quot;step =  1  lambda =  0.244143283153437  loss:  247.540550042631&quot;
## [1] &quot;step =  2  lambda =  0.244143283153437  loss:  248.803189871992&quot;
## [1] &quot;step =  1  lambda =  0.241714016897036  loss:  246.345897863297&quot;
## [1] &quot;step =  2  lambda =  0.241714016897036  loss:  247.589293052651&quot;
## [1] &quot;step =  1  lambda =  0.239308922243755  loss:  245.143933909028&quot;
## [1] &quot;step =  2  lambda =  0.239308922243755  loss:  246.311352340491&quot;
## [1] &quot;step =  1  lambda =  0.236927758682122  loss:  243.879136222432&quot;
## [1] &quot;step =  2  lambda =  0.236927758682122  loss:  244.98957904814&quot;
## [1] &quot;step =  1  lambda =  0.234570288093798  loss:  242.570369798591&quot;
## [1] &quot;step =  2  lambda =  0.234570288093798  loss:  243.674427559235&quot;
## [1] &quot;step =  1  lambda =  0.232236274729759  loss:  241.268157829072&quot;
## [1] &quot;step =  2  lambda =  0.232236274729759  loss:  242.363777993493&quot;
## [1] &quot;step =  1  lambda =  0.229925485186724  loss:  239.970402346319&quot;
## [1] &quot;step =  2  lambda =  0.229925485186724  loss:  241.056340979257&quot;
## [1] &quot;step =  1  lambda =  0.227637688383813  loss:  238.675827314715&quot;
## [1] &quot;step =  2  lambda =  0.227637688383813  loss:  239.750644886475&quot;
## [1] &quot;step =  1  lambda =  0.225372655539439  loss:  237.382976091212&quot;
## [1] &quot;step =  2  lambda =  0.225372655539439  loss:  238.445401709151&quot;
## [1] &quot;step =  1  lambda =  0.22313016014843  loss:  236.090573741337&quot;
## [1] &quot;step =  2  lambda =  0.22313016014843  loss:  237.139621682581&quot;
## [1] &quot;step =  1  lambda =  0.220909977959378  loss:  234.7976405456&quot;
## [1] &quot;step =  2  lambda =  0.220909977959378  loss:  235.832602228644&quot;
## [1] &quot;step =  1  lambda =  0.218711886952215  loss:  233.503481071585&quot;
## [1] &quot;step =  2  lambda =  0.218711886952215  loss:  234.523832757021&quot;
## [1] &quot;step =  1  lambda =  0.216535667316007  loss:  232.207459744122&quot;
## [1] &quot;step =  2  lambda =  0.216535667316007  loss:  233.212674762641&quot;
## [1] &quot;step =  1  lambda =  0.214381101426978  loss:  230.909207329191&quot;
## [1] &quot;step =  2  lambda =  0.214381101426978  loss:  231.89909131373&quot;
## [1] &quot;step =  1  lambda =  0.212247973826743  loss:  229.608554635957&quot;
## [1] &quot;step =  2  lambda =  0.212247973826743  loss:  230.58303296718&quot;
## [1] &quot;step =  1  lambda =  0.210136071200765  loss:  228.305452781003&quot;
## [1] &quot;step =  2  lambda =  0.210136071200765  loss:  229.264554232375&quot;
## [1] &quot;step =  1  lambda =  0.20804518235702  loss:  226.999955783662&quot;
## [1] &quot;step =  2  lambda =  0.20804518235702  loss:  227.943784945164&quot;
## [1] &quot;step =  1  lambda =  0.205975098204883  loss:  225.692192227222&quot;
## [1] &quot;step =  2  lambda =  0.205975098204883  loss:  226.620908962902&quot;
## [1] &quot;step =  1  lambda =  0.203925611734213  loss:  224.382344168111&quot;
## [1] &quot;step =  2  lambda =  0.203925611734213  loss:  225.26446832059&quot;
## [1] &quot;step =  1  lambda =  0.201896517994655  loss:  223.039663378321&quot;
## [1] &quot;step =  2  lambda =  0.201896517994655  loss:  223.908116522007&quot;
## [1] &quot;step =  1  lambda =  0.199887614075145  loss:  221.696730229952&quot;
## [1] &quot;step =  2  lambda =  0.199887614075145  loss:  222.568471418563&quot;
## [1] &quot;step =  1  lambda =  0.197898699083615  loss:  220.370276754558&quot;
## [1] &quot;step =  2  lambda =  0.197898699083615  loss:  221.236608642503&quot;
## [1] &quot;step =  1  lambda =  0.19592957412691  loss:  219.051529727339&quot;
## [1] &quot;step =  2  lambda =  0.19592957412691  loss:  219.907209293946&quot;
## [1] &quot;step =  1  lambda =  0.193980042290892  loss:  217.735223340418&quot;
## [1] &quot;step =  2  lambda =  0.193980042290892  loss:  218.578747324246&quot;
## [1] &quot;step =  1  lambda =  0.192049908620754  loss:  216.419846656641&quot;
## [1] &quot;step =  2  lambda =  0.192049908620754  loss:  217.25104617719&quot;
## [1] &quot;step =  1  lambda =  0.19013898010152  loss:  215.105224791879&quot;
## [1] &quot;step =  2  lambda =  0.19013898010152  loss:  215.924335920104&quot;
## [1] &quot;step =  1  lambda =  0.188247065638747  loss:  213.791585450949&quot;
## [1] &quot;step =  2  lambda =  0.188247065638747  loss:  214.598930223018&quot;
## [1] &quot;step =  1  lambda =  0.18637397603941  loss:  212.479239122055&quot;
## [1] &quot;step =  2  lambda =  0.18637397603941  loss:  213.275126630611&quot;
## [1] &quot;step =  1  lambda =  0.184519523992989  loss:  211.168480337887&quot;
## [1] &quot;step =  2  lambda =  0.184519523992989  loss:  211.953183303396&quot;
## [1] &quot;step =  1  lambda =  0.182683524052735  loss:  209.859564645321&quot;
## [1] &quot;step =  2  lambda =  0.182683524052735  loss:  210.633320098221&quot;
## [1] &quot;step =  1  lambda =  0.180865792617122  loss:  208.552709673203&quot;
## [1] &quot;step =  2  lambda =  0.180865792617122  loss:  209.315725942574&quot;
## [1] &quot;step =  1  lambda =  0.179066147911493  loss:  207.248102432065&quot;
## [1] &quot;step =  2  lambda =  0.179066147911493  loss:  208.000566579163&quot;
## [1] &quot;step =  1  lambda =  0.177284409969878  loss:  205.945906980898&quot;
## [1] &quot;step =  2  lambda =  0.177284409969878  loss:  206.687991008235&quot;
## [1] &quot;step =  1  lambda =  0.175520400616997  loss:  204.646270804906&quot;
## [1] &quot;step =  2  lambda =  0.175520400616997  loss:  205.435912176384&quot;
## [1] &quot;step =  1  lambda =  0.173773943450445  loss:  203.405962601942&quot;
## [1] &quot;step =  2  lambda =  0.173773943450445  loss:  204.1592487182&quot;
## [1] &quot;step =  1  lambda =  0.172044863823051  loss:  202.141885647265&quot;
## [1] &quot;step =  2  lambda =  0.172044863823051  loss:  202.873099609895&quot;
## [1] &quot;step =  1  lambda =  0.17033298882541  loss:  200.868421177837&quot;
## [1] &quot;step =  2  lambda =  0.17033298882541  loss:  201.579380412155&quot;
## [1] &quot;step =  1  lambda =  0.168638147268596  loss:  199.587464648311&quot;
## [1] &quot;step =  2  lambda =  0.168638147268596  loss:  200.279590014996&quot;
## [1] &quot;step =  1  lambda =  0.166960169667041  loss:  198.300499859084&quot;
## [1] &quot;step =  2  lambda =  0.166960169667041  loss:  198.975431507675&quot;
## [1] &quot;step =  1  lambda =  0.165298888221586  loss:  197.009212863184&quot;
## [1] &quot;step =  2  lambda =  0.165298888221586  loss:  197.66851960155&quot;
## [1] &quot;step =  1  lambda =  0.163654136802704  loss:  195.715202176477&quot;
## [1] &quot;step =  2  lambda =  0.163654136802704  loss:  196.360262159103&quot;
## [1] &quot;step =  1  lambda =  0.162025750933881  loss:  194.419861498788&quot;
## [1] &quot;step =  2  lambda =  0.162025750933881  loss:  195.051847294094&quot;
## [1] &quot;step =  1  lambda =  0.160413567775173  loss:  193.124366960427&quot;
## [1] &quot;step =  2  lambda =  0.160413567775173  loss:  193.744267650845&quot;
## [1] &quot;step =  1  lambda =  0.158817426106921  loss:  191.829701170739&quot;
## [1] &quot;step =  2  lambda =  0.158817426106921  loss:  192.438352444638&quot;
## [1] &quot;step =  1  lambda =  0.157237166313628  loss:  190.536684945419&quot;
## [1] &quot;step =  2  lambda =  0.157237166313628  loss:  191.134797103078&quot;
## [1] &quot;step =  1  lambda =  0.155672630367997  loss:  189.246006655565&quot;
## [1] &quot;step =  2  lambda =  0.155672630367997  loss:  189.834187957961&quot;
## [1] &quot;step =  1  lambda =  0.154123661815132  loss:  187.958246674706&quot;
## [1] &quot;step =  2  lambda =  0.154123661815132  loss:  188.537022071236&quot;
## [1] &quot;step =  1  lambda =  0.152590105756884  loss:  186.673897007309&quot;
## [1] &quot;step =  2  lambda =  0.152590105756884  loss:  187.243722999319&quot;
## [1] &quot;step =  1  lambda =  0.151071808836371  loss:  185.393376895512&quot;
## [1] &quot;step =  2  lambda =  0.151071808836371  loss:  185.954653353763&quot;
## [1] &quot;step =  1  lambda =  0.149568619222635  loss:  184.117045253838&quot;
## [1] &quot;step =  2  lambda =  0.149568619222635  loss:  184.670124878499&quot;
## [1] &quot;step =  1  lambda =  0.148080386595462  loss:  182.845210645088&quot;
## [1] &quot;step =  2  lambda =  0.148080386595462  loss:  183.390406601742&quot;
## [1] &quot;step =  1  lambda =  0.14660696213035  loss:  181.57813935007&quot;
## [1] &quot;step =  2  lambda =  0.14660696213035  loss:  182.115731483527&quot;
## [1] &quot;step =  1  lambda =  0.145148198483624  loss:  180.316061947986&quot;
## [1] &quot;step =  2  lambda =  0.145148198483624  loss:  180.846301875367&quot;
## [1] &quot;step =  1  lambda =  0.143703949777703  loss:  179.059178720844&quot;
## [1] &quot;step =  2  lambda =  0.143703949777703  loss:  179.582294031999&quot;
## [1] &quot;step =  1  lambda =  0.142274071586514  loss:  177.807664119481&quot;
## [1] &quot;step =  2  lambda =  0.142274071586514  loss:  178.323861859513&quot;
## [1] &quot;step =  1  lambda =  0.140858420921045  loss:  176.561670473662&quot;
## [1] &quot;step =  2  lambda =  0.140858420921045  loss:  177.071140043414&quot;
## [1] &quot;step =  1  lambda =  0.139456856215051  loss:  175.321331088375&quot;
## [1] &quot;step =  2  lambda =  0.139456856215051  loss:  175.824246669892&quot;
## [1] &quot;step =  1  lambda =  0.138069237310893  loss:  174.086762838453&quot;
## [1] &quot;step =  2  lambda =  0.138069237310893  loss:  174.583285430639&quot;
## [1] &quot;step =  1  lambda =  0.136695425445524  loss:  172.858068350958&quot;
## [1] &quot;step =  2  lambda =  0.136695425445524  loss:  173.348347483914&quot;
## [1] &quot;step =  1  lambda =  0.135335283236613  loss:  171.635337847316&quot;
## [1] &quot;step =  2  lambda =  0.135335283236613  loss:  172.11951303079&quot;
## [1] &quot;step =  1  lambda =  0.133988674668805  loss:  170.418650703492&quot;
## [1] &quot;step =  2  lambda =  0.133988674668805  loss:  170.896852654562&quot;
## [1] &quot;step =  1  lambda =  0.132655465080122  loss:  169.208076775771&quot;
## [1] &quot;step =  2  lambda =  0.132655465080122  loss:  169.680428462636&quot;
## [1] &quot;step =  1  lambda =  0.131335521148493  loss:  168.003677531001&quot;
## [1] &quot;step =  2  lambda =  0.131335521148493  loss:  168.462029962978&quot;
## [1] &quot;step =  1  lambda =  0.130028710878426  loss:  166.797366217247&quot;
## [1] &quot;step =  2  lambda =  0.130028710878426  loss:  167.234583130201&quot;
## [1] &quot;step =  1  lambda =  0.128734903587804  loss:  165.582254798035&quot;
## [1] &quot;step =  2  lambda =  0.128734903587804  loss:  166.015576668886&quot;
## [1] &quot;step =  1  lambda =  0.127453969894821  loss:  164.375295335053&quot;
## [1] &quot;step =  2  lambda =  0.127453969894821  loss:  164.804874918622&quot;
## [1] &quot;step =  1  lambda =  0.126185781705039  loss:  163.176559033753&quot;
## [1] &quot;step =  2  lambda =  0.126185781705039  loss:  163.602480457913&quot;
## [1] &quot;step =  1  lambda =  0.124930212198582  loss:  161.986048087403&quot;
## [1] &quot;step =  2  lambda =  0.124930212198582  loss:  162.408108882247&quot;
## [1] &quot;step =  1  lambda =  0.123687135817455  loss:  160.803480831115&quot;
## [1] &quot;step =  2  lambda =  0.123687135817455  loss:  161.221538336281&quot;
## [1] &quot;step =  1  lambda =  0.122456428252982  loss:  159.628637551762&quot;
## [1] &quot;step =  2  lambda =  0.122456428252982  loss:  160.042602817064&quot;
## [1] &quot;step =  1  lambda =  0.121237966433382  loss:  158.461353844639&quot;
## [1] &quot;step =  2  lambda =  0.121237966433382  loss:  158.871175527447&quot;
## [1] &quot;step =  1  lambda =  0.120031628511457  loss:  157.301504136275&quot;
## [1] &quot;step =  2  lambda =  0.120031628511457  loss:  157.707157305011&quot;
## [1] &quot;step =  1  lambda =  0.11883729385241  loss:  156.148990226472&quot;
## [1] &quot;step =  2  lambda =  0.11883729385241  loss:  156.550468559475&quot;
## [1] &quot;step =  1  lambda =  0.117654843021779  loss:  155.003733302024&quot;
## [1] &quot;step =  2  lambda =  0.117654843021779  loss:  155.401043581387&quot;
## [1] &quot;step =  1  lambda =  0.116484157773497  loss:  153.865668298589&quot;
## [1] &quot;step =  2  lambda =  0.116484157773497  loss:  154.2588265661&quot;
## [1] &quot;step =  1  lambda =  0.115325121038063  loss:  152.734739961718&quot;
## [1] &quot;step =  2  lambda =  0.115325121038063  loss:  153.123768869271&quot;
## [1] &quot;step =  1  lambda =  0.114177616910836  loss:  151.610900128002&quot;
## [1] &quot;step =  2  lambda =  0.114177616910836  loss:  151.995827119287&quot;
## [1] &quot;step =  1  lambda =  0.11304153064045  loss:  150.494105855207&quot;
## [1] &quot;step =  2  lambda =  0.11304153064045  loss:  150.874961910326&quot;
## [1] &quot;step =  1  lambda =  0.111916748617329  loss:  149.384318127711&quot;
## [1] &quot;step =  2  lambda =  0.111916748617329  loss:  149.761136882997&quot;
## [1] &quot;step =  1  lambda =  0.110803158362334  loss:  148.281500945913&quot;
## [1] &quot;step =  2  lambda =  0.110803158362334  loss:  148.654318062615&quot;
## [1] &quot;step =  1  lambda =  0.109700648515511  loss:  147.185620670894&quot;
## [1] &quot;step =  2  lambda =  0.109700648515511  loss:  147.554473369649&quot;
## [1] &quot;step =  1  lambda =  0.108609108824958  loss:  146.09664553966&quot;
## [1] &quot;step =  2  lambda =  0.108609108824958  loss:  146.461572246739&quot;
## [1] &quot;step =  1  lambda =  0.107528430135795  loss:  145.014545295851&quot;
## [1] &quot;step =  2  lambda =  0.107528430135795  loss:  145.375585366195&quot;
## [1] &quot;step =  1  lambda =  0.106458504379253  loss:  143.93929090017&quot;
## [1] &quot;step =  2  lambda =  0.106458504379253  loss:  144.296484394459&quot;
## [1] &quot;step =  1  lambda =  0.105399224561864  loss:  142.870854297246&quot;
## [1] &quot;step =  2  lambda =  0.105399224561864  loss:  143.224241798121&quot;
## [1] &quot;step =  1  lambda =  0.104350484754765  loss:  141.80920822363&quot;
## [1] &quot;step =  2  lambda =  0.104350484754765  loss:  142.158830681228&quot;
## [1] &quot;step =  1  lambda =  0.1033121800831  loss:  140.75432604678&quot;
## [1] &quot;step =  2  lambda =  0.1033121800831  loss:  141.100224646966&quot;
## [1] &quot;step =  1  lambda =  0.102284206715537  loss:  139.706181628184&quot;
## [1] &quot;step =  2  lambda =  0.102284206715537  loss:  140.048397678982&quot;
## [1] &quot;step =  1  lambda =  0.101266461853883  loss:  138.664749205912&quot;
## [1] &quot;step =  2  lambda =  0.101266461853883  loss:  139.003324038984&quot;
## [1] &quot;step =  1  lambda =  0.100258843722804  loss:  137.63000329328&quot;
## [1] &quot;step =  2  lambda =  0.100258843722804  loss:  137.964978178183&quot;
## [1] &quot;step =  1  lambda =  0.0992612515596457  loss:  136.601918591221&quot;
## [1] &quot;step =  2  lambda =  0.0992612515596457  loss:  136.933334660764&quot;
## [1] &quot;step =  1  lambda =  0.0982735856043615  loss:  135.580469912549&quot;
## [1] &quot;step =  2  lambda =  0.0982735856043615  loss:  135.908368097942&quot;
## [1] &quot;step =  1  lambda =  0.0972957470895328  loss:  134.565632116702&quot;
## [1] &quot;step =  2  lambda =  0.0972957470895328  loss:  134.890053091445&quot;
## [1] &quot;step =  1  lambda =  0.096327638230493  loss:  133.55738005381&quot;
## [1] &quot;step =  2  lambda =  0.096327638230493  loss:  133.878364185425&quot;
## [1] &quot;step =  1  lambda =  0.0953691622155497  loss:  132.555688517108&quot;
## [1] &quot;step =  2  lambda =  0.0953691622155497  loss:  132.873275825959&quot;
## [1] &quot;step =  1  lambda =  0.0944202231963024  loss:  131.56053220285&quot;
## [1] &quot;step =  2  lambda =  0.0944202231963024  loss:  131.874762327341&quot;
## [1] &quot;step =  1  lambda =  0.0934807262780585  loss:  130.571885676951&quot;
## [1] &quot;step =  2  lambda =  0.0934807262780585  loss:  130.88279784448&quot;
## [1] &quot;step =  1  lambda =  0.0925505775103433  loss:  129.589723347672&quot;
## [1] &quot;step =  2  lambda =  0.0925505775103433  loss:  129.897356350758&quot;
## [1] &quot;step =  1  lambda =  0.0916296838775049  loss:  128.614019443697&quot;
## [1] &quot;step =  2  lambda =  0.0916296838775049  loss:  128.918411620731&quot;
## [1] &quot;step =  1  lambda =  0.0907179532894126  loss:  127.644747997017&quot;
## [1] &quot;step =  2  lambda =  0.0907179532894126  loss:  127.945937217145&quot;
## [1] &quot;step =  1  lambda =  0.0898152945722476  loss:  126.681882830071&quot;
## [1] &quot;step =  2  lambda =  0.0898152945722476  loss:  126.979906481725&quot;
## [1] &quot;step =  1  lambda =  0.0889216174593863  loss:  125.725397546635&quot;
## [1] &quot;step =  2  lambda =  0.0889216174593863  loss:  126.020292529293&quot;
## [1] &quot;step =  1  lambda =  0.0880368325823726  loss:  124.775265525991&quot;
## [1] &quot;step =  2  lambda =  0.0880368325823726  loss:  125.067068244754&quot;
## [1] &quot;step =  1  lambda =  0.0871608514619813  loss:  123.831459919943&quot;
## [1] &quot;step =  2  lambda =  0.0871608514619813  loss:  124.120206282573&quot;
## [1] &quot;step =  1  lambda =  0.0862935864993705  loss:  122.893953652301&quot;
## [1] &quot;step =  2  lambda =  0.0862935864993705  loss:  123.179679068367&quot;
## [1] &quot;step =  1  lambda =  0.0854349509673212  loss:  121.962719420451&quot;
## [1] &quot;step =  2  lambda =  0.0854349509673212  loss:  122.245458802305&quot;
## [1] &quot;step =  1  lambda =  0.0845848590015647  loss:  121.037729698716&quot;
## [1] &quot;step =  2  lambda =  0.0845848590015647  loss:  121.317517463999&quot;
## [1] &quot;step =  1  lambda =  0.083743225592196  loss:  120.118956743202&quot;
## [1] &quot;step =  2  lambda =  0.083743225592196  loss:  120.395826818657&quot;
## [1] &quot;step =  1  lambda =  0.0829099665751727  loss:  119.206372597877&quot;
## [1] &quot;step =  2  lambda =  0.0829099665751727  loss:  119.480358424237&quot;
## [1] &quot;step =  1  lambda =  0.0820849986238988  loss:  118.299949101654&quot;
## [1] &quot;step =  2  lambda =  0.0820849986238988  loss:  118.571083639418&quot;
## [1] &quot;step =  1  lambda =  0.0812682392408917  loss:  117.39965789628&quot;
## [1] &quot;step =  2  lambda =  0.0812682392408917  loss:  117.667973632206&quot;
## [1] &quot;step =  1  lambda =  0.0804596067495325  loss:  116.505470434847&quot;
## [1] &quot;step =  2  lambda =  0.0804596067495325  loss:  116.770999389011&quot;
## [1] &quot;step =  1  lambda =  0.079659020285898  loss:  115.617357990782&quot;
## [1] &quot;step =  2  lambda =  0.079659020285898  loss:  115.880131724078&quot;
## [1] &quot;step =  1  lambda =  0.0788663997906749  loss:  114.735291667171&quot;
## [1] &quot;step =  2  lambda =  0.0788663997906749  loss:  114.99534128914&quot;
## [1] &quot;step =  1  lambda =  0.0780816660011532  loss:  113.859242406318&quot;
## [1] &quot;step =  2  lambda =  0.0780816660011532  loss:  114.11659858321&quot;
## [1] &quot;step =  1  lambda =  0.0773047404432998  loss:  112.989180999431&quot;
## [1] &quot;step =  2  lambda =  0.0773047404432998  loss:  113.243873962411&quot;
## [1] &quot;step =  1  lambda =  0.0765355454239115  loss:  112.125078096353&quot;
## [1] &quot;step =  2  lambda =  0.0765355454239115  loss:  112.377137649799&quot;
## [1] &quot;step =  1  lambda =  0.0757740040228455  loss:  111.266904215282&quot;
## [1] &quot;step =  2  lambda =  0.0757740040228455  loss:  111.516359745097&quot;
## [1] &quot;step =  1  lambda =  0.075020040085327  loss:  110.414629752403&quot;
## [1] &quot;step =  2  lambda =  0.075020040085327  loss:  110.66151023431&quot;
## [1] &quot;step =  1  lambda =  0.0742735782143339  loss:  109.568224991402&quot;
## [1] &quot;step =  2  lambda =  0.0742735782143339  loss:  109.81255899917&quot;
## [1] &quot;step =  1  lambda =  0.0735345437630571  loss:  108.727660112818&quot;
## [1] &quot;step =  2  lambda =  0.0735345437630571  loss:  108.969475826395&quot;
## [1] &quot;step =  1  lambda =  0.0728028628274356  loss:  107.8929052032&quot;
## [1] &quot;step =  2  lambda =  0.0728028628274356  loss:  108.132230416719&quot;
## [1] &quot;step =  1  lambda =  0.0720784622387661  loss:  107.063930264046&quot;
## [1] &quot;step =  2  lambda =  0.0720784622387661  loss:  107.300792393694&quot;
## [1] &quot;step =  1  lambda =  0.0713612695563861  loss:  106.240705220514&quot;
## [1] &quot;step =  2  lambda =  0.0713612695563861  loss:  106.475131312233&quot;
## [1] &quot;step =  1  lambda =  0.0706512130604296  loss:  105.423199929876&quot;
## [1] &quot;step =  2  lambda =  0.0706512130604296  loss:  105.655216666903&quot;
## [1] &quot;step =  1  lambda =  0.0699482217446554  loss:  104.611384189722&quot;
## [1] &quot;step =  2  lambda =  0.0699482217446554  loss:  104.841017899942&quot;
## [1] &quot;step =  1  lambda =  0.069252225309346  loss:  103.805227745899&quot;
## [1] &quot;step =  2  lambda =  0.069252225309346  loss:  104.032504409015&quot;
## [1] &quot;step =  1  lambda =  0.0685631541542779  loss:  103.004700300182&quot;
## [1] &quot;step =  2  lambda =  0.0685631541542779  loss:  103.229645554697&quot;
## [1] &quot;step =  1  lambda =  0.0678809393717615  loss:  102.209771517677&quot;
## [1] &quot;step =  2  lambda =  0.0678809393717615  loss:  102.432410667685&quot;
## [1] &quot;step =  1  lambda =  0.0672055127397498  loss:  101.420411033967&quot;
## [1] &quot;step =  2  lambda =  0.0672055127397498  loss:  101.640769055754&quot;
## [1] &quot;step =  1  lambda =  0.0665368067150169  loss:  100.636588461987&quot;
## [1] &quot;step =  2  lambda =  0.0665368067150169  loss:  100.854690010441&quot;
## [1] &quot;step =  1  lambda =  0.065874754426403  loss:  99.8582733986406&quot;
## [1] &quot;step =  2  lambda =  0.065874754426403  loss:  100.074142813479&quot;
## [1] &quot;step =  1  lambda =  0.0652192896681276  loss:  99.0854354311691&quot;
## [1] &quot;step =  2  lambda =  0.0652192896681276  loss:  99.299096742981&quot;
## [1] &quot;step =  1  lambda =  0.0645703468931685  loss:  98.3180441432683&quot;
## [1] &quot;step =  2  lambda =  0.0645703468931685  loss:  98.529521079378&quot;
## [1] &quot;step =  1  lambda =  0.0639278612067076  loss:  97.5560691209662&quot;
## [1] &quot;step =  2  lambda =  0.0639278612067076  loss:  97.7653851111222&quot;
## [1] &quot;step =  1  lambda =  0.0632917683596407  loss:  96.7994799582654&quot;
## [1] &quot;step =  2  lambda =  0.0632917683596407  loss:  97.0066581401593&quot;
## [1] &quot;step =  1  lambda =  0.0626620047421532  loss:  96.0482462625597&quot;
## [1] &quot;step =  2  lambda =  0.0626620047421532  loss:  96.2533094871804&quot;
## [1] &quot;step =  1  lambda =  0.0620385073773583  loss:  95.3023376598298&quot;
## [1] &quot;step =  2  lambda =  0.0620385073773583  loss:  95.505308496658&quot;
## [1] &quot;step =  1  lambda =  0.0614212139150001  loss:  94.5617237996274&quot;
## [1] &quot;step =  2  lambda =  0.0614212139150001  loss:  94.7626245416761&quot;
## [1] &quot;step =  1  lambda =  0.060810062625218  loss:  93.8263743598543&quot;
## [1] &quot;step =  2  lambda =  0.060810062625218  loss:  94.0252270285593&quot;
## [1] &quot;step =  1  lambda =  0.0602049923923736  loss:  93.0962590513434&quot;
## [1] &quot;step =  2  lambda =  0.0602049923923736  loss:  93.2930854013111&quot;
## [1] &quot;step =  1  lambda =  0.0596059427089393  loss:  92.3713476222505&quot;
## [1] &quot;step =  2  lambda =  0.0596059427089393  loss:  92.5661691458662&quot;
## [1] &quot;step =  1  lambda =  0.0590128536694478  loss:  91.6516098622617&quot;
## [1] &quot;step =  2  lambda =  0.0590128536694478  loss:  91.8444477941653&quot;
## [1] &quot;step =  1  lambda =  0.0584256659645008  loss:  90.9370156066257&quot;
## [1] &quot;step =  2  lambda =  0.0584256659645008  loss:  91.1278909280585&quot;
## [1] &quot;step =  1  lambda =  0.0578443208748385  loss:  90.2275347400168&quot;
## [1] &quot;step =  2  lambda =  0.0578443208748385  loss:  90.4164681830449&quot;
## [1] &quot;step =  1  lambda =  0.0572687602654674  loss:  89.5231372002341&quot;
## [1] &quot;step =  2  lambda =  0.0572687602654674  loss:  89.7101492518532&quot;
## [1] &quot;step =  1  lambda =  0.0566989265798469  loss:  88.8237929817458&quot;
## [1] &quot;step =  2  lambda =  0.0566989265798469  loss:  89.0089038878714&quot;
## [1] &quot;step =  1  lambda =  0.0561347628341337  loss:  88.1294721390825&quot;
## [1] &quot;step =  2  lambda =  0.0561347628341337  loss:  88.3127019084316&quot;
## [1] &quot;step =  1  lambda =  0.0555762126114831  loss:  87.4401447900866&quot;
## [1] &quot;step =  2  lambda =  0.0555762126114831  loss:  87.6215131979528&quot;
## [1] &quot;step =  1  lambda =  0.0550232200564073  loss:  86.7557811190234&quot;
## [1] &quot;step =  2  lambda =  0.0550232200564073  loss:  86.935307710951&quot;
## [1] &quot;step =  1  lambda =  0.0544757298691899  loss:  86.0763513795593&quot;
## [1] &quot;step =  2  lambda =  0.0544757298691899  loss:  86.2540554749202&quot;
## [1] &quot;step =  1  lambda =  0.053933687300356  loss:  85.4018258976123&quot;
## [1] &quot;step =  2  lambda =  0.053933687300356  loss:  85.5777265930899&quot;
## [1] &quot;step =  1  lambda =  0.0533970381451971  loss:  84.7321750740803&quot;
## [1] &quot;step =  2  lambda =  0.0533970381451971  loss:  84.9062912470627&quot;
## [1] &quot;step =  1  lambda =  0.0528657287383504  loss:  84.0673693874513&quot;
## [1] &quot;step =  2  lambda =  0.0528657287383504  loss:  84.2397196993384&quot;
## [1] &quot;step =  1  lambda =  0.0523397059484324  loss:  83.4073793963014&quot;
## [1] &quot;step =  2  lambda =  0.0523397059484324  loss:  83.5779822957289&quot;
## [1] &quot;step =  1  lambda =  0.0518189171727258  loss:  82.7521757416839&quot;
## [1] &quot;step =  2  lambda =  0.0518189171727258  loss:  82.9210494676679&quot;
## [1] &quot;step =  1  lambda =  0.0513033103319191  loss:  82.1017291494143&quot;
## [1] &quot;step =  2  lambda =  0.0513033103319191  loss:  82.2688917344184&quot;
## [1] &quot;step =  1  lambda =  0.0507928338648985  loss:  81.4560104322559&quot;
## [1] &quot;step =  2  lambda =  0.0507928338648985  loss:  81.6214797051845&quot;
## [1] &quot;step =  1  lambda =  0.0502874367235919  loss:  80.814990492008&quot;
## [1] &quot;step =  2  lambda =  0.0502874367235919  loss:  80.9787840811291&quot;
## [1] &quot;step =  1  lambda =  0.0497870683678639  loss:  80.1786403215038&quot;
## [1] &quot;step =  2  lambda =  0.0497870683678639  loss:  80.3407756573023&quot;
## [1] &quot;step =  1  lambda =  0.0492916787604622  loss:  79.5469310065173&quot;
## [1] &quot;step =  2  lambda =  0.0492916787604622  loss:  79.7074253244837&quot;
## [1] &quot;step =  1  lambda =  0.048801218362013  loss:  78.9198337275869&quot;
## [1] &quot;step =  2  lambda =  0.048801218362013  loss:  79.0787040709409&quot;
## [1] &quot;step =  1  lambda =  0.0483156381260678  loss:  78.2973197617562&quot;
## [1] &quot;step =  2  lambda =  0.0483156381260678  loss:  78.4545829841103&quot;
## [1] &quot;step =  1  lambda =  0.0478348894941984  loss:  77.6793604842357&quot;
## [1] &quot;step =  2  lambda =  0.0478348894941984  loss:  77.8350332521994&quot;
## [1] &quot;step =  1  lambda =  0.0473589243911409  loss:  77.0659273699894&quot;
## [1] &quot;step =  2  lambda =  0.0473589243911409  loss:  77.2200261657166&quot;
## [1] &quot;step =  1  lambda =  0.0468876952199885  loss:  76.4569919952484&quot;
## [1] &quot;step =  2  lambda =  0.0468876952199885  loss:  76.6095331189297&quot;
## [1] &quot;step =  1  lambda =  0.0464211548574313  loss:  75.8525260389536&quot;
## [1] &quot;step =  2  lambda =  0.0464211548574313  loss:  76.0035256112561&quot;
## [1] &quot;step =  1  lambda =  0.0459592566490442  loss:  75.2525012841327&quot;
## [1] &quot;step =  2  lambda =  0.0459592566490442  loss:  75.4019752485879&quot;
## [1] &quot;step =  1  lambda =  0.0455019544046216  loss:  74.6568896192098&quot;
## [1] &quot;step =  2  lambda =  0.0455019544046216  loss:  74.8048537445534&quot;
## [1] &quot;step =  1  lambda =  0.0450492023935578  loss:  74.0656630392547&quot;
## [1] &quot;step =  2  lambda =  0.0450492023935578  loss:  74.2121329217176&quot;
## [1] &quot;step =  1  lambda =  0.0446009553402746  loss:  73.4787936471713&quot;
## [1] &quot;step =  2  lambda =  0.0446009553402746  loss:  73.6237847127246&quot;
## [1] &quot;step =  1  lambda =  0.0441571684196929  loss:  72.8962536548272&quot;
## [1] &quot;step =  2  lambda =  0.0441571684196929  loss:  73.0397811613838&quot;
## [1] &quot;step =  1  lambda =  0.0437177972527509  loss:  72.3180153841291&quot;
## [1] &quot;step =  2  lambda =  0.0437177972527509  loss:  72.4600944237008&quot;
## [1] &quot;step =  1  lambda =  0.0432827979019659  loss:  71.7440512680426&quot;
## [1] &quot;step =  2  lambda =  0.0432827979019659  loss:  71.8846967688563&quot;
## [1] &quot;step =  1  lambda =  0.0428521268670402  loss:  71.174333851562&quot;
## [1] &quot;step =  2  lambda =  0.0428521268670402  loss:  71.3135605801356&quot;
## [1] &quot;step =  1  lambda =  0.0424257410805114  loss:  70.6088357926289&quot;
## [1] &quot;step =  2  lambda =  0.0424257410805114  loss:  70.7466583558079&quot;
## [1] &quot;step =  1  lambda =  0.0420035979034456  loss:  70.0475298630032&quot;
## [1] &quot;step =  2  lambda =  0.0420035979034456  loss:  70.1839627099597&quot;
## [1] &quot;step =  1  lambda =  0.0415856551211732  loss:  69.4903889490878&quot;
## [1] &quot;step =  2  lambda =  0.0415856551211732  loss:  69.6254463732825&quot;
## [1] &quot;step =  1  lambda =  0.0411718709390678  loss:  68.9373860527079&quot;
## [1] &quot;step =  2  lambda =  0.0411718709390678  loss:  69.0710821938169&quot;
## [1] &quot;step =  1  lambda =  0.0407622039783662  loss:  68.388494291848&quot;
## [1] &quot;step =  2  lambda =  0.0407622039783662  loss:  68.5208431376552&quot;
## [1] &quot;step =  1  lambda =  0.0403566132720311  loss:  67.8436869013471&quot;
## [1] &quot;step =  2  lambda =  0.0403566132720311  loss:  67.9747022896031&quot;
## [1] &quot;step =  1  lambda =  0.0399550582606539  loss:  67.302937233553&quot;
## [1] &quot;step =  2  lambda =  0.0399550582606539  loss:  67.4326328538017&quot;
## [1] &quot;step =  1  lambda =  0.0395574987883987  loss:  66.7662187589389&quot;
## [1] &quot;step =  2  lambda =  0.0395574987883987  loss:  66.8946081543122&quot;
## [1] &quot;step =  1  lambda =  0.0391638950989871  loss:  66.2335050666819&quot;
## [1] &quot;step =  2  lambda =  0.0391638950989871  loss:  66.3606016356645&quot;
## [1] &quot;step =  1  lambda =  0.038774207831722  loss:  65.7047698652061&quot;
## [1] &quot;step =  2  lambda =  0.038774207831722  loss:  65.83058686337&quot;
## [1] &quot;step =  1  lambda =  0.0383883980175521  loss:  65.1799869826895&quot;
## [1] &quot;step =  2  lambda =  0.0383883980175521  loss:  65.3045375244005&quot;
## [1] &quot;step =  1  lambda =  0.0380064270751743  loss:  64.6591303675392&quot;
## [1] &quot;step =  2  lambda =  0.0380064270751743  loss:  64.7824274276341&quot;
## [1] &quot;step =  1  lambda =  0.0376282568071762  loss:  64.1421740888325&quot;
## [1] &quot;step =  2  lambda =  0.0376282568071762  loss:  64.2642305042704&quot;
## [1] &quot;step =  1  lambda =  0.0372538493962158  loss:  63.6290923367273&quot;
## [1] &quot;step =  2  lambda =  0.0372538493962158  loss:  63.7499208082136&quot;
## [1] &quot;step =  1  lambda =  0.03688316740124  loss:  63.1198594228424&quot;
## [1] &quot;step =  2  lambda =  0.03688316740124  loss:  63.2394725164269&quot;
## [1] &quot;step =  1  lambda =  0.0365161737537404  loss:  62.6144497806077&quot;
## [1] &quot;step =  2  lambda =  0.0365161737537404  loss:  62.7328599292583&quot;
## [1] &quot;step =  1  lambda =  0.0361528317540464  loss:  62.1128379655874&quot;
## [1] &quot;step =  2  lambda =  0.0361528317540464  loss:  62.2300574707383&quot;
## [1] &quot;step =  1  lambda =  0.0357931050676553  loss:  61.6149986557744&quot;
## [1] &quot;step =  2  lambda =  0.0357931050676553  loss:  61.7310396888517&quot;
## [1] &quot;step =  1  lambda =  0.0354369577215986  loss:  61.1209066518596&quot;
## [1] &quot;step =  2  lambda =  0.0354369577215986  loss:  61.2357812557824&quot;
## [1] &quot;step =  1  lambda =  0.035084354100845  loss:  60.6305368774746&quot;
## [1] &quot;step =  2  lambda =  0.035084354100845  loss:  60.7442569681342&quot;
## [1] &quot;step =  1  lambda =  0.0347352589447386  loss:  60.1438643794104&quot;
## [1] &quot;step =  2  lambda =  0.0347352589447386  loss:  60.2564417471276&quot;
## [1] &quot;step =  1  lambda =  0.0343896373434727  loss:  59.6608643278118&quot;
## [1] &quot;step =  2  lambda =  0.0343896373434727  loss:  59.7723106387725&quot;
## [1] &quot;step =  1  lambda =  0.0340474547345993  loss:  59.1815120163495&quot;
## [1] &quot;step =  2  lambda =  0.0340474547345993  loss:  59.2918388140184&quot;
## [1] &quot;step =  1  lambda =  0.0337086768995724  loss:  58.7057828623686&quot;
## [1] &quot;step =  2  lambda =  0.0337086768995724  loss:  58.8150015688839&quot;
## [1] &quot;step =  1  lambda =  0.0333732699603261  loss:  58.2336524070164&quot;
## [1] &quot;step =  2  lambda =  0.0333732699603261  loss:  58.3417743245632&quot;
## [1] &quot;step =  1  lambda =  0.0330412003758869  loss:  57.7650963153496&quot;
## [1] &quot;step =  2  lambda =  0.0330412003758869  loss:  57.8721326275147&quot;
## [1] &quot;step =  1  lambda =  0.0327124349390198  loss:  57.3000903764204&quot;
## [1] &quot;step =  2  lambda =  0.0327124349390198  loss:  57.4060521495271&quot;
## [1] &quot;step =  1  lambda =  0.0323869407729071  loss:  56.8386105033434&quot;
## [1] &quot;step =  2  lambda =  0.0323869407729071  loss:  56.9435086877685&quot;
## [1] &quot;step =  1  lambda =  0.0320646853278608  loss:  56.3806327333439&quot;
## [1] &quot;step =  2  lambda =  0.0320646853278608  loss:  56.4844781648159&quot;
## [1] &quot;step =  1  lambda =  0.0317456363780679  loss:  55.9261332277878&quot;
## [1] &quot;step =  2  lambda =  0.0317456363780679  loss:  56.0289366286671&quot;
## [1] &quot;step =  1  lambda =  0.0314297620183677  loss:  55.4750882721937&quot;
## [1] &quot;step =  2  lambda =  0.0314297620183677  loss:  55.5768602527358&quot;
## [1] &quot;step =  1  lambda =  0.0311170306610609  loss:  55.0274742762274&quot;
## [1] &quot;step =  2  lambda =  0.0311170306610609  loss:  55.128225335829&quot;
## [1] &quot;step =  1  lambda =  0.0308074110327511  loss:  54.5832677736808&quot;
## [1] &quot;step =  2  lambda =  0.0308074110327511  loss:  54.6830083021085&quot;
## [1] &quot;step =  1  lambda =  0.0305008721712175  loss:  54.142445422434&quot;
## [1] &quot;step =  2  lambda =  0.0305008721712175  loss:  54.2411857010375&quot;
## [1] &quot;step =  1  lambda =  0.0301973834223185  loss:  53.704984004402&quot;
## [1] &quot;step =  2  lambda =  0.0301973834223185  loss:  53.8027342073108&quot;
## [1] &quot;step =  1  lambda =  0.0298969144369263  loss:  53.2708604254667&quot;
## [1] &quot;step =  2  lambda =  0.0298969144369263  loss:  53.367630620771&quot;
## [1] &quot;step =  1  lambda =  0.029599435167892  loss:  52.8400517153938&quot;
## [1] &quot;step =  2  lambda =  0.029599435167892  loss:  52.9358518663103&quot;
## [1] &quot;step =  1  lambda =  0.0293049158670407  loss:  52.4125350277362&quot;
## [1] &quot;step =  2  lambda =  0.0293049158670407  loss:  52.5073749937585&quot;
## [1] &quot;step =  1  lambda =  0.0290133270821971  loss:  51.9882876397231&quot;
## [1] &quot;step =  2  lambda =  0.0290133270821971  loss:  52.0821771777576&quot;
## [1] &quot;step =  1  lambda =  0.0287246396542394  loss:  51.5672869521366&quot;
## [1] &quot;step =  2  lambda =  0.0287246396542394  loss:  51.6602357176239&quot;
## [1] &quot;step =  1  lambda =  0.0284388247141845  loss:  51.1495104891746&quot;
## [1] &quot;step =  2  lambda =  0.0284388247141845  loss:  51.2415280371963&quot;
## [1] &quot;step =  1  lambda =  0.0281558536803001  loss:  50.7349358983026&quot;
## [1] &quot;step =  2  lambda =  0.0281558536803001  loss:  50.8260316846743&quot;
## [1] &quot;step =  1  lambda =  0.027875698255247  loss:  50.3235409500919&quot;
## [1] &quot;step =  2  lambda =  0.027875698255247  loss:  50.413724332443&quot;
## [1] &quot;step =  1  lambda =  0.0275983304232493  loss:  49.9153035380478&quot;
## [1] &quot;step =  2  lambda =  0.0275983304232493  loss:  50.0045837768868&quot;
## [1] &quot;step =  1  lambda =  0.0273237224472926  loss:  49.5102016784251&quot;
## [1] &quot;step =  2  lambda =  0.0273237224472926  loss:  49.598587938193&quot;
## [1] &quot;step =  1  lambda =  0.0270518468663504  loss:  49.108213510034&quot;
## [1] &quot;step =  2  lambda =  0.0270518468663504  loss:  49.1957148601435&quot;
## [1] &quot;step =  1  lambda =  0.0267826764926382  loss:  48.7093172940343&quot;
## [1] &quot;step =  2  lambda =  0.0267826764926382  loss:  48.7959427098969&quot;
## [1] &quot;step =  1  lambda =  0.0265161844088942  loss:  48.3134914137203&quot;
## [1] &quot;step =  2  lambda =  0.0265161844088942  loss:  48.3992497777613&quot;
## [1] &quot;step =  1  lambda =  0.026252343965688  loss:  47.9207143742954&quot;
## [1] &quot;step =  2  lambda =  0.026252343965688  loss:  48.0056144769554&quot;
## [1] &quot;step =  1  lambda =  0.0259911287787554  loss:  47.5309648026368&quot;
## [1] &quot;step =  2  lambda =  0.0259911287787554  loss:  47.6150153433622&quot;
## [1] &quot;step =  1  lambda =  0.0257325127263599  loss:  47.1442214470517&quot;
## [1] &quot;step =  2  lambda =  0.0257325127263599  loss:  47.2274310352728&quot;
## [1] &quot;step =  1  lambda =  0.025476469946681  loss:  46.760463177024&quot;
## [1] &quot;step =  2  lambda =  0.025476469946681  loss:  46.8428403331214&quot;
## [1] &quot;step =  1  lambda =  0.0252229748352272  loss:  46.3796689829523&quot;
## [1] &quot;step =  2  lambda =  0.0252229748352272  loss:  46.4612221392124&quot;
## [1] &quot;step =  1  lambda =  0.0249720020422762  loss:  46.0018179758797&quot;
## [1] &quot;step =  2  lambda =  0.0249720020422762  loss:  46.0825554774377&quot;
## [1] &quot;step =  1  lambda =  0.0247235264703394  loss:  45.6268893872151&quot;
## [1] &quot;step =  2  lambda =  0.0247235264703394  loss:  45.7068194929882&quot;
## [1] &quot;step =  1  lambda =  0.0244775232716527  loss:  45.2548625684473&quot;
## [1] &quot;step =  2  lambda =  0.0244775232716527  loss:  45.3339934520562&quot;
## [1] &quot;step =  1  lambda =  0.0242339678456911  loss:  44.8857169908508&quot;
## [1] &quot;step =  2  lambda =  0.0242339678456911  loss:  44.9640567415303&quot;
## [1] &quot;step =  1  lambda =  0.0239928358367092  loss:  44.5194322451843&quot;
## [1] &quot;step =  2  lambda =  0.0239928358367092  loss:  44.5969888686839&quot;
## [1] &quot;step =  1  lambda =  0.023754103131305  loss:  44.1559880413825&quot;
## [1] &quot;step =  2  lambda =  0.023754103131305  loss:  44.2327694608559&quot;
## [1] &quot;step =  1  lambda =  0.0235177458560091  loss:  43.7953642082403&quot;
## [1] &quot;step =  2  lambda =  0.0235177458560091  loss:  43.8713782651251&quot;
## [1] &quot;step =  1  lambda =  0.023283740374897  loss:  43.4375406930909&quot;
## [1] &quot;step =  2  lambda =  0.023283740374897  loss:  43.5127951479781&quot;
## [1] &quot;step =  1  lambda =  0.0230520632872256  loss:  43.0824975614775&quot;
## [1] &quot;step =  2  lambda =  0.0230520632872256  loss:  43.1570000949705&quot;
## [1] &quot;step =  1  lambda =  0.022822691425093  loss:  42.7302149968176&quot;
## [1] &quot;step =  2  lambda =  0.022822691425093  loss:  42.8039732103823&quot;
## [1] &quot;step =  1  lambda =  0.0225956018511219  loss:  42.3806733000631&quot;
## [1] &quot;step =  2  lambda =  0.0225956018511219  loss:  42.4536947168672&quot;
## [1] &quot;step =  1  lambda =  0.0223707718561656  loss:  42.0338528893527&quot;
## [1] &quot;step =  2  lambda =  0.0223707718561656  loss:  42.1061449550964&quot;
## [1] &quot;step =  1  lambda =  0.0221481789570373  loss:  41.6897342996598&quot;
## [1] &quot;step =  2  lambda =  0.0221481789570373  loss:  41.7613043833967&quot;
## [1] &quot;step =  1  lambda =  0.0219278008942616  loss:  41.3482981824346&quot;
## [1] &quot;step =  2  lambda =  0.0219278008942616  loss:  41.4191535773832&quot;
## [1] &quot;step =  1  lambda =  0.0217096156298486  loss:  41.0095253052411&quot;
## [1] &quot;step =  2  lambda =  0.0217096156298486  loss:  41.0796732295873&quot;
## [1] &quot;step =  1  lambda =  0.0214936013450899  loss:  40.6733965513881&quot;
## [1] &quot;step =  2  lambda =  0.0214936013450899  loss:  40.7428441490787&quot;
## [1] &quot;step =  1  lambda =  0.0212797364383772  loss:  40.3398929195565&quot;
## [1] &quot;step =  2  lambda =  0.0212797364383772  loss:  40.4086472610836&quot;
## [1] &quot;step =  1  lambda =  0.0210679995230414  loss:  40.0089955234214&quot;
## [1] &quot;step =  2  lambda =  0.0210679995230414  loss:  40.0770636065985&quot;
## [1] &quot;step =  1  lambda =  0.0208583694252147  loss:  39.6806855912689&quot;
## [1] &quot;step =  2  lambda =  0.0208583694252147  loss:  39.7480743419976&quot;
## [1] &quot;step =  1  lambda =  0.0206508251817126  loss:  39.3549444656097&quot;
## [1] &quot;step =  2  lambda =  0.0206508251817126  loss:  39.4216607386385&quot;
## [1] &quot;step =  1  lambda =  0.0204453460379377  loss:  39.0317536027869&quot;
## [1] &quot;step =  2  lambda =  0.0204453460379377  loss:  39.0978041824616&quot;
## [1] &quot;step =  1  lambda =  0.0202419114458044  loss:  38.7110945725816&quot;
## [1] &quot;step =  2  lambda =  0.0202419114458044  loss:  38.7764861735863&quot;
## [1] &quot;step =  1  lambda =  0.020040501061684  loss:  38.3929490578122&quot;
## [1] &quot;step =  2  lambda =  0.020040501061684  loss:  38.457688325904&quot;
## [1] &quot;step =  1  lambda =  0.0198410947443703  loss:  38.0772988539321&quot;
## [1] &quot;step =  2  lambda =  0.0198410947443703  loss:  38.141392366666&quot;
## [1] &quot;step =  1  lambda =  0.0196436725530653  loss:  37.7641258686223&quot;
## [1] &quot;step =  2  lambda =  0.0196436725530653  loss:  37.8275801360684&quot;
## [1] &quot;step =  1  lambda =  0.0194482147453854  loss:  37.4534121213804&quot;
## [1] &quot;step =  2  lambda =  0.0194482147453854  loss:  37.5162335868343&quot;
## [1] &quot;step =  1  lambda =  0.0192547017753869  loss:  37.1451397431071&quot;
## [1] &quot;step =  2  lambda =  0.0192547017753869  loss:  37.2073347837913&quot;
## [1] &quot;step =  1  lambda =  0.0190631142916116  loss:  36.8392909756889&quot;
## [1] &quot;step =  2  lambda =  0.0190631142916116  loss:  36.9008659034471&quot;
## [1] &quot;step =  1  lambda =  0.0188734331351515  loss:  36.5358481715771&quot;
## [1] &quot;step =  2  lambda =  0.0188734331351515  loss:  36.5968092335605&quot;
## [1] &quot;step =  1  lambda =  0.0186856393377328  loss:  36.2347937933649&quot;
## [1] &quot;step =  2  lambda =  0.0186856393377328  loss:  36.2951471727114&quot;
## [1] &quot;step =  1  lambda =  0.0184997141198192  loss:  35.93611041336&quot;
## [1] &quot;step =  2  lambda =  0.0184997141198192  loss:  35.995862229866&quot;
## [1] &quot;step =  1  lambda =  0.0183156388887342  loss:  35.6397807131564&quot;
## [1] &quot;step =  2  lambda =  0.0183156388887342  loss:  35.6989370239407&quot;
## [1] &quot;step =  1  lambda =  0.0181333952368011  loss:  35.3457874832015&quot;
## [1] &quot;step =  2  lambda =  0.0181333952368011  loss:  35.4043542833628&quot;
## [1] &quot;step =  1  lambda =  0.0179529649395029  loss:  35.0541136223618&quot;
## [1] &quot;step =  2  lambda =  0.0179529649395029  loss:  35.1120968456285&quot;
## [1] &quot;step =  1  lambda =  0.0177743299536594  loss:  34.764742137486&quot;
## [1] &quot;step =  2  lambda =  0.0177743299536594  loss:  34.822147656859&quot;
## [1] &quot;step =  1  lambda =  0.0175974724156234  loss:  34.4776561429653&quot;
## [1] &quot;step =  2  lambda =  0.0175974724156234  loss:  34.5344897713539&quot;
## [1] &quot;step =  1  lambda =  0.0174223746394935  loss:  34.1928388602917&quot;
## [1] &quot;step =  2  lambda =  0.0174223746394935  loss:  34.2491063511429&quot;
## [1] &quot;step =  1  lambda =  0.0172490191153463  loss:  33.9102736176142&quot;
## [1] &quot;step =  2  lambda =  0.0172490191153463  loss:  33.9659806655345&quot;
## [1] &quot;step =  1  lambda =  0.0170773885074848  loss:  33.6299438492927&quot;
## [1] &quot;step =  2  lambda =  0.0170773885074848  loss:  33.6850960906641&quot;
## [1] &quot;step =  1  lambda =  0.0169074656527053  loss:  33.3518330954503&quot;
## [1] &quot;step =  2  lambda =  0.0169074656527053  loss:  33.4064361090389&quot;
## [1] &quot;step =  1  lambda =  0.0167392335585806  loss:  33.0759250015231&quot;
## [1] &quot;step =  2  lambda =  0.0167392335585806  loss:  33.1299843090815&quot;
## [1] &quot;step =  1  lambda =  0.0165726754017613  loss:  32.802203317809&quot;
## [1] &quot;step =  2  lambda =  0.0165726754017613  loss:  32.8557243846717&quot;
## [1] &quot;step =  1  lambda =  0.0164077745262926  loss:  32.5306518990139&quot;
## [1] &quot;step =  2  lambda =  0.0164077745262926  loss:  32.5836401346876&quot;
## [1] &quot;step =  1  lambda =  0.0162445144419499  loss:  32.2612547037974&quot;
## [1] &quot;step =  2  lambda =  0.0162445144419499  loss:  32.3137154625431&quot;
## [1] &quot;step =  1  lambda =  0.0160828788225884  loss:  31.9939957943157&quot;
## [1] &quot;step =  2  lambda =  0.0160828788225884  loss:  32.0459343757261&quot;
## [1] &quot;step =  1  lambda =  0.0159228515045117  loss:  31.7288593357641&quot;
## [1] &quot;step =  2  lambda =  0.0159228515045117  loss:  31.780280985334&quot;
## [1] &quot;step =  1  lambda =  0.0157644164848545  loss:  31.4658295959176&quot;
## [1] &quot;step =  2  lambda =  0.0157644164848545  loss:  31.5167395056084&quot;
## [1] &quot;step =  1  lambda =  0.0156075579199828  loss:  31.2048909446705&quot;
## [1] &quot;step =  2  lambda =  0.0156075579199828  loss:  31.2552942534688&quot;
## [1] &quot;step =  1  lambda =  0.0154522601239095  loss:  30.9460278535745&quot;
## [1] &quot;step =  2  lambda =  0.0154522601239095  loss:  30.9959296480442&quot;
## [1] &quot;step =  1  lambda =  0.0152985075667255  loss:  30.6892248953758&quot;
## [1] &quot;step =  2  lambda =  0.0152985075667255  loss:  30.738630210205&quot;
## [1] &quot;step =  1  lambda =  0.015146284873047  loss:  30.4344667435513&quot;
## [1] &quot;step =  2  lambda =  0.015146284873047  loss:  30.4833805620929&quot;
## [1] &quot;step =  1  lambda =  0.0149955768204777  loss:  30.1817381718438&quot;
## [1] &quot;step =  2  lambda =  0.0149955768204777  loss:  30.2301654266502&quot;
## [1] &quot;step =  1  lambda =  0.0148463683380868  loss:  29.9310240537957&quot;
## [1] &quot;step =  2  lambda =  0.0148463683380868  loss:  29.9789696271484&quot;
## [1] &quot;step =  1  lambda =  0.0146986445049018  loss:  29.6823093622831&quot;
## [1] &quot;step =  2  lambda =  0.0146986445049018  loss:  29.7297780867158&quot;
## [1] &quot;step =  1  lambda =  0.0145523905484161  loss:  29.4355791690476&quot;
## [1] &quot;step =  2  lambda =  0.0145523905484161  loss:  29.4825758278645&quot;
## [1] &quot;step =  1  lambda =  0.0144075918431123  loss:  29.1908186442287&quot;
## [1] &quot;step =  2  lambda =  0.0144075918431123  loss:  29.2373479720166&quot;
## [1] &quot;step =  1  lambda =  0.0142642339089993  loss:  28.9480130558948&quot;
## [1] &quot;step =  2  lambda =  0.0142642339089993  loss:  28.9940797390305&quot;
## [1] &quot;step =  1  lambda =  0.014122302410164  loss:  28.7071477695739&quot;
## [1] &quot;step =  2  lambda =  0.014122302410164  loss:  28.7527564467254&quot;
## [1] &quot;step =  1  lambda =  0.0139817831533383  loss:  28.4682082477837&quot;
## [1] &quot;step =  2  lambda =  0.0139817831533383  loss:  28.5133635104065&quot;
## [1] &quot;step =  1  lambda =  0.0138426620864795  loss:  28.2311800495612&quot;
## [1] &quot;step =  2  lambda =  0.0138426620864795  loss:  28.2758864423893&quot;
## [1] &quot;step =  1  lambda =  0.0137049252973649  loss:  27.9960488299922&quot;
## [1] &quot;step =  2  lambda =  0.0137049252973649  loss:  28.0403108515239&quot;
## [1] &quot;step =  1  lambda =  0.0135685590122009  loss:  27.7628003397401&quot;
## [1] &quot;step =  2  lambda =  0.0135685590122009  loss:  27.8066224427183&quot;
## [1] &quot;step =  1  lambda =  0.0134335495942453  loss:  27.5314204245746&quot;
## [1] &quot;step =  2  lambda =  0.0134335495942453  loss:  27.5748070164624&quot;
## [1] &quot;step =  1  lambda =  0.0132998835424438  loss:  27.3018950248999&quot;
## [1] &quot;step =  2  lambda =  0.0132998835424438  loss:  27.3448504683508&quot;
## [1] &quot;step =  1  lambda =  0.0131675474900798  loss:  27.0742101752834&quot;
## [1] &quot;step =  2  lambda =  0.0131675474900798  loss:  27.1167387886069&quot;
## [1] &quot;step =  1  lambda =  0.0130365282034377  loss:  26.8483520039834&quot;
## [1] &quot;step =  2  lambda =  0.0130365282034377  loss:  26.8904580616052&quot;
## [1] &quot;step =  1  lambda =  0.0129068125804799  loss:  26.6243067324776&quot;
## [1] &quot;step =  2  lambda =  0.0129068125804799  loss:  26.6659944653953&quot;
## [1] &quot;step =  1  lambda =  0.0127783876495358  loss:  26.4020606749907&quot;
## [1] &quot;step =  2  lambda =  0.0127783876495358  loss:  26.4433342712245&quot;
## [1] &quot;step =  1  lambda =  0.0126512405680053  loss:  26.1816002380227&quot;
## [1] &quot;step =  2  lambda =  0.0126512405680053  loss:  26.2224638430612&quot;
## [1] &quot;step =  1  lambda =  0.0125253586210744  loss:  25.9629119198774&quot;
## [1] &quot;step =  2  lambda =  0.0125253586210744  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0124007292204434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0122773399030684  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0121551783299149  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0120342322847238  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0119144896727896  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0117959385197516  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0116785669703954  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0115623632874685  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0114473158505057  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0113334131546674  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0112206438095891  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0111089965382423  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0109984601758069  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0108890236685545  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0107806760727431  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0106734065535229  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0105672043838527  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0104620589434268  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0103579597176137  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.010254896296404  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0101528583733698  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0100518357446336  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00995181830784842  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00985279606118726  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0097547591023429  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00965769762753778  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00956160193054351  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00946646240171032  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00937226952700606  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00927901388706474  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00918668615624467  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00909527710169582  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00900477758243656  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00891517854843955  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00882647103972673  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00873864618547329  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00865169520312063  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00856560939749806  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00848038015995327  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00839599896749147  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00831245738192312  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00822974704902003  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00814785969767999  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00806678713909961  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0079865212659555  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00790705405159344  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00782837754922577  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00775048389113669  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00767336528789549  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00759701402757757  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00752142247499327  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00744658307092434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00737248833136801  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00729913084678858  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00722650328137646  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00715459837231459  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00708340892905212  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00701292783258542  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00694314803474611  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00687406255749626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00680566449223054  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00673794699908547  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00667090330625527  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00660452670931481  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00653881057054906  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0064737483182894  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00640933344625638  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00634555951290912  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00628242014080112  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00621990901594257  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0061580198871689  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00609674656551564  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00603608292359956  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00597602289500594  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00591656047368186  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00585768971333562  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00579940472684215  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0057416996856542  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0056845688192196  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00562800641440407  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00557200681492  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00551656442076077  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00546167368764078  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00540732912644096  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00535352530265991  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0053002568358704  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00524751839918138  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00519530471870523  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00514361057303038  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00509243079269919  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00504176025969098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00499159390691022  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00494192671767982  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00489275372523948  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00484407001224897  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00479587071029642  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00474815099941148  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00470090610758328  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00465413131028327  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00460782192999275  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0045619733357351  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00451658094261267  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00447164021134833  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00442714664783151  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00438309580266878  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0043394832707389  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00429630469075234  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00425355574481513  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00421123215799704  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00416932969790412  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00412784417425544  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00408677143846407  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0040461073832222  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00400584794209042  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00396598908909106  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00392652683830562  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00388745724347613  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00384877639761054  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00381048043259204  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00377256551879221  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00373502786468807  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00369786371648293  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00366106935773101  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00362464110896576  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00358857532733195  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00355286840622136  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00351751677491213  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00348251689821166  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00344786527610313  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00341355844339543  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00337959296937672  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00334596545747127  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00331267254489989  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00327971090234357  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00324707723361059  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00321476827530687  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00318278079650967  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00315111159844444  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00311975751416499  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00308871540823677  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00305798217642331  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00302755474537582  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00299743007232583  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00296760514478094  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00293807698022355  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00290884262581258  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00287989915808824  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00285124368267963  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00282287333401534  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00279478527503684  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00276697669691485  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00273944481876837  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00271218688738664  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00268520017695382  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00265848198877637  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0026320296510132  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0026058405184085  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00257991197202718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.002554241418993  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00252882629222926  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0025036640502021  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00247875217666636  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00245408818041392  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0024296695950246  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00240549397861951  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00238155891361687  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00235786200649023  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00233440088752913  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00231117321060213  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00228817665292217  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00226540891481432  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0022428677194858  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0022205508127983  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00219845596304253  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00217658096071513  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00215492361829761  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00213348177003771  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00211225327173271  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00209123600051511  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00207042785464026  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00204982675327624  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00202943063629574  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00200923746407006  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00198924521726516  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0019694518966397  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00194985552284512  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00193045413622771  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00191124579663264  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00189222858320994  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00187340059422243  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0018547599468555  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00183630477702891  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00181803323921027  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00179994350623059  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00178203376910149  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00176430223683434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00174674713626112  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00172936671185716  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00171215922556552  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00169512295662325  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00167825620138925  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00166155727317393  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00164502450207057  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00162865623478828  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00161245083448668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00159640668061225  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00158052216873622  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00156479571039417  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00154922573292716  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00153381067932446  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00151854900806788  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00150343919297757  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00148847972305943  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.001473669102354  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00145900584978686  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00144448849902054  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00143011559830787  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0014158857103468  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00140179741213667  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00138784929483593  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00137403996362121  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00136036803754789  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00134683214941197  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00133343094561336  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0013201630860205  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00130702724383639  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00129402210546585  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00128114637038421  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00126839875100724  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00125577797256237  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00124328277296124  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00123091190267348  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00121866412460175  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00120653821395804  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00119453295814118  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00118264715661557  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00117087962079117  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00115922917390459  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00114769465090143  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00113627489831977  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00112496877417484  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0011137751478448  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0011026928999577  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00109172092227951  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00108085811760332  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0010701033996396  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00105945569290761  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00104891393262779  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00103847706461533  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00102814404517473  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00101791384099544  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00100778542904851  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000997757796484312  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000987829940531229  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000978000868395395  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000968269597161403  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000958635153694021  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000949096574540873  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000939652905836096  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000930303203204949  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000921046531669378  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000911881965554516  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000902808588396114  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000893825492848894  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000884931780595815  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000876126562258242  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000867408957307003  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000858778093974336  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000850233109166719  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000841773148378549  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000833397365606696  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000825104923265905  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000816894992105029  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000808766751124111  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000800719387492281  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000792752096466468  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000784864081310932  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000777054553217582  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000769322731227101  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000761667842150847  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000754089120493534  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00074658580837668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00073915715546282  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000731802418880473  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000724520863149851  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000717311760109313  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000710174388842549  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000703108035606483  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000696111993759903  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000689185563692794  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000682328052756377  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000675538775193844  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000668817052071782  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000662162211212276  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000655573587125696  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000649050520944141  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000642592360355558  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000636198459538506  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000629868179097574  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000623600885999444  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000617395953509583  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000611252761129572  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000605170694535053  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000599149145514298  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000593187511907387  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000587285197545991  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000581441612193756  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000575656171487276  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00056992829687766  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000564257415572674  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000558642960479461  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000553084370147834  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000547581088714126  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000542132565845609  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000536738256685455  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000531397621798253  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000526110127116064  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000520875243885012  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000515692448612414  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000510561223014422  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0005054810539642  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000500451433440611  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00049547185847741  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000490541831112951  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000485660858340389  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00048082845205838  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000476044129022269  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000471307410795765  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000466617823703098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000461974898781651  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000457378171735063  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000452827182886797  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000448321477134178  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000443860603902874  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000439444117101845  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000435071575078732  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000430742540575688  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000426456580685654  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00042221326680907  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000418012174611013  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000413852883978762  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000409734978979787  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000405658047820157  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000401621682803358  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000397625480289526  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000393669040655078  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000389751968252755  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000385873871372051  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000382034362200047  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000378233056782626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000374469574986078  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000370743540459088  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000367054580595098  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000363402326495048  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000359786412930483  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000356206478307034  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000352662164628256  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000349153117459826  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000345678985894105  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000342239422515039  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000338834083363426  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000335462627902512  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000332124718983941  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00032882002281404  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000325548208920438  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000322308950119019  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000319101922481203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000315926805301555  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000312783281065711  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000309671035418626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000306589757133144  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000303539138078867  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000300518873191348  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000297528660441581  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0002945682008058  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000291637198235574  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000288735359628203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000285862394797409  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000283018016444314  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000280201940128712  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000277413884240626  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000274653569972143  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000271920721289535  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000269215064905658  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000266536330252618  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000263884249454718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000261258557301668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000258658991222064  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000256085291257132  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00025353720003473  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000251014462743614  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000248516827107952  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000246044043362099  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000243595864225619  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000241172044878559  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000238772342936964  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000236396518428641  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000234044333769158  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00023171555373809  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000229409945455492  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000227127278358615  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000224867324178848  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000222629856918889  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000220414652830147  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000218221490390368  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000216050150281479  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000213900415367661  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000211772070673631  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000209664903363145  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000207578702717718  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000205513260115544  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000203468369010644  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000201443824912203  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000199439425364123  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000197454969924779  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000195490260146975  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000193545099558094  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000191619293640457  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000189712649811868  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000187824977406354  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000185956087655102  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000184105793667579  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000182273910412845  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000180460254701048  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000178664645165106  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000176886902242567  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000175126848157658  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000173384306903506  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00017165910422453  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000169951067599028  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000168260026221911  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000166585810987634  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000164928254473277  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000163287190921808  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000161662456225505  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000160053887909543  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000158461325115751  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000156884608586522  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00015532358064889  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000153778085198759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000152247967685297  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000150733075095477  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000149233255938777  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000147748360232034  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000146278239484437  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000144822746682688  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000143381736276293  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000141955064163011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000140542587674442  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000139144165561759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000137759657981586  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000136388926482011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000135031833988743  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0001336882447914  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000132358024529944  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000131041040181239  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000129737160045754  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000128446253734388  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000127168192155434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012590284750167  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000124650093237575  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012340980408668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000122181856019035  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012096612623881  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000119762493172015  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000118570836454339  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000117391036919118  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000116222976585415  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000115066538646224  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000113921607456786  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000112788068523029  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000111665808490115  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000110554715131105  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000109454677335737  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000108365585099315  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000107287329511708  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000106219802746459  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000105162898050001  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000104116509730984  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000103080533149705  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000102054864707641  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000101039401837093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00010003404299093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.9038687632427e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.80532362252201e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.70775902233471e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.61116520613947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.51553251447417e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.42085138398996e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.32711234649488e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.23430602800707e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.14242314781733e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.05145451756109e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.96139104029952e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.87222370960982e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.78394360868463e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.69654190944029e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.61000987163404e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.52433884198997e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.43952025333374e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.3555456237358e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.27240655566322e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1900947351399e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1086019309152e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.02791999364078e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.94804085505568e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.86895652717947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.79065910151345e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.71314074824984e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.63639371548869e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.56041032846277e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.48518298877006e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.4107041736139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.33696643505071e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.26396239924518e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.1916847657329e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.12012630669027e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.04927986621178e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.97913835959434e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.90969477262882e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.84094216089865e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.77287364908539e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.70548243028111e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.63876176530778e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.5727049820433e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.50730547475429e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.44255670343554e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.37845219315595e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.31498553341106e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.25215037748203e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.18994044180088e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.12834950532221e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.06737140890104e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.00700005467694e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.94722940546415e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.88805348414794e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.82946637308688e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.77146221352103e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.71403520498611e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.65717960473339e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.60088972715548e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.54515994321769e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.48998467989523e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.43535841961575e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.38127569970771e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.32773111185406e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.27471930155139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.22223496757448e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.1702728614462e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.11882778691264e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.06789459942348e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.01746820561753e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.96754356281337e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.91811567850513e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.86917960986318e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.82073046323988e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.7727633936802e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.72527360443719e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.67825634649237e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.63170691808076e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.58562066422073e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.53999297624849e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0  loss:  NaN&quot;
## [1] &quot;fold:  4&quot;
## [1] &quot;step =  1  lambda =  22026.4657948067  loss:  73.8690413882394&quot;
## [1] &quot;step =  2  lambda =  22026.4657948067  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  21807.2987982302  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  21590.3125497062  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  21375.4853504291  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  21162.7957175002  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  20952.2223817786  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  20743.7442857556  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  20537.3405814475  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  20332.9906283122  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  20130.6739911839  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  19930.3704382303  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  19732.0599389292  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  19535.7226620655  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  19341.3389737478  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  19148.8894354453  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  18958.3548020439  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  18769.7160199212  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  18582.9542250422  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  18398.0507410714  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  18214.9870775064  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  18033.7449278285  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  17854.3061676715  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  17676.65285301  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  17500.7672183642  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  17326.6316750244  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  17154.228809291  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  16983.5413807338  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  16814.5523204676  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  16647.2447294456  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  16481.6018767693  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  16317.6071980154  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  16155.2442935794  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  15994.4969270355  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  15835.3490235131  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  15677.7846680892  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  15521.788104197  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  15367.34373205  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  15214.4361070824  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  15063.0499384043  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  14913.1700872726  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  14764.7815655773  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  14617.8695343426  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  14472.4193022429  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  14328.4163241338  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  14185.8461995975  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  14044.6946715028  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  13904.9476245792  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  13766.5910840055  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  13629.6112140124  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  13493.9943164988  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  13359.7268296619  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  13226.7953266411  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  13095.1865141752  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12964.8872312735  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12835.8844478991  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12708.165263666  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12581.7169065495  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12456.5267316084  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12332.582219721  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12209.8709763327  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  12088.380730217  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11968.099332248  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11849.0147541856  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11731.1150874729  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11614.3885420449  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11498.8234451498  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11384.4082401816  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11271.1314855245  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11158.9818534085  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  11047.9481287771  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10938.0192081652  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10829.1840985891  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10721.4319164473  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10614.7518864317  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10509.1333404504  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10404.5657165607  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10301.0385579133  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10198.5415117058  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  10097.0643281483  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9996.59685943788  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9897.12905874391  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9798.65097920349  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9701.15277292652  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9604.6246900112  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9509.05707756873  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9414.44037875829  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9320.76513183108  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9228.02196918439  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9136.2016164247  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  9045.29489144014  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8955.29270348252  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8866.186052258  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8777.96602702724  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8690.62380571415  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8604.15065402384  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8518.53792456912  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8433.77705600563  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8349.85957217593  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8266.77708126167  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8184.52127494457  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8103.08392757538  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  8022.45689535158  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7942.63211550269  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7863.60160548423  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7785.35746217936  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7707.89186110851  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7631.19705564706  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7555.2653762505  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7480.08922968767  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7405.66109828121  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7331.97353915601  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7259.01918349469  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7186.79073580093  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7115.28097316979  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  7044.48274456536  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6974.38897010583  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6904.9926403553  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6836.286815623  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6768.26462526917  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6700.91926701811  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6634.24400627789  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6568.23217546683  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6502.87717334688  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6438.17246436333  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6374.1115779914  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6310.68810808902  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6247.8957122564  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6185.72811120158  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6124.17908811267  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6063.24248803609  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  6002.91221726102  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5943.18224271013  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5884.04659133616  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5825.49934952474  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5767.53466250285  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5710.14673375352  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5653.32982443602  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5597.07825281208  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5541.3863936777  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5486.2486778005  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5431.65959136299  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5377.61367541099  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5324.10552530792  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5271.12979019412  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5218.68117245197  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5166.754427176  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5115.34436164836  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5064.44583481972  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  5014.05375679492  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4964.16308832421  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4914.76884029913  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4865.86607325376  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4817.44989687059  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4769.51546949167  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4722.05799763432  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4675.07273551178  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4628.55498455872  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4582.50009296124  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4536.90345519183  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4491.76051154869  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4447.06674769987  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4402.8176942317  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4359.00892620198  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4315.63606269742  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4272.69476639549  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4230.1807431308  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4188.08974146558  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4146.4175522646  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4105.16000827419  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4064.31298370559  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  4023.87239382231  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3983.83419453164  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3944.19438198031  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3904.948992154  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3866.09410048106  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3827.62582143991  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3789.54030817061  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3751.83375209008  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3714.50238251129  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3677.5424662662  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3640.95030733235  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3604.72224646338  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3568.854660823  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3533.34396362276  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3498.18660376333  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3463.37906547946  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3428.91786798828  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3394.79956514135  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3361.02074507995  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3327.5780298939  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3294.46807528385  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3261.68757022671  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3229.23323664469  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3197.10182907735  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3165.29013435719  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3133.79497128823  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3102.61319032788  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3071.7416732721  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3041.17733294343  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  3010.91711288239  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2980.95798704173  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2951.29695948392  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2921.93106408148  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2892.85736422039  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2864.07295250646  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2835.57495047451  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2807.3605083006  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2779.42680451698  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2751.77104573003  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2724.39046634078  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2697.28232826851  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2670.44392067681  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2643.87255970255  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2617.5655881875  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2591.52037541257  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2565.7343168348  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2540.20483382683  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2514.92937341909  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2489.90540804446  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2465.13043528557  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2440.6019776245  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2416.31758219502  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2392.27482053738  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2368.47128835535  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2344.9046052759  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2321.57241461106  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2298.47238312234  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2275.60220078732  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2252.95958056872  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2230.54225818566  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2208.34799188721  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2186.37456222824  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2164.61977184748  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2143.08144524776  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2121.75742857847  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2100.64558942018  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2079.74381657137  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2059.05001983734  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2038.56212982119  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  2018.27809771681  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1998.19589510412  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1978.3135137461  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1958.62896538806  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1939.14028155876  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1919.84551337356  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1900.74273133958  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1881.83002516269  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1863.10550355652  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1844.56729405329  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1826.21354281661  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1808.04241445606  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1790.05209184367  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1772.24077593218  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1754.60668557515  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1737.14805734885  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1719.86314537592  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1702.75022115076  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1685.80757336666  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1669.03350774476  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1652.42634686448  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1635.98442999593  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1619.7061129337  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1603.58976783251  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1587.63378304445  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1571.83656295772  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1556.19652783716  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1540.71211366621  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1525.38177199056  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1510.20396976326  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1495.17718919145  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1480.29992758455  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1465.57069720398  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1450.98802511446  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1436.5504530366  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1422.25653720118  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1408.1048482047  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1394.09397086646  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1380.22250408705  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1366.48906070825  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1352.89226737426  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1339.43076439442  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1326.10320560722  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1312.90825824566  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1299.84460280403  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1286.91093290588  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1274.10595517346  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1261.4283890983  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1248.87696691325  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1236.45043346563  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1224.14754609174  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1211.96707449258  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1199.90780061084  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1187.9685185091  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1176.14803424917  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1164.4451657728  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1152.85874278339  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1141.38760662897  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1130.03061018637  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1118.78661774649  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1107.65450490071  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1096.63315842846  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1085.72147618592  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1074.91836699577  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1064.22275053809  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1053.63355724232  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1043.1497281803  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1032.7702149604  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1022.49397962264  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1012.31999453492  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  1002.24724229025  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  992.274715605024  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  982.401417218259  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  972.626359791883  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  962.948565812015  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  953.367067491183  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  943.88090667158  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  934.48913472921  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  925.19081247906  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  915.98501008115  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  906.870806947571  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  897.847291650418  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  888.913561830636  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  880.068724107804  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  871.311893990772  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  862.642195789238  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  854.058762526152  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  845.560735851038  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  837.147265954143  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  828.817511481469  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  820.570639450629  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  812.405825167543  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  804.322252143983  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  796.319112015905  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  788.395604462634  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  780.550937126804  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  772.784325535149  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  765.094993020038  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  757.482170641809  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  749.945097111883  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  742.483018716622  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  735.095189241974  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  727.780869898828  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  720.539329249161  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  713.369843132868  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  706.271694595365  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  699.244173815886  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  692.286578036491  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  685.39821149181  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  678.578385339442  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  671.826417591095  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  665.141633044362  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  658.523363215222  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  651.970946271172  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  645.483726965061  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  639.061056569553  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  632.702292812253  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  626.40679981149  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  620.173948012713  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  614.003114125553  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  607.893681061474  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  601.845037872081  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  595.856579688017  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  589.927707658468  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  584.057828891295  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  578.246356393726  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  572.492709013672  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  566.796311381596  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  561.156593852992  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  555.572992451403  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  550.044948812038  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  544.571910125929  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  539.153329084642  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  533.788663825562  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  528.477377877687  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  523.218940108002  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  518.012824668342  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  512.85851094283  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  507.755483495794  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  502.703232020238  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  497.701251286808  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  492.749041093256  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  487.846106214441  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  482.991956352786  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  478.186106089262  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  473.428074834835  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  468.717386782416  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  464.053570859277  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  459.436160679934  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  454.864694499525  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  450.338715167621  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  445.857770082518  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  441.421411145971  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  437.029194718392  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  432.680681574476  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  428.375436859286  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  424.113030044765  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  419.893034886674  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  415.715029381986  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  411.578595726666  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  407.483320273902  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  403.428793492735  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  399.41460992711  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  395.440368155324  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  391.505670749889  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  387.610124237784  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  383.753339061112  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  379.934929538142  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  376.154513824739  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  372.411713876182  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  368.706155409357  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  365.037467865329  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  361.405284372286  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  357.809241708853  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  354.248980267766  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  350.724144019914  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  347.234380478734  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  343.779340664966  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  340.358679071749  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  336.972053630071  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  333.619125674568  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  330.299559909649  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  327.013024375971  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  323.759190417243  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  320.537732647356  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  317.34832891785  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  314.190660285694  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  311.064410981393  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  307.969268377411  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  304.904922956909  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  301.87106828279  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  298.867400967061  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  295.893620640484  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  292.94942992255  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  290.034534391735  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  287.148642556054  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  284.291465823921  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  281.46271847528  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  278.66211763304  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  275.889383234783  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  273.144238004757  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  270.426407426153  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  267.735619713647  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  265.071605786227  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  262.434099240279  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  259.822836322951  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  257.237555905775  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  254.677999458555  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  252.143911023513  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  249.635037189694  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  247.151127067624  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  244.69193226422  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  242.257206857954  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  239.846707374255  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  237.460192761167  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  235.097424365239  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  232.758165907662  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  230.442183460642  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  228.149245424004  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  225.879122502033  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  223.631587680546  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  221.406416204187  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  219.203385553955  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  217.022275424948  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  214.862867704336  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  212.724946449547  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  210.608297866674  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  208.512710289096  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  206.437974156308  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  204.383881992968  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  202.350228388148  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  200.336809974792  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  198.343425409381  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  196.369875351799  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  194.415962445393  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  192.481491297246  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  190.56626845863  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  188.670102405666  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  186.792803520168  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  184.934184070684  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  183.094058193718  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  181.272241875151  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  179.468552931832  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  177.682810993364  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  175.914837484065  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  174.164455605111  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  172.431490316854  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  170.715768321323  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  169.017118044887  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  167.335369621104  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  165.67035487373  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  164.021907299902  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  162.389862053489  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  160.774055928607  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  159.174327343297  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  157.590516323367  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  156.022464486395  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  154.470015025891  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  152.933012695615  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  151.411303794053  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  149.904736149047  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  148.413159102577  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  146.936423495695  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  145.47438165361  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  144.02688737092  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  142.593795896989  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  141.174963921477  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  139.770249560003  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  138.379512339961  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  137.002613186469  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  135.639414408465  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  134.289779684936  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  132.953574051283  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  131.63066388583  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  130.320916896459  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  129.024202107378  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  127.740389846029  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  126.469351730115  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  125.210960654765  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  123.965090779824  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  122.731617517265  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  121.510417518735  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  120.301368663216  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  119.104350044814  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  117.919241960671  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  116.74592589899  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  115.584284527188  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  114.434201680159  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  113.29556234866  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  112.168252667809  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  111.052159905699  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  109.947172452124  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  108.853179807416  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  107.7700725714  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  106.697742432451  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  105.63608215666  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  104.584985577114  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  103.544347583281  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  102.514064110494  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  101.494032129546  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  100.484149636389  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  99.4843156419338  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  98.4944301619463  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  97.514394207054  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  96.5441097728447  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  95.5834798300663  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  94.6324083149241  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  93.6908001194741  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  92.7585610821118  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  91.8355979781567  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  90.9218185105295  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  90.0171313005218  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  89.1214458786587  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  88.2346726756515  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  87.3567230134411  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  86.4875090963295  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  85.6269440022007  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  84.774941673828  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  83.9314169102688  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  83.0962853583438  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  82.2694635042017  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  81.4508686649681  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  80.6404189804771  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  79.8380334050846  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  79.0436316995645  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  78.2571344230842  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  77.4784629252608  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  76.7075393382956  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  75.9442865691873  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  75.1886282920231  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  74.4404889403455  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  73.6997936995958  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  72.9664684996329  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  72.2404400073254  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  71.5216356192192  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  70.8099834542765  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  70.1054123466879  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  69.4078518387552  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  68.7172321738465  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  68.0334842894197  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  67.3565398101166  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  66.6863310409252  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  66.0227909604099  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  65.3658532140099  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  64.715452107403  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  64.0715225999366  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  63.4340002981233  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  62.8028214492017  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  62.1779229347609  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  61.5592422644286  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  60.9467175696222  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  60.340287597362  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  59.7398917041452  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  59.1454698498823  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  58.5569625918924  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  57.9743110789593  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  57.3974570454462  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  56.8263428054691  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  56.2609112471279  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  55.7011058267956  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  55.1468705634638  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  54.5981500331442  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  54.0548893633266  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  53.5170342274912  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  52.9845308396762  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  52.4573259490991  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  51.9353668348315  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  51.4186013005269  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  50.9069776692014  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  50.4004447780655  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  49.8989519734079  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  49.4024491055302  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  48.9108865237319  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  48.4242150713452  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  47.9423860808193  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  47.4653513688535  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  46.9930632315793  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  46.5254744397892  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  46.0625382342145  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  45.6042083208487  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  45.1504388663187  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  44.7011844933009  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  44.2564002759834  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  43.816041735574  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  43.3800648358516  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  42.948425978763  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  42.5210820000628  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  42.0979901649969  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  41.6791081640293  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  41.2643941086108  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  40.8538065269903  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  40.4473043600674  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  40.0448469572867  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  39.6463940725726  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  39.2519058603045  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  38.8613428713325  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  38.4746660490321  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  38.091836725399  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  37.7128166171818  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  37.3375678220537  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  36.9660528148225  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  36.598234443678  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  36.2340759264765  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  35.8735408470628  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  35.5165931516285  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  35.1631971451066  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  34.813317487602  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  34.4669191908574  loss:  73.7927217105101&quot;
## [1] &quot;step =  1  lambda =  34.1239676147544  loss:  76.3807484417781&quot;
## [1] &quot;step =  2  lambda =  34.1239676147544  loss:  76.0488716415838&quot;
## [1] &quot;step =  3  lambda =  34.1239676147544  loss:  76.0904723113968&quot;
## [1] &quot;step =  1  lambda =  33.7844284638495  loss:  86.9246911293527&quot;
## [1] &quot;step =  2  lambda =  33.7844284638495  loss:  85.7225288997158&quot;
## [1] &quot;step =  3  lambda =  33.7844284638495  loss:  85.8261951100062&quot;
## [1] &quot;step =  1  lambda =  33.4482677839449  loss:  96.4124265201086&quot;
## [1] &quot;step =  2  lambda =  33.4482677839449  loss:  95.495101450503&quot;
## [1] &quot;step =  3  lambda =  33.4482677839449  loss:  95.477776612176&quot;
## [1] &quot;step =  1  lambda =  33.1154519586923  loss:  105.847795524664&quot;
## [1] &quot;step =  2  lambda =  33.1154519586923  loss:  105.138034349772&quot;
## [1] &quot;step =  3  lambda =  33.1154519586923  loss:  105.076269910418&quot;
## [1] &quot;step =  1  lambda =  32.7859477062319  loss:  115.276957727376&quot;
## [1] &quot;step =  2  lambda =  32.7859477062319  loss:  114.677662132851&quot;
## [1] &quot;step =  3  lambda =  32.7859477062319  loss:  114.623059939793&quot;
## [1] &quot;step =  1  lambda =  32.4597220758638  loss:  124.701288527283&quot;
## [1] &quot;step =  2  lambda =  32.4597220758638  loss:  124.121626494839&quot;
## [1] &quot;step =  3  lambda =  32.4597220758638  loss:  124.121837462148&quot;
## [1] &quot;step =  1  lambda =  32.1367424447532  loss:  134.125640200758&quot;
## [1] &quot;step =  2  lambda =  32.1367424447532  loss:  133.479062796515&quot;
## [1] &quot;step =  3  lambda =  32.1367424447532  loss:  133.576789657223&quot;
## [1] &quot;step =  1  lambda =  31.8169765146677  loss:  143.554983403988&quot;
## [1] &quot;step =  2  lambda =  31.8169765146677  loss:  142.836999256342&quot;
## [1] &quot;step =  3  lambda =  31.8169765146677  loss:  142.974966119699&quot;
## [1] &quot;step =  1  lambda =  31.500392308748  loss:  153.026697409079&quot;
## [1] &quot;step =  2  lambda =  31.500392308748  loss:  152.452546156632&quot;
## [1] &quot;step =  3  lambda =  31.500392308748  loss:  152.234453591537&quot;
## [1] &quot;step =  4  lambda =  31.500392308748  loss:  152.397971162177&quot;
## [1] &quot;step =  1  lambda =  31.1869581683094  loss:  162.446439182542&quot;
## [1] &quot;step =  2  lambda =  31.1869581683094  loss:  162.030925346008&quot;
## [1] &quot;step =  3  lambda =  31.1869581683094  loss:  161.774669329874&quot;
## [1] &quot;step =  4  lambda =  31.1869581683094  loss:  161.657321421216&quot;
## [1] &quot;step =  5  lambda =  31.1869581683094  loss:  161.790252143469&quot;
## [1] &quot;step =  1  lambda =  30.876642749677  loss:  171.936400504578&quot;
## [1] &quot;step =  2  lambda =  30.876642749677  loss:  171.645251069236&quot;
## [1] &quot;step =  3  lambda =  30.876642749677  loss:  171.463944062768&quot;
## [1] &quot;step =  4  lambda =  30.876642749677  loss:  171.345650476366&quot;
## [1] &quot;step =  5  lambda =  30.876642749677  loss:  171.265920778378&quot;
## [1] &quot;step =  6  lambda =  30.876642749677  loss:  171.211863812944&quot;
## [1] &quot;step =  7  lambda =  30.876642749677  loss:  171.175064421285&quot;
## [1] &quot;step =  8  lambda =  30.876642749677  loss:  171.166695748412&quot;
## [1] &quot;step =  1  lambda =  30.5694150210502  loss:  181.484945182025&quot;
## [1] &quot;step =  2  lambda =  30.5694150210502  loss:  181.289690483628&quot;
## [1] &quot;step =  3  lambda =  30.5694150210502  loss:  181.188547672583&quot;
## [1] &quot;step =  4  lambda =  30.5694150210502  loss:  181.105816675274&quot;
## [1] &quot;step =  5  lambda =  30.5694150210502  loss:  181.048348653391&quot;
## [1] &quot;step =  6  lambda =  30.5694150210502  loss:  181.008365633326&quot;
## [1] &quot;step =  7  lambda =  30.5694150210502  loss:  180.980454200698&quot;
## [1] &quot;step =  8  lambda =  30.5694150210502  loss:  180.960921917645&quot;
## [1] &quot;step =  1  lambda =  30.2652442594001  loss:  191.200783318208&quot;
## [1] &quot;step =  2  lambda =  30.2652442594001  loss:  191.069660642752&quot;
## [1] &quot;step =  3  lambda =  30.2652442594001  loss:  191.027077131337&quot;
## [1] &quot;step =  4  lambda =  30.2652442594001  loss:  190.957024780382&quot;
## [1] &quot;step =  5  lambda =  30.2652442594001  loss:  190.906469619224&quot;
## [1] &quot;step =  6  lambda =  30.2652442594001  loss:  190.870507303989&quot;
## [1] &quot;step =  7  lambda =  30.2652442594001  loss:  190.844853768007&quot;
## [1] &quot;step =  8  lambda =  30.2652442594001  loss:  190.826507795292&quot;
## [1] &quot;step =  1  lambda =  29.964100047397  loss:  200.974398893591&quot;
## [1] &quot;step =  2  lambda =  29.964100047397  loss:  200.911509865536&quot;
## [1] &quot;step =  3  lambda =  29.964100047397  loss:  200.933332691817&quot;
## [1] &quot;step =  1  lambda =  29.6659522703689  loss:  210.935931218746&quot;
## [1] &quot;step =  2  lambda =  29.6659522703689  loss:  210.994787007698&quot;
## [1] &quot;step =  1  lambda =  29.3707711132895  loss:  221.037953487855&quot;
## [1] &quot;step =  2  lambda =  29.3707711132895  loss:  221.038983108119&quot;
## [1] &quot;step =  1  lambda =  29.0785270577971  loss:  231.145242655709&quot;
## [1] &quot;step =  2  lambda =  29.0785270577971  loss:  231.118620816142&quot;
## [1] &quot;step =  3  lambda =  29.0785270577971  loss:  231.49148228278&quot;
## [1] &quot;step =  1  lambda =  28.7891908792427  loss:  241.114909743035&quot;
## [1] &quot;step =  2  lambda =  28.7891908792427  loss:  241.710466339891&quot;
## [1] &quot;step =  1  lambda =  28.5027336437673  loss:  251.472120769789&quot;
## [1] &quot;step =  2  lambda =  28.5027336437673  loss:  252.029583112715&quot;
## [1] &quot;step =  1  lambda =  28.2191267054086  loss:  261.985774226418&quot;
## [1] &quot;step =  2  lambda =  28.2191267054086  loss:  262.483191955835&quot;
## [1] &quot;step =  1  lambda =  27.9383417032365  loss:  272.702254025833&quot;
## [1] &quot;step =  2  lambda =  27.9383417032365  loss:  273.437769485943&quot;
## [1] &quot;step =  1  lambda =  27.6603505585168  loss:  283.674693562193&quot;
## [1] &quot;step =  2  lambda =  27.6603505585168  loss:  284.699103827166&quot;
## [1] &quot;step =  1  lambda =  27.3851254719032  loss:  294.959862043065&quot;
## [1] &quot;step =  2  lambda =  27.3851254719032  loss:  296.331654321956&quot;
## [1] &quot;step =  1  lambda =  27.1126389206579  loss:  306.621377396006&quot;
## [1] &quot;step =  2  lambda =  27.1126389206579  loss:  308.410246539325&quot;
## [1] &quot;step =  1  lambda =  26.8428636558986  loss:  318.733201505787&quot;
## [1] &quot;step =  2  lambda =  26.8428636558986  loss:  321.024122530341&quot;
## [1] &quot;step =  1  lambda =  26.575772699874  loss:  331.383618325957&quot;
## [1] &quot;step =  2  lambda =  26.575772699874  loss:  334.281944930222&quot;
## [1] &quot;step =  1  lambda =  26.3113393432659  loss:  344.680162567092&quot;
## [1] &quot;step =  2  lambda =  26.3113393432659  loss:  348.318361115676&quot;
## [1] &quot;step =  1  lambda =  26.0495371425183  loss:  358.756093086498&quot;
## [1] &quot;step =  2  lambda =  26.0495371425183  loss:  363.302955951393&quot;
## [1] &quot;step =  1  lambda =  25.7903399171931  loss:  373.779225500918&quot;
## [1] &quot;step =  2  lambda =  25.7903399171931  loss:  379.452798818786&quot;
## [1] &quot;step =  1  lambda =  25.5337217473515  loss:  389.964310591594&quot;
## [1] &quot;step =  2  lambda =  25.5337217473515  loss:  397.050404995105&quot;
## [1] &quot;step =  1  lambda =  25.2796569709629  loss:  407.590749938739&quot;
## [1] &quot;step =  2  lambda =  25.2796569709629  loss:  416.469916243908&quot;
## [1] &quot;step =  1  lambda =  25.0281201813378  loss:  427.028408744423&quot;
## [1] &quot;step =  2  lambda =  25.0281201813378  loss:  438.215865796615&quot;
## [1] &quot;step =  1  lambda =  24.7790862245877  loss:  448.77581922066&quot;
## [1] &quot;step =  2  lambda =  24.7790862245877  loss:  462.98131331106&quot;
## [1] &quot;step =  1  lambda =  24.5325301971094  loss:  473.517445365585&quot;
## [1] &quot;step =  2  lambda =  24.5325301971094  loss:  491.735713552447&quot;
## [1] &quot;step =  1  lambda =  24.2884274430945  loss:  502.21019395455&quot;
## [1] &quot;step =  2  lambda =  24.2884274430945  loss:  525.85759919001&quot;
## [1] &quot;step =  1  lambda =  24.0467535520645  loss:  536.782726081294&quot;
## [1] &quot;step =  2  lambda =  24.0467535520645  loss:  567.174384172455&quot;
## [1] &quot;step =  1  lambda =  23.8074843564287  loss:  580.613759653237&quot;
## [1] &quot;step =  2  lambda =  23.8074843564287  loss:  622.749402536096&quot;
## [1] &quot;step =  1  lambda =  23.5705959290681  loss:  642.659720970808&quot;
## [1] &quot;step =  2  lambda =  23.5705959290681  loss:  720.312703368118&quot;
## [1] &quot;step =  1  lambda =  23.3360645809427  loss:  742.040267870644&quot;
## [1] &quot;step =  2  lambda =  23.3360645809427  loss:  884.496842899547&quot;
## [1] &quot;step =  1  lambda =  23.1038668587222  loss:  904.968693551265&quot;
## [1] &quot;step =  1  lambda =  22.8739795424408  loss:  924.697184269109&quot;
## [1] &quot;step =  1  lambda =  22.6463796431754  loss:  943.687891070837&quot;
## [1] &quot;step =  1  lambda =  22.4210444007463  loss:  963.072336987561&quot;
## [1] &quot;step =  1  lambda =  22.1979512814416  loss:  987.283185868403&quot;
## [1] &quot;step =  1  lambda =  21.9770779757634  loss:  1010.60225610214&quot;
## [1] &quot;step =  1  lambda =  21.7584023961971  loss:  1033.03227224416&quot;
## [1] &quot;step =  1  lambda =  21.5419026750024  loss:  1063.71764928364&quot;
## [1] &quot;step =  1  lambda =  21.3275571620269  loss:  1112.71735924847&quot;
## [1] &quot;step =  1  lambda =  21.1153444225406  loss:  1163.51580362944&quot;
## [1] &quot;step =  1  lambda =  20.9052432350928  loss:  1210.54696648448&quot;
## [1] &quot;step =  1  lambda =  20.6972325893895  loss:  1254.11886527103&quot;
## [1] &quot;step =  1  lambda =  20.4912916841929  loss:  1294.48521673503&quot;
## [1] &quot;step =  1  lambda =  20.2873999252409  loss:  1331.8566482018&quot;
## [1] &quot;step =  1  lambda =  20.0855369231877  loss:  1369.25051605924&quot;
## [1] &quot;step =  1  lambda =  19.8856824915647  loss:  1406.2774145574&quot;
## [1] &quot;step =  1  lambda =  19.6878166447624  loss:  1440.17044228106&quot;
## [1] &quot;step =  1  lambda =  19.4919195960311  loss:  1471.15231710182&quot;
## [1] &quot;step =  1  lambda =  19.2979717555028  loss:  1499.43203603449&quot;
## [1] &quot;step =  1  lambda =  19.1059537282317  loss:  1525.20514229174&quot;
## [1] &quot;step =  1  lambda =  18.915846312255  loss:  1548.65368740322&quot;
## [1] &quot;step =  1  lambda =  18.7276304966729  loss:  1569.94632004137&quot;
## [1] &quot;step =  1  lambda =  18.5412874597469  loss:  1589.23862667892&quot;
## [1] &quot;step =  1  lambda =  18.3567985670179  loss:  1608.97713771762&quot;
## [1] &quot;step =  1  lambda =  18.1741453694431  loss:  1626.79423466516&quot;
## [1] &quot;step =  1  lambda =  17.9933096015503  loss:  1642.56598437448&quot;
## [1] &quot;step =  1  lambda =  17.8142731796122  loss:  1658.39984140287&quot;
## [1] &quot;step =  1  lambda =  17.6370181998373  loss:  1674.52751818507&quot;
## [1] &quot;step =  1  lambda =  17.46152693658  loss:  1688.97431864378&quot;
## [1] &quot;step =  1  lambda =  17.2877818405676  loss:  1702.02573355669&quot;
## [1] &quot;step =  1  lambda =  17.1157655371459  loss:  1713.91022038062&quot;
## [1] &quot;step =  1  lambda =  16.945460824541  loss:  1724.39129045206&quot;
## [1] &quot;step =  1  lambda =  16.7768506721399  loss:  1733.5894650757&quot;
## [1] &quot;step =  1  lambda =  16.6099182187867  loss:  1741.60986085683&quot;
## [1] &quot;step =  1  lambda =  16.4446467710971  loss:  1748.54505504972&quot;
## [1] &quot;step =  1  lambda =  16.2810198017884  loss:  1754.47727096914&quot;
## [1] &quot;step =  1  lambda =  16.1190209480276  loss:  1760.48690267261&quot;
## [1] &quot;step =  1  lambda =  15.958634009794  loss:  1766.12921375512&quot;
## [1] &quot;step =  1  lambda =  15.7998429482604  loss:  1772.54453074092&quot;
## [1] &quot;step =  1  lambda =  15.6426318841882  loss:  1778.17264886963&quot;
## [1] &quot;step =  1  lambda =  15.4869850963399  loss:  1783.06429689163&quot;
## [1] &quot;step =  1  lambda =  15.3328870199072  loss:  1787.26516316892&quot;
## [1] &quot;step =  1  lambda =  15.1803222449539  loss:  1790.81652261369&quot;
## [1] &quot;step =  1  lambda =  15.0292755148754  loss:  1793.75577803575&quot;
## [1] &quot;step =  1  lambda =  14.8797317248728  loss:  1796.11693010808&quot;
## [1] &quot;step =  1  lambda =  14.7316759204426  loss:  1797.93098694633&quot;
## [1] &quot;step =  1  lambda =  14.5850932958808  loss:  1799.22632197918&quot;
## [1] &quot;step =  1  lambda =  14.4399691928029  loss:  1800.02898709885&quot;
## [1] &quot;step =  1  lambda =  14.2962890986776  loss:  1800.36298683941&quot;
## [1] &quot;step =  1  lambda =  14.1540386453758  loss:  1800.25051840303&quot;
## [1] &quot;step =  1  lambda =  14.0132036077336  loss:  1799.71218164791&quot;
## [1] &quot;step =  1  lambda =  13.8737699021299  loss:  1798.76716260199&quot;
## [1] &quot;step =  1  lambda =  13.7357235850779  loss:  1797.43339362627&quot;
## [1] &quot;step =  1  lambda =  13.5990508518309  loss:  1795.72769299068&quot;
## [1] &quot;step =  1  lambda =  13.4637380350017  loss:  1793.9303610885&quot;
## [1] &quot;step =  1  lambda =  13.3297716031958  loss:  1791.88778918564&quot;
## [1] &quot;step =  1  lambda =  13.1971381596584  loss:  1789.68233021998&quot;
## [1] &quot;step =  1  lambda =  13.0658244409346  loss:  1787.31382354303&quot;
## [1] &quot;step =  1  lambda =  12.9358173155431  loss:  1784.6828543957&quot;
## [1] &quot;step =  1  lambda =  12.807103782663  loss:  1781.79773041214&quot;
## [1] &quot;step =  1  lambda =  12.6796709708339  loss:  1778.66647328388&quot;
## [1] &quot;step =  1  lambda =  12.5535061366682  loss:  1775.29684202518&quot;
## [1] &quot;step =  1  lambda =  12.4285966635775  loss:  1771.69635326922&quot;
## [1] &quot;step =  1  lambda =  12.3049300605104  loss:  1767.87229895305&quot;
## [1] &quot;step =  1  lambda =  12.1824939607035  loss:  1763.83176171476&quot;
## [1] &quot;step =  1  lambda =  12.0612761204447  loss:  1760.08518356369&quot;
## [1] &quot;step =  1  lambda =  11.9412644178491  loss:  1756.13584115422&quot;
## [1] &quot;step =  1  lambda =  11.8224468516464  loss:  1751.97223929369&quot;
## [1] &quot;step =  1  lambda =  11.7048115399809  loss:  1747.60065028732&quot;
## [1] &quot;step =  1  lambda =  11.5883467192234  loss:  1743.02727882331&quot;
## [1] &quot;step =  1  lambda =  11.4730407427948  loss:  1738.25825904312&quot;
## [1] &quot;step =  1  lambda =  11.3588820800015  loss:  1733.2996511702&quot;
## [1] &quot;step =  1  lambda =  11.2458593148818  loss:  1728.15743790864&quot;
## [1] &quot;step =  1  lambda =  11.1339611450653  loss:  1722.83752078128&quot;
## [1] &quot;step =  1  lambda =  11.0231763806416  loss:  1717.34571654022&quot;
## [1] &quot;step =  1  lambda =  10.913493943042  loss:  1711.68775375103&quot;
## [1] &quot;step =  1  lambda =  10.8049028639313  loss:  1706.20000952729&quot;
## [1] &quot;step =  1  lambda =  10.6973922841111  loss:  1701.14557866904&quot;
## [1] &quot;step =  1  lambda =  10.5909514524338  loss:  1695.91450565041&quot;
## [1] &quot;step =  1  lambda =  10.4855697247276  loss:  1690.51283758799&quot;
## [1] &quot;step =  1  lambda =  10.3812365627318  loss:  1684.94650465836&quot;
## [1] &quot;step =  1  lambda =  10.2779415330434  loss:  1679.22131791048&quot;
## [1] &quot;step =  1  lambda =  10.1756743060733  loss:  1673.34296766773&quot;
## [1] &quot;step =  1  lambda =  10.0744246550136  loss:  1667.31702246964&quot;
## [1] &quot;step =  1  lambda =  9.97418245481473  loss:  1661.14892850281&quot;
## [1] &quot;step =  1  lambda =  9.87493768117319  loss:  1654.84400947126&quot;
## [1] &quot;step =  1  lambda =  9.77668040952892  loss:  1648.40746685769&quot;
## [1] &quot;step =  1  lambda =  9.67940081407284  loss:  1641.84438052974&quot;
## [1] &quot;step =  1  lambda =  9.58308916676438  loss:  1635.15970964769&quot;
## [1] &quot;step =  1  lambda =  9.48773583635853  loss:  1628.35829383332&quot;
## [1] &quot;step =  1  lambda =  9.39333128744278  loss:  1621.44485456249&quot;
## [1] &quot;step =  1  lambda =  9.29986607948359  loss:  1614.42399674725&quot;
## [1] &quot;step =  1  lambda =  9.20733086588226  loss:  1607.3002104766&quot;
## [1] &quot;step =  1  lambda =  9.11571639304031  loss:  1600.07787288778&quot;
## [1] &quot;step =  1  lambda =  9.02501349943413  loss:  1592.76125014299&quot;
## [1] &quot;step =  1  lambda =  8.93521311469874  loss:  1585.35449948934&quot;
## [1] &quot;step =  1  lambda =  8.84630625872088  loss:  1577.86167138202&quot;
## [1] &quot;step =  1  lambda =  8.75828404074083  loss:  1570.28671165348&quot;
## [1] &quot;step =  1  lambda =  8.67113765846346  loss:  1562.63346371318&quot;
## [1] &quot;step =  1  lambda =  8.5848583971779  loss:  1554.90567076483&quot;
## [1] &quot;step =  1  lambda =  8.49943762888613  loss:  1547.10697802953&quot;
## [1] &quot;step =  1  lambda =  8.41486681144014  loss:  1539.24093496503&quot;
## [1] &quot;step =  1  lambda =  8.3311374876877  loss:  1531.3109974727&quot;
## [1] &quot;step =  1  lambda =  8.24824128462666  loss:  1523.32053008519&quot;
## [1] &quot;step =  1  lambda =  8.16616991256765  loss:  1515.27280812878&quot;
## [1] &quot;step =  1  lambda =  8.08491516430506  loss:  1507.17101985557&quot;
## [1] &quot;step =  1  lambda =  8.00446891429635  loss:  1499.01826854151&quot;
## [1] &quot;step =  1  lambda =  7.92482311784949  loss:  1490.81757454699&quot;
## [1] &quot;step =  1  lambda =  7.84596981031845  loss:  1482.57187733763&quot;
## [1] &quot;step =  1  lambda =  7.76790110630678  loss:  1474.28403746312&quot;
## [1] &quot;step =  1  lambda =  7.69060919887901  loss:  1465.95683849293&quot;
## [1] &quot;step =  1  lambda =  7.61408635877998  loss:  1457.59298890775&quot;
## [1] &quot;step =  1  lambda =  7.53832493366192  loss:  1447.5201717831&quot;
## [1] &quot;step =  1  lambda =  7.46331734731919  loss:  1437.21615416047&quot;
## [1] &quot;step =  1  lambda =  7.38905609893065  loss:  1426.94356698388&quot;
## [1] &quot;step =  1  lambda =  7.31553376230957  loss:  1416.70345829323&quot;
## [1] &quot;step =  1  lambda =  7.24274298516102  loss:  1406.49683161562&quot;
## [1] &quot;step =  1  lambda =  7.17067648834662  loss:  1396.32464750943&quot;
## [1] &quot;step =  1  lambda =  7.09932706515664  loss:  1386.18782506097&quot;
## [1] &quot;step =  1  lambda =  7.0286875805893  loss:  1376.08724333362&quot;
## [1] &quot;step =  1  lambda =  6.95875097063727  loss:  1366.02374277011&quot;
## [1] &quot;step =  1  lambda =  6.88951024158129  loss:  1355.99812654833&quot;
## [1] &quot;step =  1  lambda =  6.82095846929075  loss:  1346.01116189168&quot;
## [1] &quot;step =  1  lambda =  6.75308879853129  loss:  1336.06358133458&quot;
## [1] &quot;step =  1  lambda =  6.68589444227927  loss:  1326.15608394444&quot;
## [1] &quot;step =  1  lambda =  6.61936868104308  loss:  1316.28933650091&quot;
## [1] &quot;step =  1  lambda =  6.55350486219115  loss:  1305.75614998626&quot;
## [1] &quot;step =  1  lambda =  6.48829639928672  loss:  1295.23828202222&quot;
## [1] &quot;step =  1  lambda =  6.42373677142913  loss:  1284.78814300942&quot;
## [1] &quot;step =  1  lambda =  6.35981952260183  loss:  1274.40558768514&quot;
## [1] &quot;step =  1  lambda =  6.29653826102666  loss:  1264.09046068184&quot;
## [1] &quot;step =  1  lambda =  6.23388665852472  loss:  1253.84259727993&quot;
## [1] &quot;step =  1  lambda =  6.17185844988355  loss:  1243.66182410894&quot;
## [1] &quot;step =  1  lambda =  6.11044743223061  loss:  1233.54795979991&quot;
## [1] &quot;step =  1  lambda =  6.04964746441295  loss:  1223.50081559154&quot;
## [1] &quot;step =  1  lambda =  5.98945246638312  loss:  1213.52019589289&quot;
## [1] &quot;step =  1  lambda =  5.92985641859114  loss:  1203.60589880504&quot;
## [1] &quot;step =  1  lambda =  5.8708533613826  loss:  1193.75771660417&quot;
## [1] &quot;step =  1  lambda =  5.81243739440259  loss:  1183.97543618837&quot;
## [1] &quot;step =  1  lambda =  5.75460267600573  loss:  1174.25883949053&quot;
## [1] &quot;step =  1  lambda =  5.69734342267199  loss:  1164.60770385931&quot;
## [1] &quot;step =  1  lambda =  5.64065390842832  loss:  1155.02180241039&quot;
## [1] &quot;step =  1  lambda =  5.58452846427606  loss:  1145.50090434994&quot;
## [1] &quot;step =  1  lambda =  5.52896147762401  loss:  1136.04477527207&quot;
## [1] &quot;step =  1  lambda =  5.47394739172721  loss:  1126.65317743233&quot;
## [1] &quot;step =  1  lambda =  5.4194807051312  loss:  1117.32586999861&quot;
## [1] &quot;step =  1  lambda =  5.36555597112197  loss:  1108.06260928144&quot;
## [1] &quot;step =  1  lambda =  5.31216779718117  loss:  1098.86314894487&quot;
## [1] &quot;step =  1  lambda =  5.2593108444469  loss:  1089.72724019969&quot;
## [1] &quot;step =  1  lambda =  5.20697982717985  loss:  1080.61130746015&quot;
## [1] &quot;step =  1  lambda =  5.15516951223468  loss:  1071.34104888789&quot;
## [1] &quot;step =  1  lambda =  5.10387471853673  loss:  1062.10654851146&quot;
## [1] &quot;step =  1  lambda =  5.05309031656387  loss:  1052.79876992817&quot;
## [1] &quot;step =  1  lambda =  5.00281122783359  loss:  1043.56228728814&quot;
## [1] &quot;step =  1  lambda =  4.95303242439511  loss:  1034.39665165323&quot;
## [1] &quot;step =  1  lambda =  4.90374892832662  loss:  1025.29858434346&quot;
## [1] &quot;step =  1  lambda =  4.85495581123743  loss:  1016.27160915593&quot;
## [1] &quot;step =  1  lambda =  4.80664819377518  loss:  1007.31540193693&quot;
## [1] &quot;step =  1  lambda =  4.75882124513786  loss:  998.429628370909&quot;
## [1] &quot;step =  1  lambda =  4.71147018259074  loss:  989.613944846875&quot;
## [1] &quot;step =  1  lambda =  4.66459027098813  loss:  980.867999270604&quot;
## [1] &quot;step =  1  lambda =  4.61817682229978  loss:  972.191431825415&quot;
## [1] &quot;step =  1  lambda =  4.57222519514216  loss:  963.583875684344&quot;
## [1] &quot;step =  1  lambda =  4.52673079431425  loss:  955.044957676306&quot;
## [1] &quot;step =  1  lambda =  4.48168907033806  loss:  946.569870866975&quot;
## [1] &quot;step =  1  lambda =  4.43709551900367  loss:  938.101358342135&quot;
## [1] &quot;step =  1  lambda =  4.39294568091876  loss:  929.70303026167&quot;
## [1] &quot;step =  1  lambda =  4.34923514106274  loss:  921.374441180872&quot;
## [1] &quot;step =  1  lambda =  4.30595952834521  loss:  913.115138919883&quot;
## [1] &quot;step =  1  lambda =  4.26311451516882  loss:  904.924665510058&quot;
## [1] &quot;step =  1  lambda =  4.22069581699655  loss:  896.680268665268&quot;
## [1] &quot;step =  1  lambda =  4.17869919192325  loss:  888.251833720331&quot;
## [1] &quot;step =  1  lambda =  4.13712044025139  loss:  879.901646229022&quot;
## [1] &quot;step =  1  lambda =  4.09595540407118  loss:  871.628989396405&quot;
## [1] &quot;step =  1  lambda =  4.05519996684468  loss:  863.433149801167&quot;
## [1] &quot;step =  1  lambda =  4.0148500529942  loss:  855.313417810734&quot;
## [1] &quot;step =  1  lambda =  3.97490162749475  loss:  847.269087953826&quot;
## [1] &quot;step =  1  lambda =  3.93535069547048  loss:  839.299459253295&quot;
## [1] &quot;step =  1  lambda =  3.89619330179521  loss:  831.403835521987&quot;
## [1] &quot;step =  1  lambda =  3.85742553069697  loss:  823.581525624263&quot;
## [1] &quot;step =  1  lambda =  3.81904350536634  loss:  815.831843705625&quot;
## [1] &quot;step =  1  lambda =  3.78104338756878  loss:  808.154109392854&quot;
## [1] &quot;step =  1  lambda =  3.74342137726086  loss:  800.547647966865&quot;
## [1] &quot;step =  1  lambda =  3.7061737122102  loss:  793.011790510418&quot;
## [1] &quot;step =  1  lambda =  3.66929666761925  loss:  785.545874032672&quot;
## [1] &quot;step =  1  lambda =  3.63278655575281  loss:  778.149241572482&quot;
## [1] &quot;step =  1  lambda =  3.59663972556928  loss:  770.821242282185&quot;
## [1] &quot;step =  1  lambda =  3.56085256235552  loss:  763.561231493573&quot;
## [1] &quot;step =  1  lambda =  3.52542148736538  loss:  756.36857076758&quot;
## [1] &quot;step =  1  lambda =  3.49034295746184  loss:  749.242627929167&quot;
## [1] &quot;step =  1  lambda =  3.45561346476268  loss:  742.18277708875&quot;
## [1] &quot;step =  1  lambda =  3.42122953628968  loss:  735.188398651458&quot;
## [1] &quot;step =  1  lambda =  3.38718773362134  loss:  728.258879315387&quot;
## [1] &quot;step =  1  lambda =  3.35348465254903  loss:  721.393612059963&quot;
## [1] &quot;step =  1  lambda =  3.32011692273655  loss:  714.591996125426&quot;
## [1] &quot;step =  1  lambda =  3.28708120738312  loss:  707.853436984382&quot;
## [1] &quot;step =  1  lambda =  3.25437420288967  loss:  701.177346306308&quot;
## [1] &quot;step =  1  lambda =  3.2219926385285  loss:  694.563141915785&quot;
## [1] &quot;step =  1  lambda =  3.18993327611618  loss:  688.010247745243&quot;
## [1] &quot;step =  1  lambda =  3.15819290968977  loss:  681.518093782875&quot;
## [1] &quot;step =  1  lambda =  3.12676836518616  loss:  675.086116016358&quot;
## [1] &quot;step =  1  lambda =  3.09565650012471  loss:  668.713756372949&quot;
## [1] &quot;step =  1  lambda =  3.06485420329301  loss:  662.400462656499&quot;
## [1] &quot;step =  1  lambda =  3.03435839443567  loss:  656.145688481849&quot;
## [1] &quot;step =  1  lambda =  3.00416602394643  loss:  649.948893207057&quot;
## [1] &quot;step =  1  lambda =  2.97427407256306  loss:  643.809541863851&quot;
## [1] &quot;step =  1  lambda =  2.94467955106552  loss:  637.727105086675&quot;
## [1] &quot;step =  1  lambda =  2.915379499977  loss:  631.701059040647&quot;
## [1] &quot;step =  1  lambda =  2.88637098926796  loss:  625.730885348739&quot;
## [1] &quot;step =  1  lambda =  2.85765111806317  loss:  619.816071018427&quot;
## [1] &quot;step =  1  lambda =  2.82921701435156  loss:  613.956108368074&quot;
## [1] &quot;step =  1  lambda =  2.80106583469908  loss:  608.150494953235&quot;
## [1] &quot;step =  1  lambda =  2.7731947639643  loss:  602.398733493102&quot;
## [1] &quot;step =  1  lambda =  2.74560101501692  loss:  596.700331797233&quot;
## [1] &quot;step =  1  lambda =  2.71828182845905  loss:  591.054802692737&quot;
## [1] &quot;step =  1  lambda =  2.69123447234926  loss:  585.461663952035&quot;
## [1] &quot;step =  1  lambda =  2.66445624192942  loss:  579.920438221322&quot;
## [1] &quot;step =  1  lambda =  2.63794445935415  loss:  574.430652949819&quot;
## [1] &quot;step =  1  lambda =  2.61169647342312  loss:  568.991840319921&quot;
## [1] &quot;step =  1  lambda =  2.58570965931585  loss:  563.603537178298&quot;
## [1] &quot;step =  1  lambda =  2.55998141832927  loss:  558.265284968018&quot;
## [1] &quot;step =  1  lambda =  2.53450917761785  loss:  552.976629661762&quot;
## [1] &quot;step =  1  lambda =  2.5092903899363  loss:  547.737121696138&quot;
## [1] &quot;step =  1  lambda =  2.48432253338482  loss:  542.546315907177&quot;
## [1] &quot;step =  1  lambda =  2.45960311115695  loss:  537.403771466991&quot;
## [1] &quot;step =  1  lambda =  2.43512965128988  loss:  532.309051821658&quot;
## [1] &quot;step =  1  lambda =  2.41089970641721  loss:  527.26172463031&quot;
## [1] &quot;step =  1  lambda =  2.38691085352428  loss:  522.261361705464&quot;
## [1] &quot;step =  1  lambda =  2.36316069370579  loss:  517.307538954581&quot;
## [1] &quot;step =  1  lambda =  2.33964685192599  loss:  512.39983632286&quot;
## [1] &quot;step =  1  lambda =  2.31636697678109  loss:  507.537837737264&quot;
## [1] &quot;step =  1  lambda =  2.29331874026418  loss:  502.721131051769&quot;
## [1] &quot;step =  1  lambda =  2.27049983753241  loss:  497.949307993823&quot;
## [1] &quot;step =  1  lambda =  2.24790798667647  loss:  493.209052565996&quot;
## [1] &quot;step =  1  lambda =  2.22554092849247  loss:  488.495051586758&quot;
## [1] &quot;step =  1  lambda =  2.20339642625594  loss:  483.825752967147&quot;
## [1] &quot;step =  1  lambda =  2.1814722654982  loss:  479.200735555409&quot;
## [1] &quot;step =  1  lambda =  2.15976625378491  loss:  474.61958252363&quot;
## [1] &quot;step =  1  lambda =  2.13827622049682  loss:  470.081881307015&quot;
## [1] &quot;step =  1  lambda =  2.11700001661267  loss:  465.587223544231&quot;
## [1] &quot;step =  1  lambda =  2.09593551449437  loss:  461.135205018773&quot;
## [1] &quot;step =  1  lambda =  2.07508060767412  loss:  456.725425601391&quot;
## [1] &quot;step =  1  lambda =  2.05443321064389  loss:  452.357489193578&quot;
## [1] &quot;step =  1  lambda =  2.03399125864675  loss:  448.031003672117&quot;
## [1] &quot;step =  1  lambda =  2.01375270747048  loss:  443.745580834687&quot;
## [1] &quot;step =  1  lambda =  1.99371553324308  loss:  439.500836346522&quot;
## [1] &quot;step =  1  lambda =  1.97387773223045  loss:  435.296389688125&quot;
## [1] &quot;step =  1  lambda =  1.95423732063594  loss:  431.131864104013&quot;
## [1] &quot;step =  1  lambda =  1.93479233440203  loss:  427.006886552492&quot;
## [1] &quot;step =  1  lambda =  1.9155408290139  loss:  422.921087656455&quot;
## [1] &quot;step =  1  lambda =  1.89648087930495  loss:  418.874101655171&quot;
## [1] &quot;step =  1  lambda =  1.87761057926434  loss:  414.865566357074&quot;
## [1] &quot;step =  1  lambda =  1.85892804184634  loss:  410.895123093507&quot;
## [1] &quot;step =  1  lambda =  1.84043139878164  loss:  406.962416673437&quot;
## [1] &quot;step =  1  lambda =  1.82211880039051  loss:  403.067095339092&quot;
## [1] &quot;step =  1  lambda =  1.80398841539786  loss:  399.208810722523&quot;
## [1] &quot;step =  1  lambda =  1.78603843075007  loss:  395.387217803059&quot;
## [1] &quot;step =  1  lambda =  1.76826705143374  loss:  391.60197486565&quot;
## [1] &quot;step =  1  lambda =  1.7506725002961  loss:  387.852743460062&quot;
## [1] &quot;step =  1  lambda =  1.7332530178674  loss:  384.139188360914&quot;
## [1] &quot;step =  1  lambda =  1.71600686218486  loss:  380.460977528537&quot;
## [1] &quot;step =  1  lambda =  1.69893230861855  loss:  376.817782070629&quot;
## [1] &quot;step =  1  lambda =  1.68202764969889  loss:  373.209276204701&quot;
## [1] &quot;step =  1  lambda =  1.66529119494589  loss:  369.63513722127&quot;
## [1] &quot;step =  1  lambda =  1.64872127070013  loss:  366.095045447801&quot;
## [1] &quot;step =  1  lambda =  1.63231621995538  loss:  362.588684213372&quot;
## [1] &quot;step =  1  lambda =  1.61607440219289  loss:  359.115739814038&quot;
## [1] &quot;step =  1  lambda =  1.59999419321736  loss:  355.675901478879&quot;
## [1] &quot;step =  1  lambda =  1.58407398499448  loss:  352.268861336718&quot;
## [1] &quot;step =  1  lambda =  1.56831218549017  loss:  348.894314383477&quot;
## [1] &quot;step =  1  lambda =  1.55270721851134  loss:  345.551958450171&quot;
## [1] &quot;step =  1  lambda =  1.53725752354828  loss:  342.24149417151&quot;
## [1] &quot;step =  1  lambda =  1.52196155561863  loss:  338.962624955091&quot;
## [1] &quot;step =  1  lambda =  1.50681778511285  loss:  335.715056951171&quot;
## [1] &quot;step =  1  lambda =  1.49182469764127  loss:  332.498499023001&quot;
## [1] &quot;step =  1  lambda =  1.47698079388264  loss:  329.312648273541&quot;
## [1] &quot;step =  1  lambda =  1.46228458943423  loss:  326.155734292623&quot;
## [1] &quot;step =  1  lambda =  1.44773461466333  loss:  323.029019083911&quot;
## [1] &quot;step =  1  lambda =  1.43332941456034  loss:  319.932221063356&quot;
## [1] &quot;step =  1  lambda =  1.41906754859326  loss:  316.865061237352&quot;
## [1] &quot;step =  1  lambda =  1.40494759056359  loss:  313.827263175929&quot;
## [1] &quot;step =  1  lambda =  1.39096812846378  loss:  310.818552986399&quot;
## [1] &quot;step =  1  lambda =  1.37712776433596  loss:  307.838659287443&quot;
## [1] &quot;step =  1  lambda =  1.36342511413218  loss:  304.887313183615&quot;
## [1] &quot;step =  1  lambda =  1.349858807576  loss:  301.964248240271&quot;
## [1] &quot;step =  1  lambda =  1.33642748802547  loss:  299.069200458886&quot;
## [1] &quot;step =  1  lambda =  1.32312981233744  loss:  296.201908252775&quot;
## [1] &quot;step =  1  lambda =  1.30996445073325  loss:  293.362112423186&quot;
## [1] &quot;step =  1  lambda =  1.29693008666577  loss:  290.549556135763&quot;
## [1] &quot;step =  1  lambda =  1.28402541668774  loss:  287.763984897372&quot;
## [1] &quot;step =  1  lambda =  1.2712491503214  loss:  285.005146533269&quot;
## [1] &quot;step =  1  lambda =  1.25860000992948  loss:  282.272791164614&quot;
## [1] &quot;step =  1  lambda =  1.24607673058738  loss:  279.566671186309&quot;
## [1] &quot;step =  1  lambda =  1.23367805995674  loss:  276.886541245164&quot;
## [1] &quot;step =  1  lambda =  1.22140275816017  loss:  274.232158218367&quot;
## [1] &quot;step =  1  lambda =  1.20924959765725  loss:  271.603281192264&quot;
## [1] &quot;step =  1  lambda =  1.19721736312181  loss:  268.999671441433&quot;
## [1] &quot;step =  1  lambda =  1.18530485132037  loss:  266.421092408042&quot;
## [1] &quot;step =  1  lambda =  1.17351087099181  loss:  263.867309681494&quot;
## [1] &quot;step =  1  lambda =  1.16183424272828  loss:  261.33809097834&quot;
## [1] &quot;step =  1  lambda =  1.15027379885723  loss:  258.833206122456&quot;
## [1] &quot;step =  1  lambda =  1.13882838332462  loss:  256.352427025486&quot;
## [1] &quot;step =  1  lambda =  1.12749685157938  loss:  253.895527667529&quot;
## [1] &quot;step =  1  lambda =  1.11627807045887  loss:  251.462284078077&quot;
## [1] &quot;step =  1  lambda =  1.10517091807565  loss:  249.052474317191&quot;
## [1] &quot;step =  1  lambda =  1.09417428370521  loss:  246.665878456912&quot;
## [1] &quot;step =  1  lambda =  1.08328706767496  loss:  244.302278562895&quot;
## [1] &quot;step =  1  lambda =  1.07250818125422  loss:  241.961458676273&quot;
## [1] &quot;step =  1  lambda =  1.06183654654536  loss:  239.643204795732&quot;
## [1] &quot;step =  1  lambda =  1.05127109637602  loss:  237.3473048598&quot;
## [1] &quot;step =  1  lambda =  1.04081077419239  loss:  235.073548729342&quot;
## [1] &quot;step =  1  lambda =  1.03045453395352  loss:  232.821728170262&quot;
## [1] &quot;step =  1  lambda =  1.02020134002676  loss:  230.591636836391&quot;
## [1] &quot;step =  1  lambda =  1.01005016708417  loss:  228.383070252582&quot;
## [1] &quot;step =  1  lambda =  1  loss:  226.195825797983&quot;
## [1] &quot;step =  1  lambda =  0.990049833749168  loss:  224.0297026895&quot;
## [1] &quot;step =  1  lambda =  0.980198673306756  loss:  221.884501965435&quot;
## [1] &quot;step =  1  lambda =  0.970445533548509  loss:  219.760026469312&quot;
## [1] &quot;step =  1  lambda =  0.960789439152324  loss:  217.65608083386&quot;
## [1] &quot;step =  1  lambda =  0.951229424500715  loss:  215.572471465177&quot;
## [1] &quot;step =  1  lambda =  0.941764533584248  loss:  213.509006527057&quot;
## [1] &quot;step =  1  lambda =  0.932393819905948  loss:  211.465495925479&quot;
## [1] &quot;step =  1  lambda =  0.923116346386636  loss:  209.44175129325&quot;
## [1] &quot;step =  1  lambda =  0.913931185271228  loss:  207.437585974815&quot;
## [1] &quot;step =  1  lambda =  0.90483741803596  loss:  205.452815011209&quot;
## [1] &quot;step =  1  lambda =  0.895834135296529  loss:  203.487255125167&quot;
## [1] &quot;step =  1  lambda =  0.886920436717158  loss:  201.540724706377&quot;
## [1] &quot;step =  1  lambda =  0.878095430920562  loss:  199.613043796878&quot;
## [1] &quot;step =  1  lambda =  0.869358235398805  loss:  197.70917193418&quot;
## [1] &quot;step =  1  lambda =  0.860707976425057  loss:  195.826029496742&quot;
## [1] &quot;step =  1  lambda =  0.852143788966211  loss:  193.96098616288&quot;
## [1] &quot;step =  1  lambda =  0.843664816596384  loss:  192.113873519369&quot;
## [1] &quot;step =  1  lambda =  0.835270211411272  loss:  190.284524625601&quot;
## [1] &quot;step =  1  lambda =  0.826959133943363  loss:  188.472774002144&quot;
## [1] &quot;step =  1  lambda =  0.818730753077982  loss:  186.678457619374&quot;
## [1] &quot;step =  1  lambda =  0.810584245970188  loss:  184.901412886176&quot;
## [1] &quot;step =  1  lambda =  0.802518797962478  loss:  183.141478638725&quot;
## [1] &quot;step =  1  lambda =  0.794533602503334  loss:  181.398495129338&quot;
## [1] &quot;step =  1  lambda =  0.786627861066553  loss:  179.672304015385&quot;
## [1] &quot;step =  1  lambda =  0.778800783071405  loss:  177.962748348288&quot;
## [1] &quot;step =  1  lambda =  0.771051585803566  loss:  176.269672562578&quot;
## [1] &quot;step =  1  lambda =  0.763379494336853  loss:  174.59292246503&quot;
## [1] &quot;step =  1  lambda =  0.755783741455726  loss:  172.932345223859&quot;
## [1] &quot;step =  1  lambda =  0.748263567578566  loss:  171.287789357991&quot;
## [1] &quot;step =  2  lambda =  0.748263567578566  loss:  195.562791040804&quot;
## [1] &quot;step =  1  lambda =  0.740818220681719  loss:  193.677977414786&quot;
## [1] &quot;step =  2  lambda =  0.740818220681719  loss:  213.05913439132&quot;
## [1] &quot;step =  1  lambda =  0.733446956224289  loss:  211.004147239752&quot;
## [1] &quot;step =  2  lambda =  0.733446956224289  loss:  225.392644081118&quot;
## [1] &quot;step =  1  lambda =  0.726149037073691  loss:  223.2102836498&quot;
## [1] &quot;step =  2  lambda =  0.726149037073691  loss:  233.767851222646&quot;
## [1] &quot;step =  1  lambda =  0.718923733431926  loss:  231.502695687945&quot;
## [1] &quot;step =  2  lambda =  0.718923733431926  loss:  239.443748421586&quot;
## [1] &quot;step =  1  lambda =  0.71177032276261  loss:  237.133636050893&quot;
## [1] &quot;step =  2  lambda =  0.71177032276261  loss:  244.359408662073&quot;
## [1] &quot;step =  1  lambda =  0.704688089718714  loss:  241.990782669766&quot;
## [1] &quot;step =  2  lambda =  0.704688089718714  loss:  248.981887826642&quot;
## [1] &quot;step =  1  lambda =  0.697676326071031  loss:  246.549486283475&quot;
## [1] &quot;step =  2  lambda =  0.697676326071031  loss:  252.83903107965&quot;
## [1] &quot;step =  1  lambda =  0.690734330637355  loss:  250.368067811749&quot;
## [1] &quot;step =  2  lambda =  0.690734330637355  loss:  256.134484316884&quot;
## [1] &quot;step =  1  lambda =  0.683861409212356  loss:  253.63009461281&quot;
## [1] &quot;step =  2  lambda =  0.683861409212356  loss:  258.884362402607&quot;
## [1] &quot;step =  1  lambda =  0.677056874498164  loss:  256.362494125938&quot;
## [1] &quot;step =  2  lambda =  0.677056874498164  loss:  260.350787798&quot;
## [1] &quot;step =  1  lambda =  0.670320046035639  loss:  257.809911111679&quot;
## [1] &quot;step =  2  lambda =  0.670320046035639  loss:  261.930699361738&quot;
## [1] &quot;step =  1  lambda =  0.663650250136319  loss:  259.376474113051&quot;
## [1] &quot;step =  2  lambda =  0.663650250136319  loss:  263.241841212219&quot;
## [1] &quot;step =  1  lambda =  0.657046819815057  loss:  260.680613474972&quot;
## [1] &quot;step =  2  lambda =  0.657046819815057  loss:  264.393473946652&quot;
## [1] &quot;step =  1  lambda =  0.650509094723317  loss:  261.820338626144&quot;
## [1] &quot;step =  2  lambda =  0.650509094723317  loss:  265.417716521848&quot;
## [1] &quot;step =  1  lambda =  0.644036421083142  loss:  262.833953457498&quot;
## [1] &quot;step =  2  lambda =  0.644036421083142  loss:  266.205114263311&quot;
## [1] &quot;step =  1  lambda =  0.637628151621774  loss:  263.612057830918&quot;
## [1] &quot;step =  2  lambda =  0.637628151621774  loss:  266.994926657976&quot;
## [1] &quot;step =  1  lambda =  0.631283645506927  loss:  264.38799944319&quot;
## [1] &quot;step =  2  lambda =  0.631283645506927  loss:  267.687173400735&quot;
## [1] &quot;step =  1  lambda =  0.6250022682827  loss:  265.073044606833&quot;
## [1] &quot;step =  2  lambda =  0.6250022682827  loss:  268.292290709779&quot;
## [1] &quot;step =  1  lambda =  0.618783391806141  loss:  265.671831537774&quot;
## [1] &quot;step =  2  lambda =  0.618783391806141  loss:  268.788220545867&quot;
## [1] &quot;step =  1  lambda =  0.612626394184416  loss:  266.16252190802&quot;
## [1] &quot;step =  2  lambda =  0.612626394184416  loss:  269.1718698093&quot;
## [1] &quot;step =  1  lambda =  0.606530659712633  loss:  266.543596582594&quot;
## [1] &quot;step =  2  lambda =  0.606530659712633  loss:  269.460737530808&quot;
## [1] &quot;step =  1  lambda =  0.600495578812266  loss:  266.830194030639&quot;
## [1] &quot;step =  2  lambda =  0.600495578812266  loss:  269.651220444982&quot;
## [1] &quot;step =  1  lambda =  0.594520547970195  loss:  267.018391337272&quot;
## [1] &quot;step =  2  lambda =  0.594520547970195  loss:  269.741473657748&quot;
## [1] &quot;step =  1  lambda =  0.588604969678356  loss:  267.106628565891&quot;
## [1] &quot;step =  2  lambda =  0.588604969678356  loss:  269.759234090823&quot;
## [1] &quot;step =  1  lambda =  0.58274825237399  loss:  267.123827607932&quot;
## [1] &quot;step =  2  lambda =  0.58274825237399  loss:  269.648499497414&quot;
## [1] &quot;step =  1  lambda =  0.576949810380487  loss:  267.01381988582&quot;
## [1] &quot;step =  2  lambda =  0.576949810380487  loss:  269.433977884537&quot;
## [1] &quot;step =  1  lambda =  0.571209063848815  loss:  266.801058629306&quot;
## [1] &quot;step =  2  lambda =  0.571209063848815  loss:  269.123114670298&quot;
## [1] &quot;step =  1  lambda =  0.565525438699537  loss:  266.492913582721&quot;
## [1] &quot;step =  2  lambda =  0.565525438699537  loss:  268.720714303438&quot;
## [1] &quot;step =  1  lambda =  0.559898366565402  loss:  266.094141532239&quot;
## [1] &quot;step =  2  lambda =  0.559898366565402  loss:  268.230281823992&quot;
## [1] &quot;step =  1  lambda =  0.554327284734507  loss:  265.60789785115&quot;
## [1] &quot;step =  2  lambda =  0.554327284734507  loss:  267.655801277485&quot;
## [1] &quot;step =  1  lambda =  0.548811636094027  loss:  265.038762391583&quot;
## [1] &quot;step =  2  lambda =  0.548811636094027  loss:  267.002448809185&quot;
## [1] &quot;step =  1  lambda =  0.5433508690745  loss:  264.391541365033&quot;
## [1] &quot;step =  2  lambda =  0.5433508690745  loss:  266.274645862738&quot;
## [1] &quot;step =  1  lambda =  0.537944437594675  loss:  263.670612502087&quot;
## [1] &quot;step =  2  lambda =  0.537944437594675  loss:  265.476766174072&quot;
## [1] &quot;step =  1  lambda =  0.532591801006898  loss:  262.880306362946&quot;
## [1] &quot;step =  2  lambda =  0.532591801006898  loss:  264.613118578583&quot;
## [1] &quot;step =  1  lambda =  0.527292424043048  loss:  262.024889225551&quot;
## [1] &quot;step =  2  lambda =  0.527292424043048  loss:  263.687902080307&quot;
## [1] &quot;step =  1  lambda =  0.522045776761016  loss:  261.108518578166&quot;
## [1] &quot;step =  2  lambda =  0.522045776761016  loss:  262.705165577844&quot;
## [1] &quot;step =  1  lambda =  0.516851334491699  loss:  260.135203245044&quot;
## [1] &quot;step =  2  lambda =  0.516851334491699  loss:  261.668779544633&quot;
## [1] &quot;step =  1  lambda =  0.511708577786543  loss:  259.108775355143&quot;
## [1] &quot;step =  2  lambda =  0.511708577786543  loss:  260.582420121817&quot;
## [1] &quot;step =  1  lambda =  0.50661699236559  loss:  258.03287460128&quot;
## [1] &quot;step =  2  lambda =  0.50661699236559  loss:  259.449563241216&quot;
## [1] &quot;step =  1  lambda =  0.501576069066056  loss:  256.910942428744&quot;
## [1] &quot;step =  2  lambda =  0.501576069066056  loss:  258.273485698608&quot;
## [1] &quot;step =  1  lambda =  0.49658530379141  loss:  255.746223103698&quot;
## [1] &quot;step =  2  lambda =  0.49658530379141  loss:  257.057270508176&quot;
## [1] &quot;step =  1  lambda =  0.491644197460966  loss:  254.541769018906&quot;
## [1] &quot;step =  2  lambda =  0.491644197460966  loss:  255.803814624572&quot;
## [1] &quot;step =  1  lambda =  0.486752255959971  loss:  253.300448342562&quot;
## [1] &quot;step =  2  lambda =  0.486752255959971  loss:  254.515837816635&quot;
## [1] &quot;step =  1  lambda =  0.481908990090202  loss:  252.024953806559&quot;
## [1] &quot;step =  2  lambda =  0.481908990090202  loss:  253.195891983341&quot;
## [1] &quot;step =  1  lambda =  0.477113915521034  loss:  250.71781193204&quot;
## [1] &quot;step =  2  lambda =  0.477113915521034  loss:  251.846370523089&quot;
## [1] &quot;step =  1  lambda =  0.472366552741015  loss:  249.381392307329&quot;
## [1] &quot;step =  2  lambda =  0.472366552741015  loss:  250.469517551935&quot;
## [1] &quot;step =  1  lambda =  0.467666427009909  loss:  248.017916715971&quot;
## [1] &quot;step =  2  lambda =  0.467666427009909  loss:  249.067436865601&quot;
## [1] &quot;step =  1  lambda =  0.463013068311228  loss:  246.629468010852&quot;
## [1] &quot;step =  2  lambda =  0.463013068311228  loss:  247.642100591149&quot;
## [1] &quot;step =  1  lambda =  0.458406011305224  loss:  245.217998680867&quot;
## [1] &quot;step =  2  lambda =  0.458406011305224  loss:  246.195357499895&quot;
## [1] &quot;step =  1  lambda =  0.453844795282356  loss:  243.785339082047&quot;
## [1] &quot;step =  2  lambda =  0.453844795282356  loss:  244.728940966222&quot;
## [1] &quot;step =  1  lambda =  0.449328964117222  loss:  242.333205317987&quot;
## [1] &quot;step =  2  lambda =  0.449328964117222  loss:  243.244476564036&quot;
## [1] &quot;step =  1  lambda =  0.444858066222941  loss:  240.86320676144&quot;
## [1] &quot;step =  2  lambda =  0.444858066222941  loss:  241.743489296773&quot;
## [1] &quot;step =  1  lambda =  0.440431654505999  loss:  239.376853213011&quot;
## [1] &quot;step =  2  lambda =  0.440431654505999  loss:  240.227410459644&quot;
## [1] &quot;step =  1  lambda =  0.436049286321536  loss:  237.875479185104&quot;
## [1] &quot;step =  2  lambda =  0.436049286321536  loss:  238.691105457621&quot;
## [1] &quot;step =  1  lambda =  0.43171052342908  loss:  236.352633474049&quot;
## [1] &quot;step =  2  lambda =  0.43171052342908  loss:  237.130141070508&quot;
## [1] &quot;step =  1  lambda =  0.427414931948727  loss:  234.80611778854&quot;
## [1] &quot;step =  2  lambda =  0.427414931948727  loss:  235.549329264219&quot;
## [1] &quot;step =  1  lambda =  0.423162082317749  loss:  233.240806901604&quot;
## [1] &quot;step =  2  lambda =  0.423162082317749  loss:  233.952791946863&quot;
## [1] &quot;step =  1  lambda =  0.418951549247639  loss:  231.65992770717&quot;
## [1] &quot;step =  2  lambda =  0.418951549247639  loss:  232.343150537675&quot;
## [1] &quot;step =  1  lambda =  0.414782911681582  loss:  230.066075321622&quot;
## [1] &quot;step =  2  lambda =  0.414782911681582  loss:  230.722645422573&quot;
## [1] &quot;step =  1  lambda =  0.410655752752345  loss:  228.461467803421&quot;
## [1] &quot;step =  2  lambda =  0.410655752752345  loss:  229.093230217411&quot;
## [1] &quot;step =  1  lambda =  0.406569659740599  loss:  226.848039399295&quot;
## [1] &quot;step =  2  lambda =  0.406569659740599  loss:  227.456622943623&quot;
## [1] &quot;step =  1  lambda =  0.402524224033636  loss:  225.22749115689&quot;
## [1] &quot;step =  2  lambda =  0.402524224033636  loss:  225.814341632569&quot;
## [1] &quot;step =  1  lambda =  0.398519041084514  loss:  223.60132614325&quot;
## [1] &quot;step =  2  lambda =  0.398519041084514  loss:  224.167732342401&quot;
## [1] &quot;step =  1  lambda =  0.394553710371601  loss:  221.970877165834&quot;
## [1] &quot;step =  2  lambda =  0.394553710371601  loss:  222.517992489799&quot;
## [1] &quot;step =  1  lambda =  0.390627835358521  loss:  220.33732986365&quot;
## [1] &quot;step =  2  lambda =  0.390627835358521  loss:  220.866190761326&quot;
## [1] &quot;step =  1  lambda =  0.386741023454502  loss:  218.701742416811&quot;
## [1] &quot;step =  2  lambda =  0.386741023454502  loss:  219.213284259422&quot;
## [1] &quot;step =  1  lambda =  0.382892885975112  loss:  217.065062520793&quot;
## [1] &quot;step =  2  lambda =  0.382892885975112  loss:  217.560133292287&quot;
## [1] &quot;step =  1  lambda =  0.379083038103399  loss:  215.428142029291&quot;
## [1] &quot;step =  2  lambda =  0.379083038103399  loss:  215.907514112641&quot;
## [1] &quot;step =  1  lambda =  0.375311098851399  loss:  213.791749566923&quot;
## [1] &quot;step =  2  lambda =  0.375311098851399  loss:  214.271495630188&quot;
## [1] &quot;step =  1  lambda =  0.371576691022046  loss:  212.171008569566&quot;
## [1] &quot;step =  2  lambda =  0.371576691022046  loss:  212.707808392524&quot;
## [1] &quot;step =  1  lambda =  0.367879441171442  loss:  210.622638136773&quot;
## [1] &quot;step =  2  lambda =  0.367879441171442  loss:  211.195291821221&quot;
## [1] &quot;step =  1  lambda =  0.364218979571523  loss:  209.124950775122&quot;
## [1] &quot;step =  2  lambda =  0.364218979571523  loss:  209.655921766069&quot;
## [1] &quot;step =  1  lambda =  0.360594940173078  loss:  207.600677484236&quot;
## [1] &quot;step =  2  lambda =  0.360594940173078  loss:  208.101705590503&quot;
## [1] &quot;step =  1  lambda =  0.357006960569148  loss:  206.061706250128&quot;
## [1] &quot;step =  2  lambda =  0.357006960569148  loss:  206.537052029786&quot;
## [1] &quot;step =  1  lambda =  0.35345468195878  loss:  204.512402810798&quot;
## [1] &quot;step =  2  lambda =  0.35345468195878  loss:  204.963044817853&quot;
## [1] &quot;step =  1  lambda =  0.349937749111156  loss:  202.953964959355&quot;
## [1] &quot;step =  2  lambda =  0.349937749111156  loss:  203.374720920477&quot;
## [1] &quot;step =  1  lambda =  0.346455810330057  loss:  201.381237408613&quot;
## [1] &quot;step =  2  lambda =  0.346455810330057  loss:  201.769816623484&quot;
## [1] &quot;step =  1  lambda =  0.343008517418707  loss:  199.79209867575&quot;
## [1] &quot;step =  2  lambda =  0.343008517418707  loss:  200.157124885069&quot;
## [1] &quot;step =  1  lambda =  0.339595525644939  loss:  198.195254331844&quot;
## [1] &quot;step =  2  lambda =  0.339595525644939  loss:  198.541145075824&quot;
## [1] &quot;step =  1  lambda =  0.336216493706733  loss:  196.595158903288&quot;
## [1] &quot;step =  2  lambda =  0.336216493706733  loss:  196.924849611938&quot;
## [1] &quot;step =  1  lambda =  0.33287108369808  loss:  194.994755021004&quot;
## [1] &quot;step =  2  lambda =  0.33287108369808  loss:  195.310371761066&quot;
## [1] &quot;step =  1  lambda =  0.329558961075189  loss:  193.396154433314&quot;
## [1] &quot;step =  2  lambda =  0.329558961075189  loss:  193.660639774797&quot;
## [1] &quot;step =  1  lambda =  0.32627979462304  loss:  191.764028897624&quot;
## [1] &quot;step =  2  lambda =  0.32627979462304  loss:  191.954494665254&quot;
## [1] &quot;step =  1  lambda =  0.323033256422253  loss:  190.074683547237&quot;
## [1] &quot;step =  2  lambda =  0.323033256422253  loss:  190.241657721059&quot;
## [1] &quot;step =  1  lambda =  0.319819021816304  loss:  188.378694772563&quot;
## [1] &quot;step =  2  lambda =  0.319819021816304  loss:  188.549879181148&quot;
## [1] &quot;step =  1  lambda =  0.316636769379053  loss:  186.703548540808&quot;
## [1] &quot;step =  2  lambda =  0.316636769379053  loss:  186.879745632596&quot;
## [1] &quot;step =  1  lambda =  0.313486180882605  loss:  185.049829796481&quot;
## [1] &quot;step =  2  lambda =  0.313486180882605  loss:  185.229274731717&quot;
## [1] &quot;step =  1  lambda =  0.310366941265485  loss:  183.415578936677&quot;
## [1] &quot;step =  2  lambda =  0.310366941265485  loss:  183.596666681253&quot;
## [1] &quot;step =  1  lambda =  0.307278738601131  loss:  181.79901575621&quot;
## [1] &quot;step =  2  lambda =  0.307278738601131  loss:  181.980574956856&quot;
## [1] &quot;step =  1  lambda =  0.304221264066704  loss:  180.198807980617&quot;
## [1] &quot;step =  2  lambda =  0.304221264066704  loss:  180.3800177342&quot;
## [1] &quot;step =  1  lambda =  0.301194211912202  loss:  178.613983918324&quot;
## [1] &quot;step =  2  lambda =  0.301194211912202  loss:  178.794265897786&quot;
## [1] &quot;step =  1  lambda =  0.298197279429888  loss:  177.043821799217&quot;
## [1] &quot;step =  2  lambda =  0.298197279429888  loss:  177.22276048601&quot;
## [1] &quot;step =  1  lambda =  0.295230166924014  loss:  175.487768171953&quot;
## [1] &quot;step =  2  lambda =  0.295230166924014  loss:  175.665058740945&quot;
## [1] &quot;step =  1  lambda =  0.292292577680859  loss:  173.945384567061&quot;
## [1] &quot;step =  2  lambda =  0.292292577680859  loss:  174.120800152542&quot;
## [1] &quot;step =  1  lambda =  0.289384217939051  loss:  172.41631392164&quot;
## [1] &quot;step =  2  lambda =  0.289384217939051  loss:  172.589685117034&quot;
## [1] &quot;step =  1  lambda =  0.28650479686019  loss:  170.900259472394&quot;
## [1] &quot;step =  2  lambda =  0.28650479686019  loss:  171.071461187312&quot;
## [1] &quot;step =  1  lambda =  0.28365402649977  loss:  169.396971153272&quot;
## [1] &quot;step =  2  lambda =  0.28365402649977  loss:  169.565913791478&quot;
## [1] &quot;step =  1  lambda =  0.28083162177838  loss:  167.906236410291&quot;
## [1] &quot;step =  2  lambda =  0.28083162177838  loss:  168.072859573563&quot;
## [1] &quot;step =  1  lambda =  0.278037300453194  loss:  166.427873608998&quot;
## [1] &quot;step =  2  lambda =  0.278037300453194  loss:  166.592141303656&quot;
## [1] &quot;step =  1  lambda =  0.275270783089753  loss:  164.961726994103&quot;
## [1] &quot;step =  2  lambda =  0.275270783089753  loss:  165.123623772722&quot;
## [1] &quot;step =  1  lambda =  0.272531793034013  loss:  163.507662623363&quot;
## [1] &quot;step =  2  lambda =  0.272531793034013  loss:  163.667190352607&quot;
## [1] &quot;step =  1  lambda =  0.269820056384687  loss:  162.065564959916&quot;
## [1] &quot;step =  2  lambda =  0.269820056384687  loss:  162.222740046613&quot;
## [1] &quot;step =  1  lambda =  0.26713530196585  loss:  160.635333950435&quot;
## [1] &quot;step =  2  lambda =  0.26713530196585  loss:  160.790184932193&quot;
## [1] &quot;step =  1  lambda =  0.264477261299824  loss:  159.216882491713&quot;
## [1] &quot;step =  2  lambda =  0.264477261299824  loss:  159.369447935836&quot;
## [1] &quot;step =  1  lambda =  0.261845668580326  loss:  157.810134226406&quot;
## [1] &quot;step =  2  lambda =  0.261845668580326  loss:  157.960460898909&quot;
## [1] &quot;step =  1  lambda =  0.259240260645892  loss:  156.415021627048&quot;
## [1] &quot;step =  2  lambda =  0.259240260645892  loss:  156.563162902011&quot;
## [1] &quot;step =  1  lambda =  0.256660776953556  loss:  155.031484336231&quot;
## [1] &quot;step =  2  lambda =  0.256660776953556  loss:  155.177498819692&quot;
## [1] &quot;step =  1  lambda =  0.2541069595528  loss:  153.659467735023&quot;
## [1] &quot;step =  2  lambda =  0.2541069595528  loss:  153.803418079733&quot;
## [1] &quot;step =  1  lambda =  0.251578553059757  loss:  152.29892171406&quot;
## [1] &quot;step =  2  lambda =  0.251578553059757  loss:  152.440873603001&quot;
## [1] &quot;step =  1  lambda =  0.249075304631668  loss:  150.949799623523&quot;
## [1] &quot;step =  2  lambda =  0.249075304631668  loss:  151.08982090161&quot;
## [1] &quot;step =  1  lambda =  0.246596963941606  loss:  149.612057379955&quot;
## [1] &quot;step =  2  lambda =  0.246596963941606  loss:  149.750217314944&quot;
## [1] &quot;step =  1  lambda =  0.244143283153437  loss:  148.285652709629&quot;
## [1] &quot;step =  2  lambda =  0.244143283153437  loss:  148.422021365008&quot;
## [1] &quot;step =  1  lambda =  0.241714016897036  loss:  146.97054451012&quot;
## [1] &quot;step =  2  lambda =  0.241714016897036  loss:  147.105192214522&quot;
## [1] &quot;step =  1  lambda =  0.239308922243755  loss:  145.666692313653&quot;
## [1] &quot;step =  2  lambda =  0.239308922243755  loss:  145.799689213115&quot;
## [1] &quot;step =  1  lambda =  0.236927758682122  loss:  144.374055837708&quot;
## [1] &quot;step =  2  lambda =  0.236927758682122  loss:  144.507665846849&quot;
## [1] &quot;step =  1  lambda =  0.234570288093798  loss:  143.094521637753&quot;
## [1] &quot;step =  2  lambda =  0.234570288093798  loss:  143.294344989293&quot;
## [1] &quot;step =  1  lambda =  0.232236274729759  loss:  141.892694641855&quot;
## [1] &quot;step =  2  lambda =  0.232236274729759  loss:  142.073546269055&quot;
## [1] &quot;step =  1  lambda =  0.229925485186724  loss:  140.683943860266&quot;
## [1] &quot;step =  2  lambda =  0.229925485186724  loss:  140.856940701512&quot;
## [1] &quot;step =  1  lambda =  0.227637688383813  loss:  139.479351084116&quot;
## [1] &quot;step =  2  lambda =  0.227637688383813  loss:  139.647859545138&quot;
## [1] &quot;step =  1  lambda =  0.225372655539439  loss:  138.282211576041&quot;
## [1] &quot;step =  2  lambda =  0.225372655539439  loss:  138.446137453563&quot;
## [1] &quot;step =  1  lambda =  0.22313016014843  loss:  137.092359898376&quot;
## [1] &quot;step =  2  lambda =  0.22313016014843  loss:  137.251617647215&quot;
## [1] &quot;step =  1  lambda =  0.220909977959378  loss:  135.909640044327&quot;
## [1] &quot;step =  2  lambda =  0.220909977959378  loss:  136.064348654931&quot;
## [1] &quot;step =  1  lambda =  0.218711886952215  loss:  134.734099756295&quot;
## [1] &quot;step =  2  lambda =  0.218711886952215  loss:  134.884502460548&quot;
## [1] &quot;step =  1  lambda =  0.216535667316007  loss:  133.565909235912&quot;
## [1] &quot;step =  2  lambda =  0.216535667316007  loss:  133.71230915245&quot;
## [1] &quot;step =  1  lambda =  0.214381101426978  loss:  132.40529631122&quot;
## [1] &quot;step =  2  lambda =  0.214381101426978  loss:  132.548017258429&quot;
## [1] &quot;step =  1  lambda =  0.212247973826743  loss:  131.252507099367&quot;
## [1] &quot;step =  2  lambda =  0.212247973826743  loss:  131.39187078718&quot;
## [1] &quot;step =  1  lambda =  0.210136071200765  loss:  130.107783245346&quot;
## [1] &quot;step =  2  lambda =  0.210136071200765  loss:  130.244096496007&quot;
## [1] &quot;step =  1  lambda =  0.20804518235702  loss:  128.971349303753&quot;
## [1] &quot;step =  2  lambda =  0.20804518235702  loss:  129.104897447281&quot;
## [1] &quot;step =  1  lambda =  0.205975098204883  loss:  127.843406355852&quot;
## [1] &quot;step =  2  lambda =  0.205975098204883  loss:  127.974450430593&quot;
## [1] &quot;step =  1  lambda =  0.203925611734213  loss:  126.724129457962&quot;
## [1] &quot;step =  2  lambda =  0.203925611734213  loss:  126.852905715195&quot;
## [1] &quot;step =  1  lambda =  0.201896517994655  loss:  125.613667398485&quot;
## [1] &quot;step =  2  lambda =  0.201896517994655  loss:  125.74038814576&quot;
## [1] &quot;step =  1  lambda =  0.199887614075145  loss:  124.512143785185&quot;
## [1] &quot;step =  2  lambda =  0.199887614075145  loss:  124.636998949135&quot;
## [1] &quot;step =  1  lambda =  0.197898699083615  loss:  123.419658836176&quot;
## [1] &quot;step =  2  lambda =  0.197898699083615  loss:  123.524810586005&quot;
## [1] &quot;step =  1  lambda =  0.19592957412691  loss:  122.31864247263&quot;
## [1] &quot;step =  2  lambda =  0.19592957412691  loss:  122.406021604076&quot;
## [1] &quot;step =  1  lambda =  0.193980042290892  loss:  121.210901021244&quot;
## [1] &quot;step =  2  lambda =  0.193980042290892  loss:  121.295462890589&quot;
## [1] &quot;step =  1  lambda =  0.192049908620754  loss:  120.111309489566&quot;
## [1] &quot;step =  2  lambda =  0.192049908620754  loss:  120.198844166212&quot;
## [1] &quot;step =  1  lambda =  0.19013898010152  loss:  119.02552043018&quot;
## [1] &quot;step =  2  lambda =  0.19013898010152  loss:  119.115959438219&quot;
## [1] &quot;step =  1  lambda =  0.188247065638747  loss:  117.953329886168&quot;
## [1] &quot;step =  2  lambda =  0.188247065638747  loss:  118.045624259389&quot;
## [1] &quot;step =  1  lambda =  0.18637397603941  loss:  116.893565387735&quot;
## [1] &quot;step =  2  lambda =  0.18637397603941  loss:  116.986895826802&quot;
## [1] &quot;step =  1  lambda =  0.184519523992989  loss:  115.845293647461&quot;
## [1] &quot;step =  2  lambda =  0.184519523992989  loss:  115.939205872538&quot;
## [1] &quot;step =  1  lambda =  0.182683524052735  loss:  114.807952096252&quot;
## [1] &quot;step =  2  lambda =  0.182683524052735  loss:  114.902239450684&quot;
## [1] &quot;step =  1  lambda =  0.180865792617122  loss:  113.781228904888&quot;
## [1] &quot;step =  2  lambda =  0.180865792617122  loss:  113.878655776258&quot;
## [1] &quot;step =  1  lambda =  0.179066147911493  loss:  112.767358338309&quot;
## [1] &quot;step =  2  lambda =  0.179066147911493  loss:  112.92664882719&quot;
## [1] &quot;step =  1  lambda =  0.177284409969878  loss:  111.824748016395&quot;
## [1] &quot;step =  2  lambda =  0.177284409969878  loss:  111.990619016954&quot;
## [1] &quot;step =  1  lambda =  0.175520400616997  loss:  110.897961058005&quot;
## [1] &quot;step =  2  lambda =  0.175520400616997  loss:  111.059270153426&quot;
## [1] &quot;step =  1  lambda =  0.173773943450445  loss:  109.975816161608&quot;
## [1] &quot;step =  2  lambda =  0.173773943450445  loss:  110.131813625287&quot;
## [1] &quot;step =  1  lambda =  0.172044863823051  loss:  109.057530003997&quot;
## [1] &quot;step =  2  lambda =  0.172044863823051  loss:  109.208722382117&quot;
## [1] &quot;step =  1  lambda =  0.17033298882541  loss:  108.143568992112&quot;
## [1] &quot;step =  2  lambda =  0.17033298882541  loss:  108.290526031165&quot;
## [1] &quot;step =  1  lambda =  0.168638147268596  loss:  107.234456312989&quot;
## [1] &quot;step =  2  lambda =  0.168638147268596  loss:  107.377639105584&quot;
## [1] &quot;step =  1  lambda =  0.166960169667041  loss:  106.330601685035&quot;
## [1] &quot;step =  2  lambda =  0.166960169667041  loss:  106.470376341374&quot;
## [1] &quot;step =  1  lambda =  0.165298888221586  loss:  105.432316312211&quot;
## [1] &quot;step =  2  lambda =  0.165298888221586  loss:  105.569004075517&quot;
## [1] &quot;step =  1  lambda =  0.163654136802704  loss:  104.53990972491&quot;
## [1] &quot;step =  2  lambda =  0.163654136802704  loss:  104.673730196461&quot;
## [1] &quot;step =  1  lambda =  0.162025750933881  loss:  103.653494678039&quot;
## [1] &quot;step =  2  lambda =  0.162025750933881  loss:  103.784663155234&quot;
## [1] &quot;step =  1  lambda =  0.160413567775173  loss:  102.773225379889&quot;
## [1] &quot;step =  2  lambda =  0.160413567775173  loss:  102.901952061907&quot;
## [1] &quot;step =  1  lambda =  0.158817426106921  loss:  101.899249424702&quot;
## [1] &quot;step =  2  lambda =  0.158817426106921  loss:  102.025725698383&quot;
## [1] &quot;step =  1  lambda =  0.157237166313628  loss:  101.031694300783&quot;
## [1] &quot;step =  2  lambda =  0.157237166313628  loss:  101.156090588321&quot;
## [1] &quot;step =  1  lambda =  0.155672630367997  loss:  100.170665470038&quot;
## [1] &quot;step =  2  lambda =  0.155672630367997  loss:  100.293132279792&quot;
## [1] &quot;step =  1  lambda =  0.154123661815132  loss:  99.3162476319823&quot;
## [1] &quot;step =  2  lambda =  0.154123661815132  loss:  99.4369174627873&quot;
## [1] &quot;step =  1  lambda =  0.152590105756884  loss:  98.4685068167722&quot;
## [1] &quot;step =  2  lambda =  0.152590105756884  loss:  98.5874961761276&quot;
## [1] &quot;step =  1  lambda =  0.151071808836371  loss:  97.6274925683359&quot;
## [1] &quot;step =  2  lambda =  0.151071808836371  loss:  97.7449038884816&quot;
## [1] &quot;step =  1  lambda =  0.149568619222635  loss:  96.7932400038188&quot;
## [1] &quot;step =  2  lambda =  0.149568619222635  loss:  96.9091634020434&quot;
## [1] &quot;step =  1  lambda =  0.148080386595462  loss:  95.96556853279&quot;
## [1] &quot;step =  2  lambda =  0.148080386595462  loss:  96.0796831351729&quot;
## [1] &quot;step =  1  lambda =  0.14660696213035  loss:  95.1442660077053&quot;
## [1] &quot;step =  2  lambda =  0.14660696213035  loss:  95.2566741618833&quot;
## [1] &quot;step =  1  lambda =  0.145148198483624  loss:  94.32940879614&quot;
## [1] &quot;step =  2  lambda =  0.145148198483624  loss:  94.4402512338804&quot;
## [1] &quot;step =  1  lambda =  0.143703949777703  loss:  93.5210726152105&quot;
## [1] &quot;step =  2  lambda =  0.143703949777703  loss:  93.6304620856119&quot;
## [1] &quot;step =  1  lambda =  0.142274071586514  loss:  92.7193046962394&quot;
## [1] &quot;step =  2  lambda =  0.142274071586514  loss:  92.8273327438027&quot;
## [1] &quot;step =  1  lambda =  0.140858420921045  loss:  91.9241307919524&quot;
## [1] &quot;step =  2  lambda =  0.140858420921045  loss:  92.0308752113609&quot;
## [1] &quot;step =  1  lambda =  0.139456856215051  loss:  91.1355627772666&quot;
## [1] &quot;step =  2  lambda =  0.139456856215051  loss:  91.2410919838108&quot;
## [1] &quot;step =  1  lambda =  0.138069237310893  loss:  90.3536031170965&quot;
## [1] &quot;step =  2  lambda =  0.138069237310893  loss:  90.4579783764716&quot;
## [1] &quot;step =  1  lambda =  0.136695425445524  loss:  89.5782471685064&quot;
## [1] &quot;step =  2  lambda =  0.136695425445524  loss:  89.6815237641233&quot;
## [1] &quot;step =  1  lambda =  0.135335283236613  loss:  88.8094844071776&quot;
## [1] &quot;step =  2  lambda =  0.135335283236613  loss:  88.9117123613004&quot;
## [1] &quot;step =  1  lambda =  0.133988674668805  loss:  88.0472991995847&quot;
## [1] &quot;step =  2  lambda =  0.133988674668805  loss:  88.1485238233628&quot;
## [1] &quot;step =  1  lambda =  0.132655465080122  loss:  87.2916713979867&quot;
## [1] &quot;step =  2  lambda =  0.132655465080122  loss:  87.3919337700295&quot;
## [1] &quot;step =  1  lambda =  0.131335521148493  loss:  86.5425768587678&quot;
## [1] &quot;step =  2  lambda =  0.131335521148493  loss:  86.6419142593555&quot;
## [1] &quot;step =  1  lambda =  0.130028710878426  loss:  85.7999879117615&quot;
## [1] &quot;step =  2  lambda =  0.130028710878426  loss:  85.8984342164831&quot;
## [1] &quot;step =  1  lambda =  0.128734903587804  loss:  85.0638737848116&quot;
## [1] &quot;step =  2  lambda =  0.128734903587804  loss:  85.161459817228&quot;
## [1] &quot;step =  1  lambda =  0.127453969894821  loss:  84.3342009836185&quot;
## [1] &quot;step =  2  lambda =  0.127453969894821  loss:  84.4309548278539&quot;
## [1] &quot;step =  1  lambda =  0.126185781705039  loss:  83.6109336282006&quot;
## [1] &quot;step =  2  lambda =  0.126185781705039  loss:  83.7068809041752&quot;
## [1] &quot;step =  1  lambda =  0.124930212198582  loss:  82.8940337490782&quot;
## [1] &quot;step =  2  lambda =  0.124930212198582  loss:  82.9891978541594&quot;
## [1] &quot;step =  1  lambda =  0.123687135817455  loss:  82.1834615473125&quot;
## [1] &quot;step =  2  lambda =  0.123687135817455  loss:  82.2778638684701&quot;
## [1] &quot;step =  1  lambda =  0.122456428252982  loss:  81.479175622792&quot;
## [1] &quot;step =  2  lambda =  0.122456428252982  loss:  81.5728357232046&quot;
## [1] &quot;step =  1  lambda =  0.121237966433382  loss:  80.7811331749843&quot;
## [1] &quot;step =  2  lambda =  0.121237966433382  loss:  80.8740689586744&quot;
## [1] &quot;step =  1  lambda =  0.120031628511457  loss:  80.0892901799619&quot;
## [1] &quot;step =  2  lambda =  0.120031628511457  loss:  80.181518037605&quot;
## [1] &quot;step =  1  lambda =  0.11883729385241  loss:  79.4036015470444&quot;
## [1] &quot;step =  2  lambda =  0.11883729385241  loss:  79.4951364856664&quot;
## [1] &quot;step =  1  lambda =  0.117654843021779  loss:  78.7240212579445&quot;
## [1] &quot;step =  2  lambda =  0.117654843021779  loss:  78.8148770168225&quot;
## [1] &quot;step =  1  lambda =  0.116484157773497  loss:  78.0502854768725&quot;
## [1] &quot;step =  2  lambda =  0.116484157773497  loss:  78.1401505865543&quot;
## [1] &quot;step =  1  lambda =  0.115325121038063  loss:  77.3822134077315&quot;
## [1] &quot;step =  2  lambda =  0.115325121038063  loss:  77.4711045401272&quot;
## [1] &quot;step =  1  lambda =  0.114177616910836  loss:  76.7198023931492&quot;
## [1] &quot;step =  2  lambda =  0.114177616910836  loss:  76.8077747540895&quot;
## [1] &quot;step =  1  lambda =  0.11304153064045  loss:  76.0631225038332&quot;
## [1] &quot;step =  2  lambda =  0.11304153064045  loss:  76.1503880370082&quot;
## [1] &quot;step =  1  lambda =  0.111916748617329  loss:  75.412340743767&quot;
## [1] &quot;step =  2  lambda =  0.111916748617329  loss:  75.4990672732326&quot;
## [1] &quot;step =  1  lambda =  0.110803158362334  loss:  74.7674763150068&quot;
## [1] &quot;step =  2  lambda =  0.110803158362334  loss:  74.8536337505068&quot;
## [1] &quot;step =  1  lambda =  0.109700648515511  loss:  74.1284408840738&quot;
## [1] &quot;step =  2  lambda =  0.109700648515511  loss:  74.2140130862431&quot;
## [1] &quot;step =  1  lambda =  0.108609108824958  loss:  73.4951607985427&quot;
## [1] &quot;step =  2  lambda =  0.108609108824958  loss:  73.5801403742661&quot;
## [1] &quot;step =  1  lambda =  0.107528430135795  loss:  72.8675717857374&quot;
## [1] &quot;step =  2  lambda =  0.107528430135795  loss:  72.9519552149754&quot;
## [1] &quot;step =  1  lambda =  0.106458504379253  loss:  72.2456140340162&quot;
## [1] &quot;step =  2  lambda =  0.106458504379253  loss:  72.3293995898854&quot;
## [1] &quot;step =  1  lambda =  0.105399224561864  loss:  71.6292300894073&quot;
## [1] &quot;step =  2  lambda =  0.105399224561864  loss:  71.7124169735476&quot;
## [1] &quot;step =  1  lambda =  0.104350484754765  loss:  71.0183639768306&quot;
## [1] &quot;step =  2  lambda =  0.104350484754765  loss:  71.100951931877&quot;
## [1] &quot;step =  1  lambda =  0.1033121800831  loss:  70.4129608026058&quot;
## [1] &quot;step =  2  lambda =  0.1033121800831  loss:  70.4949499196039&quot;
## [1] &quot;step =  1  lambda =  0.102284206715537  loss:  69.812966553951&quot;
## [1] &quot;step =  2  lambda =  0.102284206715537  loss:  69.8943571648815&quot;
## [1] &quot;step =  1  lambda =  0.101266461853883  loss:  69.2183279846924&quot;
## [1] &quot;step =  2  lambda =  0.101266461853883  loss:  69.2991205953858&quot;
## [1] &quot;step =  1  lambda =  0.100258843722804  loss:  68.6289925420197&quot;
## [1] &quot;step =  2  lambda =  0.100258843722804  loss:  68.709187786523&quot;
## [1] &quot;step =  1  lambda =  0.0992612515596457  loss:  68.0449083151214&quot;
## [1] &quot;step =  2  lambda =  0.0992612515596457  loss:  68.1245069231256&quot;
## [1] &quot;step =  1  lambda =  0.0982735856043615  loss:  67.4660239971771&quot;
## [1] &quot;step =  2  lambda =  0.0982735856043615  loss:  67.545026770457&quot;
## [1] &quot;step =  1  lambda =  0.0972957470895328  loss:  66.8922888565737&quot;
## [1] &quot;step =  2  lambda =  0.0972957470895328  loss:  66.9706966521851&quot;
## [1] &quot;step =  1  lambda =  0.096327638230493  loss:  66.3236527150313&quot;
## [1] &quot;step =  2  lambda =  0.096327638230493  loss:  66.4014664337608&quot;
## [1] &quot;step =  1  lambda =  0.0953691622155497  loss:  65.7600659310885&quot;
## [1] &quot;step =  2  lambda =  0.0953691622155497  loss:  65.8372865100039&quot;
## [1] &quot;step =  1  lambda =  0.0944202231963024  loss:  65.201479387764&quot;
## [1] &quot;step =  2  lambda =  0.0944202231963024  loss:  65.2781077959197&quot;
## [1] &quot;step =  1  lambda =  0.0934807262780585  loss:  64.6478444834215&quot;
## [1] &quot;step =  2  lambda =  0.0934807262780585  loss:  64.7238817199281&quot;
## [1] &quot;step =  1  lambda =  0.0925505775103433  loss:  64.0991131250311&quot;
## [1] &quot;step =  2  lambda =  0.0925505775103433  loss:  64.1745602188329&quot;
## [1] &quot;step =  1  lambda =  0.0916296838775049  loss:  63.5552377231573&quot;
## [1] &quot;step =  2  lambda =  0.0916296838775049  loss:  63.6300957339848&quot;
## [1] &quot;step =  1  lambda =  0.0907179532894126  loss:  63.016171188135&quot;
## [1] &quot;step =  2  lambda =  0.0907179532894126  loss:  63.0904412082092&quot;
## [1] &quot;step =  1  lambda =  0.0898152945722476  loss:  62.4818669270044&quot;
## [1] &quot;step =  2  lambda =  0.0898152945722476  loss:  62.5555500831634&quot;
## [1] &quot;step =  1  lambda =  0.0889216174593863  loss:  61.9522788408754&quot;
## [1] &quot;step =  2  lambda =  0.0889216174593863  loss:  62.0253762968738&quot;
## [1] &quot;step =  1  lambda =  0.0880368325823726  loss:  61.4273613224718&quot;
## [1] &quot;step =  2  lambda =  0.0880368325823726  loss:  61.499874281274&quot;
## [1] &quot;step =  1  lambda =  0.0871608514619813  loss:  60.9070692536795&quot;
## [1] &quot;step =  2  lambda =  0.0871608514619813  loss:  60.9789989596153&quot;
## [1] &quot;step =  1  lambda =  0.0862935864993705  loss:  60.391358002969&quot;
## [1] &quot;step =  2  lambda =  0.0862935864993705  loss:  60.4627057436694&quot;
## [1] &quot;step =  1  lambda =  0.0854349509673212  loss:  59.8801834226146&quot;
## [1] &quot;step =  2  lambda =  0.0854349509673212  loss:  59.9509505306731&quot;
## [1] &quot;step =  1  lambda =  0.0845848590015647  loss:  59.3735018456585&quot;
## [1] &quot;step =  2  lambda =  0.0845848590015647  loss:  59.4436896999923&quot;
## [1] &quot;step =  1  lambda =  0.083743225592196  loss:  58.8712700825994&quot;
## [1] &quot;step =  2  lambda =  0.083743225592196  loss:  58.9408801095022&quot;
## [1] &quot;step =  1  lambda =  0.0829099665751727  loss:  58.3734454177995&quot;
## [1] &quot;step =  2  lambda =  0.0829099665751727  loss:  58.4424790916915&quot;
## [1] &quot;step =  1  lambda =  0.0820849986238988  loss:  57.8799856056204&quot;
## [1] &quot;step =  2  lambda =  0.0820849986238988  loss:  57.9484444495107&quot;
## [1] &quot;step =  1  lambda =  0.0812682392408917  loss:  57.3908488663049&quot;
## [1] &quot;step =  2  lambda =  0.0812682392408917  loss:  57.4587344519882&quot;
## [1] &quot;step =  1  lambda =  0.0804596067495325  loss:  56.9059938816311&quot;
## [1] &quot;step =  2  lambda =  0.0804596067495325  loss:  56.9733078296436&quot;
## [1] &quot;step =  1  lambda =  0.079659020285898  loss:  56.4253797903653&quot;
## [1] &quot;step =  2  lambda =  0.079659020285898  loss:  56.4921237697276&quot;
## [1] &quot;step =  1  lambda =  0.0788663997906749  loss:  55.9489661835436&quot;
## [1] &quot;step =  2  lambda =  0.0788663997906749  loss:  56.0151419113187&quot;
## [1] &quot;step =  1  lambda =  0.0780816660011532  loss:  55.4767130996123&quot;
## [1] &quot;step =  2  lambda =  0.0780816660011532  loss:  55.5423223403064&quot;
## [1] &quot;step =  1  lambda =  0.0773047404432998  loss:  55.0085810194572&quot;
## [1] &quot;step =  2  lambda =  0.0773047404432998  loss:  55.0720024385951&quot;
## [1] &quot;step =  1  lambda =  0.0765355454239115  loss:  54.5429803199514&quot;
## [1] &quot;step =  2  lambda =  0.0765355454239115  loss:  54.6038858717342&quot;
## [1] &quot;step =  1  lambda =  0.0757740040228455  loss:  54.0795052805882&quot;
## [1] &quot;step =  2  lambda =  0.0757740040228455  loss:  54.139430846071&quot;
## [1] &quot;step =  1  lambda =  0.075020040085327  loss:  53.6196545982124&quot;
## [1] &quot;step =  2  lambda =  0.075020040085327  loss:  53.6790891348608&quot;
## [1] &quot;step =  1  lambda =  0.0742735782143339  loss:  53.1638758391102&quot;
## [1] &quot;step =  2  lambda =  0.0742735782143339  loss:  53.2228869149552&quot;
## [1] &quot;step =  1  lambda =  0.0735345437630571  loss:  52.7121952405196&quot;
## [1] &quot;step =  2  lambda =  0.0735345437630571  loss:  52.770775645895&quot;
## [1] &quot;step =  1  lambda =  0.0728028628274356  loss:  52.2645649211151&quot;
## [1] &quot;step =  2  lambda =  0.0728028628274356  loss:  52.3226981425764&quot;
## [1] &quot;step =  1  lambda =  0.0720784622387661  loss:  51.8209283520652&quot;
## [1] &quot;step =  2  lambda =  0.0720784622387661  loss:  51.8785997181397&quot;
## [1] &quot;step =  1  lambda =  0.0713612695563861  loss:  51.3812314287132&quot;
## [1] &quot;step =  2  lambda =  0.0713612695563861  loss:  51.4384292792129&quot;
## [1] &quot;step =  1  lambda =  0.0706512130604296  loss:  50.9454235785519&quot;
## [1] &quot;step =  2  lambda =  0.0706512130604296  loss:  51.0021388728685&quot;
## [1] &quot;step =  1  lambda =  0.0699482217446554  loss:  50.5134573257647&quot;
## [1] &quot;step =  2  lambda =  0.0699482217446554  loss:  50.5696831617596&quot;
## [1] &quot;step =  1  lambda =  0.069252225309346  loss:  50.0852877785586&quot;
## [1] &quot;step =  2  lambda =  0.069252225309346  loss:  50.1410190117649&quot;
## [1] &quot;step =  1  lambda =  0.0685631541542779  loss:  49.6608722244993&quot;
## [1] &quot;step =  2  lambda =  0.0685631541542779  loss:  49.7161051720066&quot;
## [1] &quot;step =  1  lambda =  0.0678809393717615  loss:  49.2401698154595&quot;
## [1] &quot;step =  2  lambda =  0.0678809393717615  loss:  49.294902015274&quot;
## [1] &quot;step =  1  lambda =  0.0672055127397498  loss:  48.8231413113973&quot;
## [1] &quot;step =  2  lambda =  0.0672055127397498  loss:  48.8773713197582&quot;
## [1] &quot;step =  1  lambda =  0.0665368067150169  loss:  48.409748864536&quot;
## [1] &quot;step =  2  lambda =  0.0665368067150169  loss:  48.463476082684&quot;
## [1] &quot;step =  1  lambda =  0.065874754426403  loss:  47.9999558348807&quot;
## [1] &quot;step =  2  lambda =  0.065874754426403  loss:  48.0531803611369&quot;
## [1] &quot;step =  1  lambda =  0.0652192896681276  loss:  47.5937266325554&quot;
## [1] &quot;step =  2  lambda =  0.0652192896681276  loss:  47.6464491371183&quot;
## [1] &quot;step =  1  lambda =  0.0645703468931685  loss:  47.1910265840881&quot;
## [1] &quot;step =  2  lambda =  0.0645703468931685  loss:  47.2432482042575&quot;
## [1] &quot;step =  1  lambda =  0.0639278612067076  loss:  46.7918218201357&quot;
## [1] &quot;step =  2  lambda =  0.0639278612067076  loss:  46.8435440736091&quot;
## [1] &quot;step =  1  lambda =  0.0632917683596407  loss:  46.3960791821114&quot;
## [1] &quot;step =  2  lambda =  0.0632917683596407  loss:  46.4473038959302&quot;
## [1] &quot;step =  1  lambda =  0.0626620047421532  loss:  46.0037661451458&quot;
## [1] &quot;step =  2  lambda =  0.0626620047421532  loss:  46.054495397922&quot;
## [1] &quot;step =  1  lambda =  0.0620385073773583  loss:  45.6148507548875&quot;
## [1] &quot;step =  2  lambda =  0.0620385073773583  loss:  45.6650868301025&quot;
## [1] &quot;step =  1  lambda =  0.0614212139150001  loss:  45.2293015758332&quot;
## [1] &quot;step =  2  lambda =  0.0614212139150001  loss:  45.2790469242396&quot;
## [1] &quot;step =  1  lambda =  0.060810062625218  loss:  44.8470876491351&quot;
## [1] &quot;step =  2  lambda =  0.060810062625218  loss:  44.8963708687124&quot;
## [1] &quot;step =  1  lambda =  0.0602049923923736  loss:  44.4682038364558&quot;
## [1] &quot;step =  2  lambda =  0.0602049923923736  loss:  44.517132416321&quot;
## [1] &quot;step =  1  lambda =  0.0596059427089393  loss:  44.0927237655161&quot;
## [1] &quot;step =  2  lambda =  0.0596059427089393  loss:  44.1413771370437&quot;
## [1] &quot;step =  1  lambda =  0.0590128536694478  loss:  43.7206922227341&quot;
## [1] &quot;step =  2  lambda =  0.0590128536694478  loss:  43.7690629610594&quot;
## [1] &quot;step =  1  lambda =  0.0584256659645008  loss:  43.3520676300989&quot;
## [1] &quot;step =  2  lambda =  0.0584256659645008  loss:  43.4001364174273&quot;
## [1] &quot;step =  1  lambda =  0.0578443208748385  loss:  42.9867970805218&quot;
## [1] &quot;step =  2  lambda =  0.0578443208748385  loss:  43.0345468937414&quot;
## [1] &quot;step =  1  lambda =  0.0572687602654674  loss:  42.6248304750963&quot;
## [1] &quot;step =  2  lambda =  0.0572687602654674  loss:  42.672247573133&quot;
## [1] &quot;step =  1  lambda =  0.0566989265798469  loss:  42.266121462854&quot;
## [1] &quot;step =  2  lambda =  0.0566989265798469  loss:  42.313194601871&quot;
## [1] &quot;step =  1  lambda =  0.0561347628341337  loss:  41.9106266227166&quot;
## [1] &quot;step =  2  lambda =  0.0561347628341337  loss:  41.957346411652&quot;
## [1] &quot;step =  1  lambda =  0.0555762126114831  loss:  41.5583047952501&quot;
## [1] &quot;step =  2  lambda =  0.0555762126114831  loss:  41.6046633063193&quot;
## [1] &quot;step =  1  lambda =  0.0550232200564073  loss:  41.209116674618&quot;
## [1] &quot;step =  2  lambda =  0.0550232200564073  loss:  41.255107202253&quot;
## [1] &quot;step =  1  lambda =  0.0544757298691899  loss:  40.8630245519556&quot;
## [1] &quot;step =  2  lambda =  0.0544757298691899  loss:  40.9086414437449&quot;
## [1] &quot;step =  1  lambda =  0.053933687300356  loss:  40.5199921326754&quot;
## [1] &quot;step =  2  lambda =  0.053933687300356  loss:  40.5652306554268&quot;
## [1] &quot;step =  1  lambda =  0.0533970381451971  loss:  40.1799843903322&quot;
## [1] &quot;step =  2  lambda =  0.0533970381451971  loss:  40.2248406161066&quot;
## [1] &quot;step =  1  lambda =  0.0528657287383504  loss:  39.8429674416422&quot;
## [1] &quot;step =  2  lambda =  0.0528657287383504  loss:  39.8874381480932&quot;
## [1] &quot;step =  1  lambda =  0.0523397059484324  loss:  39.5089084368278&quot;
## [1] &quot;step =  2  lambda =  0.0523397059484324  loss:  39.5529910196909&quot;
## [1] &quot;step =  1  lambda =  0.0518189171727258  loss:  39.1777754630064&quot;
## [1] &quot;step =  2  lambda =  0.0518189171727258  loss:  39.2214678596079&quot;
## [1] &quot;step =  1  lambda =  0.0513033103319191  loss:  38.8495374593853&quot;
## [1] &quot;step =  2  lambda =  0.0513033103319191  loss:  38.8928380822028&quot;
## [1] &quot;step =  1  lambda =  0.0507928338648985  loss:  38.5241641431976&quot;
## [1] &quot;step =  2  lambda =  0.0507928338648985  loss:  38.5670718224527&quot;
## [1] &quot;step =  1  lambda =  0.0502874367235919  loss:  38.2016259452731&quot;
## [1] &quot;step =  2  lambda =  0.0502874367235919  loss:  38.2441398794897&quot;
## [1] &quot;step =  1  lambda =  0.0497870683678639  loss:  37.8818939541005&quot;
## [1] &quot;step =  2  lambda =  0.0497870683678639  loss:  37.9240136675804&quot;
## [1] &quot;step =  1  lambda =  0.0492916787604622  loss:  37.5649398672659&quot;
## [1] &quot;step =  2  lambda =  0.0492916787604622  loss:  37.6066651734991&quot;
## [1] &quot;step =  1  lambda =  0.048801218362013  loss:  37.2507359492282&quot;
## [1] &quot;step =  2  lambda =  0.048801218362013  loss:  37.2920669193542&quot;
## [1] &quot;step =  1  lambda =  0.0483156381260678  loss:  36.9392549944974&quot;
## [1] &quot;step =  2  lambda =  0.0483156381260678  loss:  36.9801919300437&quot;
## [1] &quot;step =  1  lambda =  0.0478348894941984  loss:  36.6304702954004&quot;
## [1] &quot;step =  2  lambda =  0.0478348894941984  loss:  36.6710137046349&quot;
## [1] &quot;step =  1  lambda =  0.0473589243911409  loss:  36.3243556137356&quot;
## [1] &quot;step =  2  lambda =  0.0473589243911409  loss:  36.3645061910699&quot;
## [1] &quot;step =  1  lambda =  0.0468876952199885  loss:  36.0208851557219&quot;
## [1] &quot;step =  2  lambda =  0.0468876952199885  loss:  36.0606437636945&quot;
## [1] &quot;step =  1  lambda =  0.0464211548574313  loss:  35.7200335497462&quot;
## [1] &quot;step =  2  lambda =  0.0464211548574313  loss:  35.7594012031925&quot;
## [1] &quot;step =  1  lambda =  0.0459592566490442  loss:  35.421775826493&quot;
## [1] &quot;step =  2  lambda =  0.0459592566490442  loss:  35.4607536785725&quot;
## [1] &quot;step =  1  lambda =  0.0455019544046216  loss:  35.126087401108&quot;
## [1] &quot;step =  2  lambda =  0.0455019544046216  loss:  35.164676730919&quot;
## [1] &quot;step =  1  lambda =  0.0450492023935578  loss:  34.8329440571094&quot;
## [1] &quot;step =  2  lambda =  0.0450492023935578  loss:  34.8711462586626&quot;
## [1] &quot;step =  1  lambda =  0.0446009553402746  loss:  34.5423219318036&quot;
## [1] &quot;step =  2  lambda =  0.0446009553402746  loss:  34.5801385041682&quot;
## [1] &quot;step =  1  lambda =  0.0441571684196929  loss:  34.2541975030068&quot;
## [1] &quot;step =  2  lambda =  0.0441571684196929  loss:  34.2916300414704&quot;
## [1] &quot;step =  1  lambda =  0.0437177972527509  loss:  33.9685475769028&quot;
## [1] &quot;step =  2  lambda =  0.0437177972527509  loss:  34.0055977650134&quot;
## [1] &quot;step =  1  lambda =  0.0432827979019659  loss:  33.6853492768948&quot;
## [1] &quot;step =  2  lambda =  0.0432827979019659  loss:  33.7220188792752&quot;
## [1] &quot;step =  1  lambda =  0.0428521268670402  loss:  33.4045800333343&quot;
## [1] &quot;step =  2  lambda =  0.0428521268670402  loss:  33.4408708891738&quot;
## [1] &quot;step =  1  lambda =  0.0424257410805114  loss:  33.1262175740232&quot;
## [1] &quot;step =  2  lambda =  0.0424257410805114  loss:  33.1621315911667&quot;
## [1] &quot;step =  1  lambda =  0.0420035979034456  loss:  32.8502399154033&quot;
## [1] &quot;step =  2  lambda =  0.0420035979034456  loss:  32.8857790649719&quot;
## [1] &quot;step =  1  lambda =  0.0415856551211732  loss:  32.5766253543598&quot;
## [1] &quot;step =  2  lambda =  0.0415856551211732  loss:  32.6117916658413&quot;
## [1] &quot;step =  1  lambda =  0.0411718709390678  loss:  32.3053524605738&quot;
## [1] &quot;step =  2  lambda =  0.0411718709390678  loss:  32.3401480173346&quot;
## [1] &quot;step =  1  lambda =  0.0407622039783662  loss:  32.036400069368&quot;
## [1] &quot;step =  2  lambda =  0.0407622039783662  loss:  32.0708270045416&quot;
## [1] &quot;step =  1  lambda =  0.0403566132720311  loss:  31.769747274998&quot;
## [1] &quot;step =  2  lambda =  0.0403566132720311  loss:  31.8038077677105&quot;
## [1] &quot;step =  1  lambda =  0.0399550582606539  loss:  31.505373424345&quot;
## [1] &quot;step =  2  lambda =  0.0399550582606539  loss:  31.5390696962456&quot;
## [1] &quot;step =  1  lambda =  0.0395574987883987  loss:  31.2432581109737&quot;
## [1] &quot;step =  2  lambda =  0.0395574987883987  loss:  31.2765924230376&quot;
## [1] &quot;step =  1  lambda =  0.0391638950989871  loss:  30.9833811695203&quot;
## [1] &quot;step =  2  lambda =  0.0391638950989871  loss:  31.0163558190999&quot;
## [1] &quot;step =  1  lambda =  0.038774207831722  loss:  30.7257226703824&quot;
## [1] &quot;step =  2  lambda =  0.038774207831722  loss:  30.7583399884804&quot;
## [1] &quot;step =  1  lambda =  0.0383883980175521  loss:  30.4702629146827&quot;
## [1] &quot;step =  2  lambda =  0.0383883980175521  loss:  30.5025252634275&quot;
## [1] &quot;step =  1  lambda =  0.0380064270751743  loss:  30.2169824294833&quot;
## [1] &quot;step =  2  lambda =  0.0380064270751743  loss:  30.2488921997867&quot;
## [1] &quot;step =  1  lambda =  0.0376282568071762  loss:  29.9658619632287&quot;
## [1] &quot;step =  2  lambda =  0.0376282568071762  loss:  29.9974215726088&quot;
## [1] &quot;step =  1  lambda =  0.0372538493962158  loss:  29.7168824813985&quot;
## [1] &quot;step =  2  lambda =  0.0372538493962158  loss:  29.7480943719522&quot;
## [1] &quot;step =  1  lambda =  0.03688316740124  loss:  29.4700251623517&quot;
## [1] &quot;step =  2  lambda =  0.03688316740124  loss:  29.5008917988622&quot;
## [1] &quot;step =  1  lambda =  0.0365161737537404  loss:  29.2252713933464&quot;
## [1] &quot;step =  2  lambda =  0.0365161737537404  loss:  29.2557952615142&quot;
## [1] &quot;step =  1  lambda =  0.0361528317540464  loss:  28.982602766722&quot;
## [1] &quot;step =  2  lambda =  0.0361528317540464  loss:  29.0127863715066&quot;
## [1] &quot;step =  1  lambda =  0.0357931050676553  loss:  28.7420010762287&quot;
## [1] &quot;step =  2  lambda =  0.0357931050676553  loss:  28.7718469402912&quot;
## [1] &quot;step =  1  lambda =  0.0354369577215986  loss:  28.5034483134946&quot;
## [1] &quot;step =  2  lambda =  0.0354369577215986  loss:  28.5329589757319&quot;
## [1] &quot;step =  1  lambda =  0.035084354100845  loss:  28.2669266646175&quot;
## [1] &quot;step =  2  lambda =  0.035084354100845  loss:  28.296104678779&quot;
## [1] &quot;step =  1  lambda =  0.0347352589447386  loss:  28.0324185068742&quot;
## [1] &quot;step =  2  lambda =  0.0347352589447386  loss:  28.0612664402541&quot;
## [1] &quot;step =  1  lambda =  0.0343896373434727  loss:  27.7999064055361&quot;
## [1] &quot;step =  2  lambda =  0.0343896373434727  loss:  27.8284268377338&quot;
## [1] &quot;step =  1  lambda =  0.0340474547345993  loss:  27.5693731107846&quot;
## [1] &quot;step =  2  lambda =  0.0340474547345993  loss:  27.5975686325271&quot;
## [1] &quot;step =  1  lambda =  0.0337086768995724  loss:  27.3408015547187&quot;
## [1] &quot;step =  2  lambda =  0.0337086768995724  loss:  27.3686747667396&quot;
## [1] &quot;step =  1  lambda =  0.0333732699603261  loss:  27.1141748484485&quot;
## [1] &quot;step =  2  lambda =  0.0333732699603261  loss:  27.1417283604181&quot;
## [1] &quot;step =  1  lambda =  0.0330412003758869  loss:  26.8894762792678&quot;
## [1] &quot;step =  2  lambda =  0.0330412003758869  loss:  26.9167127087701&quot;
## [1] &quot;step =  1  lambda =  0.0327124349390198  loss:  26.6666893079019&quot;
## [1] &quot;step =  2  lambda =  0.0327124349390198  loss:  26.6936112794552&quot;
## [1] &quot;step =  1  lambda =  0.0323869407729071  loss:  26.4457975658255&quot;
## [1] &quot;step =  2  lambda =  0.0323869407729071  loss:  26.4724077099406&quot;
## [1] &quot;step =  1  lambda =  0.0320646853278608  loss:  26.2267848526445&quot;
## [1] &quot;step =  2  lambda =  0.0320646853278608  loss:  26.2530858049204&quot;
## [1] &quot;step =  1  lambda =  0.0317456363780679  loss:  26.0096351335412&quot;
## [1] &quot;step =  2  lambda =  0.0317456363780679  loss:  26.0356295337932&quot;
## [1] &quot;step =  1  lambda =  0.0314297620183677  loss:  25.7943325367768&quot;
## [1] &quot;step =  2  lambda =  0.0314297620183677  loss:  25.8200230281951&quot;
## [1] &quot;step =  1  lambda =  0.0311170306610609  loss:  25.5808613512487&quot;
## [1] &quot;step =  2  lambda =  0.0311170306610609  loss:  25.6062505795854&quot;
## [1] &quot;step =  1  lambda =  0.0308074110327511  loss:  25.3692060241001&quot;
## [1] &quot;step =  2  lambda =  0.0308074110327511  loss:  25.3942966368828&quot;
## [1] &quot;step =  1  lambda =  0.0305008721712175  loss:  25.1593511583801&quot;
## [1] &quot;step =  2  lambda =  0.0305008721712175  loss:  25.1841458041494&quot;
## [1] &quot;step =  1  lambda =  0.0301973834223185  loss:  24.9512815107502&quot;
## [1] &quot;step =  2  lambda =  0.0301973834223185  loss:  24.975782838321&quot;
## [1] &quot;step =  1  lambda =  0.0298969144369263  loss:  24.7449819892371&quot;
## [1] &quot;step =  2  lambda =  0.0298969144369263  loss:  24.7691926469807&quot;
## [1] &quot;step =  1  lambda =  0.029599435167892  loss:  24.5404376510281&quot;
## [1] &quot;step =  2  lambda =  0.029599435167892  loss:  24.5643602861759&quot;
## [1] &quot;step =  1  lambda =  0.0293049158670407  loss:  24.3376337003102&quot;
## [1] &quot;step =  2  lambda =  0.0293049158670407  loss:  24.3612709582754&quot;
## [1] &quot;step =  1  lambda =  0.0290133270821971  loss:  24.1365554861481&quot;
## [1] &quot;step =  2  lambda =  0.0290133270821971  loss:  24.1599100098672&quot;
## [1] &quot;step =  1  lambda =  0.0287246396542394  loss:  23.9371885004026&quot;
## [1] &quot;step =  2  lambda =  0.0287246396542394  loss:  23.9602629296939&quot;
## [1] &quot;step =  1  lambda =  0.0284388247141845  loss:  23.7395183756866&quot;
## [1] &quot;step =  2  lambda =  0.0284388247141845  loss:  23.7623153466264&quot;
## [1] &quot;step =  1  lambda =  0.0281558536803001  loss:  23.5435308833585&quot;
## [1] &quot;step =  2  lambda =  0.0281558536803001  loss:  23.5660530276732&quot;
## [1] &quot;step =  1  lambda =  0.027875698255247  loss:  23.3492119315517&quot;
## [1] &quot;step =  2  lambda =  0.027875698255247  loss:  23.3714618760263&quot;
## [1] &quot;step =  1  lambda =  0.0275983304232493  loss:  23.1565475632389&quot;
## [1] &quot;step =  2  lambda =  0.0275983304232493  loss:  23.178527929141&quot;
## [1] &quot;step =  1  lambda =  0.0273237224472926  loss:  22.9655239543317&quot;
## [1] &quot;step =  2  lambda =  0.0273237224472926  loss:  22.9872373568508&quot;
## [1] &quot;step =  1  lambda =  0.0270518468663504  loss:  22.7761274118134&quot;
## [1] &quot;step =  2  lambda =  0.0270518468663504  loss:  22.797576459515&quot;
## [1] &quot;step =  1  lambda =  0.0267826764926382  loss:  22.5883443719052&quot;
## [1] &quot;step =  2  lambda =  0.0267826764926382  loss:  22.6095316661994&quot;
## [1] &quot;step =  1  lambda =  0.0265161844088942  loss:  22.4021613982649&quot;
## [1] &quot;step =  2  lambda =  0.0265161844088942  loss:  22.4230895328897&quot;
## [1] &quot;step =  1  lambda =  0.026252343965688  loss:  22.2175651802172&quot;
## [1] &quot;step =  2  lambda =  0.026252343965688  loss:  22.2382367407357&quot;
## [1] &quot;step =  1  lambda =  0.0259911287787554  loss:  22.0345425310161&quot;
## [1] &quot;step =  2  lambda =  0.0259911287787554  loss:  22.0549600943281&quot;
## [1] &quot;step =  1  lambda =  0.0257325127263599  loss:  21.8530803861381&quot;
## [1] &quot;step =  2  lambda =  0.0257325127263599  loss:  21.873246520005&quot;
## [1] &quot;step =  1  lambda =  0.025476469946681  loss:  21.6731658016052&quot;
## [1] &quot;step =  2  lambda =  0.025476469946681  loss:  21.6930830641892&quot;
## [1] &quot;step =  1  lambda =  0.0252229748352272  loss:  21.4947859523393&quot;
## [1] &quot;step =  2  lambda =  0.0252229748352272  loss:  21.5144568917558&quot;
## [1] &quot;step =  1  lambda =  0.0249720020422762  loss:  21.3179281305446&quot;
## [1] &quot;step =  2  lambda =  0.0249720020422762  loss:  21.3373552844285&quot;
## [1] &quot;step =  1  lambda =  0.0247235264703394  loss:  21.142579744121&quot;
## [1] &quot;step =  2  lambda =  0.0247235264703394  loss:  21.1617656392056&quot;
## [1] &quot;step =  1  lambda =  0.0244775232716527  loss:  20.9687283151047&quot;
## [1] &quot;step =  2  lambda =  0.0244775232716527  loss:  20.9876754668147&quot;
## [1] &quot;step =  1  lambda =  0.0242339678456911  loss:  20.7963614781384&quot;
## [1] &quot;step =  2  lambda =  0.0242339678456911  loss:  20.8150723901959&quot;
## [1] &quot;step =  1  lambda =  0.0239928358367092  loss:  20.6254669789692&quot;
## [1] &quot;step =  2  lambda =  0.0239928358367092  loss:  20.6439441430123&quot;
## [1] &quot;step =  1  lambda =  0.023754103131305  loss:  20.4560326729739&quot;
## [1] &quot;step =  2  lambda =  0.023754103131305  loss:  20.4742785681887&quot;
## [1] &quot;step =  1  lambda =  0.0235177458560091  loss:  20.2880465237121&quot;
## [1] &quot;step =  2  lambda =  0.0235177458560091  loss:  20.3060636164776&quot;
## [1] &quot;step =  1  lambda =  0.023283740374897  loss:  20.1214966015059&quot;
## [1] &quot;step =  2  lambda =  0.023283740374897  loss:  20.1392873450514&quot;
## [1] &quot;step =  1  lambda =  0.0230520632872256  loss:  19.9563710820462&quot;
## [1] &quot;step =  2  lambda =  0.0230520632872256  loss:  19.9739379161218&quot;
## [1] &quot;step =  1  lambda =  0.022822691425093  loss:  19.7926582450259&quot;
## [1] &quot;step =  2  lambda =  0.022822691425093  loss:  19.8100035955848&quot;
## [1] &quot;step =  1  lambda =  0.0225956018511219  loss:  19.6303464727975&quot;
## [1] &quot;step =  2  lambda =  0.0225956018511219  loss:  19.6474727516918&quot;
## [1] &quot;step =  1  lambda =  0.0223707718561656  loss:  19.469424249058&quot;
## [1] &quot;step =  2  lambda =  0.0223707718561656  loss:  19.4863338537457&quot;
## [1] &quot;step =  1  lambda =  0.0221481789570373  loss:  19.3098801575571&quot;
## [1] &quot;step =  2  lambda =  0.0221481789570373  loss:  19.3265754708221&quot;
## [1] &quot;step =  1  lambda =  0.0219278008942616  loss:  19.1517028808314&quot;
## [1] &quot;step =  2  lambda =  0.0219278008942616  loss:  19.1681862705154&quot;
## [1] &quot;step =  1  lambda =  0.0217096156298486  loss:  18.9948811989626&quot;
## [1] &quot;step =  2  lambda =  0.0217096156298486  loss:  19.0111550177084&quot;
## [1] &quot;step =  1  lambda =  0.0214936013450899  loss:  18.8394039883595&quot;
## [1] &quot;step =  2  lambda =  0.0214936013450899  loss:  18.8554705733668&quot;
## [1] &quot;step =  1  lambda =  0.0212797364383772  loss:  18.6852602205634&quot;
## [1] &quot;step =  2  lambda =  0.0212797364383772  loss:  18.7011218933559&quot;
## [1] &quot;step =  1  lambda =  0.0210679995230414  loss:  18.5324389610778&quot;
## [1] &quot;step =  2  lambda =  0.0210679995230414  loss:  18.5480980272814&quot;
## [1] &quot;step =  1  lambda =  0.0208583694252147  loss:  18.3809293682192&quot;
## [1] &quot;step =  2  lambda =  0.0208583694252147  loss:  18.3963881173524&quot;
## [1] &quot;step =  1  lambda =  0.0206508251817126  loss:  18.230720691992&quot;
## [1] &quot;step =  2  lambda =  0.0206508251817126  loss:  18.2459813972664&quot;
## [1] &quot;step =  1  lambda =  0.0204453460379377  loss:  18.0818022729845&quot;
## [1] &quot;step =  2  lambda =  0.0204453460379377  loss:  18.0968671911172&quot;
## [1] &quot;step =  1  lambda =  0.0202419114458044  loss:  17.934163541287&quot;
## [1] &quot;step =  2  lambda =  0.0202419114458044  loss:  17.9490349123229&quot;
## [1] &quot;step =  1  lambda =  0.020040501061684  loss:  17.7877940154306&quot;
## [1] &quot;step =  2  lambda =  0.020040501061684  loss:  17.8024740625763&quot;
## [1] &quot;step =  1  lambda =  0.0198410947443703  loss:  17.6426833013481&quot;
## [1] &quot;step =  2  lambda =  0.0198410947443703  loss:  17.6571742308155&quot;
## [1] &quot;step =  1  lambda =  0.0196436725530653  loss:  17.4988210913539&quot;
## [1] &quot;step =  2  lambda =  0.0196436725530653  loss:  17.513125092214&quot;
## [1] &quot;step =  1  lambda =  0.0194482147453854  loss:  17.3561971631453&quot;
## [1] &quot;step =  2  lambda =  0.0194482147453854  loss:  17.3703164071927&quot;
## [1] &quot;step =  1  lambda =  0.0192547017753869  loss:  17.2148013788227&quot;
## [1] &quot;step =  2  lambda =  0.0192547017753869  loss:  17.2287380204493&quot;
## [1] &quot;step =  1  lambda =  0.0190631142916116  loss:  17.0746236839298&quot;
## [1] &quot;step =  2  lambda =  0.0190631142916116  loss:  17.0883798600086&quot;
## [1] &quot;step =  1  lambda =  0.0188734331351515  loss:  16.9356541065123&quot;
## [1] &quot;step =  2  lambda =  0.0188734331351515  loss:  16.9492319362902&quot;
## [1] &quot;step =  1  lambda =  0.0186856393377328  loss:  16.7978827561952&quot;
## [1] &quot;step =  2  lambda =  0.0186856393377328  loss:  16.8112843411956&quot;
## [1] &quot;step =  1  lambda =  0.0184997141198192  loss:  16.6612998232787&quot;
## [1] &quot;step =  2  lambda =  0.0184997141198192  loss:  16.6745272472129&quot;
## [1] &quot;step =  1  lambda =  0.0183156388887342  loss:  16.5258955778516&quot;
## [1] &quot;step =  2  lambda =  0.0183156388887342  loss:  16.5389509065389&quot;
## [1] &quot;step =  1  lambda =  0.0181333952368011  loss:  16.3916603689222&quot;
## [1] &quot;step =  2  lambda =  0.0181333952368011  loss:  16.4045456502187&quot;
## [1] &quot;step =  1  lambda =  0.0179529649395029  loss:  16.2585846235665&quot;
## [1] &quot;step =  2  lambda =  0.0179529649395029  loss:  16.2713018873026&quot;
## [1] &quot;step =  1  lambda =  0.0177743299536594  loss:  16.1266588460925&quot;
## [1] &quot;step =  2  lambda =  0.0177743299536594  loss:  16.1392101040184&quot;
## [1] &quot;step =  1  lambda =  0.0175974724156234  loss:  15.9958736172217&quot;
## [1] &quot;step =  2  lambda =  0.0175974724156234  loss:  16.0082608629609&quot;
## [1] &quot;step =  1  lambda =  0.0174223746394935  loss:  15.8662195932859&quot;
## [1] &quot;step =  2  lambda =  0.0174223746394935  loss:  15.8784448022967&quot;
## [1] &quot;step =  1  lambda =  0.0172490191153463  loss:  15.7376875054399&quot;
## [1] &quot;step =  2  lambda =  0.0172490191153463  loss:  15.7497526349842&quot;
## [1] &quot;step =  1  lambda =  0.0170773885074848  loss:  15.6102681588893&quot;
## [1] &quot;step =  2  lambda =  0.0170773885074848  loss:  15.6221751480092&quot;
## [1] &quot;step =  1  lambda =  0.0169074656527053  loss:  15.4839524321333&quot;
## [1] &quot;step =  2  lambda =  0.0169074656527053  loss:  15.4957032016348&quot;
## [1] &quot;step =  1  lambda =  0.0167392335585806  loss:  15.3587312762224&quot;
## [1] &quot;step =  2  lambda =  0.0167392335585806  loss:  15.3703277286655&quot;
## [1] &quot;step =  1  lambda =  0.0165726754017613  loss:  15.2345957140293&quot;
## [1] &quot;step =  2  lambda =  0.0165726754017613  loss:  15.2460397337259&quot;
## [1] &quot;step =  1  lambda =  0.0164077745262926  loss:  15.1115368395349&quot;
## [1] &quot;step =  2  lambda =  0.0164077745262926  loss:  15.1228302925525&quot;
## [1] &quot;step =  1  lambda =  0.0162445144419499  loss:  14.9895458171269&quot;
## [1] &quot;step =  2  lambda =  0.0162445144419499  loss:  15.000690551299&quot;
## [1] &quot;step =  1  lambda =  0.0160828788225884  loss:  14.8686138809121&quot;
## [1] &quot;step =  2  lambda =  0.0160828788225884  loss:  14.8796117258547&quot;
## [1] &quot;step =  1  lambda =  0.0159228515045117  loss:  14.7487323340415&quot;
## [1] &quot;step =  2  lambda =  0.0159228515045117  loss:  14.7595851011758&quot;
## [1] &quot;step =  1  lambda =  0.0157644164848545  loss:  14.6298925480476&quot;
## [1] &quot;step =  2  lambda =  0.0157644164848545  loss:  14.640602030628&quot;
## [1] &quot;step =  1  lambda =  0.0156075579199828  loss:  14.5120859621945&quot;
## [1] &quot;step =  2  lambda =  0.0156075579199828  loss:  14.5226539353429&quot;
## [1] &quot;step =  1  lambda =  0.0154522601239095  loss:  14.3953040828396&quot;
## [1] &quot;step =  2  lambda =  0.0154522601239095  loss:  14.4057323035846&quot;
## [1] &quot;step =  1  lambda =  0.0152985075667255  loss:  14.2795384828069&quot;
## [1] &quot;step =  2  lambda =  0.0152985075667255  loss:  14.2898286901283&quot;
## [1] &quot;step =  1  lambda =  0.015146284873047  loss:  14.1647808007718&quot;
## [1] &quot;step =  2  lambda =  0.015146284873047  loss:  14.1749347156502&quot;
## [1] &quot;step =  1  lambda =  0.0149955768204777  loss:  14.0510227406567&quot;
## [1] &quot;step =  2  lambda =  0.0149955768204777  loss:  14.0610420661283&quot;
## [1] &quot;step =  1  lambda =  0.0148463683380868  loss:  13.9382560710377&quot;
## [1] &quot;step =  2  lambda =  0.0148463683380868  loss:  13.9481424922531&quot;
## [1] &quot;step =  1  lambda =  0.0146986445049018  loss:  13.8264726245617&quot;
## [1] &quot;step =  2  lambda =  0.0146986445049018  loss:  13.8362278088496&quot;
## [1] &quot;step =  1  lambda =  0.0145523905484161  loss:  13.7156642973729&quot;
## [1] &quot;step =  2  lambda =  0.0145523905484161  loss:  13.7252898943085&quot;
## [1] &quot;step =  1  lambda =  0.0144075918431123  loss:  13.6058230485506&quot;
## [1] &quot;step =  2  lambda =  0.0144075918431123  loss:  13.6153206900274&quot;
## [1] &quot;step =  1  lambda =  0.0142642339089993  loss:  13.4969408995558&quot;
## [1] &quot;step =  2  lambda =  0.0142642339089993  loss:  13.5063121998616&quot;
## [1] &quot;step =  1  lambda =  0.014122302410164  loss:  13.3890099336867&quot;
## [1] &quot;step =  2  lambda =  0.014122302410164  loss:  13.3982564895841&quot;
## [1] &quot;step =  1  lambda =  0.0139817831533383  loss:  13.2820222955449&quot;
## [1] &quot;step =  2  lambda =  0.0139817831533383  loss:  13.2911456863547&quot;
## [1] &quot;step =  1  lambda =  0.0138426620864795  loss:  13.175970190509&quot;
## [1] &quot;step =  2  lambda =  0.0138426620864795  loss:  13.1849719781971&quot;
## [1] &quot;step =  1  lambda =  0.0137049252973649  loss:  13.0708458842172&quot;
## [1] &quot;step =  2  lambda =  0.0137049252973649  loss:  13.0797276134857&quot;
## [1] &quot;step =  1  lambda =  0.0135685590122009  loss:  12.9666417020592&quot;
## [1] &quot;step =  2  lambda =  0.0135685590122009  loss:  12.9754049004397&quot;
## [1] &quot;step =  1  lambda =  0.0134335495942453  loss:  12.8633500286751&quot;
## [1] &quot;step =  2  lambda =  0.0134335495942453  loss:  12.8719962066255&quot;
## [1] &quot;step =  1  lambda =  0.0132998835424438  loss:  12.7609633074628&quot;
## [1] &quot;step =  2  lambda =  0.0132998835424438  loss:  12.7694939584673&quot;
## [1] &quot;step =  1  lambda =  0.0131675474900798  loss:  12.6594740400932&quot;
## [1] &quot;step =  2  lambda =  0.0131675474900798  loss:  12.6678906407646&quot;
## [1] &quot;step =  1  lambda =  0.0130365282034377  loss:  12.5588747860329&quot;
## [1] &quot;step =  2  lambda =  0.0130365282034377  loss:  12.5671787962175&quot;
## [1] &quot;step =  1  lambda =  0.0129068125804799  loss:  12.4591581620739&quot;
## [1] &quot;step =  2  lambda =  0.0129068125804799  loss:  12.4673510249593&quot;
## [1] &quot;step =  1  lambda =  0.0127783876495358  loss:  12.3603168418705&quot;
## [1] &quot;step =  2  lambda =  0.0127783876495358  loss:  12.3683999840957&quot;
## [1] &quot;step =  1  lambda =  0.0126512405680053  loss:  12.2623435554838&quot;
## [1] &quot;step =  2  lambda =  0.0126512405680053  loss:  12.2703183872511&quot;
## [1] &quot;step =  1  lambda =  0.0125253586210744  loss:  12.1652310889316&quot;
## [1] &quot;step =  2  lambda =  0.0125253586210744  loss:  12.1730990041211&quot;
## [1] &quot;step =  1  lambda =  0.0124007292204434  loss:  12.0689722837462&quot;
## [1] &quot;step =  2  lambda =  0.0124007292204434  loss:  12.0767346600318&quot;
## [1] &quot;step =  1  lambda =  0.0122773399030684  loss:  11.9735600365373&quot;
## [1] &quot;step =  2  lambda =  0.0122773399030684  loss:  11.9812182355054&quot;
## [1] &quot;step =  1  lambda =  0.0121551783299149  loss:  11.878987298562&quot;
## [1] &quot;step =  2  lambda =  0.0121551783299149  loss:  11.8868151036378&quot;
## [1] &quot;step =  1  lambda =  0.0120342322847238  loss:  11.7855151466616&quot;
## [1] &quot;step =  2  lambda =  0.0120342322847238  loss:  11.7933470058686&quot;
## [1] &quot;step =  1  lambda =  0.0119144896727896  loss:  11.6929712647807&quot;
## [1] &quot;step =  2  lambda =  0.0119144896727896  loss:  11.7006457555489&quot;
## [1] &quot;step =  1  lambda =  0.0117959385197516  loss:  11.6011865109415&quot;
## [1] &quot;step =  2  lambda =  0.0117959385197516  loss:  11.6087171705993&quot;
## [1] &quot;step =  1  lambda =  0.0116785669703954  loss:  11.5101666471414&quot;
## [1] &quot;step =  2  lambda =  0.0116785669703954  loss:  11.5175636126041&quot;
## [1] &quot;step =  1  lambda =  0.0115623632874685  loss:  11.4199140267605&quot;
## [1] &quot;step =  2  lambda =  0.0115623632874685  loss:  11.4271842984255&quot;
## [1] &quot;step =  1  lambda =  0.0114473158505057  loss:  11.330427893959&quot;
## [1] &quot;step =  2  lambda =  0.0114473158505057  loss:  11.33757683371&quot;
## [1] &quot;step =  1  lambda =  0.0113334131546674  loss:  11.241705892593&quot;
## [1] &quot;step =  2  lambda =  0.0113334131546674  loss:  11.248738089373&quot;
## [1] &quot;step =  1  lambda =  0.0112206438095891  loss:  11.153744935133&quot;
## [1] &quot;step =  2  lambda =  0.0112206438095891  loss:  11.1606645126762&quot;
## [1] &quot;step =  1  lambda =  0.0111089965382423  loss:  11.0665415122078&quot;
## [1] &quot;step =  2  lambda =  0.0111089965382423  loss:  11.0733522239901&quot;
## [1] &quot;step =  1  lambda =  0.0109984601758069  loss:  10.9800917892801&quot;
## [1] &quot;step =  2  lambda =  0.0109984601758069  loss:  10.9867970554003&quot;
## [1] &quot;step =  1  lambda =  0.0108890236685545  loss:  10.8943916453198&quot;
## [1] &quot;step =  2  lambda =  0.0108890236685545  loss:  10.9009945802973&quot;
## [1] &quot;step =  1  lambda =  0.0107806760727431  loss:  10.8094367023467&quot;
## [1] &quot;step =  2  lambda =  0.0107806760727431  loss:  10.815940144716&quot;
## [1] &quot;step =  1  lambda =  0.0106734065535229  loss:  10.7252223566122&quot;
## [1] &quot;step =  2  lambda =  0.0106734065535229  loss:  10.7316289002048&quot;
## [1] &quot;step =  1  lambda =  0.0105672043838527  loss:  10.6417438112501&quot;
## [1] &quot;step =  2  lambda =  0.0105672043838527  loss:  10.6480558360201&quot;
## [1] &quot;step =  1  lambda =  0.0104620589434268  loss:  10.5589961082318&quot;
## [1] &quot;step =  2  lambda =  0.0104620589434268  loss:  10.5652158089083&quot;
## [1] &quot;step =  1  lambda =  0.0103579597176137  loss:  10.4769741579206&quot;
## [1] &quot;step =  2  lambda =  0.0103579597176137  loss:  10.4831035695757&quot;
## [1] &quot;step =  1  lambda =  0.010254896296404  loss:  10.3956727653341&quot;
## [1] &quot;step =  2  lambda =  0.010254896296404  loss:  10.4017137855896&quot;
## [1] &quot;step =  1  lambda =  0.0101528583733698  loss:  10.3150866528664&quot;
## [1] &quot;step =  2  lambda =  0.0101528583733698  loss:  10.3210410608481&quot;
## [1] &quot;step =  1  lambda =  0.0100518357446336  loss:  10.2352104796065&quot;
## [1] &quot;step =  2  lambda =  0.0100518357446336  loss:  10.2410799519491&quot;
## [1] &quot;step =  1  lambda =  0.00995181830784842  loss:  10.156038857582&quot;
## [1] &quot;step =  2  lambda =  0.00995181830784842  loss:  10.1618249818622&quot;
## [1] &quot;step =  1  lambda =  0.00985279606118726  loss:  10.0775663653263&quot;
## [1] &quot;step =  2  lambda =  0.00985279606118726  loss:  10.0832706513057&quot;
## [1] &quot;step =  1  lambda =  0.0097547591023429  loss:  9.99978755917164&quot;
## [1] &quot;step =  2  lambda =  0.0097547591023429  loss:  10.0054114482012&quot;
## [1] &quot;step =  1  lambda =  0.00965769762753778  loss:  9.92269698263388&quot;
## [1] &quot;step =  2  lambda =  0.00965769762753778  loss:  9.92824185552901&quot;
## [1] &quot;step =  1  lambda =  0.00956160193054351  loss:  9.84628917421094&quot;
## [1] &quot;step =  2  lambda =  0.00956160193054351  loss:  9.85175635786097&quot;
## [1] &quot;step =  1  lambda =  0.00946646240171032  loss:  9.77055867386965&quot;
## [1] &quot;step =  2  lambda =  0.00946646240171032  loss:  9.77594944680293&quot;
## [1] &quot;step =  1  lambda =  0.00937226952700606  loss:  9.69550002844954&quot;
## [1] &quot;step =  2  lambda =  0.00937226952700606  loss:  9.70081562553689&quot;
## [1] &quot;step =  1  lambda =  0.00927901388706474  loss:  9.62110779617362&quot;
## [1] &quot;step =  2  lambda =  0.00927901388706474  loss:  9.62634941262141&quot;
## [1] &quot;step =  1  lambda =  0.00918668615624467  loss:  9.54737655042276&quot;
## [1] &quot;step =  2  lambda =  0.00918668615624467  loss:  9.55254534517932&quot;
## [1] &quot;step =  1  lambda =  0.00909527710169582  loss:  9.47430088290165&quot;
## [1] &quot;step =  2  lambda =  0.00909527710169582  loss:  9.47939798157884&quot;
## [1] &quot;step =  1  lambda =  0.00900477758243656  loss:  9.40187540630177&quot;
## [1] &quot;step =  2  lambda =  0.00900477758243656  loss:  9.40690190369457&quot;
## [1] &quot;step =  1  lambda =  0.00891517854843955  loss:  9.33009475654707&quot;
## [1] &quot;step =  2  lambda =  0.00891517854843955  loss:  9.33505171881945&quot;
## [1] &quot;step =  1  lambda =  0.00882647103972673  loss:  9.25895359469312&quot;
## [1] &quot;step =  2  lambda =  0.00882647103972673  loss:  9.2638420612859&quot;
## [1] &quot;step =  1  lambda =  0.00873864618547329  loss:  9.18844660853733&quot;
## [1] &quot;step =  2  lambda =  0.00873864618547329  loss:  9.19326759384347&quot;
## [1] &quot;step =  1  lambda =  0.00865169520312063  loss:  9.11856851398715&quot;
## [1] &quot;step =  2  lambda =  0.00865169520312063  loss:  9.12332300883216&quot;
## [1] &quot;step =  1  lambda =  0.00856560939749806  loss:  9.04931405622556&quot;
## [1] &quot;step =  2  lambda =  0.00856560939749806  loss:  9.05400302918337&quot;
## [1] &quot;step =  1  lambda =  0.00848038015995327  loss:  8.98067801070516&quot;
## [1] &quot;step =  2  lambda =  0.00848038015995327  loss:  8.98530240927498&quot;
## [1] &quot;step =  1  lambda =  0.00839599896749147  loss:  8.91265518399734&quot;
## [1] &quot;step =  2  lambda =  0.00839599896749147  loss:  8.91721593566207&quot;
## [1] &quot;step =  1  lambda =  0.00831245738192312  loss:  8.84524041451798&quot;
## [1] &quot;step =  2  lambda =  0.00831245738192312  loss:  8.84973842770114&quot;
## [1] &quot;step =  1  lambda =  0.00822974704902003  loss:  8.77842857314736&quot;
## [1] &quot;step =  2  lambda =  0.00822974704902003  loss:  8.7828647380829&quot;
## [1] &quot;step =  1  lambda =  0.00814785969767999  loss:  8.71221456375919&quot;
## [1] &quot;step =  2  lambda =  0.00814785969767999  loss:  8.71658975328572&quot;
## [1] &quot;step =  1  lambda =  0.00806678713909961  loss:  8.6465933236709&quot;
## [1] &quot;step =  2  lambda =  0.00806678713909961  loss:  8.65090839395997&quot;
## [1] &quot;step =  1  lambda =  0.0079865212659555  loss:  8.58155982402539&quot;
## [1] &quot;step =  2  lambda =  0.0079865212659555  loss:  8.58581561525211&quot;
## [1] &quot;step =  1  lambda =  0.00790705405159344  loss:  8.51710907011267&quot;
## [1] &quot;step =  2  lambda =  0.00790705405159344  loss:  8.52130640707524&quot;
## [1] &quot;step =  1  lambda =  0.00782837754922577  loss:  8.45323610163868&quot;
## [1] &quot;step =  2  lambda =  0.00782837754922577  loss:  8.45737579433277&quot;
## [1] &quot;step =  1  lambda =  0.00775048389113669  loss:  8.38993599294738&quot;
## [1] &quot;step =  2  lambda =  0.00775048389113669  loss:  8.39401883709988&quot;
## [1] &quot;step =  1  lambda =  0.00767336528789549  loss:  8.32720385320106&quot;
## [1] &quot;step =  2  lambda =  0.00767336528789549  loss:  8.33123063076764&quot;
## [1] &quot;step =  1  lambda =  0.00759701402757757  loss:  8.26503482652339&quot;
## [1] &quot;step =  2  lambda =  0.00759701402757757  loss:  8.26900630615317&quot;
## [1] &quot;step =  1  lambda =  0.00752142247499327  loss:  8.20342409210907&quot;
## [1] &quot;step =  2  lambda =  0.00752142247499327  loss:  8.2073410295796&quot;
## [1] &quot;step =  1  lambda =  0.00744658307092434  loss:  8.1423668643033&quot;
## [1] &quot;step =  2  lambda =  0.00744658307092434  loss:  8.14623000292826&quot;
## [1] &quot;step =  1  lambda =  0.00737248833136801  loss:  8.0818583926538&quot;
## [1] &quot;step =  2  lambda =  0.00737248833136801  loss:  8.08566846366606&quot;
## [1] &quot;step =  1  lambda =  0.00729913084678858  loss:  8.0218939619382&quot;
## [1] &quot;step =  2  lambda =  0.00729913084678858  loss:  8.02565168485&quot;
## [1] &quot;step =  1  lambda =  0.00722650328137646  loss:  7.96246889216871&quot;
## [1] &quot;step =  2  lambda =  0.00722650328137646  loss:  7.96617497511092&quot;
## [1] &quot;step =  1  lambda =  0.00715459837231459  loss:  7.90357853857632&quot;
## [1] &quot;step =  2  lambda =  0.00715459837231459  loss:  7.90723367861842&quot;
## [1] &quot;step =  1  lambda =  0.00708340892905212  loss:  7.84521829157616&quot;
## [1] &quot;step =  2  lambda =  0.00708340892905212  loss:  7.84882317502823&quot;
## [1] &quot;step =  1  lambda =  0.00701292783258542  loss:  7.78738357671571&quot;
## [1] &quot;step =  2  lambda =  0.00701292783258542  loss:  7.79093887941393&quot;
## [1] &quot;step =  1  lambda =  0.00694314803474611  loss:  7.73006985460717&quot;
## [1] &quot;step =  2  lambda =  0.00694314803474611  loss:  7.73357624218398&quot;
## [1] &quot;step =  1  lambda =  0.00687406255749626  loss:  7.67327262084557&quot;
## [1] &quot;step =  2  lambda =  0.00687406255749626  loss:  7.67673074898549&quot;
## [1] &quot;step =  1  lambda =  0.00680566449223054  loss:  7.61698740591357&quot;
## [1] &quot;step =  2  lambda =  0.00680566449223054  loss:  7.62039792059585&quot;
## [1] &quot;step =  1  lambda =  0.00673794699908547  loss:  7.56120977507428&quot;
## [1] &quot;step =  2  lambda =  0.00673794699908547  loss:  7.5645733128031&quot;
## [1] &quot;step =  1  lambda =  0.00667090330625527  loss:  7.50593532825289&quot;
## [1] &quot;step =  2  lambda =  0.00667090330625527  loss:  7.50925251627621&quot;
## [1] &quot;step =  1  lambda =  0.00660452670931481  loss:  7.45115969990832&quot;
## [1] &quot;step =  2  lambda =  0.00660452670931481  loss:  7.45443115642589&quot;
## [1] &quot;step =  1  lambda =  0.00653881057054906  loss:  7.39687855889556&quot;
## [1] &quot;step =  2  lambda =  0.00653881057054906  loss:  7.40011876854921&quot;
## [1] &quot;step =  1  lambda =  0.0064737483182894  loss:  7.34310190347213&quot;
## [1] &quot;step =  2  lambda =  0.0064737483182894  loss:  7.34624187136814&quot;
## [1] &quot;step =  1  lambda =  0.00640933344625638  loss:  7.28975588724052&quot;
## [1] &quot;step =  2  lambda =  0.00640933344625638  loss:  7.29284465207929&quot;
## [1] &quot;step =  1  lambda =  0.00634555951290912  loss:  7.23688471883208&quot;
## [1] &quot;step =  2  lambda =  0.00634555951290912  loss:  7.23992947018352&quot;
## [1] &quot;step =  1  lambda =  0.00628242014080112  loss:  7.18449079342671&quot;
## [1] &quot;step =  2  lambda =  0.00628242014080112  loss:  7.18749350071741&quot;
## [1] &quot;step =  1  lambda =  0.00621990901594257  loss:  7.13257132217909&quot;
## [1] &quot;step =  2  lambda =  0.00621990901594257  loss:  7.13553336940803&quot;
## [1] &quot;step =  1  lambda =  0.0061580198871689  loss:  7.08112296799876&quot;
## [1] &quot;step =  2  lambda =  0.0061580198871689  loss:  7.08404546434946&quot;
## [1] &quot;step =  1  lambda =  0.00609674656551564  loss:  7.0301421575204&quot;
## [1] &quot;step =  2  lambda =  0.00609674656551564  loss:  7.03302598545049&quot;
## [1] &quot;step =  1  lambda =  0.00603608292359956  loss:  6.97962513040881&quot;
## [1] &quot;step =  2  lambda =  0.00603608292359956  loss:  6.98247099488975&quot;
## [1] &quot;step =  1  lambda =  0.00597602289500594  loss:  6.92956798943602&quot;
## [1] &quot;step =  2  lambda =  0.00597602289500594  loss:  6.93237646711959&quot;
## [1] &quot;step =  1  lambda =  0.00591656047368186  loss:  6.87996675009784&quot;
## [1] &quot;step =  2  lambda =  0.00591656047368186  loss:  6.88273832979353&quot;
## [1] &quot;step =  1  lambda =  0.00585768971333562  loss:  6.83081738123114&quot;
## [1] &quot;step =  2  lambda =  0.00585768971333562  loss:  6.83355249397363&quot;
## [1] &quot;step =  1  lambda =  0.00579940472684215  loss:  6.78211583500028&quot;
## [1] &quot;step =  2  lambda =  0.00579940472684215  loss:  6.78481487517715&quot;
## [1] &quot;step =  1  lambda =  0.0057416996856542  loss:  6.73385806779719&quot;
## [1] &quot;step =  2  lambda =  0.0057416996856542  loss:  6.73652140751273&quot;
## [1] &quot;step =  1  lambda =  0.0056845688192196  loss:  6.68604005428565&quot;
## [1] &quot;step =  2  lambda =  0.0056845688192196  loss:  6.68866805290338&quot;
## [1] &quot;step =  1  lambda =  0.00562800641440407  loss:  6.63865779656997&quot;
## [1] &quot;step =  2  lambda =  0.00562800641440407  loss:  6.64125080692166&quot;
## [1] &quot;step =  1  lambda =  0.00557200681492  loss:  6.59170733000072&quot;
## [1] &quot;step =  2  lambda =  0.00557200681492  loss:  6.59426570232694&quot;
## [1] &quot;step =  1  lambda =  0.00551656442076077  loss:  6.54518472669831&quot;
## [1] &quot;step =  2  lambda =  0.00551656442076077  loss:  6.54770881105848&quot;
## [1] &quot;step =  1  lambda =  0.00546167368764078  loss:  6.49908609754228&quot;
## [1] &quot;step =  2  lambda =  0.00546167368764078  loss:  6.50157624519812&quot;
## [1] &quot;step =  1  lambda =  0.00540732912644096  loss:  6.45340759313627&quot;
## [1] &quot;step =  2  lambda =  0.00540732912644096  loss:  6.4558641572512&quot;
## [1] &quot;step =  1  lambda =  0.00535352530265991  loss:  6.40814540409473&quot;
## [1] &quot;step =  2  lambda =  0.00535352530265991  loss:  6.41056873998209&quot;
## [1] &quot;step =  1  lambda =  0.0053002568358704  loss:  6.36329576088607&quot;
## [1] &quot;step =  2  lambda =  0.0053002568358704  loss:  6.36568622596482&quot;
## [1] &quot;step =  1  lambda =  0.00524751839918138  loss:  6.31885493339208&quot;
## [1] &quot;step =  2  lambda =  0.00524751839918138  loss:  6.32121288695838&quot;
## [1] &quot;step =  1  lambda =  0.00519530471870523  loss:  6.27481923029219&quot;
## [1] &quot;step =  2  lambda =  0.00519530471870523  loss:  6.27714503318094&quot;
## [1] &quot;step =  1  lambda =  0.00514361057303038  loss:  6.23118499834675&quot;
## [1] &quot;step =  2  lambda =  0.00514361057303038  loss:  6.2334790125337&quot;
## [1] &quot;step =  1  lambda =  0.00509243079269919  loss:  6.18794862162959&quot;
## [1] &quot;step =  2  lambda =  0.00509243079269919  loss:  6.19021120980786&quot;
## [1] &quot;step =  1  lambda =  0.00504176025969098  loss:  6.14510652074353&quot;
## [1] &quot;step =  2  lambda =  0.00504176025969098  loss:  6.14733804589766&quot;
## [1] &quot;step =  1  lambda =  0.00499159390691022  loss:  6.10265515204123&quot;
## [1] &quot;step =  2  lambda =  0.00499159390691022  loss:  6.10485597703331&quot;
## [1] &quot;step =  1  lambda =  0.00494192671767982  loss:  6.06059100686587&quot;
## [1] &quot;step =  2  lambda =  0.00494192671767982  loss:  6.06276149404296&quot;
## [1] &quot;step =  1  lambda =  0.00489275372523948  loss:  6.01891061082006&quot;
## [1] &quot;step =  2  lambda =  0.00489275372523948  loss:  6.02105112164851&quot;
## [1] &quot;step =  1  lambda =  0.00484407001224897  loss:  5.97761052306836&quot;
## [1] &quot;step =  2  lambda =  0.00484407001224897  loss:  5.97972141779743&quot;
## [1] &quot;step =  1  lambda =  0.00479587071029642  loss:  5.93668733567539&quot;
## [1] &quot;step =  2  lambda =  0.00479587071029642  loss:  5.93876897303121&quot;
## [1] &quot;step =  1  lambda =  0.00474815099941148  loss:  5.89613767298008&quot;
## [1] &quot;step =  2  lambda =  0.00474815099941148  loss:  5.89819040988962&quot;
## [1] &quot;step =  1  lambda =  0.00470090610758328  loss:  5.85595819100551&quot;
## [1] &quot;step =  2  lambda =  0.00470090610758328  loss:  5.8579823823494&quot;
## [1] &quot;step =  1  lambda =  0.00465413131028327  loss:  5.81614557690275&quot;
## [1] &quot;step =  2  lambda =  0.00465413131028327  loss:  5.81814157529538&quot;
## [1] &quot;step =  1  lambda =  0.00460782192999275  loss:  5.77669654842684&quot;
## [1] &quot;step =  2  lambda =  0.00460782192999275  loss:  5.77866470402189&quot;
## [1] &quot;step =  1  lambda =  0.0045619733357351  loss:  5.73760785344289&quot;
## [1] &quot;step =  2  lambda =  0.0045619733357351  loss:  5.73954851376244&quot;
## [1] &quot;step =  1  lambda =  0.00451658094261267  loss:  5.69887626946006&quot;
## [1] &quot;step =  2  lambda =  0.00451658094261267  loss:  5.70078977924507&quot;
## [1] &quot;step =  1  lambda =  0.00447164021134833  loss:  5.6604986031911&quot;
## [1] &quot;step =  2  lambda =  0.00447164021134833  loss:  5.66238530427161&quot;
## [1] &quot;step =  1  lambda =  0.00442714664783151  loss:  5.6224716901355&quot;
## [1] &quot;step =  2  lambda =  0.00442714664783151  loss:  5.62433192131853&quot;
## [1] &quot;step =  1  lambda =  0.00438309580266878  loss:  5.58479239418412&quot;
## [1] &quot;step =  2  lambda =  0.00438309580266878  loss:  5.5866264911576&quot;
## [1] &quot;step =  1  lambda =  0.0043394832707389  loss:  5.54745760724346&quot;
## [1] &quot;step =  2  lambda =  0.0043394832707389  loss:  5.54926590249466&quot;
## [1] &quot;step =  1  lambda =  0.00429630469075234  loss:  5.51046424887781&quot;
## [1] &quot;step =  2  lambda =  0.00429630469075234  loss:  5.51224707162469&quot;
## [1] &quot;step =  1  lambda =  0.00425355574481513  loss:  5.47380926596763&quot;
## [1] &quot;step =  2  lambda =  0.00425355574481513  loss:  5.47556694210189&quot;
## [1] &quot;step =  1  lambda =  0.00421123215799704  loss:  5.43748963238277&quot;
## [1] &quot;step =  2  lambda =  0.00421123215799704  loss:  5.43922248442333&quot;
## [1] &quot;step =  1  lambda =  0.00416932969790412  loss:  5.40150234866922&quot;
## [1] &quot;step =  2  lambda =  0.00416932969790412  loss:  5.40321069572501&quot;
## [1] &quot;step =  1  lambda =  0.00412784417425544  loss:  5.36584444174802&quot;
## [1] &quot;step =  2  lambda =  0.00412784417425544  loss:  5.36752859948915&quot;
## [1] &quot;step =  1  lambda =  0.00408677143846407  loss:  5.33051296462546&quot;
## [1] &quot;step =  2  lambda =  0.00408677143846407  loss:  5.33217324526188&quot;
## [1] &quot;step =  1  lambda =  0.0040461073832222  loss:  5.29550499611344&quot;
## [1] &quot;step =  2  lambda =  0.0040461073832222  loss:  5.29714170838018&quot;
## [1] &quot;step =  1  lambda =  0.00400584794209042  loss:  5.26081764055919&quot;
## [1] &quot;step =  2  lambda =  0.00400584794209042  loss:  5.26243108970759&quot;
## [1] &quot;step =  1  lambda =  0.00396598908909106  loss:  5.22644802758343&quot;
## [1] &quot;step =  2  lambda =  0.00396598908909106  loss:  5.22803851537772&quot;
## [1] &quot;step =  1  lambda =  0.00392652683830562  loss:  5.19239331182648&quot;
## [1] &quot;step =  2  lambda =  0.00392652683830562  loss:  5.19396113654498&quot;
## [1] &quot;step =  1  lambda =  0.00388745724347613  loss:  5.15865067270143&quot;
## [1] &quot;step =  2  lambda =  0.00388745724347613  loss:  5.16019612914205&quot;
## [1] &quot;step =  1  lambda =  0.00384877639761054  loss:  5.12521731415395&quot;
## [1] &quot;step =  2  lambda =  0.00384877639761054  loss:  5.12674069364332&quot;
## [1] &quot;step =  1  lambda =  0.00381048043259204  loss:  5.09209046442814&quot;
## [1] &quot;step =  2  lambda =  0.00381048043259204  loss:  5.09359205483417&quot;
## [1] &quot;step =  1  lambda =  0.00377256551879221  loss:  5.05926737583803&quot;
## [1] &quot;step =  2  lambda =  0.00377256551879221  loss:  5.06074746158534&quot;
## [1] &quot;step =  1  lambda =  0.00373502786468807  loss:  5.02674532454418&quot;
## [1] &quot;step =  2  lambda =  0.00373502786468807  loss:  5.02820418663218&quot;
## [1] &quot;step =  1  lambda =  0.00369786371648293  loss:  4.99452161033517&quot;
## [1] &quot;step =  2  lambda =  0.00369786371648293  loss:  4.99595952635837&quot;
## [1] &quot;step =  1  lambda =  0.00366106935773101  loss:  4.96259355641339&quot;
## [1] &quot;step =  2  lambda =  0.00366106935773101  loss:  4.96401080058389&quot;
## [1] &quot;step =  1  lambda =  0.00362464110896576  loss:  4.93095850918518&quot;
## [1] &quot;step =  2  lambda =  0.00362464110896576  loss:  4.93235535235682&quot;
## [1] &quot;step =  1  lambda =  0.00358857532733195  loss:  4.89961383805464&quot;
## [1] &quot;step =  2  lambda =  0.00358857532733195  loss:  4.90099054774871&quot;
## [1] &quot;step =  1  lambda =  0.00355286840622136  loss:  4.86855693522108&quot;
## [1] &quot;step =  2  lambda =  0.00355286840622136  loss:  4.86991377565352&quot;
## [1] &quot;step =  1  lambda =  0.00351751677491213  loss:  4.83778521547992&quot;
## [1] &quot;step =  2  lambda =  0.00351751677491213  loss:  4.83912244758959&quot;
## [1] &quot;step =  1  lambda =  0.00348251689821166  loss:  4.80729611602666&quot;
## [1] &quot;step =  2  lambda =  0.00348251689821166  loss:  4.80861399750473&quot;
## [1] &quot;step =  1  lambda =  0.00344786527610313  loss:  4.77708709626388&quot;
## [1] &quot;step =  2  lambda =  0.00344786527610313  loss:  4.77838588158406&quot;
## [1] &quot;step =  1  lambda =  0.00341355844339543  loss:  4.74715563761099&quot;
## [1] &quot;step =  2  lambda =  0.00341355844339543  loss:  4.74843557806051&quot;
## [1] &quot;step =  1  lambda =  0.00337959296937672  loss:  4.71749924331662&quot;
## [1] &quot;step =  2  lambda =  0.00337959296937672  loss:  4.7187605870279&quot;
## [1] &quot;step =  1  lambda =  0.00334596545747127  loss:  4.68811543827357&quot;
## [1] &quot;step =  2  lambda =  0.00334596545747127  loss:  4.68935843025638&quot;
## [1] &quot;step =  1  lambda =  0.00331267254489989  loss:  4.6590017688361&quot;
## [1] &quot;step =  2  lambda =  0.00331267254489989  loss:  4.66022665101013&quot;
## [1] &quot;step =  1  lambda =  0.00327971090234357  loss:  4.63015580263937&quot;
## [1] &quot;step =  2  lambda =  0.00327971090234357  loss:  4.63136281386725&quot;
## [1] &quot;step =  1  lambda =  0.00324707723361059  loss:  4.6015751284212&quot;
## [1] &quot;step =  2  lambda =  0.00324707723361059  loss:  4.60276450454169&quot;
## [1] &quot;step =  1  lambda =  0.00321476827530687  loss:  4.57325735584576&quot;
## [1] &quot;step =  2  lambda =  0.00321476827530687  loss:  4.57442932970721&quot;
## [1] &quot;step =  1  lambda =  0.00318278079650967  loss:  4.54520011532919&quot;
## [1] &quot;step =  2  lambda =  0.00318278079650967  loss:  4.54635491682313&quot;
## [1] &quot;step =  1  lambda =  0.00315111159844444  loss:  4.51740105786721&quot;
## [1] &quot;step =  2  lambda =  0.00315111159844444  loss:  4.51853891396198&quot;
## [1] &quot;step =  1  lambda =  0.00311975751416499  loss:  4.48985785486443&quot;
## [1] &quot;step =  2  lambda =  0.00311975751416499  loss:  4.49097898963891&quot;
## [1] &quot;step =  1  lambda =  0.00308871540823677  loss:  4.46256819796538&quot;
## [1] &quot;step =  2  lambda =  0.00308871540823677  loss:  4.4636728326427&quot;
## [1] &quot;step =  1  lambda =  0.00305798217642331  loss:  4.43552979888733&quot;
## [1] &quot;step =  2  lambda =  0.00305798217642331  loss:  4.43661815186848&quot;
## [1] &quot;step =  1  lambda =  0.00302755474537582  loss:  4.40874038925459&quot;
## [1] &quot;step =  2  lambda =  0.00302755474537582  loss:  4.40981267615195&quot;
## [1] &quot;step =  1  lambda =  0.00299743007232583  loss:  4.38219772043439&quot;
## [1] &quot;step =  2  lambda =  0.00299743007232583  loss:  4.38325415410524&quot;
## [1] &quot;step =  1  lambda =  0.00296760514478094  loss:  4.35589956337437&quot;
## [1] &quot;step =  2  lambda =  0.00296760514478094  loss:  4.35694035395408&quot;
## [1] &quot;step =  1  lambda =  0.00293807698022355  loss:  4.3298437084414&quot;
## [1] &quot;step =  2  lambda =  0.00293807698022355  loss:  4.3308690633766&quot;
## [1] &quot;step =  1  lambda =  0.00290884262581258  loss:  4.30402796526191&quot;
## [1] &quot;step =  2  lambda =  0.00290884262581258  loss:  4.30503808934338&quot;
## [1] &quot;step =  1  lambda =  0.00287989915808824  loss:  4.27845016256358&quot;
## [1] &quot;step =  2  lambda =  0.00287989915808824  loss:  4.27944525795895&quot;
## [1] &quot;step =  1  lambda =  0.00285124368267963  loss:  4.25310814801843&quot;
## [1] &quot;step =  2  lambda =  0.00285124368267963  loss:  4.2540884143046&quot;
## [1] &quot;step =  1  lambda =  0.00282287333401534  loss:  4.22799978808713&quot;
## [1] &quot;step =  2  lambda =  0.00282287333401534  loss:  4.2289654222825&quot;
## [1] &quot;step =  1  lambda =  0.00279478527503684  loss:  4.2031229678647&quot;
## [1] &quot;step =  2  lambda =  0.00279478527503684  loss:  4.20407416446113&quot;
## [1] &quot;step =  1  lambda =  0.00276697669691485  loss:  4.17847559092749&quot;
## [1] &quot;step =  2  lambda =  0.00276697669691485  loss:  4.17941254192189&quot;
## [1] &quot;step =  1  lambda =  0.00273944481876837  loss:  4.15405557918126&quot;
## [1] &quot;step =  2  lambda =  0.00273944481876837  loss:  4.154978474107&quot;
## [1] &quot;step =  1  lambda =  0.00271218688738664  loss:  4.12986087271067&quot;
## [1] &quot;step =  2  lambda =  0.00271218688738664  loss:  4.13076989866864&quot;
## [1] &quot;step =  1  lambda =  0.00268520017695382  loss:  4.10588942962979&quot;
## [1] &quot;step =  2  lambda =  0.00268520017695382  loss:  4.10678477131917&quot;
## [1] &quot;step =  1  lambda =  0.00265848198877637  loss:  4.08213922593395&quot;
## [1] &quot;step =  2  lambda =  0.00265848198877637  loss:  4.08302106568263&quot;
## [1] &quot;step =  1  lambda =  0.0026320296510132  loss:  4.05860825535261&quot;
## [1] &quot;step =  2  lambda =  0.0026320296510132  loss:  4.05947677314737&quot;
## [1] &quot;step =  1  lambda =  0.0026058405184085  loss:  4.03529452920347&quot;
## [1] &quot;step =  2  lambda =  0.0026058405184085  loss:  4.03614990271974&quot;
## [1] &quot;step =  1  lambda =  0.00257991197202718  loss:  4.01219607624765&quot;
## [1] &quot;step =  2  lambda =  0.00257991197202718  loss:  4.01303848087903&quot;
## [1] &quot;step =  1  lambda =  0.002554241418993  loss:  3.98931094254602&quot;
## [1] &quot;step =  2  lambda =  0.002554241418993  loss:  3.99014055143343&quot;
## [1] &quot;step =  1  lambda =  0.00252882629222926  loss:  3.96663719131659&quot;
## [1] &quot;step =  2  lambda =  0.00252882629222926  loss:  3.96745417537707&quot;
## [1] &quot;step =  1  lambda =  0.0025036640502021  loss:  3.94417290279303&quot;
## [1] &quot;step =  2  lambda =  0.0025036640502021  loss:  3.94497743074822&quot;
## [1] &quot;step =  1  lambda =  0.00247875217666636  loss:  3.92191617408419&quot;
## [1] &quot;step =  2  lambda =  0.00247875217666636  loss:  3.92270841248855&quot;
## [1] &quot;step =  1  lambda =  0.00245408818041392  loss:  3.89986511903475&quot;
## [1] &quot;step =  2  lambda =  0.00245408818041392  loss:  3.90064523230335&quot;
## [1] &quot;step =  1  lambda =  0.0024296695950246  loss:  3.87801786808696&quot;
## [1] &quot;step =  2  lambda =  0.0024296695950246  loss:  3.87878601852294&quot;
## [1] &quot;step =  1  lambda =  0.00240549397861951  loss:  3.85637256814323&quot;
## [1] &quot;step =  2  lambda =  0.00240549397861951  loss:  3.85712891596498&quot;
## [1] &quot;step =  1  lambda =  0.00238155891361687  loss:  3.83492738242996&quot;
## [1] &quot;step =  2  lambda =  0.00238155891361687  loss:  3.83567208579797&quot;
## [1] &quot;step =  1  lambda =  0.00235786200649023  loss:  3.81368049036228&quot;
## [1] &quot;step =  2  lambda =  0.00235786200649023  loss:  3.8144137054056&quot;
## [1] &quot;step =  1  lambda =  0.00233440088752913  loss:  3.79263008740982&quot;
## [1] &quot;step =  2  lambda =  0.00233440088752913  loss:  3.79335196825223&quot;
## [1] &quot;step =  1  lambda =  0.00231117321060213  loss:  3.77177438496349&quot;
## [1] &quot;step =  2  lambda =  0.00231117321060213  loss:  3.77248508374935&quot;
## [1] &quot;step =  1  lambda =  0.00228817665292217  loss:  3.75111161020326&quot;
## [1] &quot;step =  2  lambda =  0.00228817665292217  loss:  3.75181127712299&quot;
## [1] &quot;step =  1  lambda =  0.00226540891481432  loss:  3.73064000596687&quot;
## [1] &quot;step =  2  lambda =  0.00226540891481432  loss:  3.73132878928216&quot;
## [1] &quot;step =  1  lambda =  0.0022428677194858  loss:  3.7103578306196&quot;
## [1] &quot;step =  2  lambda =  0.0022428677194858  loss:  3.71103587668828&quot;
## [1] &quot;step =  1  lambda =  0.0022205508127983  loss:  3.69026335792498&quot;
## [1] &quot;step =  2  lambda =  0.0022205508127983  loss:  3.69093081122556&quot;
## [1] &quot;step =  1  lambda =  0.00219845596304253  loss:  3.67035487691641&quot;
## [1] &quot;step =  2  lambda =  0.00219845596304253  loss:  3.67101188007229&quot;
## [1] &quot;step =  1  lambda =  0.00217658096071513  loss:  3.65063069176983&quot;
## [1] &quot;step =  2  lambda =  0.00217658096071513  loss:  3.65127738557325&quot;
## [1] &quot;step =  1  lambda =  0.00215492361829761  loss:  3.63108912167725&quot;
## [1] &quot;step =  2  lambda =  0.00215492361829761  loss:  3.63172564511288&quot;
## [1] &quot;step =  1  lambda =  0.00213348177003771  loss:  3.61172850072131&quot;
## [1] &quot;step =  2  lambda =  0.00213348177003771  loss:  3.61235499098953&quot;
## [1] &quot;step =  1  lambda =  0.00211225327173271  loss:  3.59254717775068&quot;
## [1] &quot;step =  2  lambda =  0.00211225327173271  loss:  3.59316377029061&quot;
## [1] &quot;step =  1  lambda =  0.00209123600051511  loss:  3.57354351625646&quot;
## [1] &quot;step =  2  lambda =  0.00209123600051511  loss:  3.57415034476866&quot;
## [1] &quot;step =  1  lambda =  0.00207042785464026  loss:  3.5547158942495&quot;
## [1] &quot;step =  2  lambda =  0.00207042785464026  loss:  3.55531309071832&quot;
## [1] &quot;step =  1  lambda =  0.00204982675327624  loss:  3.53606270413859&quot;
## [1] &quot;step =  2  lambda =  0.00204982675327624  loss:  3.53665039885433&quot;
## [1] &quot;step =  1  lambda =  0.00202943063629574  loss:  3.51758235260963&quot;
## [1] &quot;step =  2  lambda =  0.00202943063629574  loss:  3.5181606741903&quot;
## [1] &quot;step =  1  lambda =  0.00200923746407006  loss:  3.49927326050564&quot;
## [1] &quot;step =  2  lambda =  0.00200923746407006  loss:  3.4998423359185&quot;
## [1] &quot;step =  1  lambda =  0.00198924521726516  loss:  3.48113386270771&quot;
## [1] &quot;step =  2  lambda =  0.00198924521726516  loss:  3.48169381729054&quot;
## [1] &quot;step =  1  lambda =  0.0019694518966397  loss:  3.46316260801686&quot;
## [1] &quot;step =  2  lambda =  0.0019694518966397  loss:  3.46371356549886&quot;
## [1] &quot;step =  1  lambda =  0.00194985552284512  loss:  3.44535795903668&quot;
## [1] &quot;step =  2  lambda =  0.00194985552284512  loss:  3.44590004155922&quot;
## [1] &quot;step =  1  lambda =  0.00193045413622771  loss:  3.42771839205707&quot;
## [1] &quot;step =  2  lambda =  0.00193045413622771  loss:  3.42825172019402&quot;
## [1] &quot;step =  1  lambda =  0.00191124579663264  loss:  3.41024239693859&quot;
## [1] &quot;step =  2  lambda =  0.00191124579663264  loss:  3.41076708971652&quot;
## [1] &quot;step =  1  lambda =  0.00189222858320994  loss:  3.39292847699791&quot;
## [1] &quot;step =  2  lambda =  0.00189222858320994  loss:  3.39344465191592&quot;
## [1] &quot;step =  1  lambda =  0.00187340059422243  loss:  3.37577514889402&quot;
## [1] &quot;step =  2  lambda =  0.00187340059422243  loss:  3.37628292194333&quot;
## [1] &quot;step =  1  lambda =  0.0018547599468555  loss:  3.35878094251524&quot;
## [1] &quot;step =  2  lambda =  0.0018547599468555  loss:  3.3592804281986&quot;
## [1] &quot;step =  1  lambda =  0.00183630477702891  loss:  3.34194440086731&quot;
## [1] &quot;step =  2  lambda =  0.00183630477702891  loss:  3.34243571221799&quot;
## [1] &quot;step =  1  lambda =  0.00181803323921027  loss:  3.32526407996204&quot;
## [1] &quot;step =  2  lambda =  0.00181803323921027  loss:  3.32574732856272&quot;
## [1] &quot;step =  1  lambda =  0.00179994350623059  loss:  3.30873854870703&quot;
## [1] &quot;step =  2  lambda =  0.00179994350623059  loss:  3.30921384470838&quot;
## [1] &quot;step =  1  lambda =  0.00178203376910149  loss:  3.29236638879615&quot;
## [1] &quot;step =  2  lambda =  0.00178203376910149  loss:  3.29283384093511&quot;
## [1] &quot;step =  1  lambda =  0.00176430223683434  loss:  3.27614619460081&quot;
## [1] &quot;step =  2  lambda =  0.00176430223683434  loss:  3.27660591021874&quot;
## [1] &quot;step =  1  lambda =  0.00174674713626112  loss:  3.26007657306217&quot;
## [1] &quot;step =  2  lambda =  0.00174674713626112  loss:  3.26052865812266&quot;
## [1] &quot;step =  1  lambda =  0.00172936671185716  loss:  3.24415614358409&quot;
## [1] &quot;step =  2  lambda =  0.00172936671185716  loss:  3.24460070269057&quot;
## [1] &quot;step =  1  lambda =  0.00171215922556552  loss:  3.22838353792692&quot;
## [1] &quot;step =  2  lambda =  0.00171215922556552  loss:  3.22882067434001&quot;
## [1] &quot;step =  1  lambda =  0.00169512295662325  loss:  3.21275740010211&quot;
## [1] &quot;step =  2  lambda =  0.00169512295662325  loss:  3.2131872157568&quot;
## [1] &quot;step =  1  lambda =  0.00167825620138925  loss:  3.19727638626762&quot;
## [1] &quot;step =  2  lambda =  0.00167825620138925  loss:  3.19769898179019&quot;
## [1] &quot;step =  1  lambda =  0.00166155727317393  loss:  3.18193916462423&quot;
## [1] &quot;step =  2  lambda =  0.00166155727317393  loss:  3.18235463934887&quot;
## [1] &quot;step =  1  lambda =  0.00164502450207057  loss:  3.16674441531246&quot;
## [1] &quot;step =  2  lambda =  0.00164502450207057  loss:  3.16715286729776&quot;
## [1] &quot;step =  1  lambda =  0.00162865623478828  loss:  3.15169083031043&quot;
## [1] &quot;step =  2  lambda =  0.00162865623478828  loss:  3.15209235635568&quot;
## [1] &quot;step =  1  lambda =  0.00161245083448668  loss:  3.13677711333254&quot;
## [1] &quot;step =  2  lambda =  0.00161245083448668  loss:  3.13717180899366&quot;
## [1] &quot;step =  1  lambda =  0.00159640668061225  loss:  3.12200197972876&quot;
## [1] &quot;step =  2  lambda =  0.00159640668061225  loss:  3.1223899393342&quot;
## [1] &quot;step =  1  lambda =  0.00158052216873622  loss:  3.10736415638491&quot;
## [1] &quot;step =  2  lambda =  0.00158052216873622  loss:  3.10774547305122&quot;
## [1] &quot;step =  1  lambda =  0.00156479571039417  loss:  3.09286238162359&quot;
## [1] &quot;step =  2  lambda =  0.00156479571039417  loss:  3.09323714727077&quot;
## [1] &quot;step =  1  lambda =  0.00154922573292716  loss:  3.07849540510588&quot;
## [1] &quot;step =  2  lambda =  0.00154922573292716  loss:  3.07886371047264&quot;
## [1] &quot;step =  1  lambda =  0.00153381067932446  loss:  3.06426198773388&quot;
## [1] &quot;step =  2  lambda =  0.00153381067932446  loss:  3.0646239223926&quot;
## [1] &quot;step =  1  lambda =  0.00151854900806788  loss:  3.05016090155397&quot;
## [1] &quot;step =  2  lambda =  0.00151854900806788  loss:  3.05051655392546&quot;
## [1] &quot;step =  1  lambda =  0.00150343919297757  loss:  3.03619092966077&quot;
## [1] &quot;step =  2  lambda =  0.00150343919297757  loss:  3.03654038702892&quot;
## [1] &quot;step =  1  lambda =  0.00148847972305943  loss:  3.02235086610199&quot;
## [1] &quot;step =  2  lambda =  0.00148847972305943  loss:  3.0226942146281&quot;
## [1] &quot;step =  1  lambda =  0.001473669102354  loss:  3.00863951578389&quot;
## [1] &quot;step =  2  lambda =  0.001473669102354  loss:  3.00897684052094&quot;
## [1] &quot;step =  1  lambda =  0.00145900584978686  loss:  2.99505569437757&quot;
## [1] &quot;step =  2  lambda =  0.00145900584978686  loss:  2.9953870792842&quot;
## [1] &quot;step =  1  lambda =  0.00144448849902054  loss:  2.98159822822596&quot;
## [1] &quot;step =  2  lambda =  0.00144448849902054  loss:  2.98192375618031&quot;
## [1] &quot;step =  1  lambda =  0.00143011559830787  loss:  2.96826595425155&quot;
## [1] &quot;step =  2  lambda =  0.00143011559830787  loss:  2.9685857070649&quot;
## [1] &quot;step =  1  lambda =  0.0014158857103468  loss:  2.95505771986485&quot;
## [1] &quot;step =  2  lambda =  0.0014158857103468  loss:  2.95537177829513&quot;
## [1] &quot;step =  1  lambda =  0.00140179741213667  loss:  2.94197238287359&quot;
## [1] &quot;step =  2  lambda =  0.00140179741213667  loss:  2.94228082663861&quot;
## [1] &quot;step =  1  lambda =  0.00138784929483593  loss:  2.92900881139261&quot;
## [1] &quot;step =  2  lambda =  0.00138784929483593  loss:  2.92931171918323&quot;
## [1] &quot;step =  1  lambda =  0.00137403996362121  loss:  2.91616588375449&quot;
## [1] &quot;step =  2  lambda =  0.00137403996362121  loss:  2.91646333324752&quot;
## [1] &quot;step =  1  lambda =  0.00136036803754789  loss:  2.90344248842087&quot;
## [1] &quot;step =  2  lambda =  0.00136036803754789  loss:  2.90373455629188&quot;
## [1] &quot;step =  1  lambda =  0.00134683214941197  loss:  2.89083752389452&quot;
## [1] &quot;step =  2  lambda =  0.00134683214941197  loss:  2.89112428583041&quot;
## [1] &quot;step =  1  lambda =  0.00133343094561336  loss:  2.87834989863202&quot;
## [1] &quot;step =  2  lambda =  0.00133343094561336  loss:  2.87863142934346&quot;
## [1] &quot;step =  1  lambda =  0.0013201630860205  loss:  2.86597853095724&quot;
## [1] &quot;step =  2  lambda =  0.0013201630860205  loss:  2.86625490419099&quot;
## [1] &quot;step =  1  lambda =  0.00130702724383639  loss:  2.85372234897549&quot;
## [1] &quot;step =  2  lambda =  0.00130702724383639  loss:  2.85399363752644&quot;
## [1] &quot;step =  1  lambda =  0.00129402210546585  loss:  2.84158029048824&quot;
## [1] &quot;step =  2  lambda =  0.00129402210546585  loss:  2.84184656621143&quot;
## [1] &quot;step =  1  lambda =  0.00128114637038421  loss:  2.82955130290873&quot;
## [1] &quot;step =  2  lambda =  0.00128114637038421  loss:  2.82981263673109&quot;
## [1] &quot;step =  1  lambda =  0.00126839875100724  loss:  2.81763434317807&quot;
## [1] &quot;step =  2  lambda =  0.00126839875100724  loss:  2.81789080511012&quot;
## [1] &quot;step =  1  lambda =  0.00125577797256237  loss:  2.80582837768213&quot;
## [1] &quot;step =  2  lambda =  0.00125577797256237  loss:  2.80608003682944&quot;
## [1] &quot;step =  1  lambda =  0.00124328277296124  loss:  2.79413238216906&quot;
## [1] &quot;step =  2  lambda =  0.00124328277296124  loss:  2.79437930674357&quot;
## [1] &quot;step =  1  lambda =  0.00123091190267348  loss:  2.78254534166743&quot;
## [1] &quot;step =  2  lambda =  0.00123091190267348  loss:  2.78278759899869&quot;
## [1] &quot;step =  1  lambda =  0.00121866412460175  loss:  2.77106625040514&quot;
## [1] &quot;step =  2  lambda =  0.00121866412460175  loss:  2.77130390695134&quot;
## [1] &quot;step =  1  lambda =  0.00120653821395804  loss:  2.7596941117289&quot;
## [1] &quot;step =  2  lambda =  0.00120653821395804  loss:  2.75992723308775&quot;
## [1] &quot;step =  1  lambda =  0.00119453295814118  loss:  2.74842793802437&quot;
## [1] &quot;step =  2  lambda =  0.00119453295814118  loss:  2.74865658894388&quot;
## [1] &quot;step =  1  lambda =  0.00118264715661557  loss:  2.73726675063698&quot;
## [1] &quot;step =  2  lambda =  0.00118264715661557  loss:  2.73749099502611&quot;
## [1] &quot;step =  1  lambda =  0.00117087962079117  loss:  2.72620957979337&quot;
## [1] &quot;step =  2  lambda =  0.00117087962079117  loss:  2.72642948073247&quot;
## [1] &quot;step =  1  lambda =  0.00115922917390459  loss:  2.71525546452349&quot;
## [1] &quot;step =  2  lambda =  0.00115922917390459  loss:  2.7154710842747&quot;
## [1] &quot;step =  1  lambda =  0.00114769465090143  loss:  2.70440345258333&quot;
## [1] &quot;step =  2  lambda =  0.00114769465090143  loss:  2.70461485260073&quot;
## [1] &quot;step =  1  lambda =  0.00113627489831977  loss:  2.69365260037821&quot;
## [1] &quot;step =  2  lambda =  0.00113627489831977  loss:  2.69385984131796&quot;
## [1] &quot;step =  1  lambda =  0.00112496877417484  loss:  2.68300197288685&quot;
## [1] &quot;step =  2  lambda =  0.00112496877417484  loss:  2.68320511461712&quot;
## [1] &quot;step =  1  lambda =  0.0011137751478448  loss:  2.6724506435859&quot;
## [1] &quot;step =  2  lambda =  0.0011137751478448  loss:  2.67264974519673&quot;
## [1] &quot;step =  1  lambda =  0.0011026928999577  loss:  2.66199769437518&quot;
## [1] &quot;step =  2  lambda =  0.0011026928999577  loss:  2.66219281418814&quot;
## [1] &quot;step =  1  lambda =  0.00109172092227951  loss:  2.65164221550354&quot;
## [1] &quot;step =  2  lambda =  0.00109172092227951  loss:  2.65183341108132&quot;
## [1] &quot;step =  1  lambda =  0.00108085811760332  loss:  2.64138330549526&quot;
## [1] &quot;step =  2  lambda =  0.00108085811760332  loss:  2.64157063365114&quot;
## [1] &quot;step =  1  lambda =  0.0010701033996396  loss:  2.63122007107711&quot;
## [1] &quot;step =  2  lambda =  0.0010701033996396  loss:  2.63140358788427&quot;
## [1] &quot;step =  1  lambda =  0.00105945569290761  loss:  2.62115162710601&quot;
## [1] &quot;step =  2  lambda =  0.00105945569290761  loss:  2.62133138790678&quot;
## [1] &quot;step =  1  lambda =  0.00104891393262779  loss:  2.61117709649726&quot;
## [1] &quot;step =  2  lambda =  0.00104891393262779  loss:  2.61135315591217&quot;
## [1] &quot;step =  1  lambda =  0.00103847706461533  loss:  2.60129561015337&quot;
## [1] &quot;step =  2  lambda =  0.00103847706461533  loss:  2.60146802209021&quot;
## [1] &quot;step =  1  lambda =  0.00102814404517473  loss:  2.59150630689354&quot;
## [1] &quot;step =  2  lambda =  0.00102814404517473  loss:  2.59167512455614&quot;
## [1] &quot;step =  1  lambda =  0.00101791384099544  loss:  2.58180833338358&quot;
## [1] &quot;step =  2  lambda =  0.00101791384099544  loss:  2.58197360928061&quot;
## [1] &quot;step =  1  lambda =  0.00100778542904851  loss:  2.57220084406659&quot;
## [1] &quot;step =  2  lambda =  0.00100778542904851  loss:  2.57236263002023&quot;
## [1] &quot;step =  1  lambda =  0.000997757796484312  loss:  2.56268300109406&quot;
## [1] &quot;step =  2  lambda =  0.000997757796484312  loss:  2.56284134824849&quot;
## [1] &quot;step =  1  lambda =  0.000987829940531229  loss:  2.55325397425765&quot;
## [1] &quot;step =  2  lambda =  0.000987829940531229  loss:  2.55340893308753&quot;
## [1] &quot;step =  1  lambda =  0.000978000868395395  loss:  2.5439129409215&quot;
## [1] &quot;step =  2  lambda =  0.000978000868395395  loss:  2.54406456124023&quot;
## [1] &quot;step =  1  lambda =  0.000968269597161403  loss:  2.53465908595506&quot;
## [1] &quot;step =  2  lambda =  0.000968269597161403  loss:  2.53480741692308&quot;
## [1] &quot;step =  1  lambda =  0.000958635153694021  loss:  2.52549160166657&quot;
## [1] &quot;step =  2  lambda =  0.000958635153694021  loss:  2.52563669179941&quot;
## [1] &quot;step =  1  lambda =  0.000949096574540873  loss:  2.51640968773704&quot;
## [1] &quot;step =  2  lambda =  0.000949096574540873  loss:  2.51655158491337&quot;
## [1] &quot;step =  1  lambda =  0.000939652905836096  loss:  2.50741255115475&quot;
## [1] &quot;step =  2  lambda =  0.000939652905836096  loss:  2.5075513026243&quot;
## [1] &quot;step =  1  lambda =  0.000930303203204949  loss:  2.4984994061504&quot;
## [1] &quot;step =  2  lambda =  0.000930303203204949  loss:  2.49863505854175&quot;
## [1] &quot;step =  1  lambda =  0.000921046531669378  loss:  2.48966947413269&quot;
## [1] &quot;step =  2  lambda =  0.000921046531669378  loss:  2.48980207346104&quot;
## [1] &quot;step =  1  lambda =  0.000911881965554516  loss:  2.48092198362457&quot;
## [1] &quot;step =  2  lambda =  0.000911881965554516  loss:  2.48105157529931&quot;
## [1] &quot;step =  1  lambda =  0.000902808588396114  loss:  2.47225617019985&quot;
## [1] &quot;step =  2  lambda =  0.000902808588396114  loss:  2.47238279903215&quot;
## [1] &quot;step =  1  lambda =  0.000893825492848894  loss:  2.46367127642054&quot;
## [1] &quot;step =  2  lambda =  0.000893825492848894  loss:  2.46379498663072&quot;
## [1] &quot;step =  1  lambda =  0.000884931780595815  loss:  2.45516655177457&quot;
## [1] &quot;step =  2  lambda =  0.000884931780595815  loss:  2.4552873869995&quot;
## [1] &quot;step =  1  lambda =  0.000876126562258242  loss:  2.44674125261407&quot;
## [1] &quot;step =  2  lambda =  0.000876126562258242  loss:  2.44685925591442&quot;
## [1] &quot;step =  1  lambda =  0.000867408957307003  loss:  2.43839464209428&quot;
## [1] &quot;step =  2  lambda =  0.000867408957307003  loss:  2.43850985596166&quot;
## [1] &quot;step =  1  lambda =  0.000858778093974336  loss:  2.43012599011277&quot;
## [1] &quot;step =  2  lambda =  0.000858778093974336  loss:  2.43023845647686&quot;
## [1] &quot;step =  1  lambda =  0.000850233109166719  loss:  2.42193457324942&quot;
## [1] &quot;step =  2  lambda =  0.000850233109166719  loss:  2.42204433348492&quot;
## [1] &quot;step =  1  lambda =  0.000841773148378549  loss:  2.41381967470665&quot;
## [1] &quot;step =  2  lambda =  0.000841773148378549  loss:  2.4139267696402&quot;
## [1] &quot;step =  1  lambda =  0.000833397365606696  loss:  2.4057805842504&quot;
## [1] &quot;step =  2  lambda =  0.000833397365606696  loss:  2.4058850541674&quot;
## [1] &quot;step =  1  lambda =  0.000825104923265905  loss:  2.39781659815141&quot;
## [1] &quot;step =  2  lambda =  0.000825104923265905  loss:  2.39791848280279&quot;
## [1] &quot;step =  1  lambda =  0.000816894992105029  loss:  2.38992701912717&quot;
## [1] &quot;step =  2  lambda =  0.000816894992105029  loss:  2.39002635773601&quot;
## [1] &quot;step =  1  lambda =  0.000808766751124111  loss:  2.38211115628423&quot;
## [1] &quot;step =  2  lambda =  0.000808766751124111  loss:  2.38220798755236&quot;
## [1] &quot;step =  1  lambda =  0.000800719387492281  loss:  2.37436832506106&quot;
## [1] &quot;step =  2  lambda =  0.000800719387492281  loss:  2.3744626871756&quot;
## [1] &quot;step =  1  lambda =  0.000792752096466468  loss:  2.36669784717144&quot;
## [1] &quot;step =  2  lambda =  0.000792752096466468  loss:  2.36678977781114&quot;
## [1] &quot;step =  1  lambda =  0.000784864081310932  loss:  2.35909905054822&quot;
## [1] &quot;step =  2  lambda =  0.000784864081310932  loss:  2.35918858688986&quot;
## [1] &quot;step =  1  lambda =  0.000777054553217582  loss:  2.35157126928768&quot;
## [1] &quot;step =  2  lambda =  0.000777054553217582  loss:  2.35165844801232&quot;
## [1] &quot;step =  1  lambda =  0.000769322731227101  loss:  2.3441138435943&quot;
## [1] &quot;step =  2  lambda =  0.000769322731227101  loss:  2.3441987008935&quot;
## [1] &quot;step =  1  lambda =  0.000761667842150847  loss:  2.33672611972601&quot;
## [1] &quot;step =  2  lambda =  0.000761667842150847  loss:  2.33680869130789&quot;
## [1] &quot;step =  1  lambda =  0.000754089120493534  loss:  2.32940744993993&quot;
## [1] &quot;step =  2  lambda =  0.000754089120493534  loss:  2.32948777103527&quot;
## [1] &quot;step =  1  lambda =  0.00074658580837668  loss:  2.32215719243854&quot;
## [1] &quot;step =  2  lambda =  0.00074658580837668  loss:  2.32223529780675&quot;
## [1] &quot;step =  1  lambda =  0.00073915715546282  loss:  2.31497471131639&quot;
## [1] &quot;step =  2  lambda =  0.00073915715546282  loss:  2.31505063525139&quot;
## [1] &quot;step =  1  lambda =  0.000731802418880473  loss:  2.30785937650716&quot;
## [1] &quot;step =  2  lambda =  0.000731802418880473  loss:  2.30793315284321&quot;
## [1] &quot;step =  1  lambda =  0.000724520863149851  loss:  2.30081056373124&quot;
## [1] &quot;step =  2  lambda =  0.000724520863149851  loss:  2.30088222584874&quot;
## [1] &quot;step =  1  lambda =  0.000717311760109313  loss:  2.29382765444379&quot;
## [1] &quot;step =  2  lambda =  0.000717311760109313  loss:  2.29389723527493&quot;
## [1] &quot;step =  1  lambda =  0.000710174388842549  loss:  2.28691003578318&quot;
## [1] &quot;step =  2  lambda =  0.000710174388842549  loss:  2.28697756781761&quot;
## [1] &quot;step =  1  lambda =  0.000703108035606483  loss:  2.28005710051994&quot;
## [1] &quot;step =  2  lambda =  0.000703108035606483  loss:  2.28012261581028&quot;
## [1] &quot;step =  1  lambda =  0.000696111993759903  loss:  2.27326824700605&quot;
## [1] &quot;step =  2  lambda =  0.000696111993759903  loss:  2.27333177717344&quot;
## [1] &quot;step =  1  lambda =  0.000689185563692794  loss:  2.26654287912481&quot;
## [1] &quot;step =  2  lambda =  0.000689185563692794  loss:  2.2666044553643&quot;
## [1] &quot;step =  1  lambda =  0.000682328052756377  loss:  2.25988040624106&quot;
## [1] &quot;step =  2  lambda =  0.000682328052756377  loss:  2.25994005932699&quot;
## [1] &quot;step =  1  lambda =  0.000675538775193844  loss:  2.25328024315179&quot;
## [1] &quot;step =  2  lambda =  0.000675538775193844  loss:  2.25333800344312&quot;
## [1] &quot;step =  1  lambda =  0.000668817052071782  loss:  2.24674181003731&quot;
## [1] &quot;step =  2  lambda =  0.000668817052071782  loss:  2.2467977074828&quot;
## [1] &quot;step =  1  lambda =  0.000662162211212276  loss:  2.2402645324127&quot;
## [1] &quot;step =  2  lambda =  0.000662162211212276  loss:  2.24031859655613&quot;
## [1] &quot;step =  1  lambda =  0.000655573587125696  loss:  2.23384784107977&quot;
## [1] &quot;step =  2  lambda =  0.000655573587125696  loss:  2.23390010106508&quot;
## [1] &quot;step =  1  lambda =  0.000649050520944141  loss:  2.22749117207946&quot;
## [1] &quot;step =  2  lambda =  0.000649050520944141  loss:  2.22754165665576&quot;
## [1] &quot;step =  1  lambda =  0.000642592360355558  loss:  2.22119396664453&quot;
## [1] &quot;step =  2  lambda =  0.000642592360355558  loss:  2.22124270417114&quot;
## [1] &quot;step =  1  lambda =  0.000636198459538506  loss:  2.21495567115281&quot;
## [1] &quot;step =  2  lambda =  0.000636198459538506  loss:  2.21500268960415&quot;
## [1] &quot;step =  1  lambda =  0.000629868179097574  loss:  2.20877573708074&quot;
## [1] &quot;step =  2  lambda =  0.000629868179097574  loss:  2.20882106405128&quot;
## [1] &quot;step =  1  lambda =  0.000623600885999444  loss:  2.20265362095741&quot;
## [1] &quot;step =  2  lambda =  0.000623600885999444  loss:  2.20269728366644&quot;
## [1] &quot;step =  1  lambda =  0.000617395953509583  loss:  2.19658878431893&quot;
## [1] &quot;step =  2  lambda =  0.000617395953509583  loss:  2.19663080961536&quot;
## [1] &quot;step =  1  lambda =  0.000611252761129572  loss:  2.19058069366324&quot;
## [1] &quot;step =  2  lambda =  0.000611252761129572  loss:  2.19062110803031&quot;
## [1] &quot;step =  1  lambda =  0.000605170694535053  loss:  2.18462882040527&quot;
## [1] &quot;step =  2  lambda =  0.000605170694535053  loss:  2.1846676499652&quot;
## [1] &quot;step =  1  lambda =  0.000599149145514298  loss:  2.17873264083257&quot;
## [1] &quot;step =  2  lambda =  0.000599149145514298  loss:  2.17876991135118&quot;
## [1] &quot;step =  1  lambda =  0.000593187511907387  loss:  2.17289163606125&quot;
## [1] &quot;step =  2  lambda =  0.000593187511907387  loss:  2.17292737295249&quot;
## [1] &quot;step =  1  lambda =  0.000587285197545991  loss:  2.16710529199236&quot;
## [1] &quot;step =  2  lambda =  0.000587285197545991  loss:  2.16713952032282&quot;
## [1] &quot;step =  1  lambda =  0.000581441612193756  loss:  2.1613730992686&quot;
## [1] &quot;step =  2  lambda =  0.000581441612193756  loss:  2.16140584376195&quot;
## [1] &quot;step =  1  lambda =  0.000575656171487276  loss:  2.15569455323146&quot;
## [1] &quot;step =  2  lambda =  0.000575656171487276  loss:  2.15572583827286&quot;
## [1] &quot;step =  1  lambda =  0.00056992829687766  loss:  2.15006915387872&quot;
## [1] &quot;step =  2  lambda =  0.00056992829687766  loss:  2.15009900351915&quot;
## [1] &quot;step =  1  lambda =  0.000564257415572674  loss:  2.14449640582232&quot;
## [1] &quot;step =  2  lambda =  0.000564257415572674  loss:  2.14452484378288&quot;
## [1] &quot;step =  1  lambda =  0.000558642960479461  loss:  2.13897581824659&quot;
## [1] &quot;step =  2  lambda =  0.000558642960479461  loss:  2.13900286792271&quot;
## [1] &quot;step =  1  lambda =  0.000553084370147834  loss:  2.13350690486687&quot;
## [1] &quot;step =  2  lambda =  0.000553084370147834  loss:  2.13353258933256&quot;
## [1] &quot;step =  1  lambda =  0.000547581088714126  loss:  2.12808918388848&quot;
## [1] &quot;step =  2  lambda =  0.000547581088714126  loss:  2.12811352590042&quot;
## [1] &quot;step =  1  lambda =  0.000542132565845609  loss:  2.12272217796601&quot;
## [1] &quot;step =  2  lambda =  0.000542132565845609  loss:  2.1227451999677&quot;
## [1] &quot;step =  1  lambda =  0.000536738256685455  loss:  2.11740541416314&quot;
## [1] &quot;step =  2  lambda =  0.000536738256685455  loss:  2.11742713828887&quot;
## [1] &quot;step =  1  lambda =  0.000531397621798253  loss:  2.11213842391251&quot;
## [1] &quot;step =  2  lambda =  0.000531397621798253  loss:  2.11215887199144&quot;
## [1] &quot;step =  1  lambda =  0.000526110127116064  loss:  2.10692074297626&quot;
## [1] &quot;step =  2  lambda =  0.000526110127116064  loss:  2.10693993653634&quot;
## [1] &quot;step =  1  lambda =  0.000520875243885012  loss:  2.10175191140673&quot;
## [1] &quot;step =  2  lambda =  0.000520875243885012  loss:  2.10176987167858&quot;
## [1] &quot;step =  1  lambda =  0.000515692448612414  loss:  2.09663147350754&quot;
## [1] &quot;step =  2  lambda =  0.000515692448612414  loss:  2.09664822142837&quot;
## [1] &quot;step =  1  lambda =  0.000510561223014422  loss:  2.09155897779504&quot;
## [1] &quot;step =  2  lambda =  0.000510561223014422  loss:  2.09157453401244&quot;
## [1] &quot;step =  1  lambda =  0.0005054810539642  loss:  2.08653397696013&quot;
## [1] &quot;step =  2  lambda =  0.0005054810539642  loss:  2.08654836183581&quot;
## [1] &quot;step =  1  lambda =  0.000500451433440611  loss:  2.08155602783033&quot;
## [1] &quot;step =  2  lambda =  0.000500451433440611  loss:  2.0815692614439&quot;
## [1] &quot;step =  1  lambda =  0.00049547185847741  loss:  2.07662469133224&quot;
## [1] &quot;step =  2  lambda =  0.00049547185847741  loss:  2.07663679348487&quot;
## [1] &quot;step =  1  lambda =  0.000490541831112951  loss:  2.07173953245434&quot;
## [1] &quot;step =  2  lambda =  0.000490541831112951  loss:  2.0717505226724&quot;
## [1] &quot;step =  1  lambda =  0.000485660858340389  loss:  2.06690012021012&quot;
## [1] &quot;step =  2  lambda =  0.000485660858340389  loss:  2.06691001774878&quot;
## [1] &quot;step =  1  lambda =  0.00048082845205838  loss:  2.06210602760149&quot;
## [1] &quot;step =  2  lambda =  0.00048082845205838  loss:  2.06211485144827&quot;
## [1] &quot;step =  1  lambda =  0.000476044129022269  loss:  2.05735683158258&quot;
## [1] &quot;step =  2  lambda =  0.000476044129022269  loss:  2.05736460046088&quot;
## [1] &quot;step =  1  lambda =  0.000471307410795765  loss:  2.05265211302378&quot;
## [1] &quot;step =  2  lambda =  0.000471307410795765  loss:  2.05265884539634&quot;
## [1] &quot;step =  1  lambda =  0.000466617823703098  loss:  2.04799145667621&quot;
## [1] &quot;step =  2  lambda =  0.000466617823703098  loss:  2.04799717074855&quot;
## [1] &quot;step =  1  lambda =  0.000461974898781651  loss:  2.04337445113638&quot;
## [1] &quot;step =  2  lambda =  0.000461974898781651  loss:  2.04337916486018&quot;
## [1] &quot;step =  1  lambda =  0.000457378171735063  loss:  2.0388006888113&quot;
## [1] &quot;step =  2  lambda =  0.000457378171735063  loss:  2.03880441988773&quot;
## [1] &quot;step =  1  lambda =  0.000452827182886797  loss:  2.03426976588371&quot;
## [1] &quot;step =  2  lambda =  0.000452827182886797  loss:  2.03427253176683&quot;
## [1] &quot;step =  1  lambda =  0.000448321477134178  loss:  2.02978128227789&quot;
## [1] &quot;step =  2  lambda =  0.000448321477134178  loss:  2.0297831001778&quot;
## [1] &quot;step =  1  lambda =  0.000443860603902874  loss:  2.02533484162545&quot;
## [1] &quot;step =  2  lambda =  0.000443860603902874  loss:  2.02533572851162&quot;
## [1] &quot;step =  1  lambda =  0.000439444117101845  loss:  2.02093005123176&quot;
## [1] &quot;step =  2  lambda =  0.000439444117101845  loss:  2.02093002383616&quot;
## [1] &quot;step =  3  lambda =  0.000439444117101845  loss:  2.02093083653352&quot;
## [1] &quot;step =  1  lambda =  0.000435071575078732  loss:  2.01656567437891&quot;
## [1] &quot;step =  2  lambda =  0.000435071575078732  loss:  2.01656556011858&quot;
## [1] &quot;step =  3  lambda =  0.000435071575078732  loss:  2.01656626409309&quot;
## [1] &quot;step =  1  lambda =  0.000430742540575688  loss:  2.01224125173266&quot;
## [1] &quot;step =  2  lambda =  0.000430742540575688  loss:  2.01224103208962&quot;
## [1] &quot;step =  3  lambda =  0.000430742540575688  loss:  2.01224161741619&quot;
## [1] &quot;step =  1  lambda =  0.000426456580685654  loss:  2.00795639270301&quot;
## [1] &quot;step =  2  lambda =  0.000426456580685654  loss:  2.00795606267782&quot;
## [1] &quot;step =  3  lambda =  0.000426456580685654  loss:  2.00795652765295&quot;
## [1] &quot;step =  1  lambda =  0.00042221326680907  loss:  2.00371073159348&quot;
## [1] &quot;step =  2  lambda =  0.00042221326680907  loss:  2.00371029156415&quot;
## [1] &quot;step =  3  lambda =  0.00042221326680907  loss:  2.00371063832323&quot;
## [1] &quot;step =  1  lambda =  0.000418012174611013  loss:  1.99950391504964&quot;
## [1] &quot;step =  2  lambda =  0.000418012174611013  loss:  1.99950336831576&quot;
## [1] &quot;step =  3  lambda =  0.000418012174611013  loss:  1.99950360129375&quot;
## [1] &quot;step =  1  lambda =  0.000413852883978762  loss:  1.99533559803685&quot;
## [1] &quot;step =  2  lambda =  0.000413852883978762  loss:  1.99533494973643&quot;
## [1] &quot;step =  3  lambda =  0.000413852883978762  loss:  1.99533507484087&quot;
## [1] &quot;step =  1  lambda =  0.000409734978979787  loss:  1.9912054419036&quot;
## [1] &quot;step =  2  lambda =  0.000409734978979787  loss:  1.99120469835186&quot;
## [1] &quot;step =  3  lambda =  0.000409734978979787  loss:  1.99120472242177&quot;
## [1] &quot;step =  1  lambda =  0.000405658047820157  loss:  1.98711311315327&quot;
## [1] &quot;step =  2  lambda =  0.000405658047820157  loss:  1.98711228139521&quot;
## [1] &quot;step =  3  lambda =  0.000405658047820157  loss:  1.98711221182935&quot;
## [1] &quot;step =  4  lambda =  0.000405658047820157  loss:  1.98711288541605&quot;
## [1] &quot;step =  1  lambda =  0.000401621682803358  loss:  1.98305744175927&quot;
## [1] &quot;step =  2  lambda =  0.000401621682803358  loss:  1.9830572613162&quot;
## [1] &quot;step =  3  lambda =  0.000401621682803358  loss:  1.98305782881474&quot;
## [1] &quot;step =  1  lambda =  0.000397625480289526  loss:  1.97903974171347&quot;
## [1] &quot;step =  2  lambda =  0.000397625480289526  loss:  1.97903947295063&quot;
## [1] &quot;step =  3  lambda =  0.000397625480289526  loss:  1.97903995000724&quot;
## [1] &quot;step =  1  lambda =  0.000393669040655078  loss:  1.9750588815969&quot;
## [1] &quot;step =  2  lambda =  0.000393669040655078  loss:  1.97505853695468&quot;
## [1] &quot;step =  3  lambda =  0.000393669040655078  loss:  1.97505893442316&quot;
## [1] &quot;step =  1  lambda =  0.000389751968252755  loss:  1.97111454978681&quot;
## [1] &quot;step =  2  lambda =  0.000389751968252755  loss:  1.97111413909718&quot;
## [1] &quot;step =  3  lambda =  0.000389751968252755  loss:  1.97111446636582&quot;
## [1] &quot;step =  1  lambda =  0.000385873871372051  loss:  1.96720643351049&quot;
## [1] &quot;step =  2  lambda =  0.000385873871372051  loss:  1.96720596566947&quot;
## [1] &quot;step =  3  lambda =  0.000385873871372051  loss:  1.96720623143291&quot;
## [1] &quot;step =  1  lambda =  0.000382034362200047  loss:  1.96333422126577&quot;
## [1] &quot;step =  2  lambda =  0.000382034362200047  loss:  1.96333370460081&quot;
## [1] &quot;step =  3  lambda =  0.000382034362200047  loss:  1.96333391705624&quot;
## [1] &quot;step =  1  lambda =  0.000378233056782626  loss:  1.95949760336122&quot;
## [1] &quot;step =  2  lambda =  0.000378233056782626  loss:  1.95949704575297&quot;
## [1] &quot;step =  3  lambda =  0.000378233056782626  loss:  1.95949721268676&quot;
## [1] &quot;step =  1  lambda =  0.000374469574986078  loss:  1.9556962721012&quot;
## [1] &quot;step =  2  lambda =  0.000374469574986078  loss:  1.95569568105099&quot;
## [1] &quot;step =  3  lambda =  0.000374469574986078  loss:  1.955695809894&quot;
## [1] &quot;step =  1  lambda =  0.000370743540459088  loss:  1.95192992188559&quot;
## [1] &quot;step =  2  lambda =  0.000370743540459088  loss:  1.95192930456205&quot;
## [1] &quot;step =  3  lambda =  0.000370743540459088  loss:  1.9519294024295&quot;
## [1] &quot;step =  1  lambda =  0.000367054580595098  loss:  1.94819824927332&quot;
## [1] &quot;step =  2  lambda =  0.000367054580595098  loss:  1.94819761254671&quot;
## [1] &quot;step =  3  lambda =  0.000367054580595098  loss:  1.94819768626795&quot;
## [1] &quot;step =  1  lambda =  0.000363402326495048  loss:  1.94450095302374&quot;
## [1] &quot;step =  2  lambda =  0.000363402326495048  loss:  1.94450030349184&quot;
## [1] &quot;step =  3  lambda =  0.000363402326495048  loss:  1.94450035963293&quot;
## [1] &quot;step =  1  lambda =  0.000359786412930483  loss:  1.94083773412246&quot;
## [1] &quot;step =  2  lambda =  0.000359786412930483  loss:  1.9408370781303&quot;
## [1] &quot;step =  3  lambda =  0.000359786412930483  loss:  1.94083712301141&quot;
## [1] &quot;step =  1  lambda =  0.000356206478307034  loss:  1.93720829579607&quot;
## [1] &quot;step =  2  lambda =  0.000356206478307034  loss:  1.9372076394513&quot;
## [1] &quot;step =  3  lambda =  0.000356206478307034  loss:  1.93720767916038&quot;
## [1] &quot;step =  1  lambda =  0.000352662164628256  loss:  1.93361234351893&quot;
## [1] &quot;step =  2  lambda =  0.000352662164628256  loss:  1.93361169270412&quot;
## [1] &quot;step =  3  lambda =  0.000352662164628256  loss:  1.93361173310804&quot;
## [1] &quot;step =  1  lambda =  0.000349153117459826  loss:  1.93004958501452&quot;
## [1] &quot;step =  2  lambda =  0.000349153117459826  loss:  1.93004894539725&quot;
## [1] &quot;step =  3  lambda =  0.000349153117459826  loss:  1.93004899215109&quot;
## [1] &quot;step =  1  lambda =  0.000345678985894105  loss:  1.92651973025285&quot;
## [1] &quot;step =  2  lambda =  0.000345678985894105  loss:  1.92651910729433&quot;
## [1] &quot;step =  3  lambda =  0.000345678985894105  loss:  1.92651916584943&quot;
## [1] &quot;step =  1  lambda =  0.000342239422515039  loss:  1.92302249144539&quot;
## [1] &quot;step =  2  lambda =  0.000342239422515039  loss:  1.923021890408&quot;
## [1] &quot;step =  3  lambda =  0.000342239422515039  loss:  1.92302196601919&quot;
## [1] &quot;step =  1  lambda =  0.000338834083363426  loss:  1.91955758303817&quot;
## [1] &quot;step =  2  lambda =  0.000338834083363426  loss:  1.9195570089924&quot;
## [1] &quot;step =  3  lambda =  0.000338834083363426  loss:  1.91955710672463&quot;
## [1] &quot;step =  1  lambda =  0.000335462627902512  loss:  1.91612472170392&quot;
## [1] &quot;step =  2  lambda =  0.000335462627902512  loss:  1.91612417953471&quot;
## [1] &quot;step =  3  lambda =  0.000335462627902512  loss:  1.91612430426933&quot;
## [1] &quot;step =  1  lambda =  0.000332124718983941  loss:  1.91272362633332&quot;
## [1] &quot;step =  2  lambda =  0.000332124718983941  loss:  1.91272312074624&quot;
## [1] &quot;step =  3  lambda =  0.000332124718983941  loss:  1.91272327718696&quot;
## [1] &quot;step =  1  lambda =  0.00032882002281404  loss:  1.90935401802595&quot;
## [1] &quot;step =  2  lambda =  0.00032882002281404  loss:  1.90935355355302&quot;
## [1] &quot;step =  3  lambda =  0.00032882002281404  loss:  1.90935374623176&quot;
## [1] &quot;step =  1  lambda =  0.000325548208920438  loss:  1.90601562008094&quot;
## [1] &quot;step =  2  lambda =  0.000325548208920438  loss:  1.90601520108639&quot;
## [1] &quot;step =  3  lambda =  0.000325548208920438  loss:  1.90601543436894&quot;
## [1] &quot;step =  1  lambda =  0.000322308950119019  loss:  1.90270815798747&quot;
## [1] &quot;step =  2  lambda =  0.000322308950119019  loss:  1.90270778867331&quot;
## [1] &quot;step =  3  lambda =  0.000322308950119019  loss:  1.90270806676487&quot;
## [1] &quot;step =  1  lambda =  0.000319101922481203  loss:  1.89943135941511&quot;
## [1] &quot;step =  2  lambda =  0.000319101922481203  loss:  1.89943104382666&quot;
## [1] &quot;step =  3  lambda =  0.000319101922481203  loss:  1.89943137077735&quot;
## [1] &quot;step =  1  lambda =  0.000315926805301555  loss:  1.89618495420424&quot;
## [1] &quot;step =  2  lambda =  0.000315926805301555  loss:  1.89618469623549&quot;
## [1] &quot;step =  3  lambda =  0.000315926805301555  loss:  1.89618507594569&quot;
## [1] &quot;step =  1  lambda =  0.000312783281065711  loss:  1.89296867435626&quot;
## [1] &quot;step =  2  lambda =  0.000312783281065711  loss:  1.89296847775519&quot;
## [1] &quot;step =  3  lambda =  0.000312783281065711  loss:  1.89296891398083&quot;
## [1] &quot;step =  1  lambda =  0.000309671035418626  loss:  1.88978225402384&quot;
## [1] &quot;step =  2  lambda =  0.000309671035418626  loss:  1.88978212239767&quot;
## [1] &quot;step =  3  lambda =  0.000309671035418626  loss:  1.88978261875539&quot;
## [1] &quot;step =  1  lambda =  0.000306589757133144  loss:  1.8866254295011&quot;
## [1] &quot;step =  2  lambda =  0.000306589757133144  loss:  1.88662536632135&quot;
## [1] &quot;step =  3  lambda =  0.000306589757133144  loss:  1.88662592629362&quot;
## [1] &quot;step =  1  lambda =  0.000303539138078867  loss:  1.88349793921372&quot;
## [1] &quot;step =  2  lambda =  0.000303539138078867  loss:  1.88349794782123&quot;
## [1] &quot;step =  1  lambda =  0.000300518873191348  loss:  1.88039999040983&quot;
## [1] &quot;step =  2  lambda =  0.000300518873191348  loss:  1.8803994698394&quot;
## [1] &quot;step =  3  lambda =  0.000300518873191348  loss:  1.88039957217758&quot;
## [1] &quot;step =  1  lambda =  0.000297528660441581  loss:  1.87733027493072&quot;
## [1] &quot;step =  2  lambda =  0.000297528660441581  loss:  1.87732984834817&quot;
## [1] &quot;step =  3  lambda =  0.000297528660441581  loss:  1.87733003628141&quot;
## [1] &quot;step =  1  lambda =  0.0002945682008058  loss:  1.87428913789179&quot;
## [1] &quot;step =  2  lambda =  0.0002945682008058  loss:  1.87428880257045&quot;
## [1] &quot;step =  3  lambda =  0.0002945682008058  loss:  1.87428907569029&quot;
## [1] &quot;step =  1  lambda =  0.000291637198235574  loss:  1.87127631717295&quot;
## [1] &quot;step =  2  lambda =  0.000291637198235574  loss:  1.87127607379805&quot;
## [1] &quot;step =  3  lambda =  0.000291637198235574  loss:  1.87127643344134&quot;
## [1] &quot;step =  1  lambda =  0.000288735359628203  loss:  1.86829155811317&quot;
## [1] &quot;step =  2  lambda =  0.000288735359628203  loss:  1.86829140825528&quot;
## [1] &quot;step =  3  lambda =  0.000288735359628203  loss:  1.86829185624348&quot;
## [1] &quot;step =  1  lambda =  0.000285862394797409  loss:  1.86533460970391&quot;
## [1] &quot;step =  2  lambda =  0.000285862394797409  loss:  1.86533455523041&quot;
## [1] &quot;step =  3  lambda =  0.000285862394797409  loss:  1.86533509358172&quot;
## [1] &quot;step =  1  lambda =  0.000283018016444314  loss:  1.8624052236935&quot;
## [1] &quot;step =  2  lambda =  0.000283018016444314  loss:  1.86240526660929&quot;
## [1] &quot;step =  1  lambda =  0.000280201940128712  loss:  1.85950353614556&quot;
## [1] &quot;step =  2  lambda =  0.000280201940128712  loss:  1.85950310403206&quot;
## [1] &quot;step =  3  lambda =  0.000280201940128712  loss:  1.85950326388352&quot;
## [1] &quot;step =  1  lambda =  0.000277413884240626  loss:  1.8566284013766&quot;
## [1] &quot;step =  2  lambda =  0.000277413884240626  loss:  1.85662808591769&quot;
## [1] &quot;step =  3  lambda =  0.000277413884240626  loss:  1.85662835428332&quot;
## [1] &quot;step =  1  lambda =  0.000274653569972143  loss:  1.85378011439623&quot;
## [1] &quot;step =  2  lambda =  0.000274653569972143  loss:  1.85377991232193&quot;
## [1] &quot;step =  3  lambda =  0.000274653569972143  loss:  1.85378028813551&quot;
## [1] &quot;step =  1  lambda =  0.000271920721289535  loss:  1.85095842771906&quot;
## [1] &quot;step =  2  lambda =  0.000271920721289535  loss:  1.85095833901913&quot;
## [1] &quot;step =  3  lambda =  0.000271920721289535  loss:  1.85095882290823&quot;
## [1] &quot;step =  1  lambda =  0.000269215064905658  loss:  1.84816310098157&quot;
## [1] &quot;step =  2  lambda =  0.000269215064905658  loss:  1.84816312653166&quot;
## [1] &quot;step =  1  lambda =  0.000266536330252618  loss:  1.84539426310752&quot;
## [1] &quot;step =  2  lambda =  0.000266536330252618  loss:  1.84539384950964&quot;
## [1] &quot;step =  3  lambda =  0.000266536330252618  loss:  1.84539400714545&quot;
## [1] &quot;step =  1  lambda =  0.000263884249454718  loss:  1.8426507961687&quot;
## [1] &quot;step =  2  lambda =  0.000263884249454718  loss:  1.84265051399677&quot;
## [1] &quot;step =  3  lambda =  0.000263884249454718  loss:  1.84265079506381&quot;
## [1] &quot;step =  1  lambda =  0.000261258557301668  loss:  1.83993300206533&quot;
## [1] &quot;step =  2  lambda =  0.000261258557301668  loss:  1.8399328476562&quot;
## [1] &quot;step =  3  lambda =  0.000261258557301668  loss:  1.83993325063221&quot;
## [1] &quot;step =  1  lambda =  0.000258658991222064  loss:  1.83724064323829&quot;
## [1] &quot;step =  2  lambda =  0.000258658991222064  loss:  1.83724061611205&quot;
## [1] &quot;step =  3  lambda =  0.000258658991222064  loss:  1.83724114115564&quot;
## [1] &quot;step =  1  lambda =  0.000256085291257132  loss:  1.83457348906926&quot;
## [1] &quot;step =  2  lambda =  0.000256085291257132  loss:  1.83457358965016&quot;
## [1] &quot;step =  1  lambda =  0.00025353720003473  loss:  1.83193157263597&quot;
## [1] &quot;step =  2  lambda =  0.00025353720003473  loss:  1.83193126689338&quot;
## [1] &quot;step =  3  lambda =  0.00025353720003473  loss:  1.8319315117955&quot;
## [1] &quot;step =  1  lambda =  0.000251014462743614  loss:  1.82931398572511&quot;
## [1] &quot;step =  2  lambda =  0.000251014462743614  loss:  1.82931382344716&quot;
## [1] &quot;step =  3  lambda =  0.000251014462743614  loss:  1.82931420408919&quot;
## [1] &quot;step =  1  lambda =  0.000248516827107952  loss:  1.8267209448993&quot;
## [1] &quot;step =  2  lambda =  0.000248516827107952  loss:  1.82672092230091&quot;
## [1] &quot;step =  3  lambda =  0.000248516827107952  loss:  1.82672143694536&quot;
## [1] &quot;step =  1  lambda =  0.000246044043362099  loss:  1.82415222257899&quot;
## [1] &quot;step =  2  lambda =  0.000246044043362099  loss:  1.82415233892383&quot;
## [1] &quot;step =  1  lambda =  0.000243595864225619  loss:  1.82160781875294&quot;
## [1] &quot;step =  2  lambda =  0.000243595864225619  loss:  1.82160755405929&quot;
## [1] &quot;step =  3  lambda =  0.000243595864225619  loss:  1.8216078247067&quot;
## [1] &quot;step =  1  lambda =  0.000241172044878559  loss:  1.81908690170454&quot;
## [1] &quot;step =  2  lambda =  0.000241172044878559  loss:  1.81908679021013&quot;
## [1] &quot;step =  3  lambda =  0.000241172044878559  loss:  1.81908720645531&quot;
## [1] &quot;step =  1  lambda =  0.000238772342936964  loss:  1.81658966458327&quot;
## [1] &quot;step =  2  lambda =  0.000238772342936964  loss:  1.81658970226504&quot;
## [1] &quot;step =  1  lambda =  0.000236396518428641  loss:  1.81411616916019&quot;
## [1] &quot;step =  2  lambda =  0.000236396518428641  loss:  1.81411584474759&quot;
## [1] &quot;step =  3  lambda =  0.000236396518428641  loss:  1.81411604535229&quot;
## [1] &quot;step =  1  lambda =  0.000234044333769158  loss:  1.81166545792076&quot;
## [1] &quot;step =  2  lambda =  0.000234044333769158  loss:  1.81166529498789&quot;
## [1] &quot;step =  3  lambda =  0.000234044333769158  loss:  1.81166564943902&quot;
## [1] &quot;step =  1  lambda =  0.00023171555373809  loss:  1.80923779750348&quot;
## [1] &quot;step =  2  lambda =  0.00023171555373809  loss:  1.80923779164339&quot;
## [1] &quot;step =  3  lambda =  0.00023171555373809  loss:  1.8092382975527&quot;
## [1] &quot;step =  1  lambda =  0.000229409945455492  loss:  1.80683297282257&quot;
## [1] &quot;step =  2  lambda =  0.000229409945455492  loss:  1.80683312266506&quot;
## [1] &quot;step =  1  lambda =  0.000227127278358615  loss:  1.80445092480625&quot;
## [1] &quot;step =  2  lambda =  0.000227127278358615  loss:  1.80445073451047&quot;
## [1] &quot;step =  3  lambda =  0.000227127278358615  loss:  1.80445105383087&quot;
## [1] &quot;step =  1  lambda =  0.000224867324178848  loss:  1.80209096341886&quot;
## [1] &quot;step =  2  lambda =  0.000224867324178848  loss:  1.80209094126063&quot;
## [1] &quot;step =  3  lambda =  0.000224867324178848  loss:  1.80209142145079&quot;
## [1] &quot;step =  1  lambda =  0.000222629856918889  loss:  1.79975323585177&quot;
## [1] &quot;step =  2  lambda =  0.000222629856918889  loss:  1.79975337766246&quot;
## [1] &quot;step =  1  lambda =  0.000220414652830147  loss:  1.79743767652989&quot;
## [1] &quot;step =  2  lambda =  0.000220414652830147  loss:  1.7974374952005&quot;
## [1] &quot;step =  3  lambda =  0.000220414652830147  loss:  1.79743781344288&quot;
## [1] &quot;step =  1  lambda =  0.000218221490390368  loss:  1.79514360882186&quot;
## [1] &quot;step =  2  lambda =  0.000218221490390368  loss:  1.79514360234496&quot;
## [1] &quot;step =  3  lambda =  0.000218221490390368  loss:  1.79514408820416&quot;
## [1] &quot;step =  1  lambda =  0.000216050150281479  loss:  1.79287118296904&quot;
## [1] &quot;step =  2  lambda =  0.000216050150281479  loss:  1.79287134693694&quot;
## [1] &quot;step =  1  lambda =  0.000213900415367661  loss:  1.790620301419&quot;
## [1] &quot;step =  2  lambda =  0.000213900415367661  loss:  1.79062015820758&quot;
## [1] &quot;step =  3  lambda =  0.000213900415367661  loss:  1.79062050444769&quot;
## [1] &quot;step =  1  lambda =  0.000211772070673631  loss:  1.78839036116616&quot;
## [1] &quot;step =  2  lambda =  0.000211772070673631  loss:  1.78839039855461&quot;
## [1] &quot;step =  1  lambda =  0.000209664903363145  loss:  1.78618170547168&quot;
## [1] &quot;step =  2  lambda =  0.000209664903363145  loss:  1.78618144774225&quot;
## [1] &quot;step =  3  lambda =  0.000209664903363145  loss:  1.78618167437984&quot;
## [1] &quot;step =  1  lambda =  0.000207578702717718  loss:  1.78399349371028&quot;
## [1] &quot;step =  2  lambda =  0.000207578702717718  loss:  1.78399342295402&quot;
## [1] &quot;step =  3  lambda =  0.000207578702717718  loss:  1.78399382919949&quot;
## [1] &quot;step =  1  lambda =  0.000205513260115544  loss:  1.78182597270546&quot;
## [1] &quot;step =  2  lambda =  0.000205513260115544  loss:  1.78182608383928&quot;
## [1] &quot;step =  1  lambda =  0.000203468369010644  loss:  1.77967908125059&quot;
## [1] &quot;step =  2  lambda =  0.000203468369010644  loss:  1.77967891110682&quot;
## [1] &quot;step =  3  lambda =  0.000203468369010644  loss:  1.77967921479589&quot;
## [1] &quot;step =  1  lambda =  0.000201443824912203  loss:  1.7775521572188&quot;
## [1] &quot;step =  2  lambda =  0.000201443824912203  loss:  1.77755217776336&quot;
## [1] &quot;step =  1  lambda =  0.000199439425364123  loss:  1.7754455832525&quot;
## [1] &quot;step =  2  lambda =  0.000199439425364123  loss:  1.77544533360621&quot;
## [1] &quot;step =  3  lambda =  0.000199439425364123  loss:  1.77544555268219&quot;
## [1] &quot;step =  1  lambda =  0.000197454969924779  loss:  1.77335853102019&quot;
## [1] &quot;step =  2  lambda =  0.000197454969924779  loss:  1.77335847753743&quot;
## [1] &quot;step =  3  lambda =  0.000197454969924779  loss:  1.77335888564306&quot;
## [1] &quot;step =  1  lambda =  0.000195490260146975  loss:  1.77129125708876&quot;
## [1] &quot;step =  2  lambda =  0.000195490260146975  loss:  1.7712913946533&quot;
## [1] &quot;step =  1  lambda =  0.000193545099558094  loss:  1.76924366005236&quot;
## [1] &quot;step =  2  lambda =  0.000193545099558094  loss:  1.76924354016304&quot;
## [1] &quot;step =  3  lambda =  0.000193545099558094  loss:  1.76924387868721&quot;
## [1] &quot;step =  1  lambda =  0.000191619293640457  loss:  1.7672151752174&quot;
## [1] &quot;step =  2  lambda =  0.000191619293640457  loss:  1.76721525436947&quot;
## [1] &quot;step =  1  lambda =  0.000189712649811868  loss:  1.76520607185379&quot;
## [1] &quot;step =  2  lambda =  0.000189712649811868  loss:  1.76520590389699&quot;
## [1] &quot;step =  3  lambda =  0.000189712649811868  loss:  1.76520618936874&quot;
## [1] &quot;step =  1  lambda =  0.000187824977406354  loss:  1.76321568269715&quot;
## [1] &quot;step =  2  lambda =  0.000187824977406354  loss:  1.76321571868592&quot;
## [1] &quot;step =  1  lambda =  0.000185956087655102  loss:  1.76124436690776&quot;
## [1] &quot;step =  2  lambda =  0.000185956087655102  loss:  1.76124416530827&quot;
## [1] &quot;step =  3  lambda =  0.000185956087655102  loss:  1.76124441186551&quot;
## [1] &quot;step =  1  lambda =  0.000184105793667579  loss:  1.75929138721295&quot;
## [1] &quot;step =  2  lambda =  0.000184105793667579  loss:  1.75929139367493&quot;
## [1] &quot;step =  1  lambda =  0.000182273910412845  loss:  1.75735716466618&quot;
## [1] &quot;step =  2  lambda =  0.000182273910412845  loss:  1.75735694262477&quot;
## [1] &quot;step =  3  lambda =  0.000182273910412845  loss:  1.75735716338666&quot;
## [1] &quot;step =  1  lambda =  0.000180460254701048  loss:  1.75544091923416&quot;
## [1] &quot;step =  2  lambda =  0.000180460254701048  loss:  1.75544090892119&quot;
## [1] &quot;step =  3  lambda =  0.000180460254701048  loss:  1.75544133461995&quot;
## [1] &quot;step =  1  lambda =  0.000178664645165106  loss:  1.75354290997195&quot;
## [1] &quot;step =  2  lambda =  0.000178664645165106  loss:  1.75354310614278&quot;
## [1] &quot;step =  1  lambda =  0.000176886902242567  loss:  1.7516629539953&quot;
## [1] &quot;step =  2  lambda =  0.000176886902242567  loss:  1.75166293246455&quot;
## [1] &quot;step =  3  lambda =  0.000176886902242567  loss:  1.75166334372372&quot;
## [1] &quot;step =  1  lambda =  0.000175126848157658  loss:  1.74980067819441&quot;
## [1] &quot;step =  2  lambda =  0.000175126848157658  loss:  1.74980086974135&quot;
## [1] &quot;step =  1  lambda =  0.000173384306903506  loss:  1.74795613387986&quot;
## [1] &quot;step =  2  lambda =  0.000173384306903506  loss:  1.74795611681703&quot;
## [1] &quot;step =  3  lambda =  0.000173384306903506  loss:  1.74795652775072&quot;
## [1] &quot;step =  1  lambda =  0.00017165910422453  loss:  1.74612895178171&quot;
## [1] &quot;step =  2  lambda =  0.00017165910422453  loss:  1.74612915172298&quot;
## [1] &quot;step =  1  lambda =  0.000169951067599028  loss:  1.74431916897321&quot;
## [1] &quot;step =  2  lambda =  0.000169951067599028  loss:  1.74431916870473&quot;
## [1] &quot;step =  3  lambda =  0.000169951067599028  loss:  1.74431959142577&quot;
## [1] &quot;step =  1  lambda =  0.000168260026221911  loss:  1.74252644789159&quot;
## [1] &quot;step =  2  lambda =  0.000168260026221911  loss:  1.74252666792575&quot;
## [1] &quot;step =  1  lambda =  0.000166585810987634  loss:  1.74075078742882&quot;
## [1] &quot;step =  2  lambda =  0.000166585810987634  loss:  1.74075081529269&quot;
## [1] &quot;step =  1  lambda =  0.000164928254473277  loss:  1.73899203205779&quot;
## [1] &quot;step =  2  lambda =  0.000164928254473277  loss:  1.73899187534127&quot;
## [1] &quot;step =  3  lambda =  0.000164928254473277  loss:  1.73899213679417&quot;
## [1] &quot;step =  1  lambda =  0.000163287190921808  loss:  1.7372497197758&quot;
## [1] &quot;step =  2  lambda =  0.000163287190921808  loss:  1.73724979249126&quot;
## [1] &quot;step =  1  lambda =  0.000161662456225505  loss:  1.73552415179999&quot;
## [1] &quot;step =  2  lambda =  0.000161662456225505  loss:  1.73552404559541&quot;
## [1] &quot;step =  3  lambda =  0.000161662456225505  loss:  1.73552435138542&quot;
## [1] &quot;step =  1  lambda =  0.000160053887909543  loss:  1.73381477094257&quot;
## [1] &quot;step =  2  lambda =  0.000160053887909543  loss:  1.73381489490907&quot;
## [1] &quot;step =  1  lambda =  0.000158461325115751  loss:  1.73212177583949&quot;
## [1] &quot;step =  2  lambda =  0.000158461325115751  loss:  1.73212172779472&quot;
## [1] &quot;step =  3  lambda =  0.000158461325115751  loss:  1.73212208638591&quot;
## [1] &quot;step =  1  lambda =  0.000156884608586522  loss:  1.73044472726677&quot;
## [1] &quot;step =  2  lambda =  0.000156884608586522  loss:  1.73044491113942&quot;
## [1] &quot;step =  1  lambda =  0.00015532358064889  loss:  1.72878370431029&quot;
## [1] &quot;step =  2  lambda =  0.00015532358064889  loss:  1.7287837231128&quot;
## [1] &quot;step =  1  lambda =  0.000153778085198759  loss:  1.72713851341794&quot;
## [1] &quot;step =  2  lambda =  0.000153778085198759  loss:  1.72713837407027&quot;
## [1] &quot;step =  3  lambda =  0.000153778085198759  loss:  1.72713863632229&quot;
## [1] &quot;step =  1  lambda =  0.000152247967685297  loss:  1.72550874367667&quot;
## [1] &quot;step =  2  lambda =  0.000152247967685297  loss:  1.72550884365597&quot;
## [1] &quot;step =  1  lambda =  0.000150733075095477  loss:  1.723894647723&quot;
## [1] &quot;step =  2  lambda =  0.000150733075095477  loss:  1.72389459448291&quot;
## [1] &quot;step =  3  lambda =  0.000150733075095477  loss:  1.72389493682751&quot;
## [1] &quot;step =  1  lambda =  0.000149233255938777  loss:  1.72229577143182&quot;
## [1] &quot;step =  2  lambda =  0.000149233255938777  loss:  1.72229595766069&quot;
## [1] &quot;step =  1  lambda =  0.000147748360232034  loss:  1.7207121941587&quot;
## [1] &quot;step =  2  lambda =  0.000147748360232034  loss:  1.7207122333423&quot;
## [1] &quot;step =  1  lambda =  0.000146278239484437  loss:  1.71914372375592&quot;
## [1] &quot;step =  2  lambda =  0.000146278239484437  loss:  1.71914362237317&quot;
## [1] &quot;step =  3  lambda =  0.000146278239484437  loss:  1.71914391140796&quot;
## [1] &quot;step =  1  lambda =  0.000144822746682688  loss:  1.71759000981523&quot;
## [1] &quot;step =  2  lambda =  0.000144822746682688  loss:  1.7175901542009&quot;
## [1] &quot;step =  1  lambda =  0.000143381736276293  loss:  1.71605121997021&quot;
## [1] &quot;step =  2  lambda =  0.000143381736276293  loss:  1.71605122825272&quot;
## [1] &quot;step =  1  lambda =  0.000141955064163011  loss:  1.71452711756415&quot;
## [1] &quot;step =  2  lambda =  0.000141955064163011  loss:  1.71452699586374&quot;
## [1] &quot;step =  3  lambda =  0.000141955064163011  loss:  1.71452725859872&quot;
## [1] &quot;step =  1  lambda =  0.000140542587674442  loss:  1.71301734516151&quot;
## [1] &quot;step =  2  lambda =  0.000140542587674442  loss:  1.71301747374063&quot;
## [1] &quot;step =  1  lambda =  0.000139144165561759  loss:  1.7115221053278&quot;
## [1] &quot;step =  2  lambda =  0.000139144165561759  loss:  1.71152210796787&quot;
## [1] &quot;step =  1  lambda =  0.000137759657981586  loss:  1.71004114480165&quot;
## [1] &quot;step =  2  lambda =  0.000137759657981586  loss:  1.71004102736961&quot;
## [1] &quot;step =  3  lambda =  0.000137759657981586  loss:  1.7100412882941&quot;
## [1] &quot;step =  1  lambda =  0.000136388926482011  loss:  1.70857412311213&quot;
## [1] &quot;step =  2  lambda =  0.000136388926482011  loss:  1.70857425980059&quot;
## [1] &quot;step =  1  lambda =  0.000135031833988743  loss:  1.70712122915978&quot;
## [1] &quot;step =  2  lambda =  0.000135031833988743  loss:  1.70712124953956&quot;
## [1] &quot;step =  1  lambda =  0.0001336882447914  loss:  1.70568221759149&quot;
## [1] &quot;step =  2  lambda =  0.0001336882447914  loss:  1.70568212730789&quot;
## [1] &quot;step =  3  lambda =  0.0001336882447914  loss:  1.70568240932018&quot;
## [1] &quot;step =  1  lambda =  0.000132358024529944  loss:  1.70425678724702&quot;
## [1] &quot;step =  2  lambda =  0.000132358024529944  loss:  1.70425695447704&quot;
## [1] &quot;step =  1  lambda =  0.000131041040181239  loss:  1.70284506798607&quot;
## [1] &quot;step =  2  lambda =  0.000131041040181239  loss:  1.70284512809407&quot;
## [1] &quot;step =  1  lambda =  0.000129737160045754  loss:  1.70144684515651&quot;
## [1] &quot;step =  2  lambda =  0.000129737160045754  loss:  1.70144680358458&quot;
## [1] &quot;step =  3  lambda =  0.000129737160045754  loss:  1.7014471283163&quot;
## [1] &quot;step =  1  lambda =  0.000128446253734388  loss:  1.70006187799325&quot;
## [1] &quot;step =  2  lambda =  0.000128446253734388  loss:  1.70006209698795&quot;
## [1] &quot;step =  1  lambda =  0.000127168192155434  loss:  1.6986901944912&quot;
## [1] &quot;step =  2  lambda =  0.000127168192155434  loss:  1.69869031515687&quot;
## [1] &quot;step =  1  lambda =  0.00012590284750167  loss:  1.6973316321706&quot;
## [1] &quot;step =  2  lambda =  0.00012590284750167  loss:  1.69733165975822&quot;
## [1] &quot;step =  1  lambda =  0.000124650093237575  loss:  1.6959860691964&quot;
## [1] &quot;step =  2  lambda =  0.000124650093237575  loss:  1.69598600816799&quot;
## [1] &quot;step =  3  lambda =  0.000124650093237575  loss:  1.69598630692277&quot;
## [1] &quot;step =  1  lambda =  0.00012340980408668  loss:  1.69465326079734&quot;
## [1] &quot;step =  2  lambda =  0.00012340980408668  loss:  1.69465346634229&quot;
## [1] &quot;step =  1  lambda =  0.000122181856019035  loss:  1.69333326647092&quot;
## [1] &quot;step =  2  lambda =  0.000122181856019035  loss:  1.69333338575448&quot;
## [1] &quot;step =  1  lambda =  0.00012096612623881  loss:  1.6920259086261&quot;
## [1] &quot;step =  2  lambda =  0.00012096612623881  loss:  1.69202594656043&quot;
## [1] &quot;step =  1  lambda =  0.000119762493172015  loss:  1.69073106984798&quot;
## [1] &quot;step =  2  lambda =  0.000119762493172015  loss:  1.69073103061129&quot;
## [1] &quot;step =  3  lambda =  0.000119762493172015  loss:  1.6907313441743&quot;
## [1] &quot;step =  1  lambda =  0.000118570836454339  loss:  1.68944854191045&quot;
## [1] &quot;step =  2  lambda =  0.000118570836454339  loss:  1.68944877372132&quot;
## [1] &quot;step =  1  lambda =  0.000117391036919118  loss:  1.68817833490492&quot;
## [1] &quot;step =  2  lambda =  0.000117391036919118  loss:  1.68817849157195&quot;
## [1] &quot;step =  1  lambda =  0.000116222976585415  loss:  1.68692029731783&quot;
## [1] &quot;step =  2  lambda =  0.000116222976585415  loss:  1.68692038349346&quot;
## [1] &quot;step =  1  lambda =  0.000115066538646224  loss:  1.68567431605631&quot;
## [1] &quot;step =  2  lambda =  0.000115066538646224  loss:  1.68567433569397&quot;
## [1] &quot;step =  1  lambda =  0.000113921607456786  loss:  1.68444027845753&quot;
## [1] &quot;step =  2  lambda =  0.000113921607456786  loss:  1.68444023510908&quot;
## [1] &quot;step =  3  lambda =  0.000113921607456786  loss:  1.68444053666867&quot;
## [1] &quot;step =  1  lambda =  0.000112788068523029  loss:  1.68321798991637&quot;
## [1] &quot;step =  2  lambda =  0.000112788068523029  loss:  1.68321822370643&quot;
## [1] &quot;step =  1  lambda =  0.000111665808490115  loss:  1.68200746123079&quot;
## [1] &quot;step =  2  lambda =  0.000111665808490115  loss:  1.68200763340703&quot;
## [1] &quot;step =  1  lambda =  0.000110554715131105  loss:  1.68080854187394&quot;
## [1] &quot;step =  2  lambda =  0.000110554715131105  loss:  1.68080865674719&quot;
## [1] &quot;step =  1  lambda =  0.000109454677335737  loss:  1.67962112390941&quot;
## [1] &quot;step =  2  lambda =  0.000109454677335737  loss:  1.67962118514074&quot;
## [1] &quot;step =  1  lambda =  0.000108365585099315  loss:  1.67844509982759&quot;
## [1] &quot;step =  2  lambda =  0.000108365585099315  loss:  1.67844511070758&quot;
## [1] &quot;step =  1  lambda =  0.000107287329511708  loss:  1.67728036281466&quot;
## [1] &quot;step =  2  lambda =  0.000107287329511708  loss:  1.67728032641638&quot;
## [1] &quot;step =  3  lambda =  0.000107287329511708  loss:  1.67728062576871&quot;
## [1] &quot;step =  1  lambda =  0.000106219802746459  loss:  1.67612674532629&quot;
## [1] &quot;step =  2  lambda =  0.000106219802746459  loss:  1.67612699256205&quot;
## [1] &quot;step =  1  lambda =  0.000105162898050001  loss:  1.67498423621151&quot;
## [1] &quot;step =  2  lambda =  0.000105162898050001  loss:  1.67498443700714&quot;
## [1] &quot;step =  1  lambda =  0.000104116509730984  loss:  1.67385269774396&quot;
## [1] &quot;step =  2  lambda =  0.000104116509730984  loss:  1.67385285604652&quot;
## [1] &quot;step =  1  lambda =  0.000103080533149705  loss:  1.67273202789262&quot;
## [1] &quot;step =  2  lambda =  0.000103080533149705  loss:  1.67273214704946&quot;
## [1] &quot;step =  1  lambda =  0.000102054864707641  loss:  1.67162212504341&quot;
## [1] &quot;step =  2  lambda =  0.000102054864707641  loss:  1.67162220806295&quot;
## [1] &quot;step =  1  lambda =  0.000101039401837093  loss:  1.67052288825033&quot;
## [1] &quot;step =  2  lambda =  0.000101039401837093  loss:  1.67052293794423&quot;
## [1] &quot;step =  1  lambda =  0.00010003404299093  loss:  1.66943421736795&quot;
## [1] &quot;step =  2  lambda =  0.00010003404299093  loss:  1.66943423641891&quot;
## [1] &quot;step =  1  lambda =  9.9038687632427e-05  loss:  1.66835601310969&quot;
## [1] &quot;step =  2  lambda =  9.9038687632427e-05  loss:  1.66835600410354&quot;
## [1] &quot;step =  3  lambda =  9.9038687632427e-05  loss:  1.66835631934946&quot;
## [1] &quot;step =  1  lambda =  9.80532362252201e-05  loss:  1.66728816021922&quot;
## [1] &quot;step =  2  lambda =  9.80532362252201e-05  loss:  1.66728844258723&quot;
## [1] &quot;step =  1  lambda =  9.70775902233471e-05  loss:  1.66623058313428&quot;
## [1] &quot;step =  2  lambda =  9.70775902233471e-05  loss:  1.66623083772554&quot;
## [1] &quot;step =  1  lambda =  9.61116520613947e-05  loss:  1.66518317885258&quot;
## [1] &quot;step =  2  lambda =  9.61116520613947e-05  loss:  1.66518340917833&quot;
## [1] &quot;step =  1  lambda =  9.51553251447417e-05  loss:  1.66414585273896&quot;
## [1] &quot;step =  2  lambda =  9.51553251447417e-05  loss:  1.66414606176961&quot;
## [1] &quot;step =  1  lambda =  9.42085138398996e-05  loss:  1.66311851055924&quot;
## [1] &quot;step =  2  lambda =  9.42085138398996e-05  loss:  1.66311870096328&quot;
## [1] &quot;step =  1  lambda =  9.32711234649488e-05  loss:  1.66210105871015&quot;
## [1] &quot;step =  2  lambda =  9.32711234649488e-05  loss:  1.66210123298331&quot;
## [1] &quot;step =  1  lambda =  9.23430602800707e-05  loss:  1.66109340433959&quot;
## [1] &quot;step =  2  lambda =  9.23430602800707e-05  loss:  1.66109356486583&quot;
## [1] &quot;step =  1  lambda =  9.14242314781733e-05  loss:  1.66009545539876&quot;
## [1] &quot;step =  2  lambda =  9.14242314781733e-05  loss:  1.66009560447868&quot;
## [1] &quot;step =  1  lambda =  9.05145451756109e-05  loss:  1.65910712066185&quot;
## [1] &quot;step =  2  lambda =  9.05145451756109e-05  loss:  1.65910726052739&quot;
## [1] &quot;step =  1  lambda =  8.96139104029952e-05  loss:  1.65812830973203&quot;
## [1] &quot;step =  2  lambda =  8.96139104029952e-05  loss:  1.65812844255544&quot;
## [1] &quot;step =  1  lambda =  8.87222370960982e-05  loss:  1.65715893304186&quot;
## [1] &quot;step =  2  lambda =  8.87222370960982e-05  loss:  1.65715906094191&quot;
## [1] &quot;step =  1  lambda =  8.78394360868463e-05  loss:  1.65619890185099&quot;
## [1] &quot;step =  2  lambda =  8.78394360868463e-05  loss:  1.65619902689777&quot;
## [1] &quot;step =  1  lambda =  8.69654190944029e-05  loss:  1.65524812824253&quot;
## [1] &quot;step =  2  lambda =  8.69654190944029e-05  loss:  1.65524825246131&quot;
## [1] &quot;step =  1  lambda =  8.61000987163404e-05  loss:  1.65430652511859&quot;
## [1] &quot;step =  2  lambda =  8.61000987163404e-05  loss:  1.65430665049309&quot;
## [1] &quot;step =  1  lambda =  8.52433884198997e-05  loss:  1.65337400619526&quot;
## [1] &quot;step =  2  lambda =  8.52433884198997e-05  loss:  1.65337413467034&quot;
## [1] &quot;step =  1  lambda =  8.43952025333374e-05  loss:  1.65245048599714&quot;
## [1] &quot;step =  2  lambda =  8.43952025333374e-05  loss:  1.65245061948118&quot;
## [1] &quot;step =  1  lambda =  8.3555456237358e-05  loss:  1.65153587985159&quot;
## [1] &quot;step =  2  lambda =  8.3555456237358e-05  loss:  1.65153602021847&quot;
## [1] &quot;step =  1  lambda =  8.27240655566322e-05  loss:  1.65063010388267&quot;
## [1] &quot;step =  2  lambda =  8.27240655566322e-05  loss:  1.65063025297349&quot;
## [1] &quot;step =  1  lambda =  8.1900947351399e-05  loss:  1.64973307500495&quot;
## [1] &quot;step =  2  lambda =  8.1900947351399e-05  loss:  1.64973323462948&quot;
## [1] &quot;step =  1  lambda =  8.1086019309152e-05  loss:  1.64884471091702&quot;
## [1] &quot;step =  2  lambda =  8.1086019309152e-05  loss:  1.64884488285509&quot;
## [1] &quot;step =  1  lambda =  8.02791999364078e-05  loss:  1.64796493009515&quot;
## [1] &quot;step =  2  lambda =  8.02791999364078e-05  loss:  1.64796511609762&quot;
## [1] &quot;step =  1  lambda =  7.94804085505568e-05  loss:  1.64709365178646&quot;
## [1] &quot;step =  2  lambda =  7.94804085505568e-05  loss:  1.6470938535763&quot;
## [1] &quot;step =  1  lambda =  7.86895652717947e-05  loss:  1.64623079600239&quot;
## [1] &quot;step =  2  lambda =  7.86895652717947e-05  loss:  1.64623101527552&quot;
## [1] &quot;step =  1  lambda =  7.79065910151345e-05  loss:  1.64537628351195&quot;
## [1] &quot;step =  2  lambda =  7.79065910151345e-05  loss:  1.64537652193794&quot;
## [1] &quot;step =  1  lambda =  7.71314074824984e-05  loss:  1.6445300358349&quot;
## [1] &quot;step =  2  lambda =  7.71314074824984e-05  loss:  1.64453029505774&quot;
## [1] &quot;step =  1  lambda =  7.63639371548869e-05  loss:  1.64369197523506&quot;
## [1] &quot;step =  2  lambda =  7.63639371548869e-05  loss:  1.64369225687366&quot;
## [1] &quot;step =  1  lambda =  7.56041032846277e-05  loss:  1.6428620247135&quot;
## [1] &quot;step =  2  lambda =  7.56041032846277e-05  loss:  1.6428623303623&quot;
## [1] &quot;step =  1  lambda =  7.48518298877006e-05  loss:  1.64204010800179&quot;
## [1] &quot;step =  2  lambda =  7.48518298877006e-05  loss:  1.64204043923122&quot;
## [1] &quot;step =  1  lambda =  7.4107041736139e-05  loss:  1.64122614955528&quot;
## [1] &quot;step =  2  lambda =  7.4107041736139e-05  loss:  1.64122650791219&quot;
## [1] &quot;step =  1  lambda =  7.33696643505071e-05  loss:  1.64042007454635&quot;
## [1] &quot;step =  2  lambda =  7.33696643505071e-05  loss:  1.64042046155444&quot;
## [1] &quot;step =  1  lambda =  7.26396239924518e-05  loss:  1.63962180885773&quot;
## [1] &quot;step =  2  lambda =  7.26396239924518e-05  loss:  1.63962222601797&quot;
## [1] &quot;step =  1  lambda =  7.1916847657329e-05  loss:  1.63883127907593&quot;
## [1] &quot;step =  2  lambda =  7.1916847657329e-05  loss:  1.63883172786686&quot;
## [1] &quot;step =  1  lambda =  7.12012630669027e-05  loss:  1.63804841248457&quot;
## [1] &quot;step =  2  lambda =  7.12012630669027e-05  loss:  1.63804889436265&quot;
## [1] &quot;step =  1  lambda =  7.04927986621178e-05  loss:  1.63727313705783&quot;
## [1] &quot;step =  2  lambda =  7.04927986621178e-05  loss:  1.63727365345778&quot;
## [1] &quot;step =  1  lambda =  6.97913835959434e-05  loss:  1.63650538145398&quot;
## [1] &quot;step =  2  lambda =  6.97913835959434e-05  loss:  1.63650593378908&quot;
## [1] &quot;step =  1  lambda =  6.90969477262882e-05  loss:  1.6357450750089&quot;
## [1] &quot;step =  2  lambda =  6.90969477262882e-05  loss:  1.63574566467123&quot;
## [1] &quot;step =  1  lambda =  6.84094216089865e-05  loss:  1.63499214772967&quot;
## [1] &quot;step =  2  lambda =  6.84094216089865e-05  loss:  1.63499277609047&quot;
## [1] &quot;step =  1  lambda =  6.77287364908539e-05  loss:  1.63424653028824&quot;
## [1] &quot;step =  2  lambda =  6.77287364908539e-05  loss:  1.6342471986981&quot;
## [1] &quot;step =  1  lambda =  6.70548243028111e-05  loss:  1.63350815401508&quot;
## [1] &quot;step =  2  lambda =  6.70548243028111e-05  loss:  1.63350886380425&quot;
## [1] &quot;step =  1  lambda =  6.63876176530778e-05  loss:  1.63277695089302&quot;
## [1] &quot;step =  2  lambda =  6.63876176530778e-05  loss:  1.63277770337167&quot;
## [1] &quot;step =  1  lambda =  6.5727049820433e-05  loss:  1.63205285355099&quot;
## [1] &quot;step =  2  lambda =  6.5727049820433e-05  loss:  1.63205365000942&quot;
## [1] &quot;step =  1  lambda =  6.50730547475429e-05  loss:  1.63133579525791&quot;
## [1] &quot;step =  2  lambda =  6.50730547475429e-05  loss:  1.63133663696682&quot;
## [1] &quot;step =  1  lambda =  6.44255670343554e-05  loss:  1.63062570991662&quot;
## [1] &quot;step =  2  lambda =  6.44255670343554e-05  loss:  1.63062659812733&quot;
## [1] &quot;step =  1  lambda =  6.37845219315595e-05  loss:  1.62992253205781&quot;
## [1] &quot;step =  2  lambda =  6.37845219315595e-05  loss:  1.62992346800251&quot;
## [1] &quot;step =  1  lambda =  6.31498553341106e-05  loss:  1.62922619683414&quot;
## [1] &quot;step =  2  lambda =  6.31498553341106e-05  loss:  1.6292271817261&quot;
## [1] &quot;step =  1  lambda =  6.25215037748203e-05  loss:  1.62853664001424&quot;
## [1] &quot;step =  2  lambda =  6.25215037748203e-05  loss:  1.62853767504804&quot;
## [1] &quot;step =  1  lambda =  6.18994044180088e-05  loss:  1.62785379797689&quot;
## [1] &quot;step =  2  lambda =  6.18994044180088e-05  loss:  1.62785488432866&quot;
## [1] &quot;step =  1  lambda =  6.12834950532221e-05  loss:  1.62717760770524&quot;
## [1] &quot;step =  2  lambda =  6.12834950532221e-05  loss:  1.62717874653282&quot;
## [1] &quot;step =  1  lambda =  6.06737140890104e-05  loss:  1.62650800678097&quot;
## [1] &quot;step =  2  lambda =  6.06737140890104e-05  loss:  1.6265091992242&quot;
## [1] &quot;step =  1  lambda =  6.00700005467694e-05  loss:  1.6258449333787&quot;
## [1] &quot;step =  2  lambda =  6.00700005467694e-05  loss:  1.62584618055961&quot;
## [1] &quot;step =  1  lambda =  5.94722940546415e-05  loss:  1.6251883262603&quot;
## [1] &quot;step =  2  lambda =  5.94722940546415e-05  loss:  1.62518962928326&quot;
## [1] &quot;step =  1  lambda =  5.88805348414794e-05  loss:  1.62453812476929&quot;
## [1] &quot;step =  2  lambda =  5.88805348414794e-05  loss:  1.6245394847213&quot;
## [1] &quot;step =  1  lambda =  5.82946637308688e-05  loss:  1.62389426882532&quot;
## [1] &quot;step =  2  lambda =  5.82946637308688e-05  loss:  1.62389568677615&quot;
## [1] &quot;step =  1  lambda =  5.77146221352103e-05  loss:  1.62325669891868&quot;
## [1] &quot;step =  2  lambda =  5.77146221352103e-05  loss:  1.62325817592109&quot;
## [1] &quot;step =  1  lambda =  5.71403520498611e-05  loss:  1.62262535610485&quot;
## [1] &quot;step =  2  lambda =  5.71403520498611e-05  loss:  1.62262689319479&quot;
## [1] &quot;step =  1  lambda =  5.65717960473339e-05  loss:  1.62200018199915&quot;
## [1] &quot;step =  2  lambda =  5.65717960473339e-05  loss:  1.62200178019598&quot;
## [1] &quot;step =  1  lambda =  5.60088972715548e-05  loss:  1.6213811187714&quot;
## [1] &quot;step =  2  lambda =  5.60088972715548e-05  loss:  1.62138277907804&quot;
## [1] &quot;step =  1  lambda =  5.54515994321769e-05  loss:  1.62076810914061&quot;
## [1] &quot;step =  2  lambda =  5.54515994321769e-05  loss:  1.62076983254375&quot;
## [1] &quot;step =  1  lambda =  5.48998467989523e-05  loss:  1.62016109636979&quot;
## [1] &quot;step =  2  lambda =  5.48998467989523e-05  loss:  1.62016288384009&quot;
## [1] &quot;step =  1  lambda =  5.43535841961575e-05  loss:  1.61956002426075&quot;
## [1] &quot;step =  2  lambda =  5.43535841961575e-05  loss:  1.61956187675298&quot;
## [1] &quot;step =  1  lambda =  5.38127569970771e-05  loss:  1.61896483714894&quot;
## [1] &quot;step =  2  lambda =  5.38127569970771e-05  loss:  1.61896675560224&quot;
## [1] &quot;step =  1  lambda =  5.32773111185406e-05  loss:  1.61837547989843&quot;
## [1] &quot;step =  2  lambda =  5.32773111185406e-05  loss:  1.61837746523643&quot;
## [1] &quot;step =  1  lambda =  5.27471930155139e-05  loss:  1.61779189789682&quot;
## [1] &quot;step =  2  lambda =  5.27471930155139e-05  loss:  1.61779395102785&quot;
## [1] &quot;step =  1  lambda =  5.22223496757448e-05  loss:  1.61721403705027&quot;
## [1] &quot;step =  2  lambda =  5.22223496757448e-05  loss:  1.61721615886752&quot;
## [1] &quot;step =  1  lambda =  5.1702728614462e-05  loss:  1.61664184377857&quot;
## [1] &quot;step =  2  lambda =  5.1702728614462e-05  loss:  1.61664403516028&quot;
## [1] &quot;step =  1  lambda =  5.11882778691264e-05  loss:  1.61607526501022&quot;
## [1] &quot;step =  2  lambda =  5.11882778691264e-05  loss:  1.61607752681983&quot;
## [1] &quot;step =  1  lambda =  5.06789459942348e-05  loss:  1.61551424817763&quot;
## [1] &quot;step =  2  lambda =  5.06789459942348e-05  loss:  1.61551658126398&quot;
## [1] &quot;step =  1  lambda =  5.01746820561753e-05  loss:  1.61495874121224&quot;
## [1] &quot;step =  2  lambda =  5.01746820561753e-05  loss:  1.61496114640975&quot;
## [1] &quot;step =  1  lambda =  4.96754356281337e-05  loss:  1.61440869253987&quot;
## [1] &quot;step =  2  lambda =  4.96754356281337e-05  loss:  1.61441117066864&quot;
## [1] &quot;step =  1  lambda =  4.91811567850513e-05  loss:  1.6138640510759&quot;
## [1] &quot;step =  2  lambda =  4.91811567850513e-05  loss:  1.61386660294197&quot;
## [1] &quot;step =  1  lambda =  4.86917960986318e-05  loss:  1.6133247662207&quot;
## [1] &quot;step =  2  lambda =  4.86917960986318e-05  loss:  1.61332739261612&quot;
## [1] &quot;step =  1  lambda =  4.82073046323988e-05  loss:  1.61279078785492&quot;
## [1] &quot;step =  2  lambda =  4.82073046323988e-05  loss:  1.61279348955798&quot;
## [1] &quot;step =  1  lambda =  4.7727633936802e-05  loss:  1.61226206633499&quot;
## [1] &quot;step =  2  lambda =  4.7727633936802e-05  loss:  1.61226484411037&quot;
## [1] &quot;step =  1  lambda =  4.72527360443719e-05  loss:  1.61173855248856&quot;
## [1] &quot;step =  2  lambda =  4.72527360443719e-05  loss:  1.61174140708747&quot;
## [1] &quot;step =  1  lambda =  4.67825634649237e-05  loss:  1.61122019760997&quot;
## [1] &quot;step =  2  lambda =  4.67825634649237e-05  loss:  1.61122312977028&quot;
## [1] &quot;step =  1  lambda =  4.63170691808076e-05  loss:  1.61070695345585&quot;
## [1] &quot;step =  2  lambda =  4.63170691808076e-05  loss:  1.61070996390231&quot;
## [1] &quot;step =  1  lambda =  4.58562066422073e-05  loss:  1.6101987722407&quot;
## [1] &quot;step =  2  lambda =  4.58562066422073e-05  loss:  1.61020186168507&quot;
## [1] &quot;step =  1  lambda =  4.53999297624849e-05  loss:  1.60969560663251&quot;
## [1] &quot;step =  2  lambda =  4.53999297624849e-05  loss:  1.60969877577367&quot;
## [1] &quot;step =  1  lambda =  0  loss:  1.55931106360552&quot;
## [1] &quot;step =  2  lambda =  0  loss:  1.55929875305478&quot;
## [1] &quot;step =  3  lambda =  0  loss:  1.55928696392609&quot;
## [1] &quot;step =  4  lambda =  0  loss:  1.55927557051871&quot;
## [1] &quot;step =  5  lambda =  0  loss:  1.55926452002297&quot;
## [1] &quot;step =  6  lambda =  0  loss:  1.55925378162392&quot;
## [1] &quot;step =  7  lambda =  0  loss:  1.55924333580901&quot;
## [1] &quot;step =  8  lambda =  0  loss:  1.55923316877163&quot;
## [1] &quot;step =  9  lambda =  0  loss:  1.55922326984502&quot;
## [1] &quot;step =  10  lambda =  0  loss:  1.559213630358&quot;
## [1] &quot;step =  11  lambda =  0  loss:  1.5592042430745&quot;
## [1] &quot;step =  12  lambda =  0  loss:  1.55919510187357&quot;
## [1] &quot;step =  13  lambda =  0  loss:  1.55918620153693&quot;
## [1] &quot;step =  14  lambda =  0  loss:  1.5591775375914&quot;
## [1] &quot;step =  15  lambda =  0  loss:  1.55916910618433&quot;
## [1] &quot;step =  16  lambda =  0  loss:  1.55916090398166&quot;
## [1] &quot;step =  17  lambda =  0  loss:  1.55915292808319&quot;
## [1] &quot;step =  18  lambda =  0  loss:  1.5591451759515&quot;
## [1] &quot;step =  19  lambda =  0  loss:  1.55913764535236&quot;
## [1] &quot;step =  20  lambda =  0  loss:  1.55913033430448&quot;
## [1] &quot;step =  21  lambda =  0  loss:  1.55912324103742&quot;
## [1] &quot;step =  22  lambda =  0  loss:  1.55911636395613&quot;
## [1] &quot;step =  23  lambda =  0  loss:  1.55910970161145&quot;
## [1] &quot;step =  24  lambda =  0  loss:  1.55910325267523&quot;
## [1] &quot;step =  25  lambda =  0  loss:  1.55909701591977&quot;
## [1] &quot;step =  26  lambda =  0  loss:  1.55909099020055&quot;
## [1] &quot;step =  27  lambda =  0  loss:  1.55908517444203&quot;
## [1] &quot;step =  28  lambda =  0  loss:  1.55907956762577&quot;
## [1] &quot;step =  29  lambda =  0  loss:  1.55907416878065&quot;
## [1] &quot;step =  30  lambda =  0  loss:  1.55906897697484&quot;
## [1] &quot;step =  31  lambda =  0  loss:  1.55906399130908&quot;
## [1] &quot;step =  32  lambda =  0  loss:  1.55905921091134&quot;
## [1] &quot;step =  33  lambda =  0  loss:  1.5590546349322&quot;
## [1] &quot;step =  34  lambda =  0  loss:  1.5590502625413&quot;
## [1] &quot;step =  35  lambda =  0  loss:  1.55904609292434&quot;
## [1] &quot;step =  36  lambda =  0  loss:  1.55904212528052&quot;
## [1] &quot;step =  37  lambda =  0  loss:  1.55903835882074&quot;
## [1] &quot;step =  38  lambda =  0  loss:  1.55903479276585&quot;
## [1] &quot;step =  39  lambda =  0  loss:  1.5590314263454&quot;
## [1] &quot;step =  40  lambda =  0  loss:  1.55902825879656&quot;
## [1] &quot;step =  41  lambda =  0  loss:  1.55902528936332&quot;
## [1] &quot;step =  42  lambda =  0  loss:  1.55902251729568&quot;
## [1] &quot;step =  43  lambda =  0  loss:  1.55901994184922&quot;
## [1] &quot;step =  44  lambda =  0  loss:  1.55901756228459&quot;
## [1] &quot;step =  45  lambda =  0  loss:  1.5590153778671&quot;
## [1] &quot;step =  46  lambda =  0  loss:  1.5590133878665&quot;
## [1] &quot;step =  47  lambda =  0  loss:  1.55901159155672&quot;
## [1] &quot;step =  48  lambda =  0  loss:  1.55900998821563&quot;
## [1] &quot;step =  49  lambda =  0  loss:  1.55900857712488&quot;
## [1] &quot;step =  50  lambda =  0  loss:  1.55900735756984&quot;
## [1] &quot;step =  51  lambda =  0  loss:  1.55900632883941&quot;
## [1] &quot;step =  52  lambda =  0  loss:  1.55900549022597&quot;
## [1] &quot;step =  53  lambda =  0  loss:  1.55900484102524&quot;
## [1] &quot;step =  54  lambda =  0  loss:  1.55900438053627&quot;
## [1] &quot;step =  55  lambda =  0  loss:  1.55900410806137&quot;
## [1] &quot;step =  56  lambda =  0  loss:  1.559004022906&quot;
## [1] &quot;step =  57  lambda =  0  loss:  1.55900412437878&quot;
## [1] &quot;fold:  5&quot;
## [1] &quot;step =  1  lambda =  22026.4657948067  loss:  74.5163924083676&quot;
## [1] &quot;step =  2  lambda =  22026.4657948067  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  21807.2987982302  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  21590.3125497062  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  21375.4853504291  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  21162.7957175002  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  20952.2223817786  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  20743.7442857556  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  20537.3405814475  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  20332.9906283122  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  20130.6739911839  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  19930.3704382303  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  19732.0599389292  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  19535.7226620655  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  19341.3389737478  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  19148.8894354453  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  18958.3548020439  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  18769.7160199212  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  18582.9542250422  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  18398.0507410714  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  18214.9870775064  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  18033.7449278285  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  17854.3061676715  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  17676.65285301  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  17500.7672183642  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  17326.6316750244  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  17154.228809291  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  16983.5413807338  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  16814.5523204676  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  16647.2447294456  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  16481.6018767693  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  16317.6071980154  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  16155.2442935794  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  15994.4969270355  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  15835.3490235131  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  15677.7846680892  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  15521.788104197  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  15367.34373205  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  15214.4361070824  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  15063.0499384043  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  14913.1700872726  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  14764.7815655773  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  14617.8695343426  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  14472.4193022429  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  14328.4163241338  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  14185.8461995975  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  14044.6946715028  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  13904.9476245792  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  13766.5910840055  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  13629.6112140124  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  13493.9943164988  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  13359.7268296619  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  13226.7953266411  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  13095.1865141752  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12964.8872312735  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12835.8844478991  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12708.165263666  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12581.7169065495  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12456.5267316084  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12332.582219721  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12209.8709763327  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  12088.380730217  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11968.099332248  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11849.0147541856  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11731.1150874729  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11614.3885420449  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11498.8234451498  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11384.4082401816  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11271.1314855245  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11158.9818534085  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  11047.9481287771  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10938.0192081652  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10829.1840985891  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10721.4319164473  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10614.7518864317  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10509.1333404504  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10404.5657165607  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10301.0385579133  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10198.5415117058  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  10097.0643281483  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9996.59685943788  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9897.12905874391  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9798.65097920349  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9701.15277292652  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9604.6246900112  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9509.05707756873  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9414.44037875829  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9320.76513183108  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9228.02196918439  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9136.2016164247  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  9045.29489144014  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8955.29270348252  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8866.186052258  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8777.96602702724  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8690.62380571415  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8604.15065402384  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8518.53792456912  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8433.77705600563  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8349.85957217593  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8266.77708126167  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8184.52127494457  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8103.08392757538  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  8022.45689535158  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7942.63211550269  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7863.60160548423  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7785.35746217936  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7707.89186110851  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7631.19705564706  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7555.2653762505  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7480.08922968767  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7405.66109828121  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7331.97353915601  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7259.01918349469  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7186.79073580093  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7115.28097316979  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  7044.48274456536  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6974.38897010583  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6904.9926403553  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6836.286815623  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6768.26462526917  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6700.91926701811  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6634.24400627789  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6568.23217546683  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6502.87717334688  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6438.17246436333  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6374.1115779914  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6310.68810808902  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6247.8957122564  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6185.72811120158  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6124.17908811267  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6063.24248803609  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  6002.91221726102  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5943.18224271013  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5884.04659133616  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5825.49934952474  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5767.53466250285  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5710.14673375352  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5653.32982443602  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5597.07825281208  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5541.3863936777  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5486.2486778005  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5431.65959136299  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5377.61367541099  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5324.10552530792  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5271.12979019412  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5218.68117245197  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5166.754427176  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5115.34436164836  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5064.44583481972  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  5014.05375679492  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4964.16308832421  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4914.76884029913  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4865.86607325376  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4817.44989687059  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4769.51546949167  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4722.05799763432  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4675.07273551178  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4628.55498455872  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4582.50009296124  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4536.90345519183  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4491.76051154869  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4447.06674769987  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4402.8176942317  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4359.00892620198  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4315.63606269742  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4272.69476639549  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4230.1807431308  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4188.08974146558  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4146.4175522646  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4105.16000827419  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4064.31298370559  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  4023.87239382231  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3983.83419453164  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3944.19438198031  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3904.948992154  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3866.09410048106  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3827.62582143991  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3789.54030817061  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3751.83375209008  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3714.50238251129  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3677.5424662662  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3640.95030733235  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3604.72224646338  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3568.854660823  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3533.34396362276  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3498.18660376333  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3463.37906547946  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3428.91786798828  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3394.79956514135  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3361.02074507995  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3327.5780298939  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3294.46807528385  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3261.68757022671  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3229.23323664469  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3197.10182907735  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3165.29013435719  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3133.79497128823  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3102.61319032788  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3071.7416732721  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3041.17733294343  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  3010.91711288239  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2980.95798704173  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2951.29695948392  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2921.93106408148  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2892.85736422039  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2864.07295250646  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2835.57495047451  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2807.3605083006  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2779.42680451698  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2751.77104573003  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2724.39046634078  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2697.28232826851  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2670.44392067681  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2643.87255970255  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2617.5655881875  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2591.52037541257  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2565.7343168348  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2540.20483382683  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2514.92937341909  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2489.90540804446  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2465.13043528557  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2440.6019776245  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2416.31758219502  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2392.27482053738  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2368.47128835535  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2344.9046052759  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2321.57241461106  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2298.47238312234  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2275.60220078732  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2252.95958056872  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2230.54225818566  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2208.34799188721  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2186.37456222824  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2164.61977184748  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2143.08144524776  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2121.75742857847  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2100.64558942018  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2079.74381657137  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2059.05001983734  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2038.56212982119  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  2018.27809771681  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1998.19589510412  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1978.3135137461  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1958.62896538806  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1939.14028155876  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1919.84551337356  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1900.74273133958  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1881.83002516269  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1863.10550355652  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1844.56729405329  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1826.21354281661  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1808.04241445606  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1790.05209184367  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1772.24077593218  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1754.60668557515  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1737.14805734885  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1719.86314537592  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1702.75022115076  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1685.80757336666  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1669.03350774476  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1652.42634686448  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1635.98442999593  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1619.7061129337  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1603.58976783251  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1587.63378304445  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1571.83656295772  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1556.19652783716  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1540.71211366621  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1525.38177199056  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1510.20396976326  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1495.17718919145  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1480.29992758455  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1465.57069720398  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1450.98802511446  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1436.5504530366  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1422.25653720118  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1408.1048482047  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1394.09397086646  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1380.22250408705  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1366.48906070825  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1352.89226737426  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1339.43076439442  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1326.10320560722  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1312.90825824566  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1299.84460280403  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1286.91093290588  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1274.10595517346  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1261.4283890983  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1248.87696691325  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1236.45043346563  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1224.14754609174  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1211.96707449258  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1199.90780061084  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1187.9685185091  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1176.14803424917  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1164.4451657728  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1152.85874278339  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1141.38760662897  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1130.03061018637  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1118.78661774649  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1107.65450490071  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1096.63315842846  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1085.72147618592  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1074.91836699577  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1064.22275053809  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1053.63355724232  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1043.1497281803  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1032.7702149604  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1022.49397962264  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1012.31999453492  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  1002.24724229025  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  992.274715605024  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  982.401417218259  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  972.626359791883  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  962.948565812015  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  953.367067491183  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  943.88090667158  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  934.48913472921  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  925.19081247906  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  915.98501008115  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  906.870806947571  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  897.847291650418  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  888.913561830636  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  880.068724107804  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  871.311893990772  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  862.642195789238  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  854.058762526152  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  845.560735851038  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  837.147265954143  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  828.817511481469  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  820.570639450629  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  812.405825167543  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  804.322252143983  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  796.319112015905  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  788.395604462634  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  780.550937126804  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  772.784325535149  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  765.094993020038  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  757.482170641809  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  749.945097111883  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  742.483018716622  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  735.095189241974  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  727.780869898828  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  720.539329249161  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  713.369843132868  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  706.271694595365  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  699.244173815886  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  692.286578036491  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  685.39821149181  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  678.578385339442  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  671.826417591095  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  665.141633044362  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  658.523363215222  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  651.970946271172  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  645.483726965061  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  639.061056569553  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  632.702292812253  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  626.40679981149  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  620.173948012713  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  614.003114125553  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  607.893681061474  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  601.845037872081  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  595.856579688017  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  589.927707658468  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  584.057828891295  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  578.246356393726  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  572.492709013672  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  566.796311381596  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  561.156593852992  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  555.572992451403  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  550.044948812038  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  544.571910125929  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  539.153329084642  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  533.788663825562  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  528.477377877687  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  523.218940108002  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  518.012824668342  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  512.85851094283  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  507.755483495794  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  502.703232020238  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  497.701251286808  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  492.749041093256  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  487.846106214441  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  482.991956352786  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  478.186106089262  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  473.428074834835  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  468.717386782416  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  464.053570859277  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  459.436160679934  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  454.864694499525  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  450.338715167621  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  445.857770082518  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  441.421411145971  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  437.029194718392  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  432.680681574476  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  428.375436859286  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  424.113030044765  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  419.893034886674  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  415.715029381986  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  411.578595726666  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  407.483320273902  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  403.428793492735  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  399.41460992711  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  395.440368155324  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  391.505670749889  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  387.610124237784  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  383.753339061112  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  379.934929538142  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  376.154513824739  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  372.411713876182  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  368.706155409357  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  365.037467865329  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  361.405284372286  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  357.809241708853  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  354.248980267766  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  350.724144019914  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  347.234380478734  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  343.779340664966  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  340.358679071749  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  336.972053630071  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  333.619125674568  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  330.299559909649  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  327.013024375971  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  323.759190417243  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  320.537732647356  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  317.34832891785  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  314.190660285694  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  311.064410981393  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  307.969268377411  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  304.904922956909  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  301.87106828279  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  298.867400967061  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  295.893620640484  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  292.94942992255  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  290.034534391735  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  287.148642556054  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  284.291465823921  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  281.46271847528  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  278.66211763304  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  275.889383234783  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  273.144238004757  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  270.426407426153  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  267.735619713647  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  265.071605786227  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  262.434099240279  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  259.822836322951  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  257.237555905775  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  254.677999458555  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  252.143911023513  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  249.635037189694  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  247.151127067624  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  244.69193226422  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  242.257206857954  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  239.846707374255  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  237.460192761167  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  235.097424365239  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  232.758165907662  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  230.442183460642  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  228.149245424004  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  225.879122502033  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  223.631587680546  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  221.406416204187  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  219.203385553955  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  217.022275424948  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  214.862867704336  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  212.724946449547  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  210.608297866674  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  208.512710289096  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  206.437974156308  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  204.383881992968  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  202.350228388148  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  200.336809974792  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  198.343425409381  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  196.369875351799  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  194.415962445393  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  192.481491297246  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  190.56626845863  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  188.670102405666  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  186.792803520168  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  184.934184070684  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  183.094058193718  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  181.272241875151  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  179.468552931832  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  177.682810993364  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  175.914837484065  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  174.164455605111  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  172.431490316854  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  170.715768321323  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  169.017118044887  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  167.335369621104  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  165.67035487373  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  164.021907299902  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  162.389862053489  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  160.774055928607  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  159.174327343297  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  157.590516323367  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  156.022464486395  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  154.470015025891  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  152.933012695615  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  151.411303794053  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  149.904736149047  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  148.413159102577  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  146.936423495695  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  145.47438165361  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  144.02688737092  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  142.593795896989  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  141.174963921477  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  139.770249560003  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  138.379512339961  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  137.002613186469  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  135.639414408465  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  134.289779684936  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  132.953574051283  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  131.63066388583  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  130.320916896459  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  129.024202107378  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  127.740389846029  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  126.469351730115  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  125.210960654765  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  123.965090779824  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  122.731617517265  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  121.510417518735  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  120.301368663216  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  119.104350044814  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  117.919241960671  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  116.74592589899  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  115.584284527188  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  114.434201680159  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  113.29556234866  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  112.168252667809  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  111.052159905699  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  109.947172452124  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  108.853179807416  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  107.7700725714  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  106.697742432451  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  105.63608215666  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  104.584985577114  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  103.544347583281  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  102.514064110494  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  101.494032129546  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  100.484149636389  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  99.4843156419338  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  98.4944301619463  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  97.514394207054  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  96.5441097728447  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  95.5834798300663  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  94.6324083149241  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  93.6908001194741  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  92.7585610821118  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  91.8355979781567  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  90.9218185105295  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  90.0171313005218  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  89.1214458786587  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  88.2346726756515  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  87.3567230134411  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  86.4875090963295  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  85.6269440022007  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  84.774941673828  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  83.9314169102688  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  83.0962853583438  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  82.2694635042017  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  81.4508686649681  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  80.6404189804771  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  79.8380334050846  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  79.0436316995645  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  78.2571344230842  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  77.4784629252608  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  76.7075393382956  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  75.9442865691873  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  75.1886282920231  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  74.4404889403455  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  73.6997936995958  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  72.9664684996329  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  72.2404400073254  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  71.5216356192192  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  70.8099834542765  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  70.1054123466879  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  69.4078518387552  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  68.7172321738465  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  68.0334842894197  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  67.3565398101166  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  66.6863310409252  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  66.0227909604099  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  65.3658532140099  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  64.715452107403  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  64.0715225999366  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  63.4340002981233  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  62.8028214492017  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  62.1779229347609  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  61.5592422644286  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  60.9467175696222  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  60.340287597362  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  59.7398917041452  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  59.1454698498823  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  58.5569625918924  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  57.9743110789593  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  57.3974570454462  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  56.8263428054691  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  56.2609112471279  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  55.7011058267956  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  55.1468705634638  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  54.5981500331442  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  54.0548893633266  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  53.5170342274912  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  52.9845308396762  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  52.4573259490991  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  51.9353668348315  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  51.4186013005269  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  50.9069776692014  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  50.4004447780655  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  49.8989519734079  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  49.4024491055302  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  48.9108865237319  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  48.4242150713452  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  47.9423860808193  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  47.4653513688535  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  46.9930632315793  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  46.5254744397892  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  46.0625382342145  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  45.6042083208487  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  45.1504388663187  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  44.7011844933009  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  44.2564002759834  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  43.816041735574  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  43.3800648358516  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  42.948425978763  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  42.5210820000628  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  42.0979901649969  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  41.6791081640293  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  41.2643941086108  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  40.8538065269903  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  40.4473043600674  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  40.0448469572867  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  39.6463940725726  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  39.2519058603045  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  38.8613428713325  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  38.4746660490321  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  38.091836725399  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  37.7128166171818  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  37.3375678220537  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  36.9660528148225  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  36.598234443678  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  36.2340759264765  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  35.8735408470628  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  35.5165931516285  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  35.1631971451066  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  34.813317487602  loss:  74.4581261685425&quot;
## [1] &quot;step =  1  lambda =  34.4669191908574  loss:  84.204018551985&quot;
## [1] &quot;step =  2  lambda =  34.4669191908574  loss:  83.061125983488&quot;
## [1] &quot;step =  3  lambda =  34.4669191908574  loss:  83.1822709750677&quot;
## [1] &quot;step =  1  lambda =  34.1239676147544  loss:  94.1404252124269&quot;
## [1] &quot;step =  2  lambda =  34.1239676147544  loss:  93.1038222005946&quot;
## [1] &quot;step =  3  lambda =  34.1239676147544  loss:  93.1862220382018&quot;
## [1] &quot;step =  1  lambda =  33.7844284638495  loss:  103.810457724765&quot;
## [1] &quot;step =  2  lambda =  33.7844284638495  loss:  103.084329161853&quot;
## [1] &quot;step =  3  lambda =  33.7844284638495  loss:  103.122990329413&quot;
## [1] &quot;step =  1  lambda =  33.4482677839449  loss:  113.418742975269&quot;
## [1] &quot;step =  2  lambda =  33.4482677839449  loss:  112.989109619135&quot;
## [1] &quot;step =  3  lambda =  33.4482677839449  loss:  113.000727556533&quot;
## [1] &quot;step =  1  lambda =  33.1154519586923  loss:  122.973262270931&quot;
## [1] &quot;step =  2  lambda =  33.1154519586923  loss:  122.825577423407&quot;
## [1] &quot;step =  1  lambda =  32.7859477062319  loss:  132.480013313428&quot;
## [1] &quot;step =  2  lambda =  32.7859477062319  loss:  132.601536495187&quot;
## [1] &quot;step =  1  lambda =  32.4597220758638  loss:  141.942868803366&quot;
## [1] &quot;step =  2  lambda =  32.4597220758638  loss:  142.324907115032&quot;
## [1] &quot;step =  1  lambda =  32.1367424447532  loss:  151.358094955012&quot;
## [1] &quot;step =  2  lambda =  32.1367424447532  loss:  152.003034843486&quot;
## [1] &quot;step =  1  lambda =  31.8169765146677  loss:  160.732882442968&quot;
## [1] &quot;step =  2  lambda =  31.8169765146677  loss:  161.64359730076&quot;
## [1] &quot;step =  1  lambda =  31.500392308748  loss:  170.339045952855&quot;
## [1] &quot;step =  2  lambda =  31.500392308748  loss:  171.158798565093&quot;
## [1] &quot;step =  1  lambda =  31.1869581683094  loss:  180.090002148576&quot;
## [1] &quot;step =  2  lambda =  31.1869581683094  loss:  180.522927044787&quot;
## [1] &quot;step =  1  lambda =  30.876642749677  loss:  189.916258019059&quot;
## [1] &quot;step =  2  lambda =  30.876642749677  loss:  189.741193756961&quot;
## [1] &quot;step =  3  lambda =  30.876642749677  loss:  190.612435427222&quot;
## [1] &quot;step =  1  lambda =  30.5694150210502  loss:  199.468597091282&quot;
## [1] &quot;step =  2  lambda =  30.5694150210502  loss:  199.495142352685&quot;
## [1] &quot;step =  1  lambda =  30.2652442594001  loss:  209.523562502636&quot;
## [1] &quot;step =  2  lambda =  30.2652442594001  loss:  209.592946953227&quot;
## [1] &quot;step =  1  lambda =  29.964100047397  loss:  219.609254629522&quot;
## [1] &quot;step =  2  lambda =  29.964100047397  loss:  219.883640398766&quot;
## [1] &quot;step =  1  lambda =  29.6659522703689  loss:  229.748269073201&quot;
## [1] &quot;step =  2  lambda =  29.6659522703689  loss:  230.255231276147&quot;
## [1] &quot;step =  1  lambda =  29.3707711132895  loss:  239.966171467966&quot;
## [1] &quot;step =  2  lambda =  29.3707711132895  loss:  240.733046346709&quot;
## [1] &quot;step =  1  lambda =  29.0785270577971  loss:  250.287907041681&quot;
## [1] &quot;step =  2  lambda =  29.0785270577971  loss:  251.342996279764&quot;
## [1] &quot;step =  1  lambda =  28.7891908792427  loss:  260.739078192351&quot;
## [1] &quot;step =  2  lambda =  28.7891908792427  loss:  262.112650742726&quot;
## [1] &quot;step =  1  lambda =  28.5027336437673  loss:  271.346970594962&quot;
## [1] &quot;step =  2  lambda =  28.5027336437673  loss:  273.072106788011&quot;
## [1] &quot;step =  1  lambda =  28.2191267054086  loss:  282.141391218935&quot;
## [1] &quot;step =  2  lambda =  28.2191267054086  loss:  284.254786695779&quot;
## [1] &quot;step =  1  lambda =  27.9383417032365  loss:  293.190897428867&quot;
## [1] &quot;step =  2  lambda =  27.9383417032365  loss:  295.669872611748&quot;
## [1] &quot;step =  1  lambda =  27.6603505585168  loss:  304.55610034518&quot;
## [1] &quot;step =  2  lambda =  27.6603505585168  loss:  307.314392574022&quot;
## [1] &quot;step =  1  lambda =  27.3851254719032  loss:  316.256765846605&quot;
## [1] &quot;step =  2  lambda =  27.3851254719032  loss:  319.210785924633&quot;
## [1] &quot;step =  1  lambda =  27.1126389206579  loss:  328.346184784869&quot;
## [1] &quot;step =  2  lambda =  27.1126389206579  loss:  331.387809163617&quot;
## [1] &quot;step =  1  lambda =  26.8428636558986  loss:  340.890022309994&quot;
## [1] &quot;step =  2  lambda =  26.8428636558986  loss:  343.880263236623&quot;
## [1] &quot;step =  1  lambda =  26.575772699874  loss:  353.967437773407&quot;
## [1] &quot;step =  2  lambda =  26.575772699874  loss:  357.326209043278&quot;
## [1] &quot;step =  1  lambda =  26.3113393432659  loss:  367.674650964933&quot;
## [1] &quot;step =  2  lambda =  26.3113393432659  loss:  371.780326087912&quot;
## [1] &quot;step =  1  lambda =  26.0495371425183  loss:  382.130016245785&quot;
## [1] &quot;step =  2  lambda =  26.0495371425183  loss:  387.133827269714&quot;
## [1] &quot;step =  1  lambda =  25.7903399171931  loss:  397.481107616267&quot;
## [1] &quot;step =  2  lambda =  25.7903399171931  loss:  403.575474332044&quot;
## [1] &quot;step =  1  lambda =  25.5337217473515  loss:  413.91474366205&quot;
## [1] &quot;step =  2  lambda =  25.5337217473515  loss:  421.348238430724&quot;
## [1] &quot;step =  1  lambda =  25.2796569709629  loss:  431.671408013876&quot;
## [1] &quot;step =  2  lambda =  25.2796569709629  loss:  440.770656035247&quot;
## [1] &quot;step =  1  lambda =  25.0281201813378  loss:  451.066348007929&quot;
## [1] &quot;step =  2  lambda =  25.0281201813378  loss:  462.268619313891&quot;
## [1] &quot;step =  1  lambda =  24.7790862245877  loss:  472.520954926982&quot;
## [1] &quot;step =  2  lambda =  24.7790862245877  loss:  486.423392119842&quot;
## [1] &quot;step =  1  lambda =  24.5325301971094  loss:  496.610115058409&quot;
## [1] &quot;step =  2  lambda =  24.5325301971094  loss:  514.044882987857&quot;
## [1] &quot;step =  1  lambda =  24.2884274430945  loss:  524.134394684879&quot;
## [1] &quot;step =  2  lambda =  24.2884274430945  loss:  546.283697567236&quot;
## [1] &quot;step =  1  lambda =  24.0467535520645  loss:  556.230317495716&quot;
## [1] &quot;step =  2  lambda =  24.0467535520645  loss:  584.800234157947&quot;
## [1] &quot;step =  1  lambda =  23.8074843564287  loss:  595.923277074081&quot;
## [1] &quot;step =  2  lambda =  23.8074843564287  loss:  632.627949710161&quot;
## [1] &quot;step =  1  lambda =  23.5705959290681  loss:  646.600933094439&quot;
## [1] &quot;step =  2  lambda =  23.5705959290681  loss:  701.532379048712&quot;
## [1] &quot;step =  1  lambda =  23.3360645809427  loss:  722.339196943963&quot;
## [1] &quot;step =  2  lambda =  23.3360645809427  loss:  823.616135652661&quot;
## [1] &quot;step =  1  lambda =  23.1038668587222  loss:  843.800449779313&quot;
## [1] &quot;step =  1  lambda =  22.8739795424408  loss:  863.307884228294&quot;
## [1] &quot;step =  1  lambda =  22.6463796431754  loss:  882.136592890958&quot;
## [1] &quot;step =  1  lambda =  22.4210444007463  loss:  900.28739103429&quot;
## [1] &quot;step =  1  lambda =  22.1979512814416  loss:  917.763350051258&quot;
## [1] &quot;step =  1  lambda =  21.9770779757634  loss:  940.129967727012&quot;
## [1] &quot;step =  1  lambda =  21.7584023961971  loss:  962.160507774374&quot;
## [1] &quot;step =  1  lambda =  21.5419026750024  loss:  983.386385375236&quot;
## [1] &quot;step =  1  lambda =  21.3275571620269  loss:  1006.97975633576&quot;
## [1] &quot;step =  1  lambda =  21.1153444225406  loss:  1037.6127821468&quot;
## [1] &quot;step =  1  lambda =  20.9052432350928  loss:  1067.78101882067&quot;
## [1] &quot;step =  1  lambda =  20.6972325893895  loss:  1096.73148307529&quot;
## [1] &quot;step =  1  lambda =  20.4912916841929  loss:  1124.44722514522&quot;
## [1] &quot;step =  1  lambda =  20.2873999252409  loss:  1154.02420935515&quot;
## [1] &quot;step =  1  lambda =  20.0855369231877  loss:  1193.35161108645&quot;
## [1] &quot;step =  1  lambda =  19.8856824915647  loss:  1229.58218869085&quot;
## [1] &quot;step =  1  lambda =  19.6878166447624  loss:  1262.96751831143&quot;
## [1] &quot;step =  1  lambda =  19.4919195960311  loss:  1293.72841061162&quot;
## [1] &quot;step =  1  lambda =  19.2979717555028  loss:  1322.66659140676&quot;
## [1] &quot;step =  1  lambda =  19.1059537282317  loss:  1352.66808917306&quot;
## [1] &quot;step =  1  lambda =  18.915846312255  loss:  1380.11756488335&quot;
## [1] &quot;step =  1  lambda =  18.7276304966729  loss:  1405.57587821888&quot;
## [1] &quot;step =  1  lambda =  18.5412874597469  loss:  1432.7159902395&quot;
## [1] &quot;step =  1  lambda =  18.3567985670179  loss:  1457.60052725336&quot;
## [1] &quot;step =  1  lambda =  18.1741453694431  loss:  1480.38809229439&quot;
## [1] &quot;step =  1  lambda =  17.9933096015503  loss:  1501.22481992686&quot;
## [1] &quot;step =  1  lambda =  17.8142731796122  loss:  1520.24528180369&quot;
## [1] &quot;step =  1  lambda =  17.6370181998373  loss:  1537.57337221768&quot;
## [1] &quot;step =  1  lambda =  17.46152693658  loss:  1553.32316945987&quot;
## [1] &quot;step =  1  lambda =  17.2877818405676  loss:  1568.6885031863&quot;
## [1] &quot;step =  1  lambda =  17.1157655371459  loss:  1583.52463419603&quot;
## [1] &quot;step =  1  lambda =  16.945460824541  loss:  1596.75549012162&quot;
## [1] &quot;step =  1  lambda =  16.7768506721399  loss:  1608.57238493428&quot;
## [1] &quot;step =  1  lambda =  16.6099182187867  loss:  1619.50439303404&quot;
## [1] &quot;step =  1  lambda =  16.4446467710971  loss:  1630.39735290471&quot;
## [1] &quot;step =  1  lambda =  16.2810198017884  loss:  1640.19894697379&quot;
## [1] &quot;step =  1  lambda =  16.1190209480276  loss:  1648.87232376382&quot;
## [1] &quot;step =  1  lambda =  15.958634009794  loss:  1656.50457901649&quot;
## [1] &quot;step =  1  lambda =  15.7998429482604  loss:  1663.17334706298&quot;
## [1] &quot;step =  1  lambda =  15.6426318841882  loss:  1668.94820723365&quot;
## [1] &quot;step =  1  lambda =  15.4869850963399  loss:  1673.89181947316&quot;
## [1] &quot;step =  1  lambda =  15.3328870199072  loss:  1678.19669258107&quot;
## [1] &quot;step =  1  lambda =  15.1803222449539  loss:  1683.01336978989&quot;
## [1] &quot;step =  1  lambda =  15.0292755148754  loss:  1687.14552023341&quot;
## [1] &quot;step =  1  lambda =  14.8797317248728  loss:  1690.6355178184&quot;
## [1] &quot;step =  1  lambda =  14.7316759204426  loss:  1693.52193165587&quot;
## [1] &quot;step =  1  lambda =  14.5850932958808  loss:  1695.83997787653&quot;
## [1] &quot;step =  1  lambda =  14.4399691928029  loss:  1697.6219096261&quot;
## [1] &quot;step =  1  lambda =  14.2962890986776  loss:  1698.89735460397&quot;
## [1] &quot;step =  1  lambda =  14.1540386453758  loss:  1699.69360784487&quot;
## [1] &quot;step =  1  lambda =  14.0132036077336  loss:  1700.03588615713&quot;
## [1] &quot;step =  1  lambda =  13.8737699021299  loss:  1699.94754961654&quot;
## [1] &quot;step =  1  lambda =  13.7357235850779  loss:  1699.45029470086&quot;
## [1] &quot;step =  1  lambda =  13.5990508518309  loss:  1698.64273932121&quot;
## [1] &quot;step =  1  lambda =  13.4637380350017  loss:  1697.51263746818&quot;
## [1] &quot;step =  1  lambda =  13.3297716031958  loss:  1696.06410346612&quot;
## [1] &quot;step =  1  lambda =  13.1971381596584  loss:  1694.30980436232&quot;
## [1] &quot;step =  1  lambda =  13.0658244409346  loss:  1692.26174764277&quot;
## [1] &quot;step =  1  lambda =  12.9358173155431  loss:  1689.93134032592&quot;
## [1] &quot;step =  1  lambda =  12.807103782663  loss:  1687.32944078419&quot;
## [1] &quot;step =  1  lambda =  12.6796709708339  loss:  1684.46640428946&quot;
## [1] &quot;step =  1  lambda =  12.5535061366682  loss:  1681.35212314394&quot;
## [1] &quot;step =  1  lambda =  12.4285966635775  loss:  1677.9960621405&quot;
## [1] &quot;step =  1  lambda =  12.3049300605104  loss:  1674.40728999355&quot;
## [1] &quot;step =  1  lambda =  12.1824939607035  loss:  1670.59450729186&quot;
## [1] &quot;step =  1  lambda =  12.0612761204447  loss:  1666.56607144686&quot;
## [1] &quot;step =  1  lambda =  11.9412644178491  loss:  1662.33001904174&quot;
## [1] &quot;step =  1  lambda =  11.8224468516464  loss:  1657.89408592819&quot;
## [1] &quot;step =  1  lambda =  11.7048115399809  loss:  1653.26572536678&quot;
## [1] &quot;step =  1  lambda =  11.5883467192234  loss:  1648.45212446323&quot;
## [1] &quot;step =  1  lambda =  11.4730407427948  loss:  1643.46021911568&quot;
## [1] &quot;step =  1  lambda =  11.3588820800015  loss:  1638.29670765612&quot;
## [1] &quot;step =  1  lambda =  11.2458593148818  loss:  1632.98631615154&quot;
## [1] &quot;step =  1  lambda =  11.1339611450653  loss:  1627.73794198217&quot;
## [1] &quot;step =  1  lambda =  11.0231763806416  loss:  1622.32886355972&quot;
## [1] &quot;step =  1  lambda =  10.913493943042  loss:  1616.76503931873&quot;
## [1] &quot;step =  1  lambda =  10.8049028639313  loss:  1611.05225516599&quot;
## [1] &quot;step =  1  lambda =  10.6973922841111  loss:  1605.19613097872&quot;
## [1] &quot;step =  1  lambda =  10.5909514524338  loss:  1599.20212677931&quot;
## [1] &quot;step =  1  lambda =  10.4855697247276  loss:  1593.07554860569&quot;
## [1] &quot;step =  1  lambda =  10.3812365627318  loss:  1586.82155409244&quot;
## [1] &quot;step =  1  lambda =  10.2779415330434  loss:  1580.4451577758&quot;
## [1] &quot;step =  1  lambda =  10.1756743060733  loss:  1573.95123613431&quot;
## [1] &quot;step =  1  lambda =  10.0744246550136  loss:  1567.34453237581&quot;
## [1] &quot;step =  1  lambda =  9.97418245481473  loss:  1560.62966098137&quot;
## [1] &quot;step =  1  lambda =  9.87493768117319  loss:  1553.81111201656&quot;
## [1] &quot;step =  1  lambda =  9.77668040952892  loss:  1546.8932552204&quot;
## [1] &quot;step =  1  lambda =  9.67940081407284  loss:  1539.88034388239&quot;
## [1] &quot;step =  1  lambda =  9.58308916676438  loss:  1532.77651851848&quot;
## [1] &quot;step =  1  lambda =  9.48773583635853  loss:  1525.58581035628&quot;
## [1] &quot;step =  1  lambda =  9.39333128744278  loss:  1518.31214464042&quot;
## [1] &quot;step =  1  lambda =  9.29986607948359  loss:  1510.95934376842&quot;
## [1] &quot;step =  1  lambda =  9.20733086588226  loss:  1503.53113026764&quot;
## [1] &quot;step =  1  lambda =  9.11571639304031  loss:  1496.03112962342&quot;
## [1] &quot;step =  1  lambda =  9.02501349943413  loss:  1488.46287296828&quot;
## [1] &quot;step =  1  lambda =  8.93521311469874  loss:  1480.82979964185&quot;
## [1] &quot;step =  1  lambda =  8.84630625872088  loss:  1473.13525963053&quot;
## [1] &quot;step =  1  lambda =  8.75828404074083  loss:  1465.38251589575&quot;
## [1] &quot;step =  1  lambda =  8.67113765846346  loss:  1457.57474659892&quot;
## [1] &quot;step =  1  lambda =  8.5848583971779  loss:  1449.71504723097&quot;
## [1] &quot;step =  1  lambda =  8.49943762888613  loss:  1441.80643265361&quot;
## [1] &quot;step =  1  lambda =  8.41486681144014  loss:  1433.85183905923&quot;
## [1] &quot;step =  1  lambda =  8.3311374876877  loss:  1425.85412585557&quot;
## [1] &quot;step =  1  lambda =  8.24824128462666  loss:  1417.81607748093&quot;
## [1] &quot;step =  1  lambda =  8.16616991256765  loss:  1409.74040515546&quot;
## [1] &quot;step =  1  lambda =  8.08491516430506  loss:  1401.62974857305&quot;
## [1] &quot;step =  1  lambda =  8.00446891429635  loss:  1393.48667753852&quot;
## [1] &quot;step =  1  lambda =  7.92482311784949  loss:  1385.31369355404&quot;
## [1] &quot;step =  1  lambda =  7.84596981031845  loss:  1377.11323135842&quot;
## [1] &quot;step =  1  lambda =  7.76790110630678  loss:  1367.85187402073&quot;
## [1] &quot;step =  1  lambda =  7.69060919887901  loss:  1357.78106749618&quot;
## [1] &quot;step =  1  lambda =  7.61408635877998  loss:  1347.94052520965&quot;
## [1] &quot;step =  1  lambda =  7.53832493366192  loss:  1338.17957659171&quot;
## [1] &quot;step =  1  lambda =  7.46331734731919  loss:  1328.45315228529&quot;
## [1] &quot;step =  1  lambda =  7.38905609893065  loss:  1318.76215760044&quot;
## [1] &quot;step =  1  lambda =  7.31553376230957  loss:  1309.10745314063&quot;
## [1] &quot;step =  1  lambda =  7.24274298516102  loss:  1299.48985657274&quot;
## [1] &quot;step =  1  lambda =  7.17067648834662  loss:  1289.91014433907&quot;
## [1] &quot;step =  1  lambda =  7.09932706515664  loss:  1280.36905331097&quot;
## [1] &quot;step =  1  lambda =  7.0286875805893  loss:  1270.86728238439&quot;
## [1] &quot;step =  1  lambda =  6.95875097063727  loss:  1261.40549401786&quot;
## [1] &quot;step =  1  lambda =  6.88951024158129  loss:  1251.98431571347&quot;
## [1] &quot;step =  1  lambda =  6.82095846929075  loss:  1242.60434144192&quot;
## [1] &quot;step =  1  lambda =  6.75308879853129  loss:  1233.26613301277&quot;
## [1] &quot;step =  1  lambda =  6.68589444227927  loss:  1223.97022139102&quot;
## [1] &quot;step =  1  lambda =  6.61936868104308  loss:  1214.71710796145&quot;
## [1] &quot;step =  1  lambda =  6.55350486219115  loss:  1205.50726574218&quot;
## [1] &quot;step =  1  lambda =  6.48829639928672  loss:  1196.34114054894&quot;
## [1] &quot;step =  1  lambda =  6.42373677142913  loss:  1187.21915211163&quot;
## [1] &quot;step =  1  lambda =  6.35981952260183  loss:  1178.14169514466&quot;
## [1] &quot;step =  1  lambda =  6.29653826102666  loss:  1169.11549714763&quot;
## [1] &quot;step =  1  lambda =  6.23388665852472  loss:  1160.37634042796&quot;
## [1] &quot;step =  1  lambda =  6.17185844988355  loss:  1151.70138073583&quot;
## [1] &quot;step =  1  lambda =  6.11044743223061  loss:  1143.06171485757&quot;
## [1] &quot;step =  1  lambda =  6.04964746441295  loss:  1134.40390735002&quot;
## [1] &quot;step =  1  lambda =  5.98945246638312  loss:  1125.01404426159&quot;
## [1] &quot;step =  1  lambda =  5.92985641859114  loss:  1115.68554925825&quot;
## [1] &quot;step =  1  lambda =  5.8708533613826  loss:  1106.41844916575&quot;
## [1] &quot;step =  1  lambda =  5.81243739440259  loss:  1097.21275159005&quot;
## [1] &quot;step =  1  lambda =  5.75460267600573  loss:  1088.06844596716&quot;
## [1] &quot;step =  1  lambda =  5.69734342267199  loss:  1078.98550455869&quot;
## [1] &quot;step =  1  lambda =  5.64065390842832  loss:  1069.96388339543&quot;
## [1] &quot;step =  1  lambda =  5.58452846427606  loss:  1061.00352317139&quot;
## [1] &quot;step =  1  lambda =  5.52896147762401  loss:  1052.10435009069&quot;
## [1] &quot;step =  1  lambda =  5.47394739172721  loss:  1043.26627666939&quot;
## [1] &quot;step =  1  lambda =  5.4194807051312  loss:  1034.48920249461&quot;
## [1] &quot;step =  1  lambda =  5.36555597112197  loss:  1025.77301494295&quot;
## [1] &quot;step =  1  lambda =  5.31216779718117  loss:  1017.11758986023&quot;
## [1] &quot;step =  1  lambda =  5.2593108444469  loss:  1008.52279220459&quot;
## [1] &quot;step =  1  lambda =  5.20697982717985  loss:  999.988476654839&quot;
## [1] &quot;step =  1  lambda =  5.15516951223468  loss:  991.514488185711&quot;
## [1] &quot;step =  1  lambda =  5.10387471853673  loss:  983.10066261201&quot;
## [1] &quot;step =  1  lambda =  5.05309031656387  loss:  974.746827103131&quot;
## [1] &quot;step =  1  lambda =  5.00281122783359  loss:  966.452800669644&quot;
## [1] &quot;step =  1  lambda =  4.95303242439511  loss:  958.218394623455&quot;
## [1] &quot;step =  1  lambda =  4.90374892832662  loss:  950.043413013003&quot;
## [1] &quot;step =  1  lambda =  4.85495581123743  loss:  941.927653034894&quot;
## [1] &quot;step =  1  lambda =  4.80664819377518  loss:  933.870905423304&quot;
## [1] &quot;step =  1  lambda =  4.75882124513786  loss:  925.872954818426&quot;
## [1] &quot;step =  1  lambda =  4.71147018259074  loss:  917.817442287731&quot;
## [1] &quot;step =  1  lambda =  4.66459027098813  loss:  909.794769500787&quot;
## [1] &quot;step =  1  lambda =  4.61817682229978  loss:  901.83361654327&quot;
## [1] &quot;step =  1  lambda =  4.57222519514216  loss:  893.933706657535&quot;
## [1] &quot;step =  1  lambda =  4.52673079431425  loss:  886.094756893463&quot;
## [1] &quot;step =  1  lambda =  4.48168907033806  loss:  878.316478573943&quot;
## [1] &quot;step =  1  lambda =  4.43709551900367  loss:  870.59857773309&quot;
## [1] &quot;step =  1  lambda =  4.39294568091876  loss:  862.940755529002&quot;
## [1] &quot;step =  1  lambda =  4.34923514106274  loss:  855.342708632714&quot;
## [1] &quot;step =  1  lambda =  4.30595952834521  loss:  847.804129594797&quot;
## [1] &quot;step =  1  lambda =  4.26311451516882  loss:  840.324707190964&quot;
## [1] &quot;step =  1  lambda =  4.22069581699655  loss:  832.904126747894&quot;
## [1] &quot;step =  1  lambda =  4.17869919192325  loss:  825.542070450394&quot;
## [1] &quot;step =  1  lambda =  4.13712044025139  loss:  818.238217630913&quot;
## [1] &quot;step =  1  lambda =  4.09595540407118  loss:  810.98968469043&quot;
## [1] &quot;step =  1  lambda =  4.05519996684468  loss:  803.798165805554&quot;
## [1] &quot;step =  1  lambda =  4.0148500529942  loss:  796.647089105434&quot;
## [1] &quot;step =  1  lambda =  3.97490162749475  loss:  789.547706788906&quot;
## [1] &quot;step =  1  lambda =  3.93535069547048  loss:  782.505656409188&quot;
## [1] &quot;step =  1  lambda =  3.89619330179521  loss:  775.520589930857&quot;
## [1] &quot;step =  1  lambda =  3.85742553069697  loss:  768.592158178641&quot;
## [1] &quot;step =  1  lambda =  3.81904350536634  loss:  761.7200110306&quot;
## [1] &quot;step =  1  lambda =  3.78104338756878  loss:  754.903797598874&quot;
## [1] &quot;step =  1  lambda =  3.74342137726086  loss:  748.143166398685&quot;
## [1] &quot;step =  1  lambda =  3.7061737122102  loss:  741.43776550624&quot;
## [1] &quot;step =  1  lambda =  3.66929666761925  loss:  734.787242706184&quot;
## [1] &quot;step =  1  lambda =  3.63278655575281  loss:  728.19124562919&quot;
## [1] &quot;step =  1  lambda =  3.59663972556928  loss:  721.649421880249&quot;
## [1] &quot;step =  1  lambda =  3.56085256235552  loss:  715.16141915823&quot;
## [1] &quot;step =  1  lambda =  3.52542148736538  loss:  708.726885367194&quot;
## [1] &quot;step =  1  lambda =  3.49034295746184  loss:  702.345468719992&quot;
## [1] &quot;step =  1  lambda =  3.45561346476268  loss:  696.016817834587&quot;
## [1] &quot;step =  1  lambda =  3.42122953628968  loss:  689.740581823566&quot;
## [1] &quot;step =  1  lambda =  3.38718773362134  loss:  683.516410377268&quot;
## [1] &quot;step =  1  lambda =  3.35348465254903  loss:  677.343953840919&quot;
## [1] &quot;step =  1  lambda =  3.32011692273655  loss:  671.222863286168&quot;
## [1] &quot;step =  1  lambda =  3.28708120738312  loss:  665.152790577389&quot;
## [1] &quot;step =  1  lambda =  3.25437420288967  loss:  659.133388433091&quot;
## [1] &quot;step =  1  lambda =  3.2219926385285  loss:  653.164310482759&quot;
## [1] &quot;step =  1  lambda =  3.18993327611618  loss:  647.245211319429&quot;
## [1] &quot;step =  1  lambda =  3.15819290968977  loss:  641.375746548316&quot;
## [1] &quot;step =  1  lambda =  3.12676836518616  loss:  635.55557283173&quot;
## [1] &quot;step =  1  lambda =  3.09565650012471  loss:  629.784347930562&quot;
## [1] &quot;step =  1  lambda =  3.06485420329301  loss:  624.06173074258&quot;
## [1] &quot;step =  1  lambda =  3.03435839443567  loss:  618.38738133776&quot;
## [1] &quot;step =  1  lambda =  3.00416602394643  loss:  612.760960990865&quot;
## [1] &quot;step =  1  lambda =  2.97427407256306  loss:  607.182132211475&quot;
## [1] &quot;step =  1  lambda =  2.94467955106552  loss:  601.650558771655&quot;
## [1] &quot;step =  1  lambda =  2.915379499977  loss:  596.165905731441&quot;
## [1] &quot;step =  1  lambda =  2.88637098926796  loss:  590.727839462295&quot;
## [1] &quot;step =  1  lambda =  2.85765111806317  loss:  585.336027668703&quot;
## [1] &quot;step =  1  lambda =  2.82921701435156  loss:  579.990139408038&quot;
## [1] &quot;step =  1  lambda =  2.80106583469908  loss:  574.689845108829&quot;
## [1] &quot;step =  1  lambda =  2.7731947639643  loss:  569.434816587573&quot;
## [1] &quot;step =  1  lambda =  2.74560101501692  loss:  564.22472706417&quot;
## [1] &quot;step =  1  lambda =  2.71828182845905  loss:  559.05925117613&quot;
## [1] &quot;step =  1  lambda =  2.69123447234926  loss:  553.938064991621&quot;
## [1] &quot;step =  1  lambda =  2.66445624192942  loss:  548.860846021452&quot;
## [1] &quot;step =  1  lambda =  2.63794445935415  loss:  543.827273230094&quot;
## [1] &quot;step =  1  lambda =  2.61169647342312  loss:  538.837027045796&quot;
## [1] &quot;step =  1  lambda =  2.58570965931585  loss:  533.889789369874&quot;
## [1] &quot;step =  1  lambda =  2.55998141832927  loss:  528.985243585249&quot;
## [1] &quot;step =  1  lambda =  2.53450917761785  loss:  524.12307456428&quot;
## [1] &quot;step =  1  lambda =  2.5092903899363  loss:  519.177543255767&quot;
## [1] &quot;step =  1  lambda =  2.48432253338482  loss:  514.275057580432&quot;
## [1] &quot;step =  1  lambda =  2.45960311115695  loss:  509.417630972629&quot;
## [1] &quot;step =  1  lambda =  2.43512965128988  loss:  504.604874012651&quot;
## [1] &quot;step =  1  lambda =  2.41089970641721  loss:  499.836400391967&quot;
## [1] &quot;step =  1  lambda =  2.38691085352428  loss:  495.111826888615&quot;
## [1] &quot;step =  1  lambda =  2.36316069370579  loss:  490.430773342733&quot;
## [1] &quot;step =  1  lambda =  2.33964685192599  loss:  485.792862632229&quot;
## [1] &quot;step =  1  lambda =  2.31636697678109  loss:  481.197720648608&quot;
## [1] &quot;step =  1  lambda =  2.29331874026418  loss:  476.644976272986&quot;
## [1] &quot;step =  1  lambda =  2.27049983753241  loss:  472.134261352286&quot;
## [1] &quot;step =  1  lambda =  2.24790798667647  loss:  467.665210675636&quot;
## [1] &quot;step =  1  lambda =  2.22554092849247  loss:  463.237461950973&quot;
## [1] &quot;step =  1  lambda =  2.20339642625594  loss:  458.850655781874&quot;
## [1] &quot;step =  1  lambda =  2.1814722654982  loss:  454.504435644595&quot;
## [1] &quot;step =  1  lambda =  2.15976625378491  loss:  450.198447865355&quot;
## [1] &quot;step =  1  lambda =  2.13827622049682  loss:  445.932341597842&quot;
## [1] &quot;step =  1  lambda =  2.11700001661267  loss:  441.705768800954&quot;
## [1] &quot;step =  1  lambda =  2.09593551449437  loss:  437.518384216786&quot;
## [1] &quot;step =  1  lambda =  2.07508060767412  loss:  433.369845348846&quot;
## [1] &quot;step =  1  lambda =  2.05443321064389  loss:  429.259812440515&quot;
## [1] &quot;step =  1  lambda =  2.03399125864675  loss:  425.187948453749&quot;
## [1] &quot;step =  1  lambda =  2.01375270747048  loss:  421.15391904801&quot;
## [1] &quot;step =  1  lambda =  1.99371553324308  loss:  417.157392559438&quot;
## [1] &quot;step =  1  lambda =  1.97387773223045  loss:  413.198039980263&quot;
## [1] &quot;step =  1  lambda =  1.95423732063594  loss:  409.275534938438&quot;
## [1] &quot;step =  1  lambda =  1.93479233440203  loss:  405.389553677514&quot;
## [1] &quot;step =  1  lambda =  1.9155408290139  loss:  401.539775036727&quot;
## [1] &quot;step =  1  lambda =  1.89648087930495  loss:  397.725880431327&quot;
## [1] &quot;step =  1  lambda =  1.87761057926434  loss:  393.947553833108&quot;
## [1] &quot;step =  1  lambda =  1.85892804184634  loss:  390.204481751167&quot;
## [1] &quot;step =  1  lambda =  1.84043139878164  loss:  386.496353212871&quot;
## [1] &quot;step =  1  lambda =  1.82211880039051  loss:  382.822859745034&quot;
## [1] &quot;step =  1  lambda =  1.80398841539786  loss:  379.173571846694&quot;
## [1] &quot;step =  1  lambda =  1.78603843075007  loss:  375.552272406622&quot;
## [1] &quot;step =  1  lambda =  1.76826705143374  loss:  371.965144503265&quot;
## [1] &quot;step =  1  lambda =  1.7506725002961  loss:  368.411880192569&quot;
## [1] &quot;step =  1  lambda =  1.7332530178674  loss:  364.889282847344&quot;
## [1] &quot;step =  1  lambda =  1.71600686218486  loss:  361.399811776566&quot;
## [1] &quot;step =  1  lambda =  1.69893230861855  loss:  357.94339289101&quot;
## [1] &quot;step =  1  lambda =  1.68202764969889  loss:  354.519725763715&quot;
## [1] &quot;step =  1  lambda =  1.66529119494589  loss:  351.128512517472&quot;
## [1] &quot;step =  1  lambda =  1.64872127070013  loss:  347.769457805111&quot;
## [1] &quot;step =  1  lambda =  1.63231621995538  loss:  344.44226878992&quot;
## [1] &quot;step =  1  lambda =  1.61607440219289  loss:  341.146655126208&quot;
## [1] &quot;step =  1  lambda =  1.59999419321736  loss:  337.882328940007&quot;
## [1] &quot;step =  1  lambda =  1.58407398499448  loss:  334.64900480992&quot;
## [1] &quot;step =  1  lambda =  1.56831218549017  loss:  331.446399748102&quot;
## [1] &quot;step =  1  lambda =  1.55270721851134  loss:  328.274233181382&quot;
## [1] &quot;step =  1  lambda =  1.53725752354828  loss:  325.132226932533&quot;
## [1] &quot;step =  1  lambda =  1.52196155561863  loss:  322.020105201664&quot;
## [1] &quot;step =  1  lambda =  1.50681778511285  loss:  318.937594547765&quot;
## [1] &quot;step =  1  lambda =  1.49182469764127  loss:  315.884423870377&quot;
## [1] &quot;step =  1  lambda =  1.47698079388264  loss:  312.860324391408&quot;
## [1] &quot;step =  1  lambda =  1.46228458943423  loss:  309.865029637074&quot;
## [1] &quot;step =  1  lambda =  1.44773461466333  loss:  306.898275419977&quot;
## [1] &quot;step =  1  lambda =  1.43332941456034  loss:  303.959799821321&quot;
## [1] &quot;step =  1  lambda =  1.41906754859326  loss:  301.049343173249&quot;
## [1] &quot;step =  1  lambda =  1.40494759056359  loss:  298.166648041319&quot;
## [1] &quot;step =  1  lambda =  1.39096812846378  loss:  295.311459207105&quot;
## [1] &quot;step =  1  lambda =  1.37712776433596  loss:  292.483523650927&quot;
## [1] &quot;step =  1  lambda =  1.36342511413218  loss:  289.682590534706&quot;
## [1] &quot;step =  1  lambda =  1.349858807576  loss:  286.908411184948&quot;
## [1] &quot;step =  1  lambda =  1.33642748802547  loss:  284.160739075848&quot;
## [1] &quot;step =  1  lambda =  1.32312981233744  loss:  281.439329812523&quot;
## [1] &quot;step =  1  lambda =  1.30996445073325  loss:  278.743941114358&quot;
## [1] &quot;step =  1  lambda =  1.29693008666577  loss:  276.074332798485&quot;
## [1] &quot;step =  1  lambda =  1.28402541668774  loss:  273.430266763368&quot;
## [1] &quot;step =  1  lambda =  1.2712491503214  loss:  270.811506972515&quot;
## [1] &quot;step =  1  lambda =  1.25860000992948  loss:  268.217819438308&quot;
## [1] &quot;step =  1  lambda =  1.24607673058738  loss:  265.648972205944&quot;
## [1] &quot;step =  1  lambda =  1.23367805995674  loss:  263.104735337495&quot;
## [1] &quot;step =  1  lambda =  1.22140275816017  loss:  260.584880896075&quot;
## [1] &quot;step =  1  lambda =  1.20924959765725  loss:  258.089182930132&quot;
## [1] &quot;step =  1  lambda =  1.19721736312181  loss:  255.617417457842&quot;
## [1] &quot;step =  1  lambda =  1.18530485132037  loss:  253.169362451611&quot;
## [1] &quot;step =  1  lambda =  1.17351087099181  loss:  250.744797822699&quot;
## [1] &quot;step =  1  lambda =  1.16183424272828  loss:  248.343505405939&quot;
## [1] &quot;step =  1  lambda =  1.15027379885723  loss:  245.965268944576&quot;
## [1] &quot;step =  1  lambda =  1.13882838332462  loss:  243.609874075198&quot;
## [1] &quot;step =  1  lambda =  1.12749685157938  loss:  241.277108312785&quot;
## [1] &quot;step =  1  lambda =  1.11627807045887  loss:  238.966761035859&quot;
## [1] &quot;step =  1  lambda =  1.10517091807565  loss:  236.678623471734&quot;
## [1] &quot;step =  1  lambda =  1.09417428370521  loss:  234.412488681871&quot;
## [1] &quot;step =  1  lambda =  1.08328706767496  loss:  232.168151547337&quot;
## [1] &quot;step =  1  lambda =  1.07250818125422  loss:  229.945408754362&quot;
## [1] &quot;step =  1  lambda =  1.06183654654536  loss:  227.744058779994&quot;
## [1] &quot;step =  1  lambda =  1.05127109637602  loss:  225.563901877863&quot;
## [1] &quot;step =  1  lambda =  1.04081077419239  loss:  223.404740064029&quot;
## [1] &quot;step =  1  lambda =  1.03045453395352  loss:  221.266377102939&quot;
## [1] &quot;step =  1  lambda =  1.02020134002676  loss:  219.148618493479&quot;
## [1] &quot;step =  1  lambda =  1.01005016708417  loss:  217.051271455113&quot;
## [1] &quot;step =  1  lambda =  1  loss:  214.974144914131&quot;
## [1] &quot;step =  1  lambda =  0.990049833749168  loss:  212.917049489981&quot;
## [1] &quot;step =  1  lambda =  0.980198673306756  loss:  210.8797974817&quot;
## [1] &quot;step =  1  lambda =  0.970445533548509  loss:  208.862202854436&quot;
## [1] &quot;step =  1  lambda =  0.960789439152324  loss:  206.864081226063&quot;
## [1] &quot;step =  1  lambda =  0.951229424500715  loss:  204.885249853889&quot;
## [1] &quot;step =  1  lambda =  0.941764533584248  loss:  202.925527621453&quot;
## [1] &quot;step =  1  lambda =  0.932393819905948  loss:  200.984735025411&quot;
## [1] &quot;step =  1  lambda =  0.923116346386636  loss:  199.062694162521&quot;
## [1] &quot;step =  1  lambda =  0.913931185271228  loss:  197.158506482814&quot;
## [1] &quot;step =  1  lambda =  0.90483741803596  loss:  195.272407194241&quot;
## [1] &quot;step =  1  lambda =  0.895834135296529  loss:  193.404566639996&quot;
## [1] &quot;step =  1  lambda =  0.886920436717158  loss:  191.554812507197&quot;
## [1] &quot;step =  1  lambda =  0.878095430920562  loss:  189.722974032021&quot;
## [1] &quot;step =  1  lambda =  0.869358235398805  loss:  187.908881987011&quot;
## [1] &quot;step =  1  lambda =  0.860707976425057  loss:  186.112368668467&quot;
## [1] &quot;step =  1  lambda =  0.852143788966211  loss:  184.333267883926&quot;
## [1] &quot;step =  1  lambda =  0.843664816596384  loss:  182.571414939726&quot;
## [1] &quot;step =  1  lambda =  0.835270211411272  loss:  180.826646628662&quot;
## [1] &quot;step =  1  lambda =  0.826959133943363  loss:  179.098801217723&quot;
## [1] &quot;step =  1  lambda =  0.818730753077982  loss:  177.387718435912&quot;
## [1] &quot;step =  1  lambda =  0.810584245970188  loss:  175.693239462157&quot;
## [1] &quot;step =  1  lambda =  0.802518797962478  loss:  174.015206913306&quot;
## [1] &quot;step =  1  lambda =  0.794533602503334  loss:  172.353464832197&quot;
## [1] &quot;step =  1  lambda =  0.786627861066553  loss:  170.707858675825&quot;
## [1] &quot;step =  2  lambda =  0.786627861066553  loss:  200.636850458604&quot;
## [1] &quot;step =  1  lambda =  0.778800783071405  loss:  198.698576947181&quot;
## [1] &quot;step =  2  lambda =  0.778800783071405  loss:  218.290566480222&quot;
## [1] &quot;step =  1  lambda =  0.771051585803566  loss:  216.178707220803&quot;
## [1] &quot;step =  2  lambda =  0.771051585803566  loss:  230.575791086053&quot;
## [1] &quot;step =  1  lambda =  0.763379494336853  loss:  228.339843845889&quot;
## [1] &quot;step =  2  lambda =  0.763379494336853  loss:  239.252053078662&quot;
## [1] &quot;step =  1  lambda =  0.755783741455726  loss:  236.946478140194&quot;
## [1] &quot;step =  2  lambda =  0.755783741455726  loss:  246.989869427602&quot;
## [1] &quot;step =  1  lambda =  0.748263567578566  loss:  244.595335014895&quot;
## [1] &quot;step =  2  lambda =  0.748263567578566  loss:  252.21512073272&quot;
## [1] &quot;step =  1  lambda =  0.740818220681719  loss:  249.77406525459&quot;
## [1] &quot;step =  2  lambda =  0.740818220681719  loss:  257.784329777619&quot;
## [1] &quot;step =  1  lambda =  0.733446956224289  loss:  255.28574019476&quot;
## [1] &quot;step =  2  lambda =  0.733446956224289  loss:  262.383435908734&quot;
## [1] &quot;step =  1  lambda =  0.726149037073691  loss:  259.83786199427&quot;
## [1] &quot;step =  2  lambda =  0.726149037073691  loss:  265.853716387005&quot;
## [1] &quot;step =  1  lambda =  0.718923733431926  loss:  263.279971262896&quot;
## [1] &quot;step =  2  lambda =  0.718923733431926  loss:  269.42405923133&quot;
## [1] &quot;step =  1  lambda =  0.71177032276261  loss:  266.810787472849&quot;
## [1] &quot;step =  2  lambda =  0.71177032276261  loss:  272.949511626794&quot;
## [1] &quot;step =  1  lambda =  0.704688089718714  loss:  270.305840184023&quot;
## [1] &quot;step =  2  lambda =  0.704688089718714  loss:  276.328430359668&quot;
## [1] &quot;step =  1  lambda =  0.697676326071031  loss:  273.650641772577&quot;
## [1] &quot;step =  2  lambda =  0.697676326071031  loss:  279.520171049978&quot;
## [1] &quot;step =  1  lambda =  0.690734330637355  loss:  276.81016858648&quot;
## [1] &quot;step =  2  lambda =  0.690734330637355  loss:  282.48261636751&quot;
## [1] &quot;step =  1  lambda =  0.683861409212356  loss:  279.745653550677&quot;
## [1] &quot;step =  2  lambda =  0.683861409212356  loss:  284.841879363522&quot;
## [1] &quot;step =  1  lambda =  0.677056874498164  loss:  282.08087928568&quot;
## [1] &quot;step =  2  lambda =  0.677056874498164  loss:  287.172760136718&quot;
## [1] &quot;step =  1  lambda =  0.670320046035639  loss:  284.382963897142&quot;
## [1] &quot;step =  2  lambda =  0.670320046035639  loss:  289.47874283901&quot;
## [1] &quot;step =  1  lambda =  0.663650250136319  loss:  286.665470520177&quot;
## [1] &quot;step =  2  lambda =  0.663650250136319  loss:  291.576832147536&quot;
## [1] &quot;step =  1  lambda =  0.657046819815057  loss:  288.742243793358&quot;
## [1] &quot;step =  2  lambda =  0.657046819815057  loss:  293.487786904592&quot;
## [1] &quot;step =  1  lambda =  0.650509094723317  loss:  290.633719304602&quot;
## [1] &quot;step =  2  lambda =  0.650509094723317  loss:  295.207018863941&quot;
## [1] &quot;step =  1  lambda =  0.644036421083142  loss:  292.335354477276&quot;
## [1] &quot;step =  2  lambda =  0.644036421083142  loss:  296.542906070201&quot;
## [1] &quot;step =  1  lambda =  0.637628151621774  loss:  293.662321115213&quot;
## [1] &quot;step =  2  lambda =  0.637628151621774  loss:  297.602185476832&quot;
## [1] &quot;step =  1  lambda =  0.631283645506927  loss:  294.713885798011&quot;
## [1] &quot;step =  2  lambda =  0.631283645506927  loss:  298.471040025935&quot;
## [1] &quot;step =  1  lambda =  0.6250022682827  loss:  295.573395560714&quot;
## [1] &quot;step =  2  lambda =  0.6250022682827  loss:  299.05270609839&quot;
## [1] &quot;step =  1  lambda =  0.618783391806141  loss:  296.150875984578&quot;
## [1] &quot;step =  2  lambda =  0.618783391806141  loss:  299.545806000443&quot;
## [1] &quot;step =  1  lambda =  0.612626394184416  loss:  296.638235689536&quot;
## [1] &quot;step =  2  lambda =  0.612626394184416  loss:  299.958180332644&quot;
## [1] &quot;step =  1  lambda =  0.606530659712633  loss:  297.045685221796&quot;
## [1] &quot;step =  2  lambda =  0.606530659712633  loss:  300.277881637783&quot;
## [1] &quot;step =  1  lambda =  0.600495578812266  loss:  297.360715739769&quot;
## [1] &quot;step =  2  lambda =  0.600495578812266  loss:  300.488805398981&quot;
## [1] &quot;step =  1  lambda =  0.594520547970195  loss:  297.568748992526&quot;
## [1] &quot;step =  2  lambda =  0.594520547970195  loss:  300.56516324571&quot;
## [1] &quot;step =  1  lambda =  0.588604969678356  loss:  297.643562620423&quot;
## [1] &quot;step =  2  lambda =  0.588604969678356  loss:  300.511795817864&quot;
## [1] &quot;step =  1  lambda =  0.58274825237399  loss:  297.589943468279&quot;
## [1] &quot;step =  2  lambda =  0.58274825237399  loss:  300.353854704134&quot;
## [1] &quot;step =  1  lambda =  0.576949810380487  loss:  297.432545147175&quot;
## [1] &quot;step =  2  lambda =  0.576949810380487  loss:  300.112931871148&quot;
## [1] &quot;step =  1  lambda =  0.571209063848815  loss:  297.19323316571&quot;
## [1] &quot;step =  2  lambda =  0.571209063848815  loss:  299.751893424066&quot;
## [1] &quot;step =  1  lambda =  0.565525438699537  loss:  296.83502727759&quot;
## [1] &quot;step =  2  lambda =  0.565525438699537  loss:  299.277534283264&quot;
## [1] &quot;step =  1  lambda =  0.559898366565402  loss:  296.364631309384&quot;
## [1] &quot;step =  2  lambda =  0.559898366565402  loss:  298.695750948632&quot;
## [1] &quot;step =  1  lambda =  0.554327284734507  loss:  295.788118361631&quot;
## [1] &quot;step =  2  lambda =  0.554327284734507  loss:  298.013403184321&quot;
## [1] &quot;step =  1  lambda =  0.548811636094027  loss:  295.1118089453&quot;
## [1] &quot;step =  2  lambda =  0.548811636094027  loss:  297.235412889626&quot;
## [1] &quot;step =  1  lambda =  0.5433508690745  loss:  294.340815628152&quot;
## [1] &quot;step =  2  lambda =  0.5433508690745  loss:  296.364367595885&quot;
## [1] &quot;step =  1  lambda =  0.537944437594675  loss:  293.477150730294&quot;
## [1] &quot;step =  2  lambda =  0.537944437594675  loss:  295.403882950994&quot;
## [1] &quot;step =  1  lambda =  0.532591801006898  loss:  292.525526684854&quot;
## [1] &quot;step =  2  lambda =  0.532591801006898  loss:  294.3608301906&quot;
## [1] &quot;step =  1  lambda =  0.527292424043048  loss:  291.491516259896&quot;
## [1] &quot;step =  2  lambda =  0.527292424043048  loss:  293.238634929375&quot;
## [1] &quot;step =  1  lambda =  0.522045776761016  loss:  290.379819677879&quot;
## [1] &quot;step =  2  lambda =  0.522045776761016  loss:  292.044831118456&quot;
## [1] &quot;step =  1  lambda =  0.516851334491699  loss:  289.197233238748&quot;
## [1] &quot;step =  2  lambda =  0.516851334491699  loss:  290.785569129738&quot;
## [1] &quot;step =  1  lambda =  0.511708577786543  loss:  287.949846657485&quot;
## [1] &quot;step =  2  lambda =  0.511708577786543  loss:  289.466555669909&quot;
## [1] &quot;step =  1  lambda =  0.50661699236559  loss:  286.643310322102&quot;
## [1] &quot;step =  2  lambda =  0.50661699236559  loss:  288.093065686464&quot;
## [1] &quot;step =  1  lambda =  0.501576069066056  loss:  285.282847075191&quot;
## [1] &quot;step =  2  lambda =  0.501576069066056  loss:  286.669989268692&quot;
## [1] &quot;step =  1  lambda =  0.49658530379141  loss:  283.873298667466&quot;
## [1] &quot;step =  2  lambda =  0.49658530379141  loss:  285.201879867643&quot;
## [1] &quot;step =  1  lambda =  0.491644197460966  loss:  282.419173513187&quot;
## [1] &quot;step =  2  lambda =  0.491644197460966  loss:  283.692992321154&quot;
## [1] &quot;step =  1  lambda =  0.486752255959971  loss:  280.924684346715&quot;
## [1] &quot;step =  2  lambda =  0.486752255959971  loss:  282.147310513384&quot;
## [1] &quot;step =  1  lambda =  0.481908990090202  loss:  279.393775612647&quot;
## [1] &quot;step =  2  lambda =  0.481908990090202  loss:  280.568567364981&quot;
## [1] &quot;step =  1  lambda =  0.477113915521034  loss:  277.830143260411&quot;
## [1] &quot;step =  2  lambda =  0.477113915521034  loss:  278.960259863943&quot;
## [1] &quot;step =  1  lambda =  0.472366552741015  loss:  276.237249627593&quot;
## [1] &quot;step =  2  lambda =  0.472366552741015  loss:  277.325661150691&quot;
## [1] &quot;step =  1  lambda =  0.467666427009909  loss:  274.618335406166&quot;
## [1] &quot;step =  2  lambda =  0.467666427009909  loss:  275.667830968464&quot;
## [1] &quot;step =  1  lambda =  0.463013068311228  loss:  272.976429990129&quot;
## [1] &quot;step =  2  lambda =  0.463013068311228  loss:  273.989625261234&quot;
## [1] &quot;step =  1  lambda =  0.458406011305224  loss:  271.314360979115&quot;
## [1] &quot;step =  2  lambda =  0.458406011305224  loss:  272.29370534998&quot;
## [1] &quot;step =  1  lambda =  0.453844795282356  loss:  269.634763264619&quot;
## [1] &quot;step =  2  lambda =  0.453844795282356  loss:  270.582546902629&quot;
## [1] &quot;step =  1  lambda =  0.449328964117222  loss:  267.94008791203&quot;
## [1] &quot;step =  2  lambda =  0.449328964117222  loss:  268.858448789069&quot;
## [1] &quot;step =  1  lambda =  0.444858066222941  loss:  266.232610928913&quot;
## [1] &quot;step =  2  lambda =  0.444858066222941  loss:  267.123541845972&quot;
## [1] &quot;step =  1  lambda =  0.440431654505999  loss:  264.51444194405&quot;
## [1] &quot;step =  2  lambda =  0.440431654505999  loss:  265.37979754335&quot;
## [1] &quot;step =  1  lambda =  0.436049286321536  loss:  262.787532789171&quot;
## [1] &quot;step =  2  lambda =  0.436049286321536  loss:  263.629036530596&quot;
## [1] &quot;step =  1  lambda =  0.43171052342908  loss:  261.053685961372&quot;
## [1] &quot;step =  2  lambda =  0.43171052342908  loss:  261.872937035439&quot;
## [1] &quot;step =  1  lambda =  0.427414931948727  loss:  259.314562939855&quot;
## [1] &quot;step =  2  lambda =  0.427414931948727  loss:  260.113043089429&quot;
## [1] &quot;step =  1  lambda =  0.423162082317749  loss:  257.571692330877&quot;
## [1] &quot;step =  2  lambda =  0.423162082317749  loss:  258.350772555626&quot;
## [1] &quot;step =  1  lambda =  0.418951549247639  loss:  255.826477816802&quot;
## [1] &quot;step =  2  lambda =  0.418951549247639  loss:  256.639078548013&quot;
## [1] &quot;step =  1  lambda =  0.414782911681582  loss:  254.129396977521&quot;
## [1] &quot;step =  2  lambda =  0.414782911681582  loss:  255.107821251357&quot;
## [1] &quot;step =  1  lambda =  0.410655752752345  loss:  252.612849879837&quot;
## [1] &quot;step =  2  lambda =  0.410655752752345  loss:  253.601045305134&quot;
## [1] &quot;step =  1  lambda =  0.406569659740599  loss:  251.120582171481&quot;
## [1] &quot;step =  2  lambda =  0.406569659740599  loss:  252.123738561775&quot;
## [1] &quot;step =  1  lambda =  0.402524224033636  loss:  249.657527508876&quot;
## [1] &quot;step =  2  lambda =  0.402524224033636  loss:  250.655007416854&quot;
## [1] &quot;step =  1  lambda =  0.398519041084514  loss:  248.202986301513&quot;
## [1] &quot;step =  2  lambda =  0.398519041084514  loss:  249.184293670608&quot;
## [1] &quot;step =  1  lambda =  0.394553710371601  loss:  246.746498943925&quot;
## [1] &quot;step =  2  lambda =  0.394553710371601  loss:  247.707234328777&quot;
## [1] &quot;step =  1  lambda =  0.390627835358521  loss:  245.283742057325&quot;
## [1] &quot;step =  2  lambda =  0.390627835358521  loss:  246.229616586988&quot;
## [1] &quot;step =  1  lambda =  0.386741023454502  loss:  243.820164619078&quot;
## [1] &quot;step =  2  lambda =  0.386741023454502  loss:  244.73713714038&quot;
## [1] &quot;step =  1  lambda =  0.382892885975112  loss:  242.342189061954&quot;
## [1] &quot;step =  2  lambda =  0.382892885975112  loss:  243.199180356785&quot;
## [1] &quot;step =  1  lambda =  0.379083038103399  loss:  240.819188692423&quot;
## [1] &quot;step =  2  lambda =  0.379083038103399  loss:  241.645593452495&quot;
## [1] &quot;step =  1  lambda =  0.375311098851399  loss:  239.280717366307&quot;
## [1] &quot;step =  2  lambda =  0.375311098851399  loss:  240.084706767699&quot;
## [1] &quot;step =  1  lambda =  0.371576691022046  loss:  237.735024136192&quot;
## [1] &quot;step =  2  lambda =  0.371576691022046  loss:  238.519817733513&quot;
## [1] &quot;step =  1  lambda =  0.367879441171442  loss:  236.185374291051&quot;
## [1] &quot;step =  2  lambda =  0.367879441171442  loss:  236.952875719631&quot;
## [1] &quot;step =  1  lambda =  0.364218979571523  loss:  234.633698079774&quot;
## [1] &quot;step =  2  lambda =  0.364218979571523  loss:  235.385259326844&quot;
## [1] &quot;step =  1  lambda =  0.360594940173078  loss:  233.081360491103&quot;
## [1] &quot;step =  2  lambda =  0.360594940173078  loss:  233.810482541617&quot;
## [1] &quot;step =  1  lambda =  0.357006960569148  loss:  231.521271517477&quot;
## [1] &quot;step =  2  lambda =  0.357006960569148  loss:  232.22489521584&quot;
## [1] &quot;step =  1  lambda =  0.35345468195878  loss:  229.951179449077&quot;
## [1] &quot;step =  2  lambda =  0.35345468195878  loss:  230.633022083247&quot;
## [1] &quot;step =  1  lambda =  0.349937749111156  loss:  228.374872064536&quot;
## [1] &quot;step =  2  lambda =  0.349937749111156  loss:  229.036691841347&quot;
## [1] &quot;step =  1  lambda =  0.346455810330057  loss:  226.79415759903&quot;
## [1] &quot;step =  2  lambda =  0.346455810330057  loss:  227.438453570621&quot;
## [1] &quot;step =  1  lambda =  0.343008517418707  loss:  225.211559449265&quot;
## [1] &quot;step =  2  lambda =  0.343008517418707  loss:  225.840059828826&quot;
## [1] &quot;step =  1  lambda =  0.339595525644939  loss:  223.628812431225&quot;
## [1] &quot;step =  2  lambda =  0.339595525644939  loss:  224.242825058624&quot;
## [1] &quot;step =  1  lambda =  0.336216493706733  loss:  222.047217698941&quot;
## [1] &quot;step =  2  lambda =  0.336216493706733  loss:  222.647848118983&quot;
## [1] &quot;step =  1  lambda =  0.33287108369808  loss:  220.467274362435&quot;
## [1] &quot;step =  2  lambda =  0.33287108369808  loss:  221.052223390945&quot;
## [1] &quot;step =  1  lambda =  0.329558961075189  loss:  218.886363323196&quot;
## [1] &quot;step =  2  lambda =  0.329558961075189  loss:  219.455572706726&quot;
## [1] &quot;step =  1  lambda =  0.32627979462304  loss:  217.305391551337&quot;
## [1] &quot;step =  2  lambda =  0.32627979462304  loss:  217.860112362908&quot;
## [1] &quot;step =  1  lambda =  0.323033256422253  loss:  215.725602414217&quot;
## [1] &quot;step =  2  lambda =  0.323033256422253  loss:  216.267044167304&quot;
## [1] &quot;step =  1  lambda =  0.319819021816304  loss:  214.148185447676&quot;
## [1] &quot;step =  2  lambda =  0.319819021816304  loss:  214.677413236914&quot;
## [1] &quot;step =  1  lambda =  0.316636769379053  loss:  212.574175246469&quot;
## [1] &quot;step =  2  lambda =  0.316636769379053  loss:  213.092101590067&quot;
## [1] &quot;step =  1  lambda =  0.313486180882605  loss:  211.004445004664&quot;
## [1] &quot;step =  2  lambda =  0.313486180882605  loss:  211.511846558831&quot;
## [1] &quot;step =  1  lambda =  0.310366941265485  loss:  209.439724694069&quot;
## [1] &quot;step =  2  lambda =  0.310366941265485  loss:  209.937263020745&quot;
## [1] &quot;step =  1  lambda =  0.307278738601131  loss:  207.880623053044&quot;
## [1] &quot;step =  2  lambda =  0.307278738601131  loss:  208.368864045193&quot;
## [1] &quot;step =  1  lambda =  0.304221264066704  loss:  206.327648018899&quot;
## [1] &quot;step =  2  lambda =  0.304221264066704  loss:  206.807078804447&quot;
## [1] &quot;step =  1  lambda =  0.301194211912202  loss:  204.781108363172&quot;
## [1] &quot;step =  2  lambda =  0.301194211912202  loss:  205.25109259907&quot;
## [1] &quot;step =  1  lambda =  0.298197279429888  loss:  203.240125927116&quot;
## [1] &quot;step =  2  lambda =  0.298197279429888  loss:  203.70531478554&quot;
## [1] &quot;step =  1  lambda =  0.295230166924014  loss:  201.709568105297&quot;
## [1] &quot;step =  2  lambda =  0.295230166924014  loss:  202.169893222743&quot;
## [1] &quot;step =  1  lambda =  0.292292577680859  loss:  200.189266868203&quot;
## [1] &quot;step =  2  lambda =  0.292292577680859  loss:  200.644516139563&quot;
## [1] &quot;step =  1  lambda =  0.289384217939051  loss:  198.678913496541&quot;
## [1] &quot;step =  2  lambda =  0.289384217939051  loss:  199.128923791545&quot;
## [1] &quot;step =  1  lambda =  0.28650479686019  loss:  197.178250732988&quot;
## [1] &quot;step =  2  lambda =  0.28650479686019  loss:  197.622912825013&quot;
## [1] &quot;step =  1  lambda =  0.28365402649977  loss:  195.687077135036&quot;
## [1] &quot;step =  2  lambda =  0.28365402649977  loss:  196.126327845142&quot;
## [1] &quot;step =  1  lambda =  0.28083162177838  loss:  194.20523874141&quot;
## [1] &quot;step =  2  lambda =  0.28083162177838  loss:  194.639052123521&quot;
## [1] &quot;step =  1  lambda =  0.278037300453194  loss:  192.732619878331&quot;
## [1] &quot;step =  2  lambda =  0.278037300453194  loss:  193.160999175841&quot;
## [1] &quot;step =  1  lambda =  0.275270783089753  loss:  191.26913482301&quot;
## [1] &quot;step =  2  lambda =  0.275270783089753  loss:  191.692105662371&quot;
## [1] &quot;step =  1  lambda =  0.272531793034013  loss:  189.814720774767&quot;
## [1] &quot;step =  2  lambda =  0.272531793034013  loss:  190.232325645975&quot;
## [1] &quot;step =  1  lambda =  0.269820056384687  loss:  188.369332169397&quot;
## [1] &quot;step =  2  lambda =  0.269820056384687  loss:  188.781626067205&quot;
## [1] &quot;step =  1  lambda =  0.26713530196585  loss:  186.93293619829&quot;
## [1] &quot;step =  2  lambda =  0.26713530196585  loss:  187.339983236543&quot;
## [1] &quot;step =  1  lambda =  0.264477261299824  loss:  185.505509334635&quot;
## [1] &quot;step =  2  lambda =  0.264477261299824  loss:  185.907380142238&quot;
## [1] &quot;step =  1  lambda =  0.261845668580326  loss:  184.087034667246&quot;
## [1] &quot;step =  2  lambda =  0.261845668580326  loss:  184.48380439584&quot;
## [1] &quot;step =  1  lambda =  0.259240260645892  loss:  182.67749986589&quot;
## [1] &quot;step =  2  lambda =  0.259240260645892  loss:  183.069246669009&quot;
## [1] &quot;step =  1  lambda =  0.256660776953556  loss:  181.276895633144&quot;
## [1] &quot;step =  2  lambda =  0.256660776953556  loss:  181.663699505854&quot;
## [1] &quot;step =  1  lambda =  0.2541069595528  loss:  179.885214528162&quot;
## [1] &quot;step =  2  lambda =  0.2541069595528  loss:  180.267156421471&quot;
## [1] &quot;step =  1  lambda =  0.251578553059757  loss:  178.502450073872&quot;
## [1] &quot;step =  2  lambda =  0.251578553059757  loss:  178.879611218667&quot;
## [1] &quot;step =  1  lambda =  0.249075304631668  loss:  177.128596080262&quot;
## [1] &quot;step =  2  lambda =  0.249075304631668  loss:  177.501057471455&quot;
## [1] &quot;step =  1  lambda =  0.246596963941606  loss:  175.763646132824&quot;
## [1] &quot;step =  2  lambda =  0.246596963941606  loss:  176.131488136515&quot;
## [1] &quot;step =  1  lambda =  0.244143283153437  loss:  174.407593207728&quot;
## [1] &quot;step =  2  lambda =  0.244143283153437  loss:  174.770895263312&quot;
## [1] &quot;step =  1  lambda =  0.241714016897036  loss:  173.060429384698&quot;
## [1] &quot;step =  2  lambda =  0.241714016897036  loss:  173.41926978065&quot;
## [1] &quot;step =  1  lambda =  0.239308922243755  loss:  171.722145635581&quot;
## [1] &quot;step =  2  lambda =  0.239308922243755  loss:  172.07660134273&quot;
## [1] &quot;step =  1  lambda =  0.236927758682122  loss:  170.39273167185&quot;
## [1] &quot;step =  2  lambda =  0.236927758682122  loss:  170.74287822175&quot;
## [1] &quot;step =  1  lambda =  0.234570288093798  loss:  169.072175838187&quot;
## [1] &quot;step =  2  lambda =  0.234570288093798  loss:  169.418087237019&quot;
## [1] &quot;step =  1  lambda =  0.232236274729759  loss:  167.760465042235&quot;
## [1] &quot;step =  2  lambda =  0.232236274729759  loss:  168.102213712827&quot;
## [1] &quot;step =  1  lambda =  0.229925485186724  loss:  166.457584712819&quot;
## [1] &quot;step =  2  lambda =  0.229925485186724  loss:  166.795241458973&quot;
## [1] &quot;step =  1  lambda =  0.227637688383813  loss:  165.163518780613&quot;
## [1] &quot;step =  2  lambda =  0.227637688383813  loss:  165.497152769172&quot;
## [1] &quot;step =  1  lambda =  0.225372655539439  loss:  163.878249676501&quot;
## [1] &quot;step =  2  lambda =  0.225372655539439  loss:  164.207928433515&quot;
## [1] &quot;step =  1  lambda =  0.22313016014843  loss:  162.601758343871&quot;
## [1] &quot;step =  2  lambda =  0.22313016014843  loss:  162.927547761972&quot;
## [1] &quot;step =  1  lambda =  0.220909977959378  loss:  161.334024261825&quot;
## [1] &quot;step =  2  lambda =  0.220909977959378  loss:  161.65598861647&quot;
## [1] &quot;step =  1  lambda =  0.218711886952215  loss:  160.075025476903&quot;
## [1] &quot;step =  2  lambda =  0.218711886952215  loss:  160.393227449597&quot;
## [1] &quot;step =  1  lambda =  0.216535667316007  loss:  158.824738641355&quot;
## [1] &quot;step =  2  lambda =  0.216535667316007  loss:  159.139239348313&quot;
## [1] &quot;step =  1  lambda =  0.214381101426978  loss:  157.583139056375&quot;
## [1] &quot;step =  2  lambda =  0.214381101426978  loss:  157.893998081376&quot;
## [1] &quot;step =  1  lambda =  0.212247973826743  loss:  156.350200719018&quot;
## [1] &quot;step =  2  lambda =  0.212247973826743  loss:  156.657476149404&quot;
## [1] &quot;step =  1  lambda =  0.210136071200765  loss:  155.125896371729&quot;
## [1] &quot;step =  2  lambda =  0.210136071200765  loss:  155.429644836707&quot;
## [1] &quot;step =  1  lambda =  0.20804518235702  loss:  153.910197553622&quot;
## [1] &quot;step =  2  lambda =  0.20804518235702  loss:  154.210474264165&quot;
## [1] &quot;step =  1  lambda =  0.205975098204883  loss:  152.703074652803&quot;
## [1] &quot;step =  2  lambda =  0.205975098204883  loss:  152.999933442575&quot;
## [1] &quot;step =  1  lambda =  0.203925611734213  loss:  151.504496959148&quot;
## [1] &quot;step =  2  lambda =  0.203925611734213  loss:  151.797990325968&quot;
## [1] &quot;step =  1  lambda =  0.201896517994655  loss:  150.314432717066&quot;
## [1] &quot;step =  2  lambda =  0.201896517994655  loss:  150.604611864526&quot;
## [1] &quot;step =  1  lambda =  0.199887614075145  loss:  149.132849177858&quot;
## [1] &quot;step =  2  lambda =  0.199887614075145  loss:  149.419764056766&quot;
## [1] &quot;step =  1  lambda =  0.197898699083615  loss:  147.959712651356&quot;
## [1] &quot;step =  2  lambda =  0.197898699083615  loss:  148.243412000742&quot;
## [1] &quot;step =  1  lambda =  0.19592957412691  loss:  146.794988556587&quot;
## [1] &quot;step =  2  lambda =  0.19592957412691  loss:  147.075519944068&quot;
## [1] &quot;step =  1  lambda =  0.193980042290892  loss:  145.638641471278&quot;
## [1] &quot;step =  2  lambda =  0.193980042290892  loss:  145.916051332601&quot;
## [1] &quot;step =  1  lambda =  0.192049908620754  loss:  144.490635180027&quot;
## [1] &quot;step =  2  lambda =  0.192049908620754  loss:  144.764968857661&quot;
## [1] &quot;step =  1  lambda =  0.19013898010152  loss:  143.350932721033&quot;
## [1] &quot;step =  2  lambda =  0.19013898010152  loss:  143.622234501712&quot;
## [1] &quot;step =  1  lambda =  0.188247065638747  loss:  142.2194964313&quot;
## [1] &quot;step =  2  lambda =  0.188247065638747  loss:  142.487809582435&quot;
## [1] &quot;step =  1  lambda =  0.18637397603941  loss:  141.096287990254&quot;
## [1] &quot;step =  2  lambda =  0.18637397603941  loss:  141.36165479516&quot;
## [1] &quot;step =  1  lambda =  0.184519523992989  loss:  139.981268461727&quot;
## [1] &quot;step =  2  lambda =  0.184519523992989  loss:  140.24373025363&quot;
## [1] &quot;step =  1  lambda =  0.182683524052735  loss:  138.874398334306&quot;
## [1] &quot;step =  2  lambda =  0.182683524052735  loss:  139.133995529118&quot;
## [1] &quot;step =  1  lambda =  0.180865792617122  loss:  137.775637560033&quot;
## [1] &quot;step =  2  lambda =  0.180865792617122  loss:  138.03240968787&quot;
## [1] &quot;step =  1  lambda =  0.179066147911493  loss:  136.684945591467&quot;
## [1] &quot;step =  2  lambda =  0.179066147911493  loss:  136.938931326938&quot;
## [1] &quot;step =  1  lambda =  0.177284409969878  loss:  135.602281417141&quot;
## [1] &quot;step =  2  lambda =  0.177284409969878  loss:  135.853518608402&quot;
## [1] &quot;step =  1  lambda =  0.175520400616997  loss:  134.527603595425&quot;
## [1] &quot;step =  2  lambda =  0.175520400616997  loss:  134.776129292028&quot;
## [1] &quot;step =  1  lambda =  0.173773943450445  loss:  133.46087028685&quot;
## [1] &quot;step =  2  lambda =  0.173773943450445  loss:  133.706720766419&quot;
## [1] &quot;step =  1  lambda =  0.172044863823051  loss:  132.402039284929&quot;
## [1] &quot;step =  2  lambda =  0.172044863823051  loss:  132.645250078681&quot;
## [1] &quot;step =  1  lambda =  0.17033298882541  loss:  131.351068045522&quot;
## [1] &quot;step =  2  lambda =  0.17033298882541  loss:  131.591673962681&quot;
## [1] &quot;step =  1  lambda =  0.168638147268596  loss:  130.307913714789&quot;
## [1] &quot;step =  2  lambda =  0.168638147268596  loss:  130.545948865925&quot;
## [1] &quot;step =  1  lambda =  0.166960169667041  loss:  129.272533155801&quot;
## [1] &quot;step =  2  lambda =  0.166960169667041  loss:  129.508030975139&quot;
## [1] &quot;step =  1  lambda =  0.165298888221586  loss:  128.244882973842&quot;
## [1] &quot;step =  2  lambda =  0.165298888221586  loss:  128.477876240574&quot;
## [1] &quot;step =  1  lambda =  0.163654136802704  loss:  127.224919540472&quot;
## [1] &quot;step =  2  lambda =  0.163654136802704  loss:  127.455440399124&quot;
## [1] &quot;step =  1  lambda =  0.162025750933881  loss:  126.212599016393&quot;
## [1] &quot;step =  2  lambda =  0.162025750933881  loss:  126.440678996289&quot;
## [1] &quot;step =  1  lambda =  0.160413567775173  loss:  125.207877373192&quot;
## [1] &quot;step =  2  lambda =  0.160413567775173  loss:  125.433547407041&quot;
## [1] &quot;step =  1  lambda =  0.158817426106921  loss:  124.210710413984&quot;
## [1] &quot;step =  2  lambda =  0.158817426106921  loss:  124.434000855654&quot;
## [1] &quot;step =  1  lambda =  0.157237166313628  loss:  123.221053793038&quot;
## [1] &quot;step =  2  lambda =  0.157237166313628  loss:  123.441994434547&quot;
## [1] &quot;step =  1  lambda =  0.155672630367997  loss:  122.238863034421&quot;
## [1] &quot;step =  2  lambda =  0.155672630367997  loss:  122.457483122179&quot;
## [1] &quot;step =  1  lambda =  0.154123661815132  loss:  121.264093549706&quot;
## [1] &quot;step =  2  lambda =  0.154123661815132  loss:  121.480421800063&quot;
## [1] &quot;step =  1  lambda =  0.152590105756884  loss:  120.296700654809&quot;
## [1] &quot;step =  2  lambda =  0.152590105756884  loss:  120.510765268924&quot;
## [1] &quot;step =  1  lambda =  0.151071808836371  loss:  119.336639585978&quot;
## [1] &quot;step =  2  lambda =  0.151071808836371  loss:  119.548468264066&quot;
## [1] &quot;step =  1  lambda =  0.149568619222635  loss:  118.383865515&quot;
## [1] &quot;step =  2  lambda =  0.149568619222635  loss:  118.593485469982&quot;
## [1] &quot;step =  1  lambda =  0.148080386595462  loss:  117.43833356365&quot;
## [1] &quot;step =  2  lambda =  0.148080386595462  loss:  117.645771534236&quot;
## [1] &quot;step =  1  lambda =  0.14660696213035  loss:  116.499998817439&quot;
## [1] &quot;step =  2  lambda =  0.14660696213035  loss:  116.705281080677&quot;
## [1] &quot;step =  1  lambda =  0.145148198483624  loss:  115.568816338684&quot;
## [1] &quot;step =  2  lambda =  0.145148198483624  loss:  115.771968722008&quot;
## [1] &quot;step =  1  lambda =  0.143703949777703  loss:  114.644741178941&quot;
## [1] &quot;step =  2  lambda =  0.143703949777703  loss:  114.845789071746&quot;
## [1] &quot;step =  1  lambda =  0.142274071586514  loss:  113.727728390846&quot;
## [1] &quot;step =  2  lambda =  0.142274071586514  loss:  113.926696755609&quot;
## [1] &quot;step =  1  lambda =  0.140858420921045  loss:  112.817733039378&quot;
## [1] &quot;step =  2  lambda =  0.140858420921045  loss:  113.014646422361&quot;
## [1] &quot;step =  1  lambda =  0.139456856215051  loss:  111.914710212588&quot;
## [1] &quot;step =  2  lambda =  0.139456856215051  loss:  112.109592754135&quot;
## [1] &quot;step =  1  lambda =  0.138069237310893  loss:  111.018615031821&quot;
## [1] &quot;step =  2  lambda =  0.138069237310893  loss:  111.211490476282&quot;
## [1] &quot;step =  1  lambda =  0.136695425445524  loss:  110.12940266145&quot;
## [1] &quot;step =  2  lambda =  0.136695425445524  loss:  110.320294366744&quot;
## [1] &quot;step =  1  lambda =  0.135335283236613  loss:  109.247028318159&quot;
## [1] &quot;step =  2  lambda =  0.135335283236613  loss:  109.435959265002&quot;
## [1] &quot;step =  1  lambda =  0.133988674668805  loss:  108.371447279794&quot;
## [1] &quot;step =  2  lambda =  0.133988674668805  loss:  108.558440080603&quot;
## [1] &quot;step =  1  lambda =  0.132655465080122  loss:  107.502614893794&quot;
## [1] &quot;step =  2  lambda =  0.132655465080122  loss:  107.687691801297&quot;
## [1] &quot;step =  1  lambda =  0.131335521148493  loss:  106.640486585251&quot;
## [1] &quot;step =  2  lambda =  0.131335521148493  loss:  106.823669500799&quot;
## [1] &quot;step =  1  lambda =  0.130028710878426  loss:  105.785017864583&quot;
## [1] &quot;step =  2  lambda =  0.130028710878426  loss:  105.966328346197&quot;
## [1] &quot;step =  1  lambda =  0.128734903587804  loss:  104.936164334866&quot;
## [1] &quot;step =  2  lambda =  0.128734903587804  loss:  105.115623605029&quot;
## [1] &quot;step =  1  lambda =  0.127453969894821  loss:  104.093881698833&quot;
## [1] &quot;step =  2  lambda =  0.127453969894821  loss:  104.271510652028&quot;
## [1] &quot;step =  1  lambda =  0.126185781705039  loss:  103.258125765553&quot;
## [1] &quot;step =  2  lambda =  0.126185781705039  loss:  103.433944975576&quot;
## [1] &quot;step =  1  lambda =  0.124930212198582  loss:  102.428852456807&quot;
## [1] &quot;step =  2  lambda =  0.124930212198582  loss:  102.602882183857&quot;
## [1] &quot;step =  1  lambda =  0.123687135817455  loss:  101.606017813187&quot;
## [1] &quot;step =  2  lambda =  0.123687135817455  loss:  101.778278010739&quot;
## [1] &quot;step =  1  lambda =  0.122456428252982  loss:  100.789577999903&quot;
## [1] &quot;step =  2  lambda =  0.122456428252982  loss:  100.960088321387&quot;
## [1] &quot;step =  1  lambda =  0.121237966433382  loss:  99.9794893123455&quot;
## [1] &quot;step =  2  lambda =  0.121237966433382  loss:  100.148269117632&quot;
## [1] &quot;step =  1  lambda =  0.120031628511457  loss:  99.1757081813907&quot;
## [1] &quot;step =  2  lambda =  0.120031628511457  loss:  99.3427765430863&quot;
## [1] &quot;step =  1  lambda =  0.11883729385241  loss:  98.3781911784659&quot;
## [1] &quot;step =  2  lambda =  0.11883729385241  loss:  98.5435668880405&quot;
## [1] &quot;step =  1  lambda =  0.117654843021779  loss:  97.5868950203891&quot;
## [1] &quot;step =  2  lambda =  0.117654843021779  loss:  97.7505965941306&quot;
## [1] &quot;step =  1  lambda =  0.116484157773497  loss:  96.8017765739891&quot;
## [1] &quot;step =  2  lambda =  0.116484157773497  loss:  96.9638222587975&quot;
## [1] &quot;step =  1  lambda =  0.115325121038063  loss:  96.0227928605156&quot;
## [1] &quot;step =  2  lambda =  0.115325121038063  loss:  96.1832006395414&quot;
## [1] &quot;step =  1  lambda =  0.114177616910836  loss:  95.2499010598481&quot;
## [1] &quot;step =  2  lambda =  0.114177616910836  loss:  95.4086886579818&quot;
## [1] &quot;step =  1  lambda =  0.11304153064045  loss:  94.4830585145117&quot;
## [1] &quot;step =  2  lambda =  0.11304153064045  loss:  94.6402434037299&quot;
## [1] &quot;step =  1  lambda =  0.111916748617329  loss:  93.7222227335076&quot;
## [1] &quot;step =  2  lambda =  0.111916748617329  loss:  93.8778221380798&quot;
## [1] &quot;step =  1  lambda =  0.110803158362334  loss:  92.9673513959646&quot;
## [1] &quot;step =  2  lambda =  0.110803158362334  loss:  93.1213822975275&quot;
## [1] &quot;step =  1  lambda =  0.109700648515511  loss:  92.2184023546189&quot;
## [1] &quot;step =  2  lambda =  0.109700648515511  loss:  92.3708814971215&quot;
## [1] &quot;step =  1  lambda =  0.108609108824958  loss:  91.4753336391293&quot;
## [1] &quot;step =  2  lambda =  0.108609108824958  loss:  91.6262775336533&quot;
## [1] &quot;step =  1  lambda =  0.107528430135795  loss:  90.7381034592318&quot;
## [1] &quot;step =  2  lambda =  0.107528430135795  loss:  90.8875283886912&quot;
## [1] &quot;step =  1  lambda =  0.106458504379253  loss:  90.0066702077412&quot;
## [1] &quot;step =  2  lambda =  0.106458504379253  loss:  90.1545922314667&quot;
## [1] &quot;step =  1  lambda =  0.105399224561864  loss:  89.2809924634047&quot;
## [1] &quot;step =  2  lambda =  0.105399224561864  loss:  89.427427421613&quot;
## [1] &quot;step =  1  lambda =  0.104350484754765  loss:  88.5610289936113&quot;
## [1] &quot;step =  2  lambda =  0.104350484754765  loss:  88.7059925117668&quot;
## [1] &quot;step =  1  lambda =  0.1033121800831  loss:  87.8467387569639&quot;
## [1] &quot;step =  2  lambda =  0.1033121800831  loss:  87.9902297992362&quot;
## [1] &quot;step =  1  lambda =  0.102284206715537  loss:  87.1380176092853&quot;
## [1] &quot;step =  2  lambda =  0.102284206715537  loss:  87.2799594354055&quot;
## [1] &quot;step =  1  lambda =  0.101266461853883  loss:  86.4347822755175&quot;
## [1] &quot;step =  2  lambda =  0.101266461853883  loss:  86.5752189504145&quot;
## [1] &quot;step =  1  lambda =  0.100258843722804  loss:  85.7370221944553&quot;
## [1] &quot;step =  2  lambda =  0.100258843722804  loss:  85.8840416106993&quot;
## [1] &quot;step =  1  lambda =  0.0992612515596457  loss:  85.0527831986601&quot;
## [1] &quot;step =  2  lambda =  0.0992612515596457  loss:  85.1995578654052&quot;
## [1] &quot;step =  1  lambda =  0.0982735856043615  loss:  84.374868828401&quot;
## [1] &quot;step =  2  lambda =  0.0982735856043615  loss:  84.5194321238622&quot;
## [1] &quot;step =  1  lambda =  0.0972957470895328  loss:  83.701483302238&quot;
## [1] &quot;step =  2  lambda =  0.0972957470895328  loss:  83.8446552943387&quot;
## [1] &quot;step =  1  lambda =  0.096327638230493  loss:  83.0333940750309&quot;
## [1] &quot;step =  2  lambda =  0.096327638230493  loss:  83.1746926841621&quot;
## [1] &quot;step =  1  lambda =  0.0953691622155497  loss:  82.3700715972316&quot;
## [1] &quot;step =  2  lambda =  0.0953691622155497  loss:  82.5093639445494&quot;
## [1] &quot;step =  1  lambda =  0.0944202231963024  loss:  81.7113371819535&quot;
## [1] &quot;step =  2  lambda =  0.0944202231963024  loss:  81.8486632872595&quot;
## [1] &quot;step =  1  lambda =  0.0934807262780585  loss:  81.0571997564564&quot;
## [1] &quot;step =  2  lambda =  0.0934807262780585  loss:  81.1926346738146&quot;
## [1] &quot;step =  1  lambda =  0.0925505775103433  loss:  80.4076731099821&quot;
## [1] &quot;step =  2  lambda =  0.0925505775103433  loss:  80.5412936813537&quot;
## [1] &quot;step =  1  lambda =  0.0916296838775049  loss:  79.7627876269955&quot;
## [1] &quot;step =  2  lambda =  0.0916296838775049  loss:  79.8946701377825&quot;
## [1] &quot;step =  1  lambda =  0.0907179532894126  loss:  79.1225728291068&quot;
## [1] &quot;step =  2  lambda =  0.0907179532894126  loss:  79.2527862330255&quot;
## [1] &quot;step =  1  lambda =  0.0898152945722476  loss:  78.4870506800169&quot;
## [1] &quot;step =  2  lambda =  0.0898152945722476  loss:  78.6156554456097&quot;
## [1] &quot;step =  1  lambda =  0.0889216174593863  loss:  77.856234520258&quot;
## [1] &quot;step =  2  lambda =  0.0889216174593863  loss:  77.9832832708279&quot;
## [1] &quot;step =  1  lambda =  0.0880368325823726  loss:  77.2301297870849&quot;
## [1] &quot;step =  2  lambda =  0.0880368325823726  loss:  77.35566843715&quot;
## [1] &quot;step =  1  lambda =  0.0871608514619813  loss:  76.6087352184133&quot;
## [1] &quot;step =  2  lambda =  0.0871608514619813  loss:  76.7328041269968&quot;
## [1] &quot;step =  1  lambda =  0.0862935864993705  loss:  75.9920440613469&quot;
## [1] &quot;step =  2  lambda =  0.0862935864993705  loss:  76.1146790423532&quot;
## [1] &quot;step =  1  lambda =  0.0854349509673212  loss:  75.3800451271912&quot;
## [1] &quot;step =  2  lambda =  0.0854349509673212  loss:  75.5012782834465&quot;
## [1] &quot;step =  1  lambda =  0.0845848590015647  loss:  74.7727236614189&quot;
## [1] &quot;step =  2  lambda =  0.0845848590015647  loss:  74.8925840552233&quot;
## [1] &quot;step =  1  lambda =  0.083743225592196  loss:  74.1700620431472&quot;
## [1] &quot;step =  2  lambda =  0.083743225592196  loss:  74.2885762296582&quot;
## [1] &quot;step =  1  lambda =  0.0829099665751727  loss:  73.5720403418696&quot;
## [1] &quot;step =  2  lambda =  0.0829099665751727  loss:  73.6892327923895&quot;
## [1] &quot;step =  1  lambda =  0.0820849986238988  loss:  72.9786367596559&quot;
## [1] &quot;step =  2  lambda =  0.0820849986238988  loss:  73.0945301983423&quot;
## [1] &quot;step =  1  lambda =  0.0812682392408917  loss:  72.3898279832356&quot;
## [1] &quot;step =  2  lambda =  0.0812682392408917  loss:  72.5044436562956&quot;
## [1] &quot;step =  1  lambda =  0.0804596067495325  loss:  71.8055894657271&quot;
## [1] &quot;step =  2  lambda =  0.0804596067495325  loss:  71.918947358049&quot;
## [1] &quot;step =  1  lambda =  0.079659020285898  loss:  71.2258956535129&quot;
## [1] &quot;step =  2  lambda =  0.079659020285898  loss:  71.3380146642828&quot;
## [1] &quot;step =  1  lambda =  0.0788663997906749  loss:  70.6507201702381&quot;
## [1] &quot;step =  2  lambda =  0.0788663997906749  loss:  70.7616182564089&quot;
## [1] &quot;step =  1  lambda =  0.0780816660011532  loss:  70.0800359671365&quot;
## [1] &quot;step =  2  lambda =  0.0780816660011532  loss:  70.1897302615507&quot;
## [1] &quot;step =  1  lambda =  0.0773047404432998  loss:  69.513815446755&quot;
## [1] &quot;step =  2  lambda =  0.0773047404432998  loss:  69.6223223561524&quot;
## [1] &quot;step =  1  lambda =  0.0765355454239115  loss:  68.952030565518&quot;
## [1] &quot;step =  2  lambda =  0.0765355454239115  loss:  69.0593658524667&quot;
## [1] &quot;step =  1  lambda =  0.0757740040228455  loss:  68.3946529193454&quot;
## [1] &quot;step =  2  lambda =  0.0757740040228455  loss:  68.5008317712333&quot;
## [1] &quot;step =  1  lambda =  0.075020040085327  loss:  67.8416538155962&quot;
## [1] &quot;step =  2  lambda =  0.075020040085327  loss:  67.9466909031311&quot;
## [1] &quot;step =  1  lambda =  0.0742735782143339  loss:  67.2930043339022&quot;
## [1] &quot;step =  2  lambda =  0.0742735782143339  loss:  67.3969138610474&quot;
## [1] &quot;step =  1  lambda =  0.0735345437630571  loss:  66.7486753779087&quot;
## [1] &quot;step =  2  lambda =  0.0735345437630571  loss:  66.851471124776&quot;
## [1] &quot;step =  1  lambda =  0.0728028628274356  loss:  66.2086377195224&quot;
## [1] &quot;step =  2  lambda =  0.0728028628274356  loss:  66.3103330794369&quot;
## [1] &quot;step =  1  lambda =  0.0720784622387661  loss:  65.6728620369433&quot;
## [1] &quot;step =  2  lambda =  0.0720784622387661  loss:  65.7734700486507&quot;
## [1] &quot;step =  1  lambda =  0.0713612695563861  loss:  65.1413189475051&quot;
## [1] &quot;step =  2  lambda =  0.0713612695563861  loss:  65.2408523233074&quot;
## [1] &quot;step =  1  lambda =  0.0706512130604296  loss:  64.6139790361541&quot;
## [1] &quot;step =  2  lambda =  0.0706512130604296  loss:  64.7124501866088&quot;
## [1] &quot;step =  1  lambda =  0.0699482217446554  loss:  64.0908128802408&quot;
## [1] &quot;step =  2  lambda =  0.0699482217446554  loss:  64.1882339359458&quot;
## [1] &quot;step =  1  lambda =  0.069252225309346  loss:  63.5717910711762&quot;
## [1] &quot;step =  2  lambda =  0.069252225309346  loss:  63.6681739020664&quot;
## [1] &quot;step =  1  lambda =  0.0685631541542779  loss:  63.0568842334086&quot;
## [1] &quot;step =  2  lambda =  0.0685631541542779  loss:  63.1522404659177&quot;
## [1] &quot;step =  1  lambda =  0.0678809393717615  loss:  62.5460630410973&quot;
## [1] &quot;step =  2  lambda =  0.0678809393717615  loss:  62.6404040734798&quot;
## [1] &quot;step =  1  lambda =  0.0672055127397498  loss:  62.039298232797&quot;
## [1] &quot;step =  2  lambda =  0.0672055127397498  loss:  62.1326352488555&quot;
## [1] &quot;step =  1  lambda =  0.0665368067150169  loss:  61.5365606244186&quot;
## [1] &quot;step =  2  lambda =  0.0665368067150169  loss:  61.6289046058423&quot;
## [1] &quot;step =  1  lambda =  0.065874754426403  loss:  61.0378211206846&quot;
## [1] &quot;step =  2  lambda =  0.065874754426403  loss:  61.1291828581752&quot;
## [1] &quot;step =  1  lambda =  0.0652192896681276  loss:  60.543050725271&quot;
## [1] &quot;step =  2  lambda =  0.0652192896681276  loss:  60.6334408286024&quot;
## [1] &quot;step =  1  lambda =  0.0645703468931685  loss:  60.0522205497924&quot;
## [1] &quot;step =  2  lambda =  0.0645703468931685  loss:  60.1416494569327&quot;
## [1] &quot;step =  1  lambda =  0.0639278612067076  loss:  59.5653018217701&quot;
## [1] &quot;step =  2  lambda =  0.0639278612067076  loss:  59.6537798071721&quot;
## [1] &quot;step =  1  lambda =  0.0632917683596407  loss:  59.082265891698&quot;
## [1] &quot;step =  2  lambda =  0.0632917683596407  loss:  59.1698030738532&quot;
## [1] &quot;step =  1  lambda =  0.0626620047421532  loss:  58.6030842393098&quot;
## [1] &quot;step =  2  lambda =  0.0626620047421532  loss:  58.6896905876469&quot;
## [1] &quot;step =  1  lambda =  0.0620385073773583  loss:  58.1277284791343&quot;
## [1] &quot;step =  2  lambda =  0.0620385073773583  loss:  58.2134138203309&quot;
## [1] &quot;step =  1  lambda =  0.0614212139150001  loss:  57.6561703654169&quot;
## [1] &quot;step =  2  lambda =  0.0614212139150001  loss:  57.7409443891864&quot;
## [1] &quot;step =  1  lambda =  0.060810062625218  loss:  57.1883817964716&quot;
## [1] &quot;step =  2  lambda =  0.060810062625218  loss:  57.2722540608793&quot;
## [1] &quot;step =  1  lambda =  0.0602049923923736  loss:  56.7243348185243&quot;
## [1] &quot;step =  2  lambda =  0.0602049923923736  loss:  56.8073147548791&quot;
## [1] &quot;step =  1  lambda =  0.0596059427089393  loss:  56.2640016290991&quot;
## [1] &quot;step =  2  lambda =  0.0596059427089393  loss:  56.3460985464618&quot;
## [1] &quot;step =  1  lambda =  0.0590128536694478  loss:  55.8073545799912&quot;
## [1] &quot;step =  2  lambda =  0.0590128536694478  loss:  55.8885776693375&quot;
## [1] &quot;step =  1  lambda =  0.0584256659645008  loss:  55.3543661798684&quot;
## [1] &quot;step =  2  lambda =  0.0584256659645008  loss:  55.4347245179382&quot;
## [1] &quot;step =  1  lambda =  0.0578443208748385  loss:  54.9050090965373&quot;
## [1] &quot;step =  2  lambda =  0.0578443208748385  loss:  54.9845116493986&quot;
## [1] &quot;step =  1  lambda =  0.0572687602654674  loss:  54.4592561589047&quot;
## [1] &quot;step =  2  lambda =  0.0572687602654674  loss:  54.5379117852599&quot;
## [1] &quot;step =  1  lambda =  0.0566989265798469  loss:  54.0170803586644&quot;
## [1] &quot;step =  2  lambda =  0.0566989265798469  loss:  54.09489781292&quot;
## [1] &quot;step =  1  lambda =  0.0561347628341337  loss:  53.5784548517341&quot;
## [1] &quot;step =  2  lambda =  0.0561347628341337  loss:  53.6554427868563&quot;
## [1] &quot;step =  1  lambda =  0.0555762126114831  loss:  53.1433529594648&quot;
## [1] &quot;step =  2  lambda =  0.0555762126114831  loss:  53.2195199296386&quot;
## [1] &quot;step =  1  lambda =  0.0550232200564073  loss:  52.7117481696456&quot;
## [1] &quot;step =  2  lambda =  0.0550232200564073  loss:  52.7871026327547&quot;
## [1] &quot;step =  1  lambda =  0.0544757298691899  loss:  52.28361413732&quot;
## [1] &quot;step =  2  lambda =  0.0544757298691899  loss:  52.3581644572625&quot;
## [1] &quot;step =  1  lambda =  0.053933687300356  loss:  51.8589246854329&quot;
## [1] &quot;step =  2  lambda =  0.053933687300356  loss:  51.9326791342869&quot;
## [1] &quot;step =  1  lambda =  0.0533970381451971  loss:  51.4376538053211&quot;
## [1] &quot;step =  2  lambda =  0.0533970381451971  loss:  51.5106205653727&quot;
## [1] &quot;step =  1  lambda =  0.0528657287383504  loss:  51.0197756570643&quot;
## [1] &quot;step =  2  lambda =  0.0528657287383504  loss:  51.0919628227086&quot;
## [1] &quot;step =  1  lambda =  0.0523397059484324  loss:  50.6052645697058&quot;
## [1] &quot;step =  2  lambda =  0.0523397059484324  loss:  50.6766801492324&quot;
## [1] &quot;step =  1  lambda =  0.0518189171727258  loss:  50.1940950413578&quot;
## [1] &quot;step =  2  lambda =  0.0518189171727258  loss:  50.2647469586295&quot;
## [1] &quot;step =  1  lambda =  0.0513033103319191  loss:  49.7862417391984&quot;
## [1] &quot;step =  2  lambda =  0.0513033103319191  loss:  49.8561378352317&quot;
## [1] &quot;step =  1  lambda =  0.0507928338648985  loss:  49.3816794993732&quot;
## [1] &quot;step =  2  lambda =  0.0507928338648985  loss:  49.4508275338285&quot;
## [1] &quot;step =  1  lambda =  0.0502874367235919  loss:  48.980383326807&quot;
## [1] &quot;step =  2  lambda =  0.0502874367235919  loss:  49.0487909793964&quot;
## [1] &quot;step =  1  lambda =  0.0497870683678639  loss:  48.5823283949365&quot;
## [1] &quot;step =  2  lambda =  0.0497870683678639  loss:  48.6500032667545&quot;
## [1] &quot;step =  1  lambda =  0.0492916787604622  loss:  48.1874900453685&quot;
## [1] &quot;step =  2  lambda =  0.0492916787604622  loss:  48.2544396601526&quot;
## [1] &quot;step =  1  lambda =  0.048801218362013  loss:  47.7958437874732&quot;
## [1] &quot;step =  2  lambda =  0.048801218362013  loss:  47.8620755927999&quot;
## [1] &quot;step =  1  lambda =  0.0483156381260678  loss:  47.407365297916&quot;
## [1] &quot;step =  2  lambda =  0.0483156381260678  loss:  47.4728866663364&quot;
## [1] &quot;step =  1  lambda =  0.0478348894941984  loss:  47.0220304201351&quot;
## [1] &quot;step =  2  lambda =  0.0478348894941984  loss:  47.0868486502563&quot;
## [1] &quot;step =  1  lambda =  0.0473589243911409  loss:  46.6398151637702&quot;
## [1] &quot;step =  2  lambda =  0.0473589243911409  loss:  46.7039374812846&quot;
## [1] &quot;step =  1  lambda =  0.0468876952199885  loss:  46.2606957040448&quot;
## [1] &quot;step =  2  lambda =  0.0468876952199885  loss:  46.3241292627141&quot;
## [1] &quot;step =  1  lambda =  0.0464211548574313  loss:  45.88464838111&quot;
## [1] &quot;step =  2  lambda =  0.0464211548574313  loss:  45.9474002637046&quot;
## [1] &quot;step =  1  lambda =  0.0459592566490442  loss:  45.5116496993504&quot;
## [1] &quot;step =  2  lambda =  0.0459592566490442  loss:  45.5737269185497&quot;
## [1] &quot;step =  1  lambda =  0.0455019544046216  loss:  45.1416763266579&quot;
## [1] &quot;step =  2  lambda =  0.0455019544046216  loss:  45.2030858259134&quot;
## [1] &quot;step =  1  lambda =  0.0450492023935578  loss:  44.7747050936756&quot;
## [1] &quot;step =  2  lambda =  0.0450492023935578  loss:  44.8354537480404&quot;
## [1] &quot;step =  1  lambda =  0.0446009553402746  loss:  44.4107129930159&quot;
## [1] &quot;step =  2  lambda =  0.0446009553402746  loss:  44.4708076099423&quot;
## [1] &quot;step =  1  lambda =  0.0441571684196929  loss:  44.0496771784547&quot;
## [1] &quot;step =  2  lambda =  0.0441571684196929  loss:  44.1091244985634&quot;
## [1] &quot;step =  1  lambda =  0.0437177972527509  loss:  43.691574964105&quot;
## [1] &quot;step =  2  lambda =  0.0437177972527509  loss:  43.7503816619269&quot;
## [1] &quot;step =  1  lambda =  0.0432827979019659  loss:  43.3363838235713&quot;
## [1] &quot;step =  2  lambda =  0.0432827979019659  loss:  43.3945565082653&quot;
## [1] &quot;step =  1  lambda =  0.0428521268670402  loss:  42.9840813890888&quot;
## [1] &quot;step =  2  lambda =  0.0428521268670402  loss:  43.0416266051361&quot;
## [1] &quot;step =  1  lambda =  0.0424257410805114  loss:  42.6346454506475&quot;
## [1] &quot;step =  2  lambda =  0.0424257410805114  loss:  42.6915696785253&quot;
## [1] &quot;step =  1  lambda =  0.0420035979034456  loss:  42.2880539551042&quot;
## [1] &quot;step =  2  lambda =  0.0420035979034456  loss:  42.3443636119395&quot;
## [1] &quot;step =  1  lambda =  0.0415856551211732  loss:  41.9442850052835&quot;
## [1] &quot;step =  2  lambda =  0.0415856551211732  loss:  41.9999864454895&quot;
## [1] &quot;step =  1  lambda =  0.0411718709390678  loss:  41.60331685907&quot;
## [1] &quot;step =  2  lambda =  0.0411718709390678  loss:  41.6584163749649&quot;
## [1] &quot;step =  1  lambda =  0.0407622039783662  loss:  41.265127928493&quot;
## [1] &quot;step =  2  lambda =  0.0407622039783662  loss:  41.3196317509043&quot;
## [1] &quot;step =  1  lambda =  0.0403566132720311  loss:  40.9296967788039&quot;
## [1] &quot;step =  2  lambda =  0.0403566132720311  loss:  40.9836110776578&quot;
## [1] &quot;step =  1  lambda =  0.0399550582606539  loss:  40.5970021275491&quot;
## [1] &quot;step =  2  lambda =  0.0399550582606539  loss:  40.6503330124472&quot;
## [1] &quot;step =  1  lambda =  0.0395574987883987  loss:  40.2670228436391&quot;
## [1] &quot;step =  2  lambda =  0.0395574987883987  loss:  40.319776364422&quot;
## [1] &quot;step =  1  lambda =  0.0391638950989871  loss:  39.9397379464135&quot;
## [1] &quot;step =  2  lambda =  0.0391638950989871  loss:  39.9919200937142&quot;
## [1] &quot;step =  1  lambda =  0.038774207831722  loss:  39.6151266047047&quot;
## [1] &quot;step =  2  lambda =  0.038774207831722  loss:  39.6667433104908&quot;
## [1] &quot;step =  1  lambda =  0.0383883980175521  loss:  39.2931681358999&quot;
## [1] &quot;step =  2  lambda =  0.0383883980175521  loss:  39.3442252740061&quot;
## [1] &quot;step =  1  lambda =  0.0380064270751743  loss:  38.9738420050026&quot;
## [1] &quot;step =  2  lambda =  0.0380064270751743  loss:  39.0243453916542&quot;
## [1] &quot;step =  1  lambda =  0.0376282568071762  loss:  38.6571278236942&quot;
## [1] &quot;step =  2  lambda =  0.0376282568071762  loss:  38.7070832180223&quot;
## [1] &quot;step =  1  lambda =  0.0372538493962158  loss:  38.3430053493968&quot;
## [1] &quot;step =  2  lambda =  0.0372538493962158  loss:  38.3924184539456&quot;
## [1] &quot;step =  1  lambda =  0.03688316740124  loss:  38.0314544843362&quot;
## [1] &quot;step =  2  lambda =  0.03688316740124  loss:  38.0803309455628&quot;
## [1] &quot;step =  1  lambda =  0.0365161737537404  loss:  37.7224552746084&quot;
## [1] &quot;step =  2  lambda =  0.0365161737537404  loss:  37.7708006833763&quot;
## [1] &quot;step =  1  lambda =  0.0361528317540464  loss:  37.4159879092468&quot;
## [1] &quot;step =  2  lambda =  0.0361528317540464  loss:  37.4638078013125&quot;
## [1] &quot;step =  1  lambda =  0.0357931050676553  loss:  37.1120327192937&quot;
## [1] &quot;step =  2  lambda =  0.0357931050676553  loss:  37.1593325757876&quot;
## [1] &quot;step =  1  lambda =  0.0354369577215986  loss:  36.8105701768734&quot;
## [1] &quot;step =  2  lambda =  0.0354369577215986  loss:  36.8573554247753&quot;
## [1] &quot;step =  1  lambda =  0.035084354100845  loss:  36.5115808942702&quot;
## [1] &quot;step =  2  lambda =  0.035084354100845  loss:  36.5578569068797&quot;
## [1] &quot;step =  1  lambda =  0.0347352589447386  loss:  36.2150456230094&quot;
## [1] &quot;step =  2  lambda =  0.0347352589447386  loss:  36.2608177204105&quot;
## [1] &quot;step =  1  lambda =  0.0343896373434727  loss:  35.9209452529425&quot;
## [1] &quot;step =  2  lambda =  0.0343896373434727  loss:  35.9662187024647&quot;
## [1] &quot;step =  1  lambda =  0.0340474547345993  loss:  35.6292608113369&quot;
## [1] &quot;step =  2  lambda =  0.0340474547345993  loss:  35.6740408280116&quot;
## [1] &quot;step =  1  lambda =  0.0337086768995724  loss:  35.3399734619701&quot;
## [1] &quot;step =  2  lambda =  0.0337086768995724  loss:  35.3842652089825&quot;
## [1] &quot;step =  1  lambda =  0.0333732699603261  loss:  35.0530645042286&quot;
## [1] &quot;step =  2  lambda =  0.0333732699603261  loss:  35.0968730933662&quot;
## [1] &quot;step =  1  lambda =  0.0330412003758869  loss:  34.7685153722116&quot;
## [1] &quot;step =  2  lambda =  0.0330412003758869  loss:  34.8118458643089&quot;
## [1] &quot;step =  1  lambda =  0.0327124349390198  loss:  34.4863076338401&quot;
## [1] &quot;step =  2  lambda =  0.0327124349390198  loss:  34.5291650392192&quot;
## [1] &quot;step =  1  lambda =  0.0323869407729071  loss:  34.2064229899708&quot;
## [1] &quot;step =  2  lambda =  0.0323869407729071  loss:  34.2488122688798&quot;
## [1] &quot;step =  1  lambda =  0.0320646853278608  loss:  33.9288432735156&quot;
## [1] &quot;step =  2  lambda =  0.0320646853278608  loss:  33.9707693365623&quot;
## [1] &quot;step =  1  lambda =  0.0317456363780679  loss:  33.6535504485659&quot;
## [1] &quot;step =  2  lambda =  0.0317456363780679  loss:  33.6950181571501&quot;
## [1] &quot;step =  1  lambda =  0.0314297620183677  loss:  33.3805266095237&quot;
## [1] &quot;step =  2  lambda =  0.0314297620183677  loss:  33.4215407762648&quot;
## [1] &quot;step =  1  lambda =  0.0311170306610609  loss:  33.1097539802365&quot;
## [1] &quot;step =  2  lambda =  0.0311170306610609  loss:  33.1503193693994&quot;
## [1] &quot;step =  1  lambda =  0.0308074110327511  loss:  32.8412149131387&quot;
## [1] &quot;step =  2  lambda =  0.0308074110327511  loss:  32.8813362410568&quot;
## [1] &quot;step =  1  lambda =  0.0305008721712175  loss:  32.5748918883991&quot;
## [1] &quot;step =  2  lambda =  0.0305008721712175  loss:  32.6145738238937&quot;
## [1] &quot;step =  1  lambda =  0.0301973834223185  loss:  32.3107675130728&quot;
## [1] &quot;step =  2  lambda =  0.0301973834223185  loss:  32.3500146778711&quot;
## [1] &quot;step =  1  lambda =  0.0298969144369263  loss:  32.0488245202599&quot;
## [1] &quot;step =  2  lambda =  0.0298969144369263  loss:  32.0876414894095&quot;
## [1] &quot;step =  1  lambda =  0.029599435167892  loss:  31.7890457682691&quot;
## [1] &quot;step =  2  lambda =  0.029599435167892  loss:  31.8274370705504&quot;
## [1] &quot;step =  1  lambda =  0.0293049158670407  loss:  31.5314142397871&quot;
## [1] &quot;step =  2  lambda =  0.0293049158670407  loss:  31.5693843581232&quot;
## [1] &quot;step =  1  lambda =  0.0290133270821971  loss:  31.2759130410542&quot;
## [1] &quot;step =  2  lambda =  0.0290133270821971  loss:  31.3134664129183&quot;
## [1] &quot;step =  1  lambda =  0.0287246396542394  loss:  31.022525401045&quot;
## [1] &quot;step =  2  lambda =  0.0287246396542394  loss:  31.0596664188652&quot;
## [1] &quot;step =  1  lambda =  0.0284388247141845  loss:  30.7712346706547&quot;
## [1] &quot;step =  2  lambda =  0.0284388247141845  loss:  30.8079676822168&quot;
## [1] &quot;step =  1  lambda =  0.0281558536803001  loss:  30.5220243218914&quot;
## [1] &quot;step =  2  lambda =  0.0281558536803001  loss:  30.5583536307393&quot;
## [1] &quot;step =  1  lambda =  0.027875698255247  loss:  30.2748779470735&quot;
## [1] &quot;step =  2  lambda =  0.027875698255247  loss:  30.310807812907&quot;
## [1] &quot;step =  1  lambda =  0.0275983304232493  loss:  30.0297792580332&quot;
## [1] &quot;step =  2  lambda =  0.0275983304232493  loss:  30.0653138971038&quot;
## [1] &quot;step =  1  lambda =  0.0273237224472926  loss:  29.7867120853249&quot;
## [1] &quot;step =  2  lambda =  0.0273237224472926  loss:  29.821855670829&quot;
## [1] &quot;step =  1  lambda =  0.0270518468663504  loss:  29.5456603774397&quot;
## [1] &quot;step =  2  lambda =  0.0270518468663504  loss:  29.5804170399097&quot;
## [1] &quot;step =  1  lambda =  0.0267826764926382  loss:  29.3066082000245&quot;
## [1] &quot;step =  2  lambda =  0.0267826764926382  loss:  29.3409820277178&quot;
## [1] &quot;step =  1  lambda =  0.0265161844088942  loss:  29.0695397351074&quot;
## [1] &quot;step =  2  lambda =  0.0265161844088942  loss:  29.1035347743928&quot;
## [1] &quot;step =  1  lambda =  0.026252343965688  loss:  28.8344392803277&quot;
## [1] &quot;step =  2  lambda =  0.026252343965688  loss:  28.8680595360696&quot;
## [1] &quot;step =  1  lambda =  0.0259911287787554  loss:  28.6012912481717&quot;
## [1] &quot;step =  2  lambda =  0.0259911287787554  loss:  28.6345406841119&quot;
## [1] &quot;step =  1  lambda =  0.0257325127263599  loss:  28.3700801652132&quot;
## [1] &quot;step =  2  lambda =  0.0257325127263599  loss:  28.4029627043509&quot;
## [1] &quot;step =  1  lambda =  0.025476469946681  loss:  28.1407906713597&quot;
## [1] &quot;step =  2  lambda =  0.025476469946681  loss:  28.1733101963288&quot;
## [1] &quot;step =  1  lambda =  0.0252229748352272  loss:  27.9134075191034&quot;
## [1] &quot;step =  2  lambda =  0.0252229748352272  loss:  27.9455678725475&quot;
## [1] &quot;step =  1  lambda =  0.0249720020422762  loss:  27.6879155727774&quot;
## [1] &quot;step =  2  lambda =  0.0249720020422762  loss:  27.7197205577227&quot;
## [1] &quot;step =  1  lambda =  0.0247235264703394  loss:  27.4642998078167&quot;
## [1] &quot;step =  2  lambda =  0.0247235264703394  loss:  27.4957531880424&quot;
## [1] &quot;step =  1  lambda =  0.0244775232716527  loss:  27.2425453100246&quot;
## [1] &quot;step =  2  lambda =  0.0244775232716527  loss:  27.2736508104313&quot;
## [1] &quot;step =  1  lambda =  0.0242339678456911  loss:  27.0226372748436&quot;
## [1] &quot;step =  2  lambda =  0.0242339678456911  loss:  27.0533985818189&quot;
## [1] &quot;step =  1  lambda =  0.0239928358367092  loss:  26.8045610066314&quot;
## [1] &quot;step =  2  lambda =  0.0239928358367092  loss:  26.8349817684136&quot;
## [1] &quot;step =  1  lambda =  0.023754103131305  loss:  26.5883019179411&quot;
## [1] &quot;step =  2  lambda =  0.023754103131305  loss:  26.6183857449805&quot;
## [1] &quot;step =  1  lambda =  0.0235177458560091  loss:  26.3738455288076&quot;
## [1] &quot;step =  2  lambda =  0.0235177458560091  loss:  26.4035959941251&quot;
## [1] &quot;step =  1  lambda =  0.023283740374897  loss:  26.1611774660366&quot;
## [1] &quot;step =  2  lambda =  0.023283740374897  loss:  26.1905981055806&quot;
## [1] &quot;step =  1  lambda =  0.0230520632872256  loss:  25.9502834625001&quot;
## [1] &quot;step =  2  lambda =  0.0230520632872256  loss:  25.9793777755001&quot;
## [1] &quot;step =  1  lambda =  0.022822691425093  loss:  25.7411493564356&quot;
## [1] &quot;step =  2  lambda =  0.022822691425093  loss:  25.7699208057543&quot;
## [1] &quot;step =  1  lambda =  0.0225956018511219  loss:  25.5337610907498&quot;
## [1] &quot;step =  2  lambda =  0.0225956018511219  loss:  25.5622131032323&quot;
## [1] &quot;step =  1  lambda =  0.0223707718561656  loss:  25.3281047123267&quot;
## [1] &quot;step =  2  lambda =  0.0223707718561656  loss:  25.3562406791477&quot;
## [1] &quot;step =  1  lambda =  0.0221481789570373  loss:  25.1241663713409&quot;
## [1] &quot;step =  2  lambda =  0.0221481789570373  loss:  25.151989648349&quot;
## [1] &quot;step =  1  lambda =  0.0219278008942616  loss:  24.9219323205743&quot;
## [1] &quot;step =  2  lambda =  0.0219278008942616  loss:  24.9494462286342&quot;
## [1] &quot;step =  1  lambda =  0.0217096156298486  loss:  24.7213889147377&quot;
## [1] &quot;step =  2  lambda =  0.0217096156298486  loss:  24.74859674007&quot;
## [1] &quot;step =  1  lambda =  0.0214936013450899  loss:  24.5225226097964&quot;
## [1] &quot;step =  2  lambda =  0.0214936013450899  loss:  24.549427604315&quot;
## [1] &quot;step =  1  lambda =  0.0212797364383772  loss:  24.3253199623001&quot;
## [1] &quot;step =  2  lambda =  0.0212797364383772  loss:  24.3519253439467&quot;
## [1] &quot;step =  1  lambda =  0.0210679995230414  loss:  24.1297676287173&quot;
## [1] &quot;step =  2  lambda =  0.0210679995230414  loss:  24.156076581794&quot;
## [1] &quot;step =  1  lambda =  0.0208583694252147  loss:  23.9358523647731&quot;
## [1] &quot;step =  2  lambda =  0.0208583694252147  loss:  23.9618680402725&quot;
## [1] &quot;step =  1  lambda =  0.0206508251817126  loss:  23.7435610247917&quot;
## [1] &quot;step =  2  lambda =  0.0206508251817126  loss:  23.7692865407242&quot;
## [1] &quot;step =  1  lambda =  0.0204453460379377  loss:  23.5528805610429&quot;
## [1] &quot;step =  2  lambda =  0.0204453460379377  loss:  23.5783190027616&quot;
## [1] &quot;step =  1  lambda =  0.0202419114458044  loss:  23.3637980230921&quot;
## [1] &quot;step =  2  lambda =  0.0202419114458044  loss:  23.3889524436158&quot;
## [1] &quot;step =  1  lambda =  0.020040501061684  loss:  23.1763005571548&quot;
## [1] &quot;step =  2  lambda =  0.020040501061684  loss:  23.2011739774877&quot;
## [1] &quot;step =  1  lambda =  0.0198410947443703  loss:  22.990375405455&quot;
## [1] &quot;step =  2  lambda =  0.0198410947443703  loss:  23.0149708149043&quot;
## [1] &quot;step =  1  lambda =  0.0196436725530653  loss:  22.8060099055867&quot;
## [1] &quot;step =  2  lambda =  0.0196436725530653  loss:  22.8303302620781&quot;
## [1] &quot;step =  1  lambda =  0.0194482147453854  loss:  22.6231914898804&quot;
## [1] &quot;step =  2  lambda =  0.0194482147453854  loss:  22.6472397202703&quot;
## [1] &quot;step =  1  lambda =  0.0192547017753869  loss:  22.4419076847722&quot;
## [1] &quot;step =  2  lambda =  0.0192547017753869  loss:  22.4656866851583&quot;
## [1] &quot;step =  1  lambda =  0.0190631142916116  loss:  22.2621461101778&quot;
## [1] &quot;step =  2  lambda =  0.0190631142916116  loss:  22.2856587462065&quot;
## [1] &quot;step =  1  lambda =  0.0188734331351515  loss:  22.0838944788694&quot;
## [1] &quot;step =  2  lambda =  0.0188734331351515  loss:  22.107143586041&quot;
## [1] &quot;step =  1  lambda =  0.0186856393377328  loss:  21.9071405958563&quot;
## [1] &quot;step =  2  lambda =  0.0186856393377328  loss:  21.9301289798277&quot;
## [1] &quot;step =  1  lambda =  0.0184997141198192  loss:  21.7318723577696&quot;
## [1] &quot;step =  2  lambda =  0.0184997141198192  loss:  21.754602794655&quot;
## [1] &quot;step =  1  lambda =  0.0183156388887342  loss:  21.5580777522505&quot;
## [1] &quot;step =  2  lambda =  0.0183156388887342  loss:  21.5805529889183&quot;
## [1] &quot;step =  1  lambda =  0.0181333952368011  loss:  21.3857448573416&quot;
## [1] &quot;step =  2  lambda =  0.0181333952368011  loss:  21.4079676117104&quot;
## [1] &quot;step =  1  lambda =  0.0179529649395029  loss:  21.214861840882&quot;
## [1] &quot;step =  2  lambda =  0.0179529649395029  loss:  21.2368348022131&quot;
## [1] &quot;step =  1  lambda =  0.0177743299536594  loss:  21.0454169599064&quot;
## [1] &quot;step =  2  lambda =  0.0177743299536594  loss:  21.0671427890941&quot;
## [1] &quot;step =  1  lambda =  0.0175974724156234  loss:  20.8773985600471&quot;
## [1] &quot;step =  2  lambda =  0.0175974724156234  loss:  20.8988798899066&quot;
## [1] &quot;step =  1  lambda =  0.0174223746394935  loss:  20.7107950749398&quot;
## [1] &quot;step =  2  lambda =  0.0174223746394935  loss:  20.7320345104926&quot;
## [1] &quot;step =  1  lambda =  0.0172490191153463  loss:  20.5455950256322&quot;
## [1] &quot;step =  2  lambda =  0.0172490191153463  loss:  20.566595144389&quot;
## [1] &quot;step =  1  lambda =  0.0170773885074848  loss:  20.3817870199971&quot;
## [1] &quot;step =  2  lambda =  0.0170773885074848  loss:  20.4025503722381&quot;
## [1] &quot;step =  1  lambda =  0.0169074656527053  loss:  20.2193597521477&quot;
## [1] &quot;step =  2  lambda =  0.0169074656527053  loss:  20.2398888612009&quot;
## [1] &quot;step =  1  lambda =  0.0167392335585806  loss:  20.058302001857&quot;
## [1] &quot;step =  2  lambda =  0.0167392335585806  loss:  20.0785993643738&quot;
## [1] &quot;step =  1  lambda =  0.0165726754017613  loss:  19.8986026339805&quot;
## [1] &quot;step =  2  lambda =  0.0165726754017613  loss:  19.9186707202084&quot;
## [1] &quot;step =  1  lambda =  0.0164077745262926  loss:  19.7402505978816&quot;
## [1] &quot;step =  2  lambda =  0.0164077745262926  loss:  19.7600918519353&quot;
## [1] &quot;step =  1  lambda =  0.0162445144419499  loss:  19.5832349268609&quot;
## [1] &quot;step =  2  lambda =  0.0162445144419499  loss:  19.6028517669903&quot;
## [1] &quot;step =  1  lambda =  0.0160828788225884  loss:  19.4275447375883&quot;
## [1] &quot;step =  2  lambda =  0.0160828788225884  loss:  19.4469395564448&quot;
## [1] &quot;step =  1  lambda =  0.0159228515045117  loss:  19.2731692295386&quot;
## [1] &quot;step =  2  lambda =  0.0159228515045117  loss:  19.2923443944381&quot;
## [1] &quot;step =  1  lambda =  0.0157644164848545  loss:  19.1200976844303&quot;
## [1] &quot;step =  2  lambda =  0.0157644164848545  loss:  19.1390555376146&quot;
## [1] &quot;step =  1  lambda =  0.0156075579199828  loss:  18.9683194656675&quot;
## [1] &quot;step =  2  lambda =  0.0156075579199828  loss:  18.9870623245627&quot;
## [1] &quot;step =  1  lambda =  0.0154522601239095  loss:  18.8178240177846&quot;
## [1] &quot;step =  2  lambda =  0.0154522601239095  loss:  18.8363541752577&quot;
## [1] &quot;step =  1  lambda =  0.0152985075667255  loss:  18.668600865895&quot;
## [1] &quot;step =  2  lambda =  0.0152985075667255  loss:  18.6869205905077&quot;
## [1] &quot;step =  1  lambda =  0.015146284873047  loss:  18.520639615142&quot;
## [1] &quot;step =  2  lambda =  0.015146284873047  loss:  18.5387511514023&quot;
## [1] &quot;step =  1  lambda =  0.0149955768204777  loss:  18.3739299501532&quot;
## [1] &quot;step =  2  lambda =  0.0149955768204777  loss:  18.3918355187646&quot;
## [1] &quot;step =  1  lambda =  0.0148463683380868  loss:  18.2284616344982&quot;
## [1] &quot;step =  2  lambda =  0.0148463683380868  loss:  18.2461634326067&quot;
## [1] &quot;step =  1  lambda =  0.0146986445049018  loss:  18.0842245101489&quot;
## [1] &quot;step =  2  lambda =  0.0146986445049018  loss:  18.1017247115878&quot;
## [1] &quot;step =  1  lambda =  0.0145523905484161  loss:  17.9412084969433&quot;
## [1] &quot;step =  2  lambda =  0.0145523905484161  loss:  17.958509252475&quot;
## [1] &quot;step =  1  lambda =  0.0144075918431123  loss:  17.7994035920516&quot;
## [1] &quot;step =  2  lambda =  0.0144075918431123  loss:  17.8165070296081&quot;
## [1] &quot;step =  1  lambda =  0.0142642339089993  loss:  17.6587998694465&quot;
## [1] &quot;step =  2  lambda =  0.0142642339089993  loss:  17.6757080943669&quot;
## [1] &quot;step =  1  lambda =  0.014122302410164  loss:  17.5193874793752&quot;
## [1] &quot;step =  2  lambda =  0.014122302410164  loss:  17.5361025746411&quot;
## [1] &quot;step =  1  lambda =  0.0139817831533383  loss:  17.3811566478352&quot;
## [1] &quot;step =  2  lambda =  0.0139817831533383  loss:  17.3976806743041&quot;
## [1] &quot;step =  1  lambda =  0.0138426620864795  loss:  17.2440976760527&quot;
## [1] &quot;step =  2  lambda =  0.0138426620864795  loss:  17.2604326726886&quot;
## [1] &quot;step =  1  lambda =  0.0137049252973649  loss:  17.1082009399642&quot;
## [1] &quot;step =  2  lambda =  0.0137049252973649  loss:  17.1243489240668&quot;
## [1] &quot;step =  1  lambda =  0.0135685590122009  loss:  16.9734568897008&quot;
## [1] &quot;step =  2  lambda =  0.0135685590122009  loss:  16.9894198571317&quot;
## [1] &quot;step =  1  lambda =  0.0134335495942453  loss:  16.8398560490755&quot;
## [1] &quot;step =  2  lambda =  0.0134335495942453  loss:  16.8556359744826&quot;
## [1] &quot;step =  1  lambda =  0.0132998835424438  loss:  16.7073890150733&quot;
## [1] &quot;step =  2  lambda =  0.0132998835424438  loss:  16.7229878521128&quot;
## [1] &quot;step =  1  lambda =  0.0131675474900798  loss:  16.5760464573444&quot;
## [1] &quot;step =  2  lambda =  0.0131675474900798  loss:  16.5914661389011&quot;
## [1] &quot;step =  1  lambda =  0.0130365282034377  loss:  16.4458191177002&quot;
## [1] &quot;step =  2  lambda =  0.0130365282034377  loss:  16.461061556105&quot;
## [1] &quot;step =  1  lambda =  0.0129068125804799  loss:  16.316697809612&quot;
## [1] &quot;step =  2  lambda =  0.0129068125804799  loss:  16.3317648968577&quot;
## [1] &quot;step =  1  lambda =  0.0127783876495358  loss:  16.1886734177126&quot;
## [1] &quot;step =  2  lambda =  0.0127783876495358  loss:  16.2035670256673&quot;
## [1] &quot;step =  1  lambda =  0.0126512405680053  loss:  16.061736897301&quot;
## [1] &quot;step =  2  lambda =  0.0126512405680053  loss:  16.0764588779197&quot;
## [1] &quot;step =  1  lambda =  0.0125253586210744  loss:  15.9358792738494&quot;
## [1] &quot;step =  2  lambda =  0.0125253586210744  loss:  15.9504314593832&quot;
## [1] &quot;step =  1  lambda =  0.0124007292204434  loss:  15.8110916425135&quot;
## [1] &quot;step =  2  lambda =  0.0124007292204434  loss:  15.8254758457171&quot;
## [1] &quot;step =  1  lambda =  0.0122773399030684  loss:  15.6873651676455&quot;
## [1] &quot;step =  2  lambda =  0.0122773399030684  loss:  15.701583181982&quot;
## [1] &quot;step =  1  lambda =  0.0121551783299149  loss:  15.5646910823097&quot;
## [1] &quot;step =  2  lambda =  0.0121551783299149  loss:  15.5787446821539&quot;
## [1] &quot;step =  1  lambda =  0.0120342322847238  loss:  15.4430606878006&quot;
## [1] &quot;step =  2  lambda =  0.0120342322847238  loss:  15.4569516286404&quot;
## [1] &quot;step =  1  lambda =  0.0119144896727896  loss:  15.3224653531647&quot;
## [1] &quot;step =  2  lambda =  0.0119144896727896  loss:  15.3361953717998&quot;
## [1] &quot;step =  1  lambda =  0.0117959385197516  loss:  15.2028965147243&quot;
## [1] &quot;step =  2  lambda =  0.0117959385197516  loss:  15.2164673294632&quot;
## [1] &quot;step =  1  lambda =  0.0116785669703954  loss:  15.0843456756039&quot;
## [1] &quot;step =  2  lambda =  0.0116785669703954  loss:  15.0977589864595&quot;
## [1] &quot;step =  1  lambda =  0.0115623632874685  loss:  14.9668044052599&quot;
## [1] &quot;step =  2  lambda =  0.0115623632874685  loss:  14.9800618941423&quot;
## [1] &quot;step =  1  lambda =  0.0114473158505057  loss:  14.8502643390126&quot;
## [1] &quot;step =  2  lambda =  0.0114473158505057  loss:  14.8633676699203&quot;
## [1] &quot;step =  1  lambda =  0.0113334131546674  loss:  14.7347171775814&quot;
## [1] &quot;step =  2  lambda =  0.0113334131546674  loss:  14.7476679967905&quot;
## [1] &quot;step =  1  lambda =  0.0112206438095891  loss:  14.6201546866218&quot;
## [1] &quot;step =  2  lambda =  0.0112206438095891  loss:  14.6329546228733&quot;
## [1] &quot;step =  1  lambda =  0.0111089965382423  loss:  14.506568696266&quot;
## [1] &quot;step =  2  lambda =  0.0111089965382423  loss:  14.5192193609515&quot;
## [1] &quot;step =  1  lambda =  0.0109984601758069  loss:  14.3939511006656&quot;
## [1] &quot;step =  2  lambda =  0.0109984601758069  loss:  14.4064540880106&quot;
## [1] &quot;step =  1  lambda =  0.0108890236685545  loss:  14.2822938575375&quot;
## [1] &quot;step =  2  lambda =  0.0108890236685545  loss:  14.2946507447829&quot;
## [1] &quot;step =  1  lambda =  0.0107806760727431  loss:  14.1715889877116&quot;
## [1] &quot;step =  2  lambda =  0.0107806760727431  loss:  14.1838013352937&quot;
## [1] &quot;step =  1  lambda =  0.0106734065535229  loss:  14.061828574682&quot;
## [1] &quot;step =  2  lambda =  0.0106734065535229  loss:  14.0738979264104&quot;
## [1] &quot;step =  1  lambda =  0.0105672043838527  loss:  13.9530047641605&quot;
## [1] &quot;step =  2  lambda =  0.0105672043838527  loss:  13.9649326473943&quot;
## [1] &quot;step =  1  lambda =  0.0104620589434268  loss:  13.8451097636325&quot;
## [1] &quot;step =  2  lambda =  0.0104620589434268  loss:  13.8568976894544&quot;
## [1] &quot;step =  1  lambda =  0.0103579597176137  loss:  13.7381358419158&quot;
## [1] &quot;step =  2  lambda =  0.0103579597176137  loss:  13.7497853053048&quot;
## [1] &quot;step =  1  lambda =  0.010254896296404  loss:  13.6320753287222&quot;
## [1] &quot;step =  2  lambda =  0.010254896296404  loss:  13.6435878087242&quot;
## [1] &quot;step =  1  lambda =  0.0101528583733698  loss:  13.526920614221&quot;
## [1] &quot;step =  2  lambda =  0.0101528583733698  loss:  13.5382975741179&quot;
## [1] &quot;step =  1  lambda =  0.0100518357446336  loss:  13.4226641486056&quot;
## [1] &quot;step =  2  lambda =  0.0100518357446336  loss:  13.4339070360825&quot;
## [1] &quot;step =  1  lambda =  0.00995181830784842  loss:  13.3192984416627&quot;
## [1] &quot;step =  2  lambda =  0.00995181830784842  loss:  13.3304086889735&quot;
## [1] &quot;step =  1  lambda =  0.00985279606118726  loss:  13.216816062344&quot;
## [1] &quot;step =  2  lambda =  0.00985279606118726  loss:  13.2277950864749&quot;
## [1] &quot;step =  1  lambda =  0.0097547591023429  loss:  13.1152096383401&quot;
## [1] &quot;step =  2  lambda =  0.0097547591023429  loss:  13.1260588411721&quot;
## [1] &quot;step =  1  lambda =  0.00965769762753778  loss:  13.0144718556576&quot;
## [1] &quot;step =  2  lambda =  0.00965769762753778  loss:  13.0251926241265&quot;
## [1] &quot;step =  1  lambda =  0.00956160193054351  loss:  12.9145954581981&quot;
## [1] &quot;step =  2  lambda =  0.00956160193054351  loss:  12.9251891644534&quot;
## [1] &quot;step =  1  lambda =  0.00946646240171032  loss:  12.8155732473399&quot;
## [1] &quot;step =  2  lambda =  0.00946646240171032  loss:  12.8260412489021&quot;
## [1] &quot;step =  1  lambda =  0.00937226952700606  loss:  12.7173980815228&quot;
## [1] &quot;step =  2  lambda =  0.00937226952700606  loss:  12.7277417214385&quot;
## [1] &quot;step =  1  lambda =  0.00927901388706474  loss:  12.6200628758343&quot;
## [1] &quot;step =  2  lambda =  0.00927901388706474  loss:  12.6302834828303&quot;
## [1] &quot;step =  1  lambda =  0.00918668615624467  loss:  12.5235606015995&quot;
## [1] &quot;step =  2  lambda =  0.00918668615624467  loss:  12.533659490235&quot;
## [1] &quot;step =  1  lambda =  0.00909527710169582  loss:  12.4278842859726&quot;
## [1] &quot;step =  2  lambda =  0.00909527710169582  loss:  12.4378627567894&quot;
## [1] &quot;step =  1  lambda =  0.00900477758243656  loss:  12.3330270115309&quot;
## [1] &quot;step =  2  lambda =  0.00900477758243656  loss:  12.3428863512033&quot;
## [1] &quot;step =  1  lambda =  0.00891517854843955  loss:  12.2389819158722&quot;
## [1] &quot;step =  2  lambda =  0.00891517854843955  loss:  12.2487233973538&quot;
## [1] &quot;step =  1  lambda =  0.00882647103972673  loss:  12.1457421912137&quot;
## [1] &quot;step =  2  lambda =  0.00882647103972673  loss:  12.1553670738836&quot;
## [1] &quot;step =  1  lambda =  0.00873864618547329  loss:  12.0533010839934&quot;
## [1] &quot;step =  2  lambda =  0.00873864618547329  loss:  12.0628106138006&quot;
## [1] &quot;step =  1  lambda =  0.00865169520312063  loss:  11.9616518944746&quot;
## [1] &quot;step =  2  lambda =  0.00865169520312063  loss:  11.9710473040811&quot;
## [1] &quot;step =  1  lambda =  0.00856560939749806  loss:  11.8707879763523&quot;
## [1] &quot;step =  2  lambda =  0.00856560939749806  loss:  11.8800704852742&quot;
## [1] &quot;step =  1  lambda =  0.00848038015995327  loss:  11.7807027363621&quot;
## [1] &quot;step =  2  lambda =  0.00848038015995327  loss:  11.7898735511098&quot;
## [1] &quot;step =  1  lambda =  0.00839599896749147  loss:  11.6913896338918&quot;
## [1] &quot;step =  2  lambda =  0.00839599896749147  loss:  11.7004499481083&quot;
## [1] &quot;step =  1  lambda =  0.00831245738192312  loss:  11.6028421805948&quot;
## [1] &quot;step =  2  lambda =  0.00831245738192312  loss:  11.6117931751926&quot;
## [1] &quot;step =  1  lambda =  0.00822974704902003  loss:  11.5150539400065&quot;
## [1] &quot;step =  2  lambda =  0.00822974704902003  loss:  11.5238967833036&quot;
## [1] &quot;step =  1  lambda =  0.00814785969767999  loss:  11.428018527163&quot;
## [1] &quot;step =  2  lambda =  0.00814785969767999  loss:  11.4367543750165&quot;
## [1] &quot;step =  1  lambda =  0.00806678713909961  loss:  11.3417296082216&quot;
## [1] &quot;step =  2  lambda =  0.00806678713909961  loss:  11.3503596041609&quot;
## [1] &quot;step =  1  lambda =  0.0079865212659555  loss:  11.2561809000846&quot;
## [1] &quot;step =  2  lambda =  0.0079865212659555  loss:  11.2647061754423&quot;
## [1] &quot;step =  1  lambda =  0.00790705405159344  loss:  11.1713661700247&quot;
## [1] &quot;step =  2  lambda =  0.00790705405159344  loss:  11.1797878440669&quot;
## [1] &quot;step =  1  lambda =  0.00782837754922577  loss:  11.0872792353129&quot;
## [1] &quot;step =  2  lambda =  0.00782837754922577  loss:  11.0955984153675&quot;
## [1] &quot;step =  1  lambda =  0.00775048389113669  loss:  11.0039139628492&quot;
## [1] &quot;step =  2  lambda =  0.00775048389113669  loss:  11.0121317444332&quot;
## [1] &quot;step =  1  lambda =  0.00767336528789549  loss:  10.9212642687947&quot;
## [1] &quot;step =  2  lambda =  0.00767336528789549  loss:  10.9293817357403&quot;
## [1] &quot;step =  1  lambda =  0.00759701402757757  loss:  10.8393241182073&quot;
## [1] &quot;step =  2  lambda =  0.00759701402757757  loss:  10.8473423427859&quot;
## [1] &quot;step =  1  lambda =  0.00752142247499327  loss:  10.7580875246784&quot;
## [1] &quot;step =  2  lambda =  0.00752142247499327  loss:  10.7660075677243&quot;
## [1] &quot;step =  1  lambda =  0.00744658307092434  loss:  10.6775485499728&quot;
## [1] &quot;step =  2  lambda =  0.00744658307092434  loss:  10.685371461005&quot;
## [1] &quot;step =  1  lambda =  0.00737248833136801  loss:  10.5977013036706&quot;
## [1] &quot;step =  2  lambda =  0.00737248833136801  loss:  10.6054281210132&quot;
## [1] &quot;step =  1  lambda =  0.00729913084678858  loss:  10.5185399428111&quot;
## [1] &quot;step =  2  lambda =  0.00729913084678858  loss:  10.5261716937131&quot;
## [1] &quot;step =  1  lambda =  0.00722650328137646  loss:  10.4400586715396&quot;
## [1] &quot;step =  2  lambda =  0.00722650328137646  loss:  10.4475963722926&quot;
## [1] &quot;step =  1  lambda =  0.00715459837231459  loss:  10.362251740756&quot;
## [1] &quot;step =  2  lambda =  0.00715459837231459  loss:  10.369696396811&quot;
## [1] &quot;step =  1  lambda =  0.00708340892905212  loss:  10.2851134477655&quot;
## [1] &quot;step =  2  lambda =  0.00708340892905212  loss:  10.2924660538489&quot;
## [1] &quot;step =  1  lambda =  0.00701292783258542  loss:  10.2086381359322&quot;
## [1] &quot;step =  2  lambda =  0.00701292783258542  loss:  10.2158996761599&quot;
## [1] &quot;step =  1  lambda =  0.00694314803474611  loss:  10.1328201943343&quot;
## [1] &quot;step =  2  lambda =  0.00694314803474611  loss:  10.1399916423247&quot;
## [1] &quot;step =  1  lambda =  0.00687406255749626  loss:  10.0576540574216&quot;
## [1] &quot;step =  2  lambda =  0.00687406255749626  loss:  10.0647363764079&quot;
## [1] &quot;step =  1  lambda =  0.00680566449223054  loss:  9.98313420467578&quot;
## [1] &quot;step =  2  lambda =  0.00680566449223054  loss:  9.99012834761663&quot;
## [1] &quot;step =  1  lambda =  0.00673794699908547  loss:  9.90925516027226&quot;
## [1] &quot;step =  2  lambda =  0.00673794699908547  loss:  9.91616206996127&quot;
## [1] &quot;step =  1  lambda =  0.00667090330625527  loss:  9.83601149274439&quot;
## [1] &quot;step =  2  lambda =  0.00667090330625527  loss:  9.84283210191861&quot;
## [1] &quot;step =  1  lambda =  0.00660452670931481  loss:  9.76339781465&quot;
## [1] &quot;step =  2  lambda =  0.00660452670931481  loss:  9.77013304609726&quot;
## [1] &quot;step =  1  lambda =  0.00653881057054906  loss:  9.69140878224004&quot;
## [1] &quot;step =  2  lambda =  0.00653881057054906  loss:  9.69805954890504&quot;
## [1] &quot;step =  1  lambda =  0.0064737483182894  loss:  9.62003909512937&quot;
## [1] &quot;step =  2  lambda =  0.0064737483182894  loss:  9.6266063002187&quot;
## [1] &quot;step =  1  lambda =  0.00640933344625638  loss:  9.54928349596968&quot;
## [1] &quot;step =  2  lambda =  0.00640933344625638  loss:  9.55576803305571&quot;
## [1] &quot;step =  1  lambda =  0.00634555951290912  loss:  9.47913677012456&quot;
## [1] &quot;step =  2  lambda =  0.00634555951290912  loss:  9.48553952324831&quot;
## [1] &quot;step =  1  lambda =  0.00628242014080112  loss:  9.40959374534685&quot;
## [1] &quot;step =  2  lambda =  0.00628242014080112  loss:  9.41591558911959&quot;
## [1] &quot;step =  1  lambda =  0.00621990901594257  loss:  9.3406492914579&quot;
## [1] &quot;step =  2  lambda =  0.00621990901594257  loss:  9.34689109116185&quot;
## [1] &quot;step =  1  lambda =  0.0061580198871689  loss:  9.27229832002907&quot;
## [1] &quot;step =  2  lambda =  0.0061580198871689  loss:  9.27846093171692&quot;
## [1] &quot;step =  1  lambda =  0.00609674656551564  loss:  9.20453578406533&quot;
## [1] &quot;step =  2  lambda =  0.00609674656551564  loss:  9.21062005465873&quot;
## [1] &quot;step =  1  lambda =  0.00603608292359956  loss:  9.13735667769092&quot;
## [1] &quot;step =  2  lambda =  0.00603608292359956  loss:  9.14336344507795&quot;
## [1] &quot;step =  1  lambda =  0.00597602289500594  loss:  9.07075603583713&quot;
## [1] &quot;step =  2  lambda =  0.00597602289500594  loss:  9.07668612896868&quot;
## [1] &quot;step =  1  lambda =  0.00591656047368186  loss:  9.00472893393216&quot;
## [1] &quot;step =  2  lambda =  0.00591656047368186  loss:  9.01058317291734&quot;
## [1] &quot;step =  1  lambda =  0.00585768971333562  loss:  8.93927048759299&quot;
## [1] &quot;step =  2  lambda =  0.00585768971333562  loss:  8.94504968379349&quot;
## [1] &quot;step =  1  lambda =  0.00579940472684215  loss:  8.87437585231939&quot;
## [1] &quot;step =  2  lambda =  0.00579940472684215  loss:  8.88008080844286&quot;
## [1] &quot;step =  1  lambda =  0.0057416996856542  loss:  8.81004022318992&quot;
## [1] &quot;step =  2  lambda =  0.0057416996856542  loss:  8.8156717333823&quot;
## [1] &quot;step =  1  lambda =  0.0056845688192196  loss:  8.74625883456003&quot;
## [1] &quot;step =  2  lambda =  0.0056845688192196  loss:  8.75181768449698&quot;
## [1] &quot;step =  1  lambda =  0.00562800641440407  loss:  8.68302695976203&quot;
## [1] &quot;step =  2  lambda =  0.00562800641440407  loss:  8.68851392673934&quot;
## [1] &quot;step =  1  lambda =  0.00557200681492  loss:  8.62033991080733&quot;
## [1] &quot;step =  2  lambda =  0.00557200681492  loss:  8.62575576383036&quot;
## [1] &quot;step =  1  lambda =  0.00551656442076077  loss:  8.55819303809048&quot;
## [1] &quot;step =  2  lambda =  0.00551656442076077  loss:  8.56353853796263&quot;
## [1] &quot;step =  1  lambda =  0.00546167368764078  loss:  8.49658173009525&quot;
## [1] &quot;step =  2  lambda =  0.00546167368764078  loss:  8.50185762950561&quot;
## [1] &quot;step =  1  lambda =  0.00540732912644096  loss:  8.43550141310283&quot;
## [1] &quot;step =  2  lambda =  0.00540732912644096  loss:  8.4407084567127&quot;
## [1] &quot;step =  1  lambda =  0.00535352530265991  loss:  8.37494755090179&quot;
## [1] &quot;step =  2  lambda =  0.00535352530265991  loss:  8.38008647543045&quot;
## [1] &quot;step =  1  lambda =  0.0053002568358704  loss:  8.31491564450018&quot;
## [1] &quot;step =  2  lambda =  0.0053002568358704  loss:  8.31998717880965&quot;
## [1] &quot;step =  1  lambda =  0.00524751839918138  loss:  8.25540123183958&quot;
## [1] &quot;step =  2  lambda =  0.00524751839918138  loss:  8.26040609701852&quot;
## [1] &quot;step =  1  lambda =  0.00519530471870523  loss:  8.19639988751095&quot;
## [1] &quot;step =  2  lambda =  0.00519530471870523  loss:  8.20133879695765&quot;
## [1] &quot;step =  1  lambda =  0.00514361057303038  loss:  8.13790722247262&quot;
## [1] &quot;step =  2  lambda =  0.00514361057303038  loss:  8.14278088197708&quot;
## [1] &quot;step =  1  lambda =  0.00509243079269919  loss:  8.07991888377003&quot;
## [1] &quot;step =  2  lambda =  0.00509243079269919  loss:  8.08472799159525&quot;
## [1] &quot;step =  1  lambda =  0.00504176025969098  loss:  8.02243055425753&quot;
## [1] &quot;step =  2  lambda =  0.00504176025969098  loss:  8.02717580121983&quot;
## [1] &quot;step =  1  lambda =  0.00499159390691022  loss:  7.96543795232202&quot;
## [1] &quot;step =  2  lambda =  0.00499159390691022  loss:  7.97012002187055&quot;
## [1] &quot;step =  1  lambda =  0.00494192671767982  loss:  7.90893683160846&quot;
## [1] &quot;step =  2  lambda =  0.00494192671767982  loss:  7.91355639990391&quot;
## [1] &quot;step =  1  lambda =  0.00489275372523948  loss:  7.85292298074734&quot;
## [1] &quot;step =  2  lambda =  0.00489275372523948  loss:  7.85748071673971&quot;
## [1] &quot;step =  1  lambda =  0.00484407001224897  loss:  7.79739222308403&quot;
## [1] &quot;step =  2  lambda =  0.00484407001224897  loss:  7.80188878858967&quot;
## [1] &quot;step =  1  lambda =  0.00479587071029642  loss:  7.7423404164099&quot;
## [1] &quot;step =  2  lambda =  0.00479587071029642  loss:  7.74677646618766&quot;
## [1] &quot;step =  1  lambda =  0.00474815099941148  loss:  7.68776345269541&quot;
## [1] &quot;step =  2  lambda =  0.00474815099941148  loss:  7.69213963452199&quot;
## [1] &quot;step =  1  lambda =  0.00470090610758328  loss:  7.63365725782499&quot;
## [1] &quot;step =  2  lambda =  0.00470090610758328  loss:  7.63797421256954&quot;
## [1] &quot;step =  1  lambda =  0.00465413131028327  loss:  7.58001779133384&quot;
## [1] &quot;step =  2  lambda =  0.00465413131028327  loss:  7.58427615303167&quot;
## [1] &quot;step =  1  lambda =  0.00460782192999275  loss:  7.52684104614638&quot;
## [1] &quot;step =  2  lambda =  0.00460782192999275  loss:  7.531041442072&quot;
## [1] &quot;step =  1  lambda =  0.0045619733357351  loss:  7.47412304831675&quot;
## [1] &quot;step =  2  lambda =  0.0045619733357351  loss:  7.47826609905603&quot;
## [1] &quot;step =  1  lambda =  0.00451658094261267  loss:  7.42185985677097&quot;
## [1] &quot;step =  2  lambda =  0.00451658094261267  loss:  7.42594617629256&quot;
## [1] &quot;step =  1  lambda =  0.00447164021134833  loss:  7.37004756305093&quot;
## [1] &quot;step =  2  lambda =  0.00447164021134833  loss:  7.37407775877695&quot;
## [1] &quot;step =  1  lambda =  0.00442714664783151  loss:  7.31868229106018&quot;
## [1] &quot;step =  2  lambda =  0.00442714664783151  loss:  7.32265696393616&quot;
## [1] &quot;step =  1  lambda =  0.00438309580266878  loss:  7.26776019681157&quot;
## [1] &quot;step =  2  lambda =  0.00438309580266878  loss:  7.27167994137554&quot;
## [1] &quot;step =  1  lambda =  0.0043394832707389  loss:  7.21727746817648&quot;
## [1] &quot;step =  2  lambda =  0.0043394832707389  loss:  7.2211428726275&quot;
## [1] &quot;step =  1  lambda =  0.00429630469075234  loss:  7.16723032463603&quot;
## [1] &quot;step =  2  lambda =  0.00429630469075234  loss:  7.17104197090181&quot;
## [1] &quot;step =  1  lambda =  0.00425355574481513  loss:  7.11761501703391&quot;
## [1] &quot;step =  2  lambda =  0.00425355574481513  loss:  7.12137348083782&quot;
## [1] &quot;step =  1  lambda =  0.00421123215799704  loss:  7.06842782733095&quot;
## [1] &quot;step =  2  lambda =  0.00421123215799704  loss:  7.0721336782583&quot;
## [1] &quot;step =  1  lambda =  0.00416932969790412  loss:  7.01966506836148&quot;
## [1] &quot;step =  2  lambda =  0.00416932969790412  loss:  7.02331886992506&quot;
## [1] &quot;step =  1  lambda =  0.00412784417425544  loss:  6.97132308359139&quot;
## [1] &quot;step =  2  lambda =  0.00412784417425544  loss:  6.97492539329633&quot;
## [1] &quot;step =  1  lambda =  0.00408677143846407  loss:  6.92339824687792&quot;
## [1] &quot;step =  2  lambda =  0.00408677143846407  loss:  6.92694961628586&quot;
## [1] &quot;step =  1  lambda =  0.0040461073832222  loss:  6.87588696223109&quot;
## [1] &quot;step =  2  lambda =  0.0040461073832222  loss:  6.87938793702365&quot;
## [1] &quot;step =  1  lambda =  0.00400584794209042  loss:  6.82878566357688&quot;
## [1] &quot;step =  2  lambda =  0.00400584794209042  loss:  6.83223678361849&quot;
## [1] &quot;step =  1  lambda =  0.00396598908909106  loss:  6.78209081452211&quot;
## [1] &quot;step =  2  lambda =  0.00396598908909106  loss:  6.78549261392213&quot;
## [1] &quot;step =  1  lambda =  0.00392652683830562  loss:  6.73579890812095&quot;
## [1] &quot;step =  2  lambda =  0.00392652683830562  loss:  6.73915191529515&quot;
## [1] &quot;step =  1  lambda =  0.00388745724347613  loss:  6.6899064666431&quot;
## [1] &quot;step =  2  lambda =  0.00388745724347613  loss:  6.69321120437446&quot;
## [1] &quot;step =  1  lambda =  0.00384877639761054  loss:  6.64441004134368&quot;
## [1] &quot;step =  2  lambda =  0.00384877639761054  loss:  6.64766702684257&quot;
## [1] &quot;step =  1  lambda =  0.00381048043259204  loss:  6.59930621223464&quot;
## [1] &quot;step =  2  lambda =  0.00381048043259204  loss:  6.60251595719837&quot;
## [1] &quot;step =  1  lambda =  0.00377256551879221  loss:  6.55459158785802&quot;
## [1] &quot;step =  2  lambda =  0.00377256551879221  loss:  6.55775459852966&quot;
## [1] &quot;step =  1  lambda =  0.00373502786468807  loss:  6.51026280506057&quot;
## [1] &quot;step =  2  lambda =  0.00373502786468807  loss:  6.51337958228728&quot;
## [1] &quot;step =  1  lambda =  0.00369786371648293  loss:  6.46631652877022&quot;
## [1] &quot;step =  2  lambda =  0.00369786371648293  loss:  6.46938756806088&quot;
## [1] &quot;step =  1  lambda =  0.00366106935773101  loss:  6.42274945177401&quot;
## [1] &quot;step =  2  lambda =  0.00366106935773101  loss:  6.42577524335623&quot;
## [1] &quot;step =  1  lambda =  0.00362464110896576  loss:  6.37955829449772&quot;
## [1] &quot;step =  2  lambda =  0.00362464110896576  loss:  6.38253932337424&quot;
## [1] &quot;step =  1  lambda =  0.00358857532733195  loss:  6.33673980478698&quot;
## [1] &quot;step =  2  lambda =  0.00358857532733195  loss:  6.33967655079154&quot;
## [1] &quot;step =  1  lambda =  0.00355286840622136  loss:  6.29429075769009&quot;
## [1] &quot;step =  2  lambda =  0.00355286840622136  loss:  6.29718369554258&quot;
## [1] &quot;step =  1  lambda =  0.00351751677491213  loss:  6.25220795524226&quot;
## [1] &quot;step =  2  lambda =  0.00351751677491213  loss:  6.25505755460341&quot;
## [1] &quot;step =  1  lambda =  0.00348251689821166  loss:  6.21048822625158&quot;
## [1] &quot;step =  2  lambda =  0.00348251689821166  loss:  6.21329495177698&quot;
## [1] &quot;step =  1  lambda =  0.00344786527610313  loss:  6.16912842608638&quot;
## [1] &quot;step =  2  lambda =  0.00344786527610313  loss:  6.17189273747995&quot;
## [1] &quot;step =  1  lambda =  0.00341355844339543  loss:  6.12812543646424&quot;
## [1] &quot;step =  2  lambda =  0.00341355844339543  loss:  6.13084778853114&quot;
## [1] &quot;step =  1  lambda =  0.00337959296937672  loss:  6.0874761652425&quot;
## [1] &quot;step =  2  lambda =  0.00337959296937672  loss:  6.09015700794145&quot;
## [1] &quot;step =  1  lambda =  0.00334596545747127  loss:  6.04717754621027&quot;
## [1] &quot;step =  2  lambda =  0.00334596545747127  loss:  6.04981732470535&quot;
## [1] &quot;step =  1  lambda =  0.00331267254489989  loss:  6.00722653888204&quot;
## [1] &quot;step =  2  lambda =  0.00331267254489989  loss:  6.00982569359387&quot;
## [1] &quot;step =  1  lambda =  0.00327971090234357  loss:  5.96762012829263&quot;
## [1] &quot;step =  2  lambda =  0.00327971090234357  loss:  5.97017909494912&quot;
## [1] &quot;step =  1  lambda =  0.00324707723361059  loss:  5.92835532479384&quot;
## [1] &quot;step =  2  lambda =  0.00324707723361059  loss:  5.93087453448025&quot;
## [1] &quot;step =  1  lambda =  0.00321476827530687  loss:  5.88942916385242&quot;
## [1] &quot;step =  2  lambda =  0.00321476827530687  loss:  5.89190904306103&quot;
## [1] &quot;step =  1  lambda =  0.00318278079650967  loss:  5.85083870584964&quot;
## [1] &quot;step =  2  lambda =  0.00318278079650967  loss:  5.85327967652875&quot;
## [1] &quot;step =  1  lambda =  0.00315111159844444  loss:  5.81258103588222&quot;
## [1] &quot;step =  2  lambda =  0.00315111159844444  loss:  5.81498351548476&quot;
## [1] &quot;step =  1  lambda =  0.00311975751416499  loss:  5.7746532635648&quot;
## [1] &quot;step =  2  lambda =  0.00311975751416499  loss:  5.77701766509633&quot;
## [1] &quot;step =  1  lambda =  0.00308871540823677  loss:  5.73705252283385&quot;
## [1] &quot;step =  2  lambda =  0.00308871540823677  loss:  5.73937925490009&quot;
## [1] &quot;step =  1  lambda =  0.00305798217642331  loss:  5.69977597175298&quot;
## [1] &quot;step =  2  lambda =  0.00305798217642331  loss:  5.70206543860681&quot;
## [1] &quot;step =  1  lambda =  0.00302755474537582  loss:  5.66282079231975&quot;
## [1] &quot;step =  2  lambda =  0.00302755474537582  loss:  5.66507339390771&quot;
## [1] &quot;step =  1  lambda =  0.00299743007232583  loss:  5.6261841902738&quot;
## [1] &quot;step =  2  lambda =  0.00299743007232583  loss:  5.62840032228212&quot;
## [1] &quot;step =  1  lambda =  0.00296760514478094  loss:  5.58986339490653&quot;
## [1] &quot;step =  2  lambda =  0.00296760514478094  loss:  5.59204344880666&quot;
## [1] &quot;step =  1  lambda =  0.00293807698022355  loss:  5.5538556588721&quot;
## [1] &quot;step =  2  lambda =  0.00293807698022355  loss:  5.5560000219657&quot;
## [1] &quot;step =  1  lambda =  0.00290884262581258  loss:  5.51815825799979&quot;
## [1] &quot;step =  2  lambda =  0.00290884262581258  loss:  5.52026731346333&quot;
## [1] &quot;step =  1  lambda =  0.00287989915808824  loss:  5.48276849110789&quot;
## [1] &quot;step =  2  lambda =  0.00287989915808824  loss:  5.4848426180367&quot;
## [1] &quot;step =  1  lambda =  0.00285124368267963  loss:  5.44768367981881&quot;
## [1] &quot;step =  2  lambda =  0.00285124368267963  loss:  5.44972325327072&quot;
## [1] &quot;step =  1  lambda =  0.00282287333401534  loss:  5.4129011683757&quot;
## [1] &quot;step =  2  lambda =  0.00282287333401534  loss:  5.41490655941413&quot;
## [1] &quot;step =  1  lambda =  0.00279478527503684  loss:  5.37841832346031&quot;
## [1] &quot;step =  2  lambda =  0.00279478527503684  loss:  5.380389899197&quot;
## [1] &quot;step =  1  lambda =  0.00276697669691485  loss:  5.34423253401231&quot;
## [1] &quot;step =  2  lambda =  0.00276697669691485  loss:  5.34617065764952&quot;
## [1] &quot;step =  1  lambda =  0.00273944481876837  loss:  5.31034121104987&quot;
## [1] &quot;step =  2  lambda =  0.00273944481876837  loss:  5.31224624192216&quot;
## [1] &quot;step =  1  lambda =  0.00271218688738664  loss:  5.27674178749165&quot;
## [1] &quot;step =  2  lambda =  0.00271218688738664  loss:  5.27861408110722&quot;
## [1] &quot;step =  1  lambda =  0.00268520017695382  loss:  5.24343171798007&quot;
## [1] &quot;step =  2  lambda =  0.00268520017695382  loss:  5.24527162606161&quot;
## [1] &quot;step =  1  lambda =  0.00265848198877637  loss:  5.2104084787059&quot;
## [1] &quot;step =  2  lambda =  0.00265848198877637  loss:  5.21221634923108&quot;
## [1] &quot;step =  1  lambda =  0.0026320296510132  loss:  5.17766956723419&quot;
## [1] &quot;step =  2  lambda =  0.0026320296510132  loss:  5.17944574447567&quot;
## [1] &quot;step =  1  lambda =  0.0026058405184085  loss:  5.14521250233149&quot;
## [1] &quot;step =  2  lambda =  0.0026058405184085  loss:  5.1469573268965&quot;
## [1] &quot;step =  1  lambda =  0.00257991197202718  loss:  5.11303482379436&quot;
## [1] &quot;step =  2  lambda =  0.00257991197202718  loss:  5.11474863266388&quot;
## [1] &quot;step =  1  lambda =  0.002554241418993  loss:  5.08113409227918&quot;
## [1] &quot;step =  2  lambda =  0.002554241418993  loss:  5.08281721884667&quot;
## [1] &quot;step =  1  lambda =  0.00252882629222926  loss:  5.04950788913318&quot;
## [1] &quot;step =  2  lambda =  0.00252882629222926  loss:  5.05116066324296&quot;
## [1] &quot;step =  1  lambda =  0.0025036640502021  loss:  5.01815381622687&quot;
## [1] &quot;step =  2  lambda =  0.0025036640502021  loss:  5.01977656421199&quot;
## [1] &quot;step =  1  lambda =  0.00247875217666636  loss:  4.98706949578757&quot;
## [1] &quot;step =  2  lambda =  0.00247875217666636  loss:  4.98866254050736&quot;
## [1] &quot;step =  1  lambda =  0.00245408818041392  loss:  4.95625257023431&quot;
## [1] &quot;step =  2  lambda =  0.00245408818041392  loss:  4.95781623111152&quot;
## [1] &quot;step =  1  lambda =  0.0024296695950246  loss:  4.92570070201394&quot;
## [1] &quot;step =  2  lambda =  0.0024296695950246  loss:  4.92723529507142&quot;
## [1] &quot;step =  1  lambda =  0.00240549397861951  loss:  4.89541157343844&quot;
## [1] &quot;step =  2  lambda =  0.00240549397861951  loss:  4.89691741133551&quot;
## [1] &quot;step =  1  lambda =  0.00238155891361687  loss:  4.86538288652353&quot;
## [1] &quot;step =  2  lambda =  0.00238155891361687  loss:  4.8668602785919&quot;
## [1] &quot;step =  1  lambda =  0.00235786200649023  loss:  4.83561236282846&quot;
## [1] &quot;step =  2  lambda =  0.00235786200649023  loss:  4.83706161510781&quot;
## [1] &quot;step =  1  lambda =  0.00233440088752913  loss:  4.80609774329696&quot;
## [1] &quot;step =  2  lambda =  0.00233440088752913  loss:  4.80751915857011&quot;
## [1] &quot;step =  1  lambda =  0.00231117321060213  loss:  4.77683678809953&quot;
## [1] &quot;step =  2  lambda =  0.00231117321060213  loss:  4.77823066592722&quot;
## [1] &quot;step =  1  lambda =  0.00228817665292217  loss:  4.74782727647678&quot;
## [1] &quot;step =  2  lambda =  0.00228817665292217  loss:  4.74919391323215&quot;
## [1] &quot;step =  1  lambda =  0.00226540891481432  loss:  4.71906700658405&quot;
## [1] &quot;step =  2  lambda =  0.00226540891481432  loss:  4.72040669548668&quot;
## [1] &quot;step =  1  lambda =  0.0022428677194858  loss:  4.69055379533723&quot;
## [1] &quot;step =  2  lambda =  0.0022428677194858  loss:  4.69186682648682&quot;
## [1] &quot;step =  1  lambda =  0.0022205508127983  loss:  4.66228547825959&quot;
## [1] &quot;step =  2  lambda =  0.0022205508127983  loss:  4.66357213866935&quot;
## [1] &quot;step =  1  lambda =  0.00219845596304253  loss:  4.63425990933001&quot;
## [1] &quot;step =  2  lambda =  0.00219845596304253  loss:  4.6355204829596&quot;
## [1] &quot;step =  1  lambda =  0.00217658096071513  loss:  4.60647496083222&quot;
## [1] &quot;step =  2  lambda =  0.00217658096071513  loss:  4.60770972862037&quot;
## [1] &quot;step =  1  lambda =  0.00215492361829761  loss:  4.57892852320515&quot;
## [1] &quot;step =  2  lambda =  0.00215492361829761  loss:  4.58013776310203&quot;
## [1] &quot;step =  1  lambda =  0.00213348177003771  loss:  4.5516185048946&quot;
## [1] &quot;step =  2  lambda =  0.00213348177003771  loss:  4.55280249189365&quot;
## [1] &quot;step =  1  lambda =  0.00211225327173271  loss:  4.52454283220587&quot;
## [1] &quot;step =  2  lambda =  0.00211225327173271  loss:  4.52570183837545&quot;
## [1] &quot;step =  1  lambda =  0.00209123600051511  loss:  4.49769944915756&quot;
## [1] &quot;step =  2  lambda =  0.00209123600051511  loss:  4.49883374367223&quot;
## [1] &quot;step =  1  lambda =  0.00207042785464026  loss:  4.47108631733659&quot;
## [1] &quot;step =  2  lambda =  0.00207042785464026  loss:  4.47219616650797&quot;
## [1] &quot;step =  1  lambda =  0.00204982675327624  loss:  4.44470141575416&quot;
## [1] &quot;step =  2  lambda =  0.00204982675327624  loss:  4.4457870830616&quot;
## [1] &quot;step =  1  lambda =  0.00202943063629574  loss:  4.41854274070298&quot;
## [1] &quot;step =  2  lambda =  0.00202943063629574  loss:  4.41960448682377&quot;
## [1] &quot;step =  1  lambda =  0.00200923746407006  loss:  4.39260830561548&quot;
## [1] &quot;step =  2  lambda =  0.00200923746407006  loss:  4.39364638845482&quot;
## [1] &quot;step =  1  lambda =  0.00198924521726516  loss:  4.36689614092317&quot;
## [1] &quot;step =  2  lambda =  0.00198924521726516  loss:  4.3679108156438&quot;
## [1] &quot;step =  1  lambda =  0.0019694518966397  loss:  4.34140429391705&quot;
## [1] &quot;step =  2  lambda =  0.0019694518966397  loss:  4.34239581296852&quot;
## [1] &quot;step =  1  lambda =  0.00194985552284512  loss:  4.3161308286091&quot;
## [1] &quot;step =  2  lambda =  0.00194985552284512  loss:  4.3170994417568&quot;
## [1] &quot;step =  1  lambda =  0.00193045413622771  loss:  4.29107382559486&quot;
## [1] &quot;step =  2  lambda =  0.00193045413622771  loss:  4.2920197799487&quot;
## [1] &quot;step =  1  lambda =  0.00191124579663264  loss:  4.26623138191703&quot;
## [1] &quot;step =  2  lambda =  0.00191124579663264  loss:  4.26715492195983&quot;
## [1] &quot;step =  1  lambda =  0.00189222858320994  loss:  4.24160161093018&quot;
## [1] &quot;step =  2  lambda =  0.00189222858320994  loss:  4.24250297854569&quot;
## [1] &quot;step =  1  lambda =  0.00187340059422243  loss:  4.21718264216641&quot;
## [1] &quot;step =  2  lambda =  0.00187340059422243  loss:  4.21806207666714&quot;
## [1] &quot;step =  1  lambda =  0.0018547599468555  loss:  4.19297262120217&quot;
## [1] &quot;step =  2  lambda =  0.0018547599468555  loss:  4.19383035935685&quot;
## [1] &quot;step =  1  lambda =  0.00183630477702891  loss:  4.168969709526&quot;
## [1] &quot;step =  2  lambda =  0.00183630477702891  loss:  4.16980598558676&quot;
## [1] &quot;step =  1  lambda =  0.00181803323921027  loss:  4.14517208440737&quot;
## [1] &quot;step =  2  lambda =  0.00181803323921027  loss:  4.1459871301366&quot;
## [1] &quot;step =  1  lambda =  0.00179994350623059  loss:  4.12157793876646&quot;
## [1] &quot;step =  2  lambda =  0.00179994350623059  loss:  4.1223719834635&quot;
## [1] &quot;step =  1  lambda =  0.00178203376910149  loss:  4.09818548104506&quot;
## [1] &quot;step =  2  lambda =  0.00178203376910149  loss:  4.09895875157244&quot;
## [1] &quot;step =  1  lambda =  0.00176430223683434  loss:  4.07499293507836&quot;
## [1] &quot;step =  2  lambda =  0.00176430223683434  loss:  4.07574565588787&quot;
## [1] &quot;step =  1  lambda =  0.00174674713626112  loss:  4.05199853996779&quot;
## [1] &quot;step =  2  lambda =  0.00174674713626112  loss:  4.05273093312625&quot;
## [1] &quot;step =  1  lambda =  0.00172936671185716  loss:  4.02920054995485&quot;
## [1] &quot;step =  2  lambda =  0.00172936671185716  loss:  4.02991283516957&quot;
## [1] &quot;step =  1  lambda =  0.00171215922556552  loss:  4.00659723429593&quot;
## [1] &quot;step =  2  lambda =  0.00171215922556552  loss:  4.00728962893995&quot;
## [1] &quot;step =  1  lambda =  0.00169512295662325  loss:  3.98418687713802&quot;
## [1] &quot;step =  2  lambda =  0.00169512295662325  loss:  3.98485959627505&quot;
## [1] &quot;step =  1  lambda =  0.00167825620138925  loss:  3.96196777739557&quot;
## [1] &quot;step =  2  lambda =  0.00167825620138925  loss:  3.96262103380462&quot;
## [1] &quot;step =  1  lambda =  0.00166155727317393  loss:  3.93993824862805&quot;
## [1] &quot;step =  2  lambda =  0.00166155727317393  loss:  3.94057225282792&quot;
## [1] &quot;step =  1  lambda =  0.00164502450207057  loss:  3.91809661891873&quot;
## [1] &quot;step =  2  lambda =  0.00164502450207057  loss:  3.91871157919211&quot;
## [1] &quot;step =  1  lambda =  0.00162865623478828  loss:  3.89644123075426&quot;
## [1] &quot;step =  2  lambda =  0.00162865623478828  loss:  3.89703735317162&quot;
## [1] &quot;step =  1  lambda =  0.00161245083448668  loss:  3.87497044090514&quot;
## [1] &quot;step =  2  lambda =  0.00161245083448668  loss:  3.87554792934841&quot;
## [1] &quot;step =  1  lambda =  0.00159640668061225  loss:  3.85368262030733&quot;
## [1] &quot;step =  2  lambda =  0.00159640668061225  loss:  3.85424167649323&quot;
## [1] &quot;step =  1  lambda =  0.00158052216873622  loss:  3.83257615394454&quot;
## [1] &quot;step =  2  lambda =  0.00158052216873622  loss:  3.83311697744777&quot;
## [1] &quot;step =  1  lambda =  0.00156479571039417  loss:  3.81164944073167&quot;
## [1] &quot;step =  2  lambda =  0.00156479571039417  loss:  3.81217222900774&quot;
## [1] &quot;step =  1  lambda =  0.00154922573292716  loss:  3.79090089339895&quot;
## [1] &quot;step =  2  lambda =  0.00154922573292716  loss:  3.79140584180689&quot;
## [1] &quot;step =  1  lambda =  0.00153381067932446  loss:  3.77032893837718&quot;
## [1] &quot;step =  2  lambda =  0.00153381067932446  loss:  3.77081624020187&quot;
## [1] &quot;step =  1  lambda =  0.00151854900806788  loss:  3.74993201568372&quot;
## [1] &quot;step =  2  lambda =  0.00151854900806788  loss:  3.7504018621581&quot;
## [1] &quot;step =  1  lambda =  0.00150343919297757  loss:  3.7297085788095&quot;
## [1] &quot;step =  2  lambda =  0.00150343919297757  loss:  3.73016115913648&quot;
## [1] &quot;step =  1  lambda =  0.00148847972305943  loss:  3.70965709460682&quot;
## [1] &quot;step =  2  lambda =  0.00148847972305943  loss:  3.71009259598095&quot;
## [1] &quot;step =  1  lambda =  0.001473669102354  loss:  3.68977604317806&quot;
## [1] &quot;step =  2  lambda =  0.001473669102354  loss:  3.69019465080703&quot;
## [1] &quot;step =  1  lambda =  0.00145900584978686  loss:  3.67006391776534&quot;
## [1] &quot;step =  2  lambda =  0.00145900584978686  loss:  3.67046581489115&quot;
## [1] &quot;step =  1  lambda =  0.00144448849902054  loss:  3.65051922464092&quot;
## [1] &quot;step =  2  lambda =  0.00144448849902054  loss:  3.65090459256093&quot;
## [1] &quot;step =  1  lambda =  0.00143011559830787  loss:  3.63114048299855&quot;
## [1] &quot;step =  2  lambda =  0.00143011559830787  loss:  3.63150950108625&quot;
## [1] &quot;step =  1  lambda =  0.0014158857103468  loss:  3.61192622484571&quot;
## [1] &quot;step =  2  lambda =  0.0014158857103468  loss:  3.61227908673898&quot;
## [1] &quot;step =  1  lambda =  0.00140179741213667  loss:  3.59287503370424&quot;
## [1] &quot;step =  2  lambda =  0.00140179741213667  loss:  3.59321085318441&quot;
## [1] &quot;step =  1  lambda =  0.00138784929483593  loss:  3.57398439942217&quot;
## [1] &quot;step =  2  lambda =  0.00138784929483593  loss:  3.5743030856587&quot;
## [1] &quot;step =  1  lambda =  0.00137403996362121  loss:  3.55525263222348&quot;
## [1] &quot;step =  2  lambda =  0.00137403996362121  loss:  3.55555524342009&quot;
## [1] &quot;step =  1  lambda =  0.00136036803754789  loss:  3.53667920140737&quot;
## [1] &quot;step =  2  lambda =  0.00136036803754789  loss:  3.53696637624425&quot;
## [1] &quot;step =  1  lambda =  0.00134683214941197  loss:  3.51826316860338&quot;
## [1] &quot;step =  2  lambda =  0.00134683214941197  loss:  3.51853530518141&quot;
## [1] &quot;step =  1  lambda =  0.00133343094561336  loss:  3.50000336781791&quot;
## [1] &quot;step =  2  lambda =  0.00133343094561336  loss:  3.50026072736524&quot;
## [1] &quot;step =  1  lambda =  0.0013201630860205  loss:  3.48189850970782&quot;
## [1] &quot;step =  2  lambda =  0.0013201630860205  loss:  3.48214127898647&quot;
## [1] &quot;step =  1  lambda =  0.00130702724383639  loss:  3.46394724423247&quot;
## [1] &quot;step =  2  lambda =  0.00130702724383639  loss:  3.46417557150989&quot;
## [1] &quot;step =  1  lambda =  0.00129402210546585  loss:  3.44614819668556&quot;
## [1] &quot;step =  2  lambda =  0.00129402210546585  loss:  3.44636221190021&quot;
## [1] &quot;step =  1  lambda =  0.00128114637038421  loss:  3.4284999878169&quot;
## [1] &quot;step =  2  lambda =  0.00128114637038421  loss:  3.42869981353078&quot;
## [1] &quot;step =  1  lambda =  0.00126839875100724  loss:  3.41100124468495&quot;
## [1] &quot;step =  2  lambda =  0.00126839875100724  loss:  3.41118700175139&quot;
## [1] &quot;step =  1  lambda =  0.00125577797256237  loss:  3.39365060619593&quot;
## [1] &quot;step =  2  lambda =  0.00125577797256237  loss:  3.39382241645194&quot;
## [1] &quot;step =  1  lambda =  0.00124328277296124  loss:  3.37644672565503&quot;
## [1] &quot;step =  2  lambda =  0.00124328277296124  loss:  3.37660471298324&quot;
## [1] &quot;step =  1  lambda =  0.00123091190267348  loss:  3.35938827168329&quot;
## [1] &quot;step =  2  lambda =  0.00123091190267348  loss:  3.35953256221816&quot;
## [1] &quot;step =  1  lambda =  0.00121866412460175  loss:  3.34247392827979&quot;
## [1] &quot;step =  2  lambda =  0.00121866412460175  loss:  3.3426046501966&quot;
## [1] &quot;step =  1  lambda =  0.00120653821395804  loss:  3.32570239446985&quot;
## [1] &quot;step =  2  lambda =  0.00120653821395804  loss:  3.32581967759871&quot;
## [1] &quot;step =  1  lambda =  0.00119453295814118  loss:  3.30907238378247&quot;
## [1] &quot;step =  2  lambda =  0.00119453295814118  loss:  3.30917635917632&quot;
## [1] &quot;step =  1  lambda =  0.00118264715661557  loss:  3.29258262368621&quot;
## [1] &quot;step =  2  lambda =  0.00118264715661557  loss:  3.29267342320684&quot;
## [1] &quot;step =  1  lambda =  0.00117087962079117  loss:  3.27623185504733&quot;
## [1] &quot;step =  2  lambda =  0.00117087962079117  loss:  3.27630961099772&quot;
## [1] &quot;step =  1  lambda =  0.00115922917390459  loss:  3.26001883163809&quot;
## [1] &quot;step =  2  lambda =  0.00115922917390459  loss:  3.26008367644979&quot;
## [1] &quot;step =  1  lambda =  0.00114769465090143  loss:  3.24394231970369&quot;
## [1] &quot;step =  2  lambda =  0.00114769465090143  loss:  3.24399438567834&quot;
## [1] &quot;step =  1  lambda =  0.00113627489831977  loss:  3.22800109758632&quot;
## [1] &quot;step =  2  lambda =  0.00113627489831977  loss:  3.22804051668586&quot;
## [1] &quot;step =  1  lambda =  0.00112496877417484  loss:  3.21219395540062&quot;
## [1] &quot;step =  2  lambda =  0.00112496877417484  loss:  3.21222085907907&quot;
## [1] &quot;step =  1  lambda =  0.0011137751478448  loss:  3.19651969475321&quot;
## [1] &quot;step =  2  lambda =  0.0011137751478448  loss:  3.19653421382293&quot;
## [1] &quot;step =  1  lambda =  0.0011026928999577  loss:  3.18097712849867&quot;
## [1] &quot;step =  2  lambda =  0.0011026928999577  loss:  3.18097939302482&quot;
## [1] &quot;step =  1  lambda =  0.00109172092227951  loss:  3.16556508052565&quot;
## [1] &quot;step =  2  lambda =  0.00109172092227951  loss:  3.16555521974299&quot;
## [1] &quot;step =  3  lambda =  0.00109172092227951  loss:  3.16554525514193&quot;
## [1] &quot;step =  4  lambda =  0.00109172092227951  loss:  3.16553516343026&quot;
## [1] &quot;step =  5  lambda =  0.00109172092227951  loss:  3.1655249302321&quot;
## [1] &quot;step =  6  lambda =  0.00109172092227951  loss:  3.16551454682934&quot;
## [1] &quot;step =  7  lambda =  0.00109172092227951  loss:  3.16550400774154&quot;
## [1] &quot;step =  8  lambda =  0.00109172092227951  loss:  3.16549330932258&quot;
## [1] &quot;step =  9  lambda =  0.00109172092227951  loss:  3.1654824489589&quot;
## [1] &quot;step =  10  lambda =  0.00109172092227951  loss:  3.16547142462163&quot;
## [1] &quot;step =  11  lambda =  0.00109172092227951  loss:  3.16546023462207&quot;
## [1] &quot;step =  12  lambda =  0.00109172092227951  loss:  3.16544887748201&quot;
## [1] &quot;step =  13  lambda =  0.00109172092227951  loss:  3.16543735186705&quot;
## [1] &quot;step =  14  lambda =  0.00109172092227951  loss:  3.16542565655403&quot;
## [1] &quot;step =  15  lambda =  0.00109172092227951  loss:  3.1654137904163&quot;
## [1] &quot;step =  16  lambda =  0.00109172092227951  loss:  3.16540175241797&quot;
## [1] &quot;step =  17  lambda =  0.00109172092227951  loss:  3.16538954161248&quot;
## [1] &quot;step =  18  lambda =  0.00109172092227951  loss:  3.16537715714263&quot;
## [1] &quot;step =  19  lambda =  0.00109172092227951  loss:  3.16536459824117&quot;
## [1] &quot;step =  20  lambda =  0.00109172092227951  loss:  3.16535186423103&quot;
## [1] &quot;step =  21  lambda =  0.00109172092227951  loss:  3.16533895452517&quot;
## [1] &quot;step =  22  lambda =  0.00109172092227951  loss:  3.1653258686257&quot;
## [1] &quot;step =  23  lambda =  0.00109172092227951  loss:  3.16531260612265&quot;
## [1] &quot;step =  24  lambda =  0.00109172092227951  loss:  3.16529916669215&quot;
## [1] &quot;step =  25  lambda =  0.00109172092227951  loss:  3.16528555009424&quot;
## [1] &quot;step =  26  lambda =  0.00109172092227951  loss:  3.16527175617043&quot;
## [1] &quot;step =  27  lambda =  0.00109172092227951  loss:  3.16525778484096&quot;
## [1] &quot;step =  28  lambda =  0.00109172092227951  loss:  3.16524363610197&quot;
## [1] &quot;step =  29  lambda =  0.00109172092227951  loss:  3.16522931002247&quot;
## [1] &quot;step =  30  lambda =  0.00109172092227951  loss:  3.16521480674135&quot;
## [1] &quot;step =  31  lambda =  0.00109172092227951  loss:  3.16520012646434&quot;
## [1] &quot;step =  32  lambda =  0.00109172092227951  loss:  3.1651852694609&quot;
## [1] &quot;step =  33  lambda =  0.00109172092227951  loss:  3.1651702360613&quot;
## [1] &quot;step =  34  lambda =  0.00109172092227951  loss:  3.16515502665363&quot;
## [1] &quot;step =  35  lambda =  0.00109172092227951  loss:  3.16513964168094&quot;
## [1] &quot;step =  36  lambda =  0.00109172092227951  loss:  3.16512408163843&quot;
## [1] &quot;step =  37  lambda =  0.00109172092227951  loss:  3.16510834707081&quot;
## [1] &quot;step =  38  lambda =  0.00109172092227951  loss:  3.16509243856964&quot;
## [1] &quot;step =  39  lambda =  0.00109172092227951  loss:  3.16507635677086&quot;
## [1] &quot;step =  40  lambda =  0.00109172092227951  loss:  3.1650601023524&quot;
## [1] &quot;step =  41  lambda =  0.00109172092227951  loss:  3.16504367603188&quot;
## [1] &quot;step =  42  lambda =  0.00109172092227951  loss:  3.16502707856444&quot;
## [1] &quot;step =  43  lambda =  0.00109172092227951  loss:  3.16501031074065&quot;
## [1] &quot;step =  44  lambda =  0.00109172092227951  loss:  3.1649933733845&quot;
## [1] &quot;step =  45  lambda =  0.00109172092227951  loss:  3.16497626735156&quot;
## [1] &quot;step =  46  lambda =  0.00109172092227951  loss:  3.16495899352718&quot;
## [1] &quot;step =  47  lambda =  0.00109172092227951  loss:  3.16494155282475&quot;
## [1] &quot;step =  48  lambda =  0.00109172092227951  loss:  3.16492394618417&quot;
## [1] &quot;step =  49  lambda =  0.00109172092227951  loss:  3.16490617457022&quot;
## [1] &quot;step =  50  lambda =  0.00109172092227951  loss:  3.16488823897123&quot;
## [1] &quot;step =  51  lambda =  0.00109172092227951  loss:  3.16487014039764&quot;
## [1] &quot;step =  52  lambda =  0.00109172092227951  loss:  3.16485187988072&quot;
## [1] &quot;step =  53  lambda =  0.00109172092227951  loss:  3.16483345847137&quot;
## [1] &quot;step =  54  lambda =  0.00109172092227951  loss:  3.16481487723898&quot;
## [1] &quot;step =  55  lambda =  0.00109172092227951  loss:  3.16479613727031&quot;
## [1] &quot;step =  56  lambda =  0.00109172092227951  loss:  3.16477723966845&quot;
## [1] &quot;step =  57  lambda =  0.00109172092227951  loss:  3.16475818555192&quot;
## [1] &quot;step =  58  lambda =  0.00109172092227951  loss:  3.16473897605368&quot;
## [1] &quot;step =  59  lambda =  0.00109172092227951  loss:  3.16471961232031&quot;
## [1] &quot;step =  60  lambda =  0.00109172092227951  loss:  3.16470009551119&quot;
## [1] &quot;step =  61  lambda =  0.00109172092227951  loss:  3.16468042679772&quot;
## [1] &quot;step =  62  lambda =  0.00109172092227951  loss:  3.1646606073626&quot;
## [1] &quot;step =  63  lambda =  0.00109172092227951  loss:  3.16464063839918&quot;
## [1] &quot;step =  64  lambda =  0.00109172092227951  loss:  3.16462052111075&quot;
## [1] &quot;step =  65  lambda =  0.00109172092227951  loss:  3.16460025671001&quot;
## [1] &quot;step =  66  lambda =  0.00109172092227951  loss:  3.16457984641841&quot;
## [1] &quot;step =  67  lambda =  0.00109172092227951  loss:  3.16455929146573&quot;
## [1] &quot;step =  68  lambda =  0.00109172092227951  loss:  3.16453859308945&quot;
## [1] &quot;step =  69  lambda =  0.00109172092227951  loss:  3.16451775253437&quot;
## [1] &quot;step =  70  lambda =  0.00109172092227951  loss:  3.16449677105207&quot;
## [1] &quot;step =  71  lambda =  0.00109172092227951  loss:  3.16447564990057&quot;
## [1] &quot;step =  72  lambda =  0.00109172092227951  loss:  3.16445439034388&quot;
## [1] &quot;step =  73  lambda =  0.00109172092227951  loss:  3.16443299365164&quot;
## [1] &quot;step =  74  lambda =  0.00109172092227951  loss:  3.16441146109875&quot;
## [1] &quot;step =  75  lambda =  0.00109172092227951  loss:  3.16438979396507&quot;
## [1] &quot;step =  76  lambda =  0.00109172092227951  loss:  3.16436799353506&quot;
## [1] &quot;step =  77  lambda =  0.00109172092227951  loss:  3.16434606109752&quot;
## [1] &quot;step =  78  lambda =  0.00109172092227951  loss:  3.16432399794531&quot;
## [1] &quot;step =  79  lambda =  0.00109172092227951  loss:  3.16430180537506&quot;
## [1] &quot;step =  80  lambda =  0.00109172092227951  loss:  3.16427948468693&quot;
## [1] &quot;step =  81  lambda =  0.00109172092227951  loss:  3.16425703718443&quot;
## [1] &quot;step =  82  lambda =  0.00109172092227951  loss:  3.16423446417412&quot;
## [1] &quot;step =  83  lambda =  0.00109172092227951  loss:  3.16421176696547&quot;
## [1] &quot;step =  84  lambda =  0.00109172092227951  loss:  3.16418894687063&quot;
## [1] &quot;step =  85  lambda =  0.00109172092227951  loss:  3.16416600520427&quot;
## [1] &quot;step =  86  lambda =  0.00109172092227951  loss:  3.1641429432834&quot;
## [1] &quot;step =  87  lambda =  0.00109172092227951  loss:  3.1641197624272&quot;
## [1] &quot;step =  88  lambda =  0.00109172092227951  loss:  3.16409646395691&quot;
## [1] &quot;step =  89  lambda =  0.00109172092227951  loss:  3.16407304919562&quot;
## [1] &quot;step =  90  lambda =  0.00109172092227951  loss:  3.16404951946822&quot;
## [1] &quot;step =  91  lambda =  0.00109172092227951  loss:  3.16402587610121&quot;
## [1] &quot;step =  92  lambda =  0.00109172092227951  loss:  3.16400212042262&quot;
## [1] &quot;step =  93  lambda =  0.00109172092227951  loss:  3.1639782537619&quot;
## [1] &quot;step =  94  lambda =  0.00109172092227951  loss:  3.16395427744977&quot;
## [1] &quot;step =  95  lambda =  0.00109172092227951  loss:  3.16393019281822&quot;
## [1] &quot;step =  96  lambda =  0.00109172092227951  loss:  3.16390600120032&quot;
## [1] &quot;step =  97  lambda =  0.00109172092227951  loss:  3.16388170393019&quot;
## [1] &quot;step =  98  lambda =  0.00109172092227951  loss:  3.16385730234294&quot;
## [1] &quot;step =  99  lambda =  0.00109172092227951  loss:  3.16383279777454&quot;
## [1] &quot;step =  100  lambda =  0.00109172092227951  loss:  3.16380819156179&quot;
## [1] &quot;step =  101  lambda =  0.00109172092227951  loss:  3.16378348504227&quot;
## [1] &quot;step =  102  lambda =  0.00109172092227951  loss:  3.16375867955422&quot;
## [1] &quot;step =  103  lambda =  0.00109172092227951  loss:  3.16373377643657&quot;
## [1] &quot;step =  104  lambda =  0.00109172092227951  loss:  3.16370877702882&quot;
## [1] &quot;step =  105  lambda =  0.00109172092227951  loss:  3.16368368267103&quot;
## [1] &quot;step =  106  lambda =  0.00109172092227951  loss:  3.16365849470375&quot;
## [1] &quot;step =  107  lambda =  0.00109172092227951  loss:  3.163633214468&quot;
## [1] &quot;step =  108  lambda =  0.00109172092227951  loss:  3.16360784330523&quot;
## [1] &quot;step =  109  lambda =  0.00109172092227951  loss:  3.16358238255726&quot;
## [1] &quot;step =  110  lambda =  0.00109172092227951  loss:  3.16355683356629&quot;
## [1] &quot;step =  111  lambda =  0.00109172092227951  loss:  3.16353119767483&quot;
## [1] &quot;step =  112  lambda =  0.00109172092227951  loss:  3.1635054762257&quot;
## [1] &quot;step =  113  lambda =  0.00109172092227951  loss:  3.16347967056198&quot;
## [1] &quot;step =  114  lambda =  0.00109172092227951  loss:  3.16345378202701&quot;
## [1] &quot;step =  115  lambda =  0.00109172092227951  loss:  3.16342781196435&quot;
## [1] &quot;step =  116  lambda =  0.00109172092227951  loss:  3.16340176171776&quot;
## [1] &quot;step =  117  lambda =  0.00109172092227951  loss:  3.1633756326312&quot;
## [1] &quot;step =  118  lambda =  0.00109172092227951  loss:  3.16334942604879&quot;
## [1] &quot;step =  119  lambda =  0.00109172092227951  loss:  3.16332314331481&quot;
## [1] &quot;step =  120  lambda =  0.00109172092227951  loss:  3.16329678577369&quot;
## [1] &quot;step =  121  lambda =  0.00109172092227951  loss:  3.16327035476995&quot;
## [1] &quot;step =  122  lambda =  0.00109172092227951  loss:  3.16324385164827&quot;
## [1] &quot;step =  123  lambda =  0.00109172092227951  loss:  3.16321727775339&quot;
## [1] &quot;step =  124  lambda =  0.00109172092227951  loss:  3.16319063443015&quot;
## [1] &quot;step =  125  lambda =  0.00109172092227951  loss:  3.1631639230235&quot;
## [1] &quot;step =  126  lambda =  0.00109172092227951  loss:  3.16313714487842&quot;
## [1] &quot;step =  127  lambda =  0.00109172092227951  loss:  3.16311030134&quot;
## [1] &quot;step =  128  lambda =  0.00109172092227951  loss:  3.16308339375332&quot;
## [1] &quot;step =  129  lambda =  0.00109172092227951  loss:  3.16305642346359&quot;
## [1] &quot;step =  130  lambda =  0.00109172092227951  loss:  3.16302939181598&quot;
## [1] &quot;step =  131  lambda =  0.00109172092227951  loss:  3.16300230015574&quot;
## [1] &quot;step =  132  lambda =  0.00109172092227951  loss:  3.16297514982815&quot;
## [1] &quot;step =  133  lambda =  0.00109172092227951  loss:  3.16294794217849&quot;
## [1] &quot;step =  134  lambda =  0.00109172092227951  loss:  3.16292067855206&quot;
## [1] &quot;step =  135  lambda =  0.00109172092227951  loss:  3.16289336029418&quot;
## [1] &quot;step =  136  lambda =  0.00109172092227951  loss:  3.16286598875018&quot;
## [1] &quot;step =  137  lambda =  0.00109172092227951  loss:  3.16283856526536&quot;
## [1] &quot;step =  138  lambda =  0.00109172092227951  loss:  3.16281109118506&quot;
## [1] &quot;step =  139  lambda =  0.00109172092227951  loss:  3.16278356785457&quot;
## [1] &quot;step =  140  lambda =  0.00109172092227951  loss:  3.16275599661919&quot;
## [1] &quot;step =  141  lambda =  0.00109172092227951  loss:  3.16272837882416&quot;
## [1] &quot;step =  142  lambda =  0.00109172092227951  loss:  3.16270071581477&quot;
## [1] &quot;step =  143  lambda =  0.00109172092227951  loss:  3.16267300893619&quot;
## [1] &quot;step =  144  lambda =  0.00109172092227951  loss:  3.16264525953363&quot;
## [1] &quot;step =  145  lambda =  0.00109172092227951  loss:  3.1626174689522&quot;
## [1] &quot;step =  146  lambda =  0.00109172092227951  loss:  3.16258963853699&quot;
## [1] &quot;step =  147  lambda =  0.00109172092227951  loss:  3.16256176963304&quot;
## [1] &quot;step =  148  lambda =  0.00109172092227951  loss:  3.16253386358533&quot;
## [1] &quot;step =  149  lambda =  0.00109172092227951  loss:  3.16250592173877&quot;
## [1] &quot;step =  150  lambda =  0.00109172092227951  loss:  3.16247794543819&quot;
## [1] &quot;step =  151  lambda =  0.00109172092227951  loss:  3.16244993602836&quot;
## [1] &quot;step =  152  lambda =  0.00109172092227951  loss:  3.16242189485395&quot;
## [1] &quot;step =  153  lambda =  0.00109172092227951  loss:  3.16239382325957&quot;
## [1] &quot;step =  154  lambda =  0.00109172092227951  loss:  3.16236572258968&quot;
## [1] &quot;step =  155  lambda =  0.00109172092227951  loss:  3.16233759418868&quot;
## [1] &quot;step =  156  lambda =  0.00109172092227951  loss:  3.16230943940083&quot;
## [1] &quot;step =  157  lambda =  0.00109172092227951  loss:  3.1622812595703&quot;
## [1] &quot;step =  158  lambda =  0.00109172092227951  loss:  3.16225305604108&quot;
## [1] &quot;step =  159  lambda =  0.00109172092227951  loss:  3.16222483015708&quot;
## [1] &quot;step =  160  lambda =  0.00109172092227951  loss:  3.16219658326202&quot;
## [1] &quot;step =  161  lambda =  0.00109172092227951  loss:  3.16216831669948&quot;
## [1] &quot;step =  162  lambda =  0.00109172092227951  loss:  3.16214003181289&quot;
## [1] &quot;step =  163  lambda =  0.00109172092227951  loss:  3.16211172994546&quot;
## [1] &quot;step =  164  lambda =  0.00109172092227951  loss:  3.16208341244029&quot;
## [1] &quot;step =  165  lambda =  0.00109172092227951  loss:  3.16205508064021&quot;
## [1] &quot;step =  166  lambda =  0.00109172092227951  loss:  3.16202673588788&quot;
## [1] &quot;step =  167  lambda =  0.00109172092227951  loss:  3.16199837952576&quot;
## [1] &quot;step =  168  lambda =  0.00109172092227951  loss:  3.16197001289605&quot;
## [1] &quot;step =  169  lambda =  0.00109172092227951  loss:  3.16194163734073&quot;
## [1] &quot;step =  170  lambda =  0.00109172092227951  loss:  3.16191325420154&quot;
## [1] &quot;step =  171  lambda =  0.00109172092227951  loss:  3.16188486481993&quot;
## [1] &quot;step =  172  lambda =  0.00109172092227951  loss:  3.1618564705371&quot;
## [1] &quot;step =  173  lambda =  0.00109172092227951  loss:  3.16182807269397&quot;
## [1] &quot;step =  174  lambda =  0.00109172092227951  loss:  3.16179967263114&quot;
## [1] &quot;step =  175  lambda =  0.00109172092227951  loss:  3.1617712716889&quot;
## [1] &quot;step =  176  lambda =  0.00109172092227951  loss:  3.16174287120725&quot;
## [1] &quot;step =  177  lambda =  0.00109172092227951  loss:  3.16171447252581&quot;
## [1] &quot;step =  178  lambda =  0.00109172092227951  loss:  3.16168607698386&quot;
## [1] &quot;step =  179  lambda =  0.00109172092227951  loss:  3.16165768592035&quot;
## [1] &quot;step =  180  lambda =  0.00109172092227951  loss:  3.16162930067379&quot;
## [1] &quot;step =  181  lambda =  0.00109172092227951  loss:  3.16160092258236&quot;
## [1] &quot;step =  182  lambda =  0.00109172092227951  loss:  3.16157255298377&quot;
## [1] &quot;step =  183  lambda =  0.00109172092227951  loss:  3.16154419321536&quot;
## [1] &quot;step =  184  lambda =  0.00109172092227951  loss:  3.161515844614&quot;
## [1] &quot;step =  185  lambda =  0.00109172092227951  loss:  3.16148750851612&quot;
## [1] &quot;step =  186  lambda =  0.00109172092227951  loss:  3.16145918625769&quot;
## [1] &quot;step =  187  lambda =  0.00109172092227951  loss:  3.16143087917417&quot;
## [1] &quot;step =  188  lambda =  0.00109172092227951  loss:  3.16140258860055&quot;
## [1] &quot;step =  189  lambda =  0.00109172092227951  loss:  3.16137431587129&quot;
## [1] &quot;step =  190  lambda =  0.00109172092227951  loss:  3.16134606232031&quot;
## [1] &quot;step =  191  lambda =  0.00109172092227951  loss:  3.16131782928101&quot;
## [1] &quot;step =  192  lambda =  0.00109172092227951  loss:  3.16128961808619&quot;
## [1] &quot;step =  193  lambda =  0.00109172092227951  loss:  3.16126143006809&quot;
## [1] &quot;step =  194  lambda =  0.00109172092227951  loss:  3.16123326655837&quot;
## [1] &quot;step =  195  lambda =  0.00109172092227951  loss:  3.16120512888803&quot;
## [1] &quot;step =  196  lambda =  0.00109172092227951  loss:  3.16117701838747&quot;
## [1] &quot;step =  197  lambda =  0.00109172092227951  loss:  3.16114893638646&quot;
## [1] &quot;step =  198  lambda =  0.00109172092227951  loss:  3.16112088421406&quot;
## [1] &quot;step =  199  lambda =  0.00109172092227951  loss:  3.16109286319866&quot;
## [1] &quot;step =  200  lambda =  0.00109172092227951  loss:  3.16106487466799&quot;
## [1] &quot;step =  201  lambda =  0.00109172092227951  loss:  3.16103691994901&quot;
## [1] &quot;step =  202  lambda =  0.00109172092227951  loss:  3.16100900036796&quot;
## [1] &quot;step =  203  lambda =  0.00109172092227951  loss:  3.16098111725035&quot;
## [1] &quot;step =  204  lambda =  0.00109172092227951  loss:  3.16095327192089&quot;
## [1] &quot;step =  205  lambda =  0.00109172092227951  loss:  3.16092546570351&quot;
## [1] &quot;step =  206  lambda =  0.00109172092227951  loss:  3.16089769992134&quot;
## [1] &quot;step =  207  lambda =  0.00109172092227951  loss:  3.16086997589668&quot;
## [1] &quot;step =  208  lambda =  0.00109172092227951  loss:  3.160842294951&quot;
## [1] &quot;step =  209  lambda =  0.00109172092227951  loss:  3.16081465840489&quot;
## [1] &quot;step =  210  lambda =  0.00109172092227951  loss:  3.16078706757806&quot;
## [1] &quot;step =  211  lambda =  0.00109172092227951  loss:  3.16075952378935&quot;
## [1] &quot;step =  212  lambda =  0.00109172092227951  loss:  3.16073202835665&quot;
## [1] &quot;step =  213  lambda =  0.00109172092227951  loss:  3.16070458259693&quot;
## [1] &quot;step =  214  lambda =  0.00109172092227951  loss:  3.16067718782623&quot;
## [1] &quot;step =  215  lambda =  0.00109172092227951  loss:  3.16064984535959&quot;
## [1] &quot;step =  216  lambda =  0.00109172092227951  loss:  3.16062255651107&quot;
## [1] &quot;step =  217  lambda =  0.00109172092227951  loss:  3.16059532259372&quot;
## [1] &quot;step =  218  lambda =  0.00109172092227951  loss:  3.16056814491959&quot;
## [1] &quot;step =  219  lambda =  0.00109172092227951  loss:  3.16054102479965&quot;
## [1] &quot;step =  220  lambda =  0.00109172092227951  loss:  3.16051396354383&quot;
## [1] &quot;step =  221  lambda =  0.00109172092227951  loss:  3.16048696246097&quot;
## [1] &quot;step =  222  lambda =  0.00109172092227951  loss:  3.16046002285883&quot;
## [1] &quot;step =  223  lambda =  0.00109172092227951  loss:  3.16043314604402&quot;
## [1] &quot;step =  224  lambda =  0.00109172092227951  loss:  3.16040633332205&quot;
## [1] &quot;step =  225  lambda =  0.00109172092227951  loss:  3.16037958599725&quot;
## [1] &quot;step =  226  lambda =  0.00109172092227951  loss:  3.1603529053728&quot;
## [1] &quot;step =  227  lambda =  0.00109172092227951  loss:  3.16032629275068&quot;
## [1] &quot;step =  228  lambda =  0.00109172092227951  loss:  3.16029974943166&quot;
## [1] &quot;step =  229  lambda =  0.00109172092227951  loss:  3.16027327671529&quot;
## [1] &quot;step =  230  lambda =  0.00109172092227951  loss:  3.16024687589989&quot;
## [1] &quot;step =  231  lambda =  0.00109172092227951  loss:  3.16022054828247&quot;
## [1] &quot;step =  232  lambda =  0.00109172092227951  loss:  3.16019429515882&quot;
## [1] &quot;step =  233  lambda =  0.00109172092227951  loss:  3.16016811782339&quot;
## [1] &quot;step =  234  lambda =  0.00109172092227951  loss:  3.16014201756935&quot;
## [1] &quot;step =  235  lambda =  0.00109172092227951  loss:  3.16011599568851&quot;
## [1] &quot;step =  236  lambda =  0.00109172092227951  loss:  3.16009005347134&quot;
## [1] &quot;step =  237  lambda =  0.00109172092227951  loss:  3.16006419220694&quot;
## [1] &quot;step =  238  lambda =  0.00109172092227951  loss:  3.16003841318302&quot;
## [1] &quot;step =  239  lambda =  0.00109172092227951  loss:  3.16001271768589&quot;
## [1] &quot;step =  240  lambda =  0.00109172092227951  loss:  3.15998710700046&quot;
## [1] &quot;step =  241  lambda =  0.00109172092227951  loss:  3.15996158241018&quot;
## [1] &quot;step =  242  lambda =  0.00109172092227951  loss:  3.15993614519704&quot;
## [1] &quot;step =  243  lambda =  0.00109172092227951  loss:  3.15991079664158&quot;
## [1] &quot;step =  244  lambda =  0.00109172092227951  loss:  3.15988553802284&quot;
## [1] &quot;step =  245  lambda =  0.00109172092227951  loss:  3.15986037061835&quot;
## [1] &quot;step =  246  lambda =  0.00109172092227951  loss:  3.15983529570412&quot;
## [1] &quot;step =  247  lambda =  0.00109172092227951  loss:  3.15981031455464&quot;
## [1] &quot;step =  248  lambda =  0.00109172092227951  loss:  3.15978542844281&quot;
## [1] &quot;step =  249  lambda =  0.00109172092227951  loss:  3.15976063864&quot;
## [1] &quot;step =  250  lambda =  0.00109172092227951  loss:  3.15973594641595&quot;
## [1] &quot;step =  251  lambda =  0.00109172092227951  loss:  3.15971135303882&quot;
## [1] &quot;step =  252  lambda =  0.00109172092227951  loss:  3.15968685977515&quot;
## [1] &quot;step =  253  lambda =  0.00109172092227951  loss:  3.15966246788983&quot;
## [1] &quot;step =  254  lambda =  0.00109172092227951  loss:  3.15963817864611&quot;
## [1] &quot;step =  255  lambda =  0.00109172092227951  loss:  3.15961399330556&quot;
## [1] &quot;step =  256  lambda =  0.00109172092227951  loss:  3.15958991312807&quot;
## [1] &quot;step =  257  lambda =  0.00109172092227951  loss:  3.15956593937185&quot;
## [1] &quot;step =  258  lambda =  0.00109172092227951  loss:  3.15954207329335&quot;
## [1] &quot;step =  259  lambda =  0.00109172092227951  loss:  3.15951831614734&quot;
## [1] &quot;step =  260  lambda =  0.00109172092227951  loss:  3.15949466918681&quot;
## [1] &quot;step =  261  lambda =  0.00109172092227951  loss:  3.15947113366299&quot;
## [1] &quot;step =  262  lambda =  0.00109172092227951  loss:  3.15944771082536&quot;
## [1] &quot;step =  263  lambda =  0.00109172092227951  loss:  3.15942440192159&quot;
## [1] &quot;step =  264  lambda =  0.00109172092227951  loss:  3.15940120819755&quot;
## [1] &quot;step =  265  lambda =  0.00109172092227951  loss:  3.15937813089727&quot;
## [1] &quot;step =  266  lambda =  0.00109172092227951  loss:  3.15935517126298&quot;
## [1] &quot;step =  267  lambda =  0.00109172092227951  loss:  3.15933233053503&quot;
## [1] &quot;step =  268  lambda =  0.00109172092227951  loss:  3.15930960995194&quot;
## [1] &quot;step =  269  lambda =  0.00109172092227951  loss:  3.15928701075031&quot;
## [1] &quot;step =  270  lambda =  0.00109172092227951  loss:  3.15926453416489&quot;
## [1] &quot;step =  271  lambda =  0.00109172092227951  loss:  3.1592421814285&quot;
## [1] &quot;step =  272  lambda =  0.00109172092227951  loss:  3.15921995377204&quot;
## [1] &quot;step =  273  lambda =  0.00109172092227951  loss:  3.15919785242451&quot;
## [1] &quot;step =  274  lambda =  0.00109172092227951  loss:  3.15917587861292&quot;
## [1] &quot;step =  275  lambda =  0.00109172092227951  loss:  3.15915403356233&quot;
## [1] &quot;step =  276  lambda =  0.00109172092227951  loss:  3.15913231849587&quot;
## [1] &quot;step =  277  lambda =  0.00109172092227951  loss:  3.15911073463463&quot;
## [1] &quot;step =  278  lambda =  0.00109172092227951  loss:  3.15908928319774&quot;
## [1] &quot;step =  279  lambda =  0.00109172092227951  loss:  3.15906796540229&quot;
## [1] &quot;step =  280  lambda =  0.00109172092227951  loss:  3.15904678246337&quot;
## [1] &quot;step =  281  lambda =  0.00109172092227951  loss:  3.15902573559402&quot;
## [1] &quot;step =  282  lambda =  0.00109172092227951  loss:  3.15900482600524&quot;
## [1] &quot;step =  283  lambda =  0.00109172092227951  loss:  3.15898405490596&quot;
## [1] &quot;step =  284  lambda =  0.00109172092227951  loss:  3.15896342350305&quot;
## [1] &quot;step =  285  lambda =  0.00109172092227951  loss:  3.15894293300129&quot;
## [1] &quot;step =  286  lambda =  0.00109172092227951  loss:  3.15892258460335&quot;
## [1] &quot;step =  287  lambda =  0.00109172092227951  loss:  3.1589023795098&quot;
## [1] &quot;step =  288  lambda =  0.00109172092227951  loss:  3.15888231891911&quot;
## [1] &quot;step =  289  lambda =  0.00109172092227951  loss:  3.1588624040276&quot;
## [1] &quot;step =  290  lambda =  0.00109172092227951  loss:  3.15884263602943&quot;
## [1] &quot;step =  291  lambda =  0.00109172092227951  loss:  3.15882301611664&quot;
## [1] &quot;step =  292  lambda =  0.00109172092227951  loss:  3.15880354547909&quot;
## [1] &quot;step =  293  lambda =  0.00109172092227951  loss:  3.15878422530448&quot;
## [1] &quot;step =  294  lambda =  0.00109172092227951  loss:  3.15876505677828&quot;
## [1] &quot;step =  295  lambda =  0.00109172092227951  loss:  3.15874604108381&quot;
## [1] &quot;step =  296  lambda =  0.00109172092227951  loss:  3.15872717940215&quot;
## [1] &quot;step =  297  lambda =  0.00109172092227951  loss:  3.1587084729122&quot;
## [1] &quot;step =  298  lambda =  0.00109172092227951  loss:  3.1586899227906&quot;
## [1] &quot;step =  299  lambda =  0.00109172092227951  loss:  3.15867153021175&quot;
## [1] &quot;step =  300  lambda =  0.00109172092227951  loss:  3.15865329634784&quot;
## [1] &quot;step =  301  lambda =  0.00109172092227951  loss:  3.15863522236874&quot;
## [1] &quot;step =  302  lambda =  0.00109172092227951  loss:  3.15861730944211&quot;
## [1] &quot;step =  303  lambda =  0.00109172092227951  loss:  3.15859955873331&quot;
## [1] &quot;step =  304  lambda =  0.00109172092227951  loss:  3.15858197140541&quot;
## [1] &quot;step =  305  lambda =  0.00109172092227951  loss:  3.15856454861919&quot;
## [1] &quot;step =  306  lambda =  0.00109172092227951  loss:  3.15854729153313&quot;
## [1] &quot;step =  307  lambda =  0.00109172092227951  loss:  3.15853020130337&quot;
## [1] &quot;step =  308  lambda =  0.00109172092227951  loss:  3.15851327908378&quot;
## [1] &quot;step =  309  lambda =  0.00109172092227951  loss:  3.15849652602584&quot;
## [1] &quot;step =  310  lambda =  0.00109172092227951  loss:  3.15847994327873&quot;
## [1] &quot;step =  311  lambda =  0.00109172092227951  loss:  3.15846353198925&quot;
## [1] &quot;step =  312  lambda =  0.00109172092227951  loss:  3.15844729330188&quot;
## [1] &quot;step =  313  lambda =  0.00109172092227951  loss:  3.15843122835869&quot;
## [1] &quot;step =  314  lambda =  0.00109172092227951  loss:  3.15841533829943&quot;
## [1] &quot;step =  315  lambda =  0.00109172092227951  loss:  3.15839962426143&quot;
## [1] &quot;step =  316  lambda =  0.00109172092227951  loss:  3.15838408737963&quot;
## [1] &quot;step =  317  lambda =  0.00109172092227951  loss:  3.15836872878659&quot;
## [1] &quot;step =  318  lambda =  0.00109172092227951  loss:  3.15835354961246&quot;
## [1] &quot;step =  319  lambda =  0.00109172092227951  loss:  3.15833855098497&quot;
## [1] &quot;step =  320  lambda =  0.00109172092227951  loss:  3.15832373402944&quot;
## [1] &quot;step =  321  lambda =  0.00109172092227951  loss:  3.15830909986876&quot;
## [1] &quot;step =  322  lambda =  0.00109172092227951  loss:  3.15829464962339&quot;
## [1] &quot;step =  323  lambda =  0.00109172092227951  loss:  3.15828038441134&quot;
## [1] &quot;step =  324  lambda =  0.00109172092227951  loss:  3.15826630534817&quot;
## [1] &quot;step =  325  lambda =  0.00109172092227951  loss:  3.158252413547&quot;
## [1] &quot;step =  326  lambda =  0.00109172092227951  loss:  3.15823871011848&quot;
## [1] &quot;step =  327  lambda =  0.00109172092227951  loss:  3.1582251961708&quot;
## [1] &quot;step =  328  lambda =  0.00109172092227951  loss:  3.15821187280964&quot;
## [1] &quot;step =  329  lambda =  0.00109172092227951  loss:  3.15819874113824&quot;
## [1] &quot;step =  330  lambda =  0.00109172092227951  loss:  3.15818580225735&quot;
## [1] &quot;step =  331  lambda =  0.00109172092227951  loss:  3.15817305726522&quot;
## [1] &quot;step =  332  lambda =  0.00109172092227951  loss:  3.15816050725757&quot;
## [1] &quot;step =  333  lambda =  0.00109172092227951  loss:  3.15814815332767&quot;
## [1] &quot;step =  334  lambda =  0.00109172092227951  loss:  3.15813599656624&quot;
## [1] &quot;step =  335  lambda =  0.00109172092227951  loss:  3.1581240380615&quot;
## [1] &quot;step =  336  lambda =  0.00109172092227951  loss:  3.15811227889913&quot;
## [1] &quot;step =  337  lambda =  0.00109172092227951  loss:  3.15810072016232&quot;
## [1] &quot;step =  338  lambda =  0.00109172092227951  loss:  3.15808936293168&quot;
## [1] &quot;step =  339  lambda =  0.00109172092227951  loss:  3.15807820828532&quot;
## [1] &quot;step =  340  lambda =  0.00109172092227951  loss:  3.15806725729879&quot;
## [1] &quot;step =  341  lambda =  0.00109172092227951  loss:  3.15805651104509&quot;
## [1] &quot;step =  342  lambda =  0.00109172092227951  loss:  3.15804597059468&quot;
## [1] &quot;step =  343  lambda =  0.00109172092227951  loss:  3.15803563701545&quot;
## [1] &quot;step =  344  lambda =  0.00109172092227951  loss:  3.15802551137273&quot;
## [1] &quot;step =  345  lambda =  0.00109172092227951  loss:  3.15801559472928&quot;
## [1] &quot;step =  346  lambda =  0.00109172092227951  loss:  3.15800588814531&quot;
## [1] &quot;step =  347  lambda =  0.00109172092227951  loss:  3.15799639267843&quot;
## [1] &quot;step =  348  lambda =  0.00109172092227951  loss:  3.15798710938368&quot;
## [1] &quot;step =  349  lambda =  0.00109172092227951  loss:  3.15797803931351&quot;
## [1] &quot;step =  350  lambda =  0.00109172092227951  loss:  3.15796918351778&quot;
## [1] &quot;step =  351  lambda =  0.00109172092227951  loss:  3.15796054304378&quot;
## [1] &quot;step =  352  lambda =  0.00109172092227951  loss:  3.15795211893618&quot;
## [1] &quot;step =  353  lambda =  0.00109172092227951  loss:  3.15794391223706&quot;
## [1] &quot;step =  354  lambda =  0.00109172092227951  loss:  3.15793592398589&quot;
## [1] &quot;step =  355  lambda =  0.00109172092227951  loss:  3.15792815521954&quot;
## [1] &quot;step =  356  lambda =  0.00109172092227951  loss:  3.15792060697227&quot;
## [1] &quot;step =  357  lambda =  0.00109172092227951  loss:  3.15791328027571&quot;
## [1] &quot;step =  358  lambda =  0.00109172092227951  loss:  3.15790617615891&quot;
## [1] &quot;step =  359  lambda =  0.00109172092227951  loss:  3.15789929564825&quot;
## [1] &quot;step =  360  lambda =  0.00109172092227951  loss:  3.15789263976753&quot;
## [1] &quot;step =  361  lambda =  0.00109172092227951  loss:  3.15788620953789&quot;
## [1] &quot;step =  362  lambda =  0.00109172092227951  loss:  3.15788000597787&quot;
## [1] &quot;step =  363  lambda =  0.00109172092227951  loss:  3.15787403010335&quot;
## [1] &quot;step =  364  lambda =  0.00109172092227951  loss:  3.1578682829276&quot;
## [1] &quot;step =  365  lambda =  0.00109172092227951  loss:  3.15786276546123&quot;
## [1] &quot;step =  366  lambda =  0.00109172092227951  loss:  3.15785747871223&quot;
## [1] &quot;step =  367  lambda =  0.00109172092227951  loss:  3.15785242368594&quot;
## [1] &quot;step =  368  lambda =  0.00109172092227951  loss:  3.15784760138503&quot;
## [1] &quot;step =  369  lambda =  0.00109172092227951  loss:  3.15784301280955&quot;
## [1] &quot;step =  370  lambda =  0.00109172092227951  loss:  3.15783865895692&quot;
## [1] &quot;step =  371  lambda =  0.00109172092227951  loss:  3.15783454082186&quot;
## [1] &quot;step =  372  lambda =  0.00109172092227951  loss:  3.15783065939646&quot;
## [1] &quot;step =  373  lambda =  0.00109172092227951  loss:  3.15782701567016&quot;
## [1] &quot;step =  374  lambda =  0.00109172092227951  loss:  3.15782361062972&quot;
## [1] &quot;step =  375  lambda =  0.00109172092227951  loss:  3.15782044525927&quot;
## [1] &quot;step =  376  lambda =  0.00109172092227951  loss:  3.15781752054026&quot;
## [1] &quot;step =  377  lambda =  0.00109172092227951  loss:  3.15781483745147&quot;
## [1] &quot;step =  378  lambda =  0.00109172092227951  loss:  3.15781239696903&quot;
## [1] &quot;step =  379  lambda =  0.00109172092227951  loss:  3.15781020006641&quot;
## [1] &quot;step =  380  lambda =  0.00109172092227951  loss:  3.15780824771438&quot;
## [1] &quot;step =  381  lambda =  0.00109172092227951  loss:  3.15780654088108&quot;
## [1] &quot;step =  382  lambda =  0.00109172092227951  loss:  3.15780508053195&quot;
## [1] &quot;step =  383  lambda =  0.00109172092227951  loss:  3.15780386762978&quot;
## [1] &quot;step =  384  lambda =  0.00109172092227951  loss:  3.15780290313466&quot;
## [1] &quot;step =  385  lambda =  0.00109172092227951  loss:  3.15780218800404&quot;
## [1] &quot;step =  386  lambda =  0.00109172092227951  loss:  3.15780172319267&quot;
## [1] &quot;step =  387  lambda =  0.00109172092227951  loss:  3.15780150965263&quot;
## [1] &quot;step =  388  lambda =  0.00109172092227951  loss:  3.15780154833333&quot;
## [1] &quot;step =  1  lambda =  0.00108085811760332  loss:  3.13835485292647&quot;
## [1] &quot;step =  2  lambda =  0.00108085811760332  loss:  3.13834517007313&quot;
## [1] &quot;step =  3  lambda =  0.00108085811760332  loss:  3.13833574587299&quot;
## [1] &quot;step =  4  lambda =  0.00108085811760332  loss:  3.13832658156447&quot;
## [1] &quot;step =  5  lambda =  0.00108085811760332  loss:  3.13831767811623&quot;
## [1] &quot;step =  6  lambda =  0.00108085811760332  loss:  3.13830903613204&quot;
## [1] &quot;step =  7  lambda =  0.00108085811760332  loss:  3.1383006561497&quot;
## [1] &quot;step =  8  lambda =  0.00108085811760332  loss:  3.13829253876921&quot;
## [1] &quot;step =  9  lambda =  0.00108085811760332  loss:  3.13828468467802&quot;
## [1] &quot;step =  10  lambda =  0.00108085811760332  loss:  3.13827709463868&quot;
## [1] &quot;step =  11  lambda =  0.00108085811760332  loss:  3.13826976946801&quot;
## [1] &quot;step =  12  lambda =  0.00108085811760332  loss:  3.13826271001823&quot;
## [1] &quot;step =  13  lambda =  0.00108085811760332  loss:  3.13825591716258&quot;
## [1] &quot;step =  14  lambda =  0.00108085811760332  loss:  3.13824939178556&quot;
## [1] &quot;step =  15  lambda =  0.00108085811760332  loss:  3.13824313477641&quot;
## [1] &quot;step =  16  lambda =  0.00108085811760332  loss:  3.13823714702519&quot;
## [1] &quot;step =  17  lambda =  0.00108085811760332  loss:  3.13823142942045&quot;
## [1] &quot;step =  18  lambda =  0.00108085811760332  loss:  3.13822598284793&quot;
## [1] &quot;step =  19  lambda =  0.00108085811760332  loss:  3.13822080818997&quot;
## [1] &quot;step =  20  lambda =  0.00108085811760332  loss:  3.13821590632522&quot;
## [1] &quot;step =  21  lambda =  0.00108085811760332  loss:  3.13821127812873&quot;
## [1] &quot;step =  22  lambda =  0.00108085811760332  loss:  3.13820692447195&quot;
## [1] &quot;step =  23  lambda =  0.00108085811760332  loss:  3.13820284622292&quot;
## [1] &quot;step =  24  lambda =  0.00108085811760332  loss:  3.13819904424651&quot;
## [1] &quot;step =  25  lambda =  0.00108085811760332  loss:  3.13819551940453&quot;
## [1] &quot;step =  26  lambda =  0.00108085811760332  loss:  3.13819227255586&quot;
## [1] &quot;step =  27  lambda =  0.00108085811760332  loss:  3.13818930455671&quot;
## [1] &quot;step =  28  lambda =  0.00108085811760332  loss:  3.13818661626066&quot;
## [1] &quot;step =  29  lambda =  0.00108085811760332  loss:  3.13818420851874&quot;
## [1] &quot;step =  30  lambda =  0.00108085811760332  loss:  3.13818208217961&quot;
## [1] &quot;step =  31  lambda =  0.00108085811760332  loss:  3.13818023808959&quot;
## [1] &quot;step =  32  lambda =  0.00108085811760332  loss:  3.1381786770927&quot;
## [1] &quot;step =  33  lambda =  0.00108085811760332  loss:  3.13817740003072&quot;
## [1] &quot;step =  34  lambda =  0.00108085811760332  loss:  3.13817640774327&quot;
## [1] &quot;step =  35  lambda =  0.00108085811760332  loss:  3.1381757010678&quot;
## [1] &quot;step =  36  lambda =  0.00108085811760332  loss:  3.13817528083961&quot;
## [1] &quot;step =  37  lambda =  0.00108085811760332  loss:  3.13817514789188&quot;
## [1] &quot;step =  38  lambda =  0.00108085811760332  loss:  3.13817530305572&quot;
## [1] &quot;step =  1  lambda =  0.0010701033996396  loss:  3.11855711056496&quot;
## [1] &quot;step =  2  lambda =  0.0010701033996396  loss:  3.11854778524197&quot;
## [1] &quot;step =  3  lambda =  0.0010701033996396  loss:  3.1185387528445&quot;
## [1] &quot;step =  4  lambda =  0.0010701033996396  loss:  3.11853001490681&quot;
## [1] &quot;step =  5  lambda =  0.0010701033996396  loss:  3.11852157261383&quot;
## [1] &quot;step =  6  lambda =  0.0010701033996396  loss:  3.11851342664566&quot;
## [1] &quot;step =  7  lambda =  0.0010701033996396  loss:  3.11850557752716&quot;
## [1] &quot;step =  8  lambda =  0.0010701033996396  loss:  3.11849802579431&quot;
## [1] &quot;step =  9  lambda =  0.0010701033996396  loss:  3.11849077204321&quot;
## [1] &quot;step =  10  lambda =  0.0010701033996396  loss:  3.11848381693134&quot;
## [1] &quot;step =  11  lambda =  0.0010701033996396  loss:  3.11847716116421&quot;
## [1] &quot;step =  12  lambda =  0.0010701033996396  loss:  3.11847080548038&quot;
## [1] &quot;step =  13  lambda =  0.0010701033996396  loss:  3.11846475063913&quot;
## [1] &quot;step =  14  lambda =  0.0010701033996396  loss:  3.11845899741145&quot;
## [1] &quot;step =  15  lambda =  0.0010701033996396  loss:  3.11845354657402&quot;
## [1] &quot;step =  16  lambda =  0.0010701033996396  loss:  3.11844839890537&quot;
## [1] &quot;step =  17  lambda =  0.0010701033996396  loss:  3.11844355518345&quot;
## [1] &quot;step =  18  lambda =  0.0010701033996396  loss:  3.1184390161844&quot;
## [1] &quot;step =  19  lambda =  0.0010701033996396  loss:  3.11843478268176&quot;
## [1] &quot;step =  20  lambda =  0.0010701033996396  loss:  3.11843085544624&quot;
## [1] &quot;step =  21  lambda =  0.0010701033996396  loss:  3.11842723524554&quot;
## [1] &quot;step =  22  lambda =  0.0010701033996396  loss:  3.11842392284448&quot;
## [1] &quot;step =  23  lambda =  0.0010701033996396  loss:  3.11842091900504&quot;
## [1] &quot;step =  24  lambda =  0.0010701033996396  loss:  3.11841822448652&quot;
## [1] &quot;step =  25  lambda =  0.0010701033996396  loss:  3.11841584004567&quot;
## [1] &quot;step =  26  lambda =  0.0010701033996396  loss:  3.11841376643678&quot;
## [1] &quot;step =  27  lambda =  0.0010701033996396  loss:  3.11841200441185&quot;
## [1] &quot;step =  28  lambda =  0.0010701033996396  loss:  3.11841055472065&quot;
## [1] &quot;step =  29  lambda =  0.0010701033996396  loss:  3.11840941811079&quot;
## [1] &quot;step =  30  lambda =  0.0010701033996396  loss:  3.1184085953278&quot;
## [1] &quot;step =  31  lambda =  0.0010701033996396  loss:  3.11840808711521&quot;
## [1] &quot;step =  32  lambda =  0.0010701033996396  loss:  3.11840789421457&quot;
## [1] &quot;step =  33  lambda =  0.0010701033996396  loss:  3.11840801736547&quot;
## [1] &quot;step =  1  lambda =  0.00105945569290761  loss:  3.09867543802705&quot;
## [1] &quot;step =  2  lambda =  0.00105945569290761  loss:  3.09866628926946&quot;
## [1] &quot;step =  3  lambda =  0.00105945569290761  loss:  3.09865746010321&quot;
## [1] &quot;step =  4  lambda =  0.00105945569290761  loss:  3.09864895228315&quot;
## [1] &quot;step =  5  lambda =  0.00105945569290761  loss:  3.09864076716391&quot;
## [1] &quot;step =  6  lambda =  0.00105945569290761  loss:  3.09863290548889&quot;
## [1] &quot;step =  7  lambda =  0.00105945569290761  loss:  3.09862536777606&quot;
## [1] &quot;step =  8  lambda =  0.00105945569290761  loss:  3.09861815451348&quot;
## [1] &quot;step =  9  lambda =  0.00105945569290761  loss:  3.09861126622673&quot;
## [1] &quot;step =  10  lambda =  0.00105945569290761  loss:  3.09860470349115&quot;
## [1] &quot;step =  11  lambda =  0.00105945569290761  loss:  3.09859846692461&quot;
## [1] &quot;step =  12  lambda =  0.00105945569290761  loss:  3.09859255717569&quot;
## [1] &quot;step =  13  lambda =  0.00105945569290761  loss:  3.09858697491307&quot;
## [1] &quot;step =  14  lambda =  0.00105945569290761  loss:  3.09858172081725&quot;
## [1] &quot;step =  15  lambda =  0.00105945569290761  loss:  3.09857679557494&quot;
## [1] &quot;step =  16  lambda =  0.00105945569290761  loss:  3.09857219987528&quot;
## [1] &quot;step =  17  lambda =  0.00105945569290761  loss:  3.09856793440745&quot;
## [1] &quot;step =  18  lambda =  0.00105945569290761  loss:  3.09856399985939&quot;
## [1] &quot;step =  19  lambda =  0.00105945569290761  loss:  3.098560396917&quot;
## [1] &quot;step =  20  lambda =  0.00105945569290761  loss:  3.09855712626376&quot;
## [1] &quot;step =  21  lambda =  0.00105945569290761  loss:  3.09855418858064&quot;
## [1] &quot;step =  22  lambda =  0.00105945569290761  loss:  3.09855158454605&quot;
## [1] &quot;step =  23  lambda =  0.00105945569290761  loss:  3.09854931483593&quot;
## [1] &quot;step =  24  lambda =  0.00105945569290761  loss:  3.09854738012382&quot;
## [1] &quot;step =  25  lambda =  0.00105945569290761  loss:  3.098545781081&quot;
## [1] &quot;step =  26  lambda =  0.00105945569290761  loss:  3.09854451837656&quot;
## [1] &quot;step =  27  lambda =  0.00105945569290761  loss:  3.09854359267751&quot;
## [1] &quot;step =  28  lambda =  0.00105945569290761  loss:  3.09854300464883&quot;
## [1] &quot;step =  29  lambda =  0.00105945569290761  loss:  3.09854275495359&quot;
## [1] &quot;step =  30  lambda =  0.00105945569290761  loss:  3.09854284425292&quot;
## [1] &quot;step =  1  lambda =  0.00104891393262779  loss:  3.07873098842856&quot;
## [1] &quot;step =  2  lambda =  0.00104891393262779  loss:  3.07872199374139&quot;
## [1] &quot;step =  3  lambda =  0.00104891393262779  loss:  3.07871334052445&quot;
## [1] &quot;step =  4  lambda =  0.00104891393262779  loss:  3.07870503070285&quot;
## [1] &quot;step =  5  lambda =  0.00104891393262779  loss:  3.07869706576818&quot;
## [1] &quot;step =  6  lambda =  0.00104891393262779  loss:  3.07868944651541&quot;
## [1] &quot;step =  7  lambda =  0.00104891393262779  loss:  3.0786821734563&quot;
## [1] &quot;step =  8  lambda =  0.00104891393262779  loss:  3.07867524703802&quot;
## [1] &quot;step =  9  lambda =  0.00104891393262779  loss:  3.07866866772601&quot;
## [1] &quot;step =  10  lambda =  0.00104891393262779  loss:  3.07866243602532&quot;
## [1] &quot;step =  11  lambda =  0.00104891393262779  loss:  3.07865655247856&quot;
## [1] &quot;step =  12  lambda =  0.00104891393262779  loss:  3.07865101765691&quot;
## [1] &quot;step =  13  lambda =  0.00104891393262779  loss:  3.07864583215091&quot;
## [1] &quot;step =  14  lambda =  0.00104891393262779  loss:  3.07864099656299&quot;
## [1] &quot;step =  15  lambda =  0.00104891393262779  loss:  3.0786365115021&quot;
## [1] &quot;step =  16  lambda =  0.00104891393262779  loss:  3.07863237758002&quot;
## [1] &quot;step =  17  lambda =  0.00104891393262779  loss:  3.07862859540912&quot;
## [1] &quot;step =  18  lambda =  0.00104891393262779  loss:  3.07862516560095&quot;
## [1] &quot;step =  19  lambda =  0.00104891393262779  loss:  3.0786220887654&quot;
## [1] &quot;step =  20  lambda =  0.00104891393262779  loss:  3.07861936551039&quot;
## [1] &quot;step =  21  lambda =  0.00104891393262779  loss:  3.07861699644159&quot;
## [1] &quot;step =  22  lambda =  0.00104891393262779  loss:  3.07861498216245&quot;
## [1] &quot;step =  23  lambda =  0.00104891393262779  loss:  3.0786133232742&quot;
## [1] &quot;step =  24  lambda =  0.00104891393262779  loss:  3.07861202037594&quot;
## [1] &quot;step =  25  lambda =  0.00104891393262779  loss:  3.0786110740647&quot;
## [1] &quot;step =  26  lambda =  0.00104891393262779  loss:  3.07861048493555&quot;
## [1] &quot;step =  27  lambda =  0.00104891393262779  loss:  3.07861025358164&quot;
## [1] &quot;step =  28  lambda =  0.00104891393262779  loss:  3.0786103805943&quot;
## [1] &quot;step =  1  lambda =  0.00103847706461533  loss:  3.0587433007342&quot;
## [1] &quot;step =  2  lambda =  0.00103847706461533  loss:  3.05873451688384&quot;
## [1] &quot;step =  3  lambda =  0.00103847706461533  loss:  3.05872609295912&quot;
## [1] &quot;step =  4  lambda =  0.00103847706461533  loss:  3.05871803101989&quot;
## [1] &quot;step =  5  lambda =  0.00103847706461533  loss:  3.05871033267097&quot;
## [1] &quot;step =  6  lambda =  0.00103847706461533  loss:  3.05870299875012&quot;
## [1] &quot;step =  7  lambda =  0.00103847706461533  loss:  3.05869602976278&quot;
## [1] &quot;step =  8  lambda =  0.00103847706461533  loss:  3.05868942611996&quot;
## [1] &quot;step =  9  lambda =  0.00103847706461533  loss:  3.05868318823397&quot;
## [1] &quot;step =  10  lambda =  0.00103847706461533  loss:  3.05867731654771&quot;
## [1] &quot;step =  11  lambda =  0.00103847706461533  loss:  3.05867181153711&quot;
## [1] &quot;step =  12  lambda =  0.00103847706461533  loss:  3.0586666737047&quot;
## [1] &quot;step =  13  lambda =  0.00103847706461533  loss:  3.05866190357166&quot;
## [1] &quot;step =  14  lambda =  0.00103847706461533  loss:  3.058657501671&quot;
## [1] &quot;step =  15  lambda =  0.00103847706461533  loss:  3.05865346854249&quot;
## [1] &quot;step =  16  lambda =  0.00103847706461533  loss:  3.05864980472913&quot;
## [1] &quot;step =  17  lambda =  0.00103847706461533  loss:  3.0586465107749&quot;
## [1] &quot;step =  18  lambda =  0.00103847706461533  loss:  3.0586435872233&quot;
## [1] &quot;step =  19  lambda =  0.00103847706461533  loss:  3.05864103461662&quot;
## [1] &quot;step =  20  lambda =  0.00103847706461533  loss:  3.0586388534954&quot;
## [1] &quot;step =  21  lambda =  0.00103847706461533  loss:  3.05863704439829&quot;
## [1] &quot;step =  22  lambda =  0.00103847706461533  loss:  3.05863560786196&quot;
## [1] &quot;step =  23  lambda =  0.00103847706461533  loss:  3.0586345444211&quot;
## [1] &quot;step =  24  lambda =  0.00103847706461533  loss:  3.0586338546085&quot;
## [1] &quot;step =  25  lambda =  0.00103847706461533  loss:  3.05863353895503&quot;
## [1] &quot;step =  26  lambda =  0.00103847706461533  loss:  3.05863359798984&quot;
## [1] &quot;step =  1  lambda =  0.00102814404517473  loss:  3.03873422099555&quot;
## [1] &quot;step =  2  lambda =  0.00102814404517473  loss:  3.03872552939532&quot;
## [1] &quot;step =  3  lambda =  0.00102814404517473  loss:  3.03871721325546&quot;
## [1] &quot;step =  4  lambda =  0.00102814404517473  loss:  3.03870927474087&quot;
## [1] &quot;step =  5  lambda =  0.00102814404517473  loss:  3.0387017155487&quot;
## [1] &quot;step =  6  lambda =  0.00102814404517473  loss:  3.03869453655144&quot;
## [1] &quot;step =  7  lambda =  0.00102814404517473  loss:  3.03868773824789&quot;
## [1] &quot;step =  8  lambda =  0.00102814404517473  loss:  3.03868132101676&quot;
## [1] &quot;step =  9  lambda =  0.00102814404517473  loss:  3.0386752852233&quot;
## [1] &quot;step =  10  lambda =  0.00102814404517473  loss:  3.03866963125537&quot;
## [1] &quot;step =  11  lambda =  0.00102814404517473  loss:  3.03866435952984&quot;
## [1] &quot;step =  12  lambda =  0.00102814404517473  loss:  3.03865947048831&quot;
## [1] &quot;step =  13  lambda =  0.00102814404517473  loss:  3.03865496459037&quot;
## [1] &quot;step =  14  lambda =  0.00102814404517473  loss:  3.03865084230735&quot;
## [1] &quot;step =  15  lambda =  0.00102814404517473  loss:  3.03864710411754&quot;
## [1] &quot;step =  16  lambda =  0.00102814404517473  loss:  3.03864375050277&quot;
## [1] &quot;step =  17  lambda =  0.00102814404517473  loss:  3.0386407819462&quot;
## [1] &quot;step =  18  lambda =  0.00102814404517473  loss:  3.03863819893088&quot;
## [1] &quot;step =  19  lambda =  0.00102814404517473  loss:  3.0386360019389&quot;
## [1] &quot;step =  20  lambda =  0.00102814404517473  loss:  3.03863419145098&quot;
## [1] &quot;step =  21  lambda =  0.00102814404517473  loss:  3.03863276794616&quot;
## [1] &quot;step =  22  lambda =  0.00102814404517473  loss:  3.03863173190176&quot;
## [1] &quot;step =  23  lambda =  0.00102814404517473  loss:  3.03863108379335&quot;
## [1] &quot;step =  24  lambda =  0.00102814404517473  loss:  3.03863082409478&quot;
## [1] &quot;step =  25  lambda =  0.00102814404517473  loss:  3.03863095327822&quot;
## [1] &quot;step =  1  lambda =  0.00101791384099544  loss:  3.01871214392471&quot;
## [1] &quot;step =  2  lambda =  0.00101791384099544  loss:  3.01870367383761&quot;
## [1] &quot;step =  3  lambda =  0.00101791384099544  loss:  3.01869559270413&quot;
## [1] &quot;step =  4  lambda =  0.00101791384099544  loss:  3.01868790277463&quot;
## [1] &quot;step =  5  lambda =  0.00101791384099544  loss:  3.01868060582521&quot;
## [1] &quot;step =  6  lambda =  0.00101791384099544  loss:  3.01867370275829&quot;
## [1] &quot;step =  7  lambda =  0.00101791384099544  loss:  3.01866719406641&quot;
## [1] &quot;step =  8  lambda =  0.00101791384099544  loss:  3.01866108009911&quot;
## [1] &quot;step =  9  lambda =  0.00101791384099544  loss:  3.01865536117923&quot;
## [1] &quot;step =  10  lambda =  0.00101791384099544  loss:  3.01865003764486&quot;
## [1] &quot;step =  11  lambda =  0.00101791384099544  loss:  3.01864510985936&quot;
## [1] &quot;step =  12  lambda =  0.00101791384099544  loss:  3.01864057820911&quot;
## [1] &quot;step =  13  lambda =  0.00101791384099544  loss:  3.0186364430978&quot;
## [1] &quot;step =  14  lambda =  0.00101791384099544  loss:  3.01863270494074&quot;
## [1] &quot;step =  15  lambda =  0.00101791384099544  loss:  3.01862936416036&quot;
## [1] &quot;step =  16  lambda =  0.00101791384099544  loss:  3.01862642118292&quot;
## [1] &quot;step =  17  lambda =  0.00101791384099544  loss:  3.01862387643629&quot;
## [1] &quot;step =  18  lambda =  0.00101791384099544  loss:  3.01862173034852&quot;
## [1] &quot;step =  19  lambda =  0.00101791384099544  loss:  3.01861998334704&quot;
## [1] &quot;step =  20  lambda =  0.00101791384099544  loss:  3.01861863585809&quot;
## [1] &quot;step =  21  lambda =  0.00101791384099544  loss:  3.01861768830652&quot;
## [1] &quot;step =  22  lambda =  0.00101791384099544  loss:  3.0186171411157&quot;
## [1] &quot;step =  23  lambda =  0.00101791384099544  loss:  3.01861699470739&quot;
## [1] &quot;step =  24  lambda =  0.00101791384099544  loss:  3.01861724950183&quot;
## [1] &quot;step =  1  lambda =  0.00100778542904851  loss:  2.9986913221762&quot;
## [1] &quot;step =  2  lambda =  0.00100778542904851  loss:  2.99868312109917&quot;
## [1] &quot;step =  3  lambda =  0.00100778542904851  loss:  2.99867532067424&quot;
## [1] &quot;step =  4  lambda =  0.00100778542904851  loss:  2.99866792322027&quot;
## [1] &quot;step =  5  lambda =  0.00100778542904851  loss:  2.99866093058037&quot;
## [1] &quot;step =  6  lambda =  0.00100778542904851  loss:  2.9986543436824&quot;
## [1] &quot;step =  7  lambda =  0.00100778542904851  loss:  2.99864816301281&quot;
## [1] &quot;step =  8  lambda =  0.00100778542904851  loss:  2.99864238889476&quot;
## [1] &quot;step =  9  lambda =  0.00100778542904851  loss:  2.99863702161264&quot;
## [1] &quot;step =  10  lambda =  0.00100778542904851  loss:  2.99863206145949&quot;
## [1] &quot;step =  11  lambda =  0.00100778542904851  loss:  2.99862750875012&quot;
## [1] &quot;step =  12  lambda =  0.00100778542904851  loss:  2.99862336382071&quot;
## [1] &quot;step =  13  lambda =  0.00100778542904851  loss:  2.99861962702412&quot;
## [1] &quot;step =  14  lambda =  0.00100778542904851  loss:  2.9986162987247&quot;
## [1] &quot;step =  15  lambda =  0.00100778542904851  loss:  2.99861337929402&quot;
## [1] &quot;step =  16  lambda =  0.00100778542904851  loss:  2.9986108691077&quot;
## [1] &quot;step =  17  lambda =  0.00100778542904851  loss:  2.99860876854323&quot;
## [1] &quot;step =  18  lambda =  0.00100778542904851  loss:  2.99860707797859&quot;
## [1] &quot;step =  19  lambda =  0.00100778542904851  loss:  2.99860579779134&quot;
## [1] &quot;step =  20  lambda =  0.00100778542904851  loss:  2.99860492835814&quot;
## [1] &quot;step =  21  lambda =  0.00100778542904851  loss:  2.99860447005444&quot;
## [1] &quot;step =  22  lambda =  0.00100778542904851  loss:  2.99860442325439&quot;
## [1] &quot;step =  23  lambda =  0.00100778542904851  loss:  2.99860478833075&quot;
## [1] &quot;step =  1  lambda =  0.000997757796484312  loss:  2.97868354420682&quot;
## [1] &quot;step =  2  lambda =  0.000997757796484312  loss:  2.97867558963463&quot;
## [1] &quot;step =  3  lambda =  0.000997757796484312  loss:  2.97866804583619&quot;
## [1] &quot;step =  4  lambda =  0.000997757796484312  loss:  2.97866091518421&quot;
## [1] &quot;step =  5  lambda =  0.000997757796484312  loss:  2.97865419957795&quot;
## [1] &quot;step =  6  lambda =  0.000997757796484312  loss:  2.97864789996659&quot;
## [1] &quot;step =  7  lambda =  0.000997757796484312  loss:  2.97864201683047&quot;
## [1] &quot;step =  8  lambda =  0.000997757796484312  loss:  2.97863655046871&quot;
## [1] &quot;step =  9  lambda =  0.000997757796484312  loss:  2.97863150113085&quot;
## [1] &quot;step =  10  lambda =  0.000997757796484312  loss:  2.97862686906904&quot;
## [1] &quot;step =  11  lambda =  0.000997757796484312  loss:  2.97862265455401&quot;
## [1] &quot;step =  12  lambda =  0.000997757796484312  loss:  2.97861885787632&quot;
## [1] &quot;step =  13  lambda =  0.000997757796484312  loss:  2.97861547934256&quot;
## [1] &quot;step =  14  lambda =  0.000997757796484312  loss:  2.97861251927066&quot;
## [1] &quot;step =  15  lambda =  0.000997757796484312  loss:  2.97860997798587&quot;
## [1] &quot;step =  16  lambda =  0.000997757796484312  loss:  2.9786078558177&quot;
## [1] &quot;step =  17  lambda =  0.000997757796484312  loss:  2.97860615309774&quot;
## [1] &quot;step =  18  lambda =  0.000997757796484312  loss:  2.9786048701583&quot;
## [1] &quot;step =  19  lambda =  0.000997757796484312  loss:  2.97860400733154&quot;
## [1] &quot;step =  20  lambda =  0.000997757796484312  loss:  2.97860356494889&quot;
## [1] &quot;step =  21  lambda =  0.000997757796484312  loss:  2.97860354334083&quot;
## [1] &quot;step =  22  lambda =  0.000997757796484312  loss:  2.97860394283666&quot;
## [1] &quot;step =  1  lambda =  0.000987829940531229  loss:  2.95869870278913&quot;
## [1] &quot;step =  2  lambda =  0.000987829940531229  loss:  2.95869091237856&quot;
## [1] &quot;step =  3  lambda =  0.000987829940531229  loss:  2.95868354148053&quot;
## [1] &quot;step =  4  lambda =  0.000987829940531229  loss:  2.95867659250895&quot;
## [1] &quot;step =  5  lambda =  0.000987829940531229  loss:  2.9586700674095&quot;
## [1] &quot;step =  6  lambda =  0.000987829940531229  loss:  2.95866396714889&quot;
## [1] &quot;step =  7  lambda =  0.000987829940531229  loss:  2.95865829220131&quot;
## [1] &quot;step =  8  lambda =  0.000987829940531229  loss:  2.95865304284395&quot;
## [1] &quot;step =  9  lambda =  0.000987829940531229  loss:  2.95864821929475&quot;
## [1] &quot;step =  10  lambda =  0.000987829940531229  loss:  2.95864382176878&quot;
## [1] &quot;step =  11  lambda =  0.000987829940531229  loss:  2.95863985049676&quot;
## [1] &quot;step =  12  lambda =  0.000987829940531229  loss:  2.95863630572778&quot;
## [1] &quot;step =  13  lambda =  0.000987829940531229  loss:  2.95863318772639&quot;
## [1] &quot;step =  14  lambda =  0.000987829940531229  loss:  2.95863049676829&quot;
## [1] &quot;step =  15  lambda =  0.000987829940531229  loss:  2.95862823313658&quot;
## [1] &quot;step =  16  lambda =  0.000987829940531229  loss:  2.95862639711877&quot;
## [1] &quot;step =  17  lambda =  0.000987829940531229  loss:  2.95862498900468&quot;
## [1] &quot;step =  18  lambda =  0.000987829940531229  loss:  2.95862400908508&quot;
## [1] &quot;step =  19  lambda =  0.000987829940531229  loss:  2.95862345765077&quot;
## [1] &quot;step =  20  lambda =  0.000987829940531229  loss:  2.95862333499204&quot;
## [1] &quot;step =  21  lambda =  0.000987829940531229  loss:  2.9586236413984&quot;
## [1] &quot;step =  1  lambda =  0.000978000868395395  loss:  2.93874527525964&quot;
## [1] &quot;step =  2  lambda =  0.000978000868395395  loss:  2.9387375156355&quot;
## [1] &quot;step =  3  lambda =  0.000978000868395395  loss:  2.9387301830534&quot;
## [1] &quot;step =  4  lambda =  0.000978000868395395  loss:  2.9387232799576&quot;
## [1] &quot;step =  5  lambda =  0.000978000868395395  loss:  2.93871680833157&quot;
## [1] &quot;step =  6  lambda =  0.000978000868395395  loss:  2.93871076915606&quot;
## [1] &quot;step =  7  lambda =  0.000978000868395395  loss:  2.93870516289911&quot;
## [1] &quot;step =  8  lambda =  0.000978000868395395  loss:  2.93869998981781&quot;
## [1] &quot;step =  9  lambda =  0.000978000868395395  loss:  2.9386952501015&quot;
## [1] &quot;step =  10  lambda =  0.000978000868395395  loss:  2.93869094393165&quot;
## [1] &quot;step =  11  lambda =  0.000978000868395395  loss:  2.93868707150268&quot;
## [1] &quot;step =  12  lambda =  0.000978000868395395  loss:  2.93868363302608&quot;
## [1] &quot;step =  13  lambda =  0.000978000868395395  loss:  2.93868062872817&quot;
## [1] &quot;step =  14  lambda =  0.000978000868395395  loss:  2.93867805884633&quot;
## [1] &quot;step =  15  lambda =  0.000978000868395395  loss:  2.93867592362531&quot;
## [1] &quot;step =  16  lambda =  0.000978000868395395  loss:  2.93867422331444&quot;
## [1] &quot;step =  17  lambda =  0.000978000868395395  loss:  2.9386729581656&quot;
## [1] &quot;step =  18  lambda =  0.000978000868395395  loss:  2.93867212843174&quot;
## [1] &quot;step =  19  lambda =  0.000978000868395395  loss:  2.93867173436607&quot;
## [1] &quot;step =  20  lambda =  0.000978000868395395  loss:  2.93867177622151&quot;
## [1] &quot;step =  1  lambda =  0.000968269597161403  loss:  2.91883072889235&quot;
## [1] &quot;step =  2  lambda =  0.000968269597161403  loss:  2.91882282327044&quot;
## [1] &quot;step =  3  lambda =  0.000968269597161403  loss:  2.91881535116617&quot;
## [1] &quot;step =  4  lambda =  0.000968269597161403  loss:  2.91880831504485&quot;
## [1] &quot;step =  5  lambda =  0.000968269597161403  loss:  2.91880171692&quot;
## [1] &quot;step =  6  lambda =  0.000968269597161403  loss:  2.91879555778328&quot;
## [1] &quot;step =  7  lambda =  0.000968269597161403  loss:  2.91878983809647&quot;
## [1] &quot;step =  8  lambda =  0.000968269597161403  loss:  2.91878455809833&quot;
## [1] &quot;step =  9  lambda =  0.000968269597161403  loss:  2.91877971795224&quot;
## [1] &quot;step =  10  lambda =  0.000968269597161403  loss:  2.91877531780921&quot;
## [1] &quot;step =  11  lambda =  0.000968269597161403  loss:  2.91877135783083&quot;
## [1] &quot;step =  12  lambda =  0.000968269597161403  loss:  2.91876783819448&quot;
## [1] &quot;step =  13  lambda =  0.000968269597161403  loss:  2.91876475909182&quot;
## [1] &quot;step =  14  lambda =  0.000968269597161403  loss:  2.91876212072536&quot;
## [1] &quot;step =  15  lambda =  0.000968269597161403  loss:  2.9187599233051&quot;
## [1] &quot;step =  16  lambda =  0.000968269597161403  loss:  2.9187581670457&quot;
## [1] &quot;step =  17  lambda =  0.000968269597161403  loss:  2.91875685216451&quot;
## [1] &quot;step =  18  lambda =  0.000968269597161403  loss:  2.91875597888018&quot;
## [1] &quot;step =  19  lambda =  0.000968269597161403  loss:  2.91875554741178&quot;
## [1] &quot;step =  20  lambda =  0.000968269597161403  loss:  2.91875555797823&quot;
## [1] &quot;step =  1  lambda =  0.000958635153694021  loss:  2.89895354929471&quot;
## [1] &quot;step =  2  lambda =  0.000958635153694021  loss:  2.89894572772254&quot;
## [1] &quot;step =  3  lambda =  0.000958635153694021  loss:  2.89893834542931&quot;
## [1] &quot;step =  4  lambda =  0.000958635153694021  loss:  2.8989314048963&quot;
## [1] &quot;step =  5  lambda =  0.000958635153694021  loss:  2.89892490816365&quot;
## [1] &quot;step =  6  lambda =  0.000958635153694021  loss:  2.898918856233&quot;
## [1] &quot;step =  7  lambda =  0.000958635153694021  loss:  2.89891324956056&quot;
## [1] &quot;step =  8  lambda =  0.000958635153694021  loss:  2.89890808836832&quot;
## [1] &quot;step =  9  lambda =  0.000958635153694021  loss:  2.89890337279571&quot;
## [1] &quot;step =  10  lambda =  0.000958635153694021  loss:  2.89889910296564&quot;
## [1] &quot;step =  11  lambda =  0.000958635153694021  loss:  2.89889527900917&quot;
## [1] &quot;step =  12  lambda =  0.000958635153694021  loss:  2.89889190107201&quot;
## [1] &quot;step =  13  lambda =  0.000958635153694021  loss:  2.89888896931357&quot;
## [1] &quot;step =  14  lambda =  0.000958635153694021  loss:  2.89888648390397&quot;
## [1] &quot;step =  15  lambda =  0.000958635153694021  loss:  2.89888444502078&quot;
## [1] &quot;step =  16  lambda =  0.000958635153694021  loss:  2.89888285284636&quot;
## [1] &quot;step =  17  lambda =  0.000958635153694021  loss:  2.89888170756589&quot;
## [1] &quot;step =  18  lambda =  0.000958635153694021  loss:  2.89888100936603&quot;
## [1] &quot;step =  19  lambda =  0.000958635153694021  loss:  2.89888075843399&quot;
## [1] &quot;step =  20  lambda =  0.000958635153694021  loss:  2.89888095495701&quot;
## [1] &quot;step =  1  lambda =  0.000949096574540873  loss:  2.87911964231688&quot;
## [1] &quot;step =  2  lambda =  0.000949096574540873  loss:  2.87911211897641&quot;
## [1] &quot;step =  3  lambda =  0.000949096574540873  loss:  2.87910504000912&quot;
## [1] &quot;step =  4  lambda =  0.000949096574540873  loss:  2.87909840790758&quot;
## [1] &quot;step =  5  lambda =  0.000949096574540873  loss:  2.87909222473516&quot;
## [1] &quot;step =  6  lambda =  0.000949096574540873  loss:  2.87908649150253&quot;
## [1] &quot;step =  7  lambda =  0.000949096574540873  loss:  2.87908120866079&quot;
## [1] &quot;step =  8  lambda =  0.000949096574540873  loss:  2.87907637641649&quot;
## [1] &quot;step =  9  lambda =  0.000949096574540873  loss:  2.87907199488693&quot;
## [1] &quot;step =  10  lambda =  0.000949096574540873  loss:  2.87906806416881&quot;
## [1] &quot;step =  11  lambda =  0.000949096574540873  loss:  2.87906458436477&quot;
## [1] &quot;step =  12  lambda =  0.000949096574540873  loss:  2.87906155559088&quot;
## [1] &quot;step =  13  lambda =  0.000949096574540873  loss:  2.87905897797638&quot;
## [1] &quot;step =  14  lambda =  0.000949096574540873  loss:  2.87905685166103&quot;
## [1] &quot;step =  15  lambda =  0.000949096574540873  loss:  2.87905517679196&quot;
## [1] &quot;step =  16  lambda =  0.000949096574540873  loss:  2.87905395352124&quot;
## [1] &quot;step =  17  lambda =  0.000949096574540873  loss:  2.87905318200383&quot;
## [1] &quot;step =  18  lambda =  0.000949096574540873  loss:  2.87905286239632&quot;
## [1] &quot;step =  19  lambda =  0.000949096574540873  loss:  2.87905299485598&quot;
## [1] &quot;step =  1  lambda =  0.000939652905836096  loss:  2.85934208733745&quot;
## [1] &quot;step =  2  lambda =  0.000939652905836096  loss:  2.85933460784961&quot;
## [1] &quot;step =  3  lambda =  0.000939652905836096  loss:  2.85932757706142&quot;
## [1] &quot;step =  4  lambda =  0.000939652905836096  loss:  2.85932099746952&quot;
## [1] &quot;step =  5  lambda =  0.000939652905836096  loss:  2.85931487115391&quot;
## [1] &quot;step =  6  lambda =  0.000939652905836096  loss:  2.85930919913118&quot;
## [1] &quot;step =  7  lambda =  0.000939652905836096  loss:  2.85930398184677&quot;
## [1] &quot;step =  8  lambda =  0.000939652905836096  loss:  2.85929921949261&quot;
## [1] &quot;step =  9  lambda =  0.000939652905836096  loss:  2.8592949121655&quot;
## [1] &quot;step =  10  lambda =  0.000939652905836096  loss:  2.85929105993805&quot;
## [1] &quot;step =  11  lambda =  0.000939652905836096  loss:  2.85928766288677&quot;
## [1] &quot;step =  12  lambda =  0.000939652905836096  loss:  2.85928472110045&quot;
## [1] &quot;step =  13  lambda =  0.000939652905836096  loss:  2.8592822346806&quot;
## [1] &quot;step =  14  lambda =  0.000939652905836096  loss:  2.85928020373895&quot;
## [1] &quot;step =  15  lambda =  0.000939652905836096  loss:  2.85927862839469&quot;
## [1] &quot;step =  16  lambda =  0.000939652905836096  loss:  2.85927750877192&quot;
## [1] &quot;step =  17  lambda =  0.000939652905836096  loss:  2.85927684499779&quot;
## [1] &quot;step =  18  lambda =  0.000939652905836096  loss:  2.85927663720119&quot;
## [1] &quot;step =  19  lambda =  0.000939652905836096  loss:  2.8592768855118&quot;
## [1] &quot;step =  1  lambda =  0.000930303203204949  loss:  2.83961771815855&quot;
## [1] &quot;step =  2  lambda =  0.000930303203204949  loss:  2.83961045980239&quot;
## [1] &quot;step =  3  lambda =  0.000930303203204949  loss:  2.83960365392212&quot;
## [1] &quot;step =  4  lambda =  0.000930303203204949  loss:  2.83959730301475&quot;
## [1] &quot;step =  5  lambda =  0.000930303203204949  loss:  2.83959140917403&quot;
## [1] &quot;step =  6  lambda =  0.000930303203204949  loss:  2.83958597342155&quot;
## [1] &quot;step =  7  lambda =  0.000930303203204949  loss:  2.83958099619726&quot;
## [1] &quot;step =  8  lambda =  0.000930303203204949  loss:  2.83957647767951&quot;
## [1] &quot;step =  9  lambda =  0.000930303203204949  loss:  2.83957241794598&quot;
## [1] &quot;step =  10  lambda =  0.000930303203204949  loss:  2.83956881704677&quot;
## [1] &quot;step =  11  lambda =  0.000930303203204949  loss:  2.83956567503391&quot;
## [1] &quot;step =  12  lambda =  0.000930303203204949  loss:  2.83956299197067&quot;
## [1] &quot;step =  13  lambda =  0.000930303203204949  loss:  2.83956076793252&quot;
## [1] &quot;step =  14  lambda =  0.000930303203204949  loss:  2.83955900300499&quot;
## [1] &quot;step =  15  lambda =  0.000930303203204949  loss:  2.83955769728097&quot;
## [1] &quot;step =  16  lambda =  0.000930303203204949  loss:  2.83955685085836&quot;
## [1] &quot;step =  17  lambda =  0.000930303203204949  loss:  2.8395564638382&quot;
## [1] &quot;step =  18  lambda =  0.000930303203204949  loss:  2.83955653632335&quot;
## [1] &quot;step =  1  lambda =  0.000921046531669378  loss:  2.81995830666002&quot;
## [1] &quot;step =  2  lambda =  0.000921046531669378  loss:  2.81995097396186&quot;
## [1] &quot;step =  3  lambda =  0.000921046531669378  loss:  2.81994409690739&quot;
## [1] &quot;step =  4  lambda =  0.000921046531669378  loss:  2.81993767798821&quot;
## [1] &quot;step =  5  lambda =  0.000921046531669378  loss:  2.81993171930619&quot;
## [1] &quot;step =  6  lambda =  0.000921046531669378  loss:  2.81992622188517&quot;
## [1] &quot;step =  7  lambda =  0.000921046531669378  loss:  2.81992118615906&quot;
## [1] &quot;step =  8  lambda =  0.000921046531669378  loss:  2.81991661229327&quot;
## [1] &quot;step =  9  lambda =  0.000921046531669378  loss:  2.81991250034779&quot;
## [1] &quot;step =  10  lambda =  0.000921046531669378  loss:  2.81990885035202&quot;
## [1] &quot;step =  11  lambda =  0.000921046531669378  loss:  2.8199056623355&quot;
## [1] &quot;step =  12  lambda =  0.000921046531669378  loss:  2.8199029363381&quot;
## [1] &quot;step =  13  lambda =  0.000921046531669378  loss:  2.81990067241134&quot;
## [1] &quot;step =  14  lambda =  0.000921046531669378  loss:  2.81989887061664&quot;
## [1] &quot;step =  15  lambda =  0.000921046531669378  loss:  2.81989753102277&quot;
## [1] &quot;step =  16  lambda =  0.000921046531669378  loss:  2.81989665370351&quot;
## [1] &quot;step =  17  lambda =  0.000921046531669378  loss:  2.81989623873589&quot;
## [1] &quot;step =  18  lambda =  0.000921046531669378  loss:  2.81989628619888&quot;
## [1] &quot;step =  1  lambda =  0.000911881965554516  loss:  2.80036003766486&quot;
## [1] &quot;step =  2  lambda =  0.000911881965554516  loss:  2.80035277914431&quot;
## [1] &quot;step =  3  lambda =  0.000911881965554516  loss:  2.8003459789815&quot;
## [1] &quot;step =  4  lambda =  0.000911881965554516  loss:  2.80033963966011&quot;
## [1] &quot;step =  5  lambda =  0.000911881965554516  loss:  2.80033376328808&quot;
## [1] &quot;step =  6  lambda =  0.000911881965554516  loss:  2.80032835089088&quot;
## [1] &quot;step =  7  lambda =  0.000911881965554516  loss:  2.80032340289679&quot;
## [1] &quot;step =  8  lambda =  0.000911881965554516  loss:  2.80031891945925&quot;
## [1] &quot;step =  9  lambda =  0.000911881965554516  loss:  2.80031490062182&quot;
## [1] &quot;step =  10  lambda =  0.000911881965554516  loss:  2.80031134639465&quot;
## [1] &quot;step =  11  lambda =  0.000911881965554516  loss:  2.80030825678636&quot;
## [1] &quot;step =  12  lambda =  0.000911881965554516  loss:  2.80030563181493&quot;
## [1] &quot;step =  13  lambda =  0.000911881965554516  loss:  2.80030347150957&quot;
## [1] &quot;step =  14  lambda =  0.000911881965554516  loss:  2.80030177590917&quot;
## [1] &quot;step =  15  lambda =  0.000911881965554516  loss:  2.8003005450599&quot;
## [1] &quot;step =  16  lambda =  0.000911881965554516  loss:  2.80029977901303&quot;
## [1] &quot;step =  17  lambda =  0.000911881965554516  loss:  2.80029947782314&quot;
## [1] &quot;step =  18  lambda =  0.000911881965554516  loss:  2.80029964154681&quot;
## [1] &quot;step =  1  lambda =  0.000902808588396114  loss:  2.78082637726525&quot;
## [1] &quot;step =  2  lambda =  0.000902808588396114  loss:  2.78081933201188&quot;
## [1] &quot;step =  3  lambda =  0.000902808588396114  loss:  2.7808127474051&quot;
## [1] &quot;step =  4  lambda =  0.000902808588396114  loss:  2.78080662591817&quot;
## [1] &quot;step =  5  lambda =  0.000902808588396114  loss:  2.78080096966304&quot;
## [1] &quot;step =  6  lambda =  0.000902808588396114  loss:  2.78079577966618&quot;
## [1] &quot;step =  7  lambda =  0.000902808588396114  loss:  2.78079105635051&quot;
## [1] &quot;step =  8  lambda =  0.000902808588396114  loss:  2.78078679985838&quot;
## [1] &quot;step =  9  lambda =  0.000902808588396114  loss:  2.7807830102181&quot;
## [1] &quot;step =  10  lambda =  0.000902808588396114  loss:  2.78077968742191&quot;
## [1] &quot;step =  11  lambda =  0.000902808588396114  loss:  2.78077683145886&quot;
## [1] &quot;step =  12  lambda =  0.000902808588396114  loss:  2.78077444232652&quot;
## [1] &quot;step =  13  lambda =  0.000902808588396114  loss:  2.78077252003314&quot;
## [1] &quot;step =  14  lambda =  0.000902808588396114  loss:  2.78077106459649&quot;
## [1] &quot;step =  15  lambda =  0.000902808588396114  loss:  2.78077007604159&quot;
## [1] &quot;step =  16  lambda =  0.000902808588396114  loss:  2.78076955439851&quot;
## [1] &quot;step =  17  lambda =  0.000902808588396114  loss:  2.78076949970074&quot;
## [1] &quot;step =  18  lambda =  0.000902808588396114  loss:  2.78076991198383&quot;
## [1] &quot;step =  1  lambda =  0.000893825492848894  loss:  2.76136059615961&quot;
## [1] &quot;step =  2  lambda =  0.000893825492848894  loss:  2.76135389436365&quot;
## [1] &quot;step =  3  lambda =  0.000893825492848894  loss:  2.76134765510353&quot;
## [1] &quot;step =  4  lambda =  0.000893825492848894  loss:  2.76134188083959&quot;
## [1] &quot;step =  5  lambda =  0.000893825492848894  loss:  2.76133657368569&quot;
## [1] &quot;step =  6  lambda =  0.000893825492848894  loss:  2.76133173466858&quot;
## [1] &quot;step =  7  lambda =  0.000893825492848894  loss:  2.76132736420592&quot;
## [1] &quot;step =  8  lambda =  0.000893825492848894  loss:  2.76132346242968&quot;
## [1] &quot;step =  9  lambda =  0.000893825492848894  loss:  2.76132002935389&quot;
## [1] &quot;step =  10  lambda =  0.000893825492848894  loss:  2.76131706495398&quot;
## [1] &quot;step =  11  lambda =  0.000893825492848894  loss:  2.76131456920066&quot;
## [1] &quot;step =  12  lambda =  0.000893825492848894  loss:  2.76131254207223&quot;
## [1] &quot;step =  13  lambda =  0.000893825492848894  loss:  2.76131098355728&quot;
## [1] &quot;step =  14  lambda =  0.000893825492848894  loss:  2.76130989365365&quot;
## [1] &quot;step =  15  lambda =  0.000893825492848894  loss:  2.76130927236634&quot;
## [1] &quot;step =  16  lambda =  0.000893825492848894  loss:  2.7613091197055&quot;
## [1] &quot;step =  17  lambda =  0.000893825492848894  loss:  2.76130943568465&quot;
## [1] &quot;step =  1  lambda =  0.000884931780595815  loss:  2.74197253139687&quot;
## [1] &quot;step =  2  lambda =  0.000884931780595815  loss:  2.74196582516204&quot;
## [1] &quot;step =  3  lambda =  0.000884931780595815  loss:  2.74195958296756&quot;
## [1] &quot;step =  4  lambda =  0.000884931780595815  loss:  2.74195380725646&quot;
## [1] &quot;step =  5  lambda =  0.000884931780595815  loss:  2.74194850013979&quot;
## [1] &quot;step =  6  lambda =  0.000884931780595815  loss:  2.74194366264196&quot;
## [1] &quot;step =  7  lambda =  0.000884931780595815  loss:  2.74193929517449&quot;
## [1] &quot;step =  8  lambda =  0.000884931780595815  loss:  2.74193539785903&quot;
## [1] &quot;step =  9  lambda =  0.000884931780595815  loss:  2.74193197069609&quot;
## [1] &quot;step =  10  lambda =  0.000884931780595815  loss:  2.74192901364535&quot;
## [1] &quot;step =  11  lambda =  0.000884931780595815  loss:  2.74192652666043&quot;
## [1] &quot;step =  12  lambda =  0.000884931780595815  loss:  2.74192450970175&quot;
## [1] &quot;step =  13  lambda =  0.000884931780595815  loss:  2.74192296273956&quot;
## [1] &quot;step =  14  lambda =  0.000884931780595815  loss:  2.7419218857532&quot;
## [1] &quot;step =  15  lambda =  0.000884931780595815  loss:  2.74192127872911&quot;
## [1] &quot;step =  16  lambda =  0.000884931780595815  loss:  2.74192114165884&quot;
## [1] &quot;step =  17  lambda =  0.000884931780595815  loss:  2.74192147453741&quot;
## [1] &quot;step =  1  lambda =  0.000876126562258242  loss:  2.722657692377&quot;
## [1] &quot;step =  2  lambda =  0.000876126562258242  loss:  2.72265109305327&quot;
## [1] &quot;step =  3  lambda =  0.000876126562258242  loss:  2.72264495893961&quot;
## [1] &quot;step =  4  lambda =  0.000876126562258242  loss:  2.72263929246055&quot;
## [1] &quot;step =  5  lambda =  0.000876126562258242  loss:  2.7226340957231&quot;
## [1] &quot;step =  6  lambda =  0.000876126562258242  loss:  2.722629369749&quot;
## [1] &quot;step =  7  lambda =  0.000876126562258242  loss:  2.72262511494396&quot;
## [1] &quot;step =  8  lambda =  0.000876126562258242  loss:  2.72262133142008&quot;
## [1] &quot;step =  9  lambda =  0.000876126562258242  loss:  2.72261801916529&quot;
## [1] &quot;step =  10  lambda =  0.000876126562258242  loss:  2.72261517812464&quot;
## [1] &quot;step =  11  lambda =  0.000876126562258242  loss:  2.72261280823579&quot;
## [1] &quot;step =  12  lambda =  0.000876126562258242  loss:  2.72261090944245&quot;
## [1] &quot;step =  13  lambda =  0.000876126562258242  loss:  2.72260948169777&quot;
## [1] &quot;step =  14  lambda =  0.000876126562258242  loss:  2.72260852496375&quot;
## [1] &quot;step =  15  lambda =  0.000876126562258242  loss:  2.72260803920944&quot;
## [1] &quot;step =  16  lambda =  0.000876126562258242  loss:  2.72260802440903&quot;
## [1] &quot;step =  17  lambda =  0.000876126562258242  loss:  2.72260848054019&quot;
## [1] &quot;step =  1  lambda =  0.000867408957307003  loss:  2.70341850052022&quot;
## [1] &quot;step =  2  lambda =  0.000867408957307003  loss:  2.70341211267373&quot;
## [1] &quot;step =  3  lambda =  0.000867408957307003  loss:  2.70340619089358&quot;
## [1] &quot;step =  4  lambda =  0.000867408957307003  loss:  2.70340073758416&quot;
## [1] &quot;step =  5  lambda =  0.000867408957307003  loss:  2.70339575484705&quot;
## [1] &quot;step =  6  lambda =  0.000867408957307003  loss:  2.70339124370083&quot;
## [1] &quot;step =  7  lambda =  0.000867408957307003  loss:  2.70338720454561&quot;
## [1] &quot;step =  8  lambda =  0.000867408957307003  loss:  2.70338363748457&quot;
## [1] &quot;step =  9  lambda =  0.000867408957307003  loss:  2.70338054249395&quot;
## [1] &quot;step =  10  lambda =  0.000867408957307003  loss:  2.70337791950516&quot;
## [1] &quot;step =  11  lambda =  0.000867408957307003  loss:  2.703375768441&quot;
## [1] &quot;step =  12  lambda =  0.000867408957307003  loss:  2.70337408922951&quot;
## [1] &quot;step =  13  lambda =  0.000867408957307003  loss:  2.70337288180782&quot;
## [1] &quot;step =  14  lambda =  0.000867408957307003  loss:  2.70337214612169&quot;
## [1] &quot;step =  15  lambda =  0.000867408957307003  loss:  2.70337188212388&quot;
## [1] &quot;step =  16  lambda =  0.000867408957307003  loss:  2.70337208977221&quot;
## [1] &quot;step =  1  lambda =  0.000858778093974336  loss:  2.68426383656389&quot;
## [1] &quot;step =  2  lambda =  0.000858778093974336  loss:  2.68425728589369&quot;
## [1] &quot;step =  3  lambda =  0.000858778093974336  loss:  2.68425120189465&quot;
## [1] &quot;step =  4  lambda =  0.000858778093974336  loss:  2.68424558694751&quot;
## [1] &quot;step =  5  lambda =  0.000858778093974336  loss:  2.68424044314428&quot;
## [1] &quot;step =  6  lambda =  0.000858778093974336  loss:  2.68423577149807&quot;
## [1] &quot;step =  7  lambda =  0.000858778093974336  loss:  2.68423157240244&quot;
## [1] &quot;step =  8  lambda =  0.000858778093974336  loss:  2.68422784595156&quot;
## [1] &quot;step =  9  lambda =  0.000858778093974336  loss:  2.68422459211048&quot;
## [1] &quot;step =  10  lambda =  0.000858778093974336  loss:  2.68422181079778&quot;
## [1] &quot;step =  11  lambda =  0.000858778093974336  loss:  2.68421950192234&quot;
## [1] &quot;step =  12  lambda =  0.000858778093974336  loss:  2.68421766539764&quot;
## [1] &quot;step =  13  lambda =  0.000858778093974336  loss:  2.68421630114588&quot;
## [1] &quot;step =  14  lambda =  0.000858778093974336  loss:  2.6842154090977&quot;
## [1] &quot;step =  15  lambda =  0.000858778093974336  loss:  2.68421498919068&quot;
## [1] &quot;step =  16  lambda =  0.000858778093974336  loss:  2.6842150413675&quot;
## [1] &quot;step =  1  lambda =  0.000850233109166719  loss:  2.665188967177&quot;
## [1] &quot;step =  2  lambda =  0.000850233109166719  loss:  2.66518234510901&quot;
## [1] &quot;step =  3  lambda =  0.000850233109166719  loss:  2.66517619005434&quot;
## [1] &quot;step =  4  lambda =  0.000850233109166719  loss:  2.66517050437001&quot;
## [1] &quot;step =  5  lambda =  0.000850233109166719  loss:  2.66516529013813&quot;
## [1] &quot;step =  6  lambda =  0.000850233109166719  loss:  2.66516054836641&quot;
## [1] &quot;step =  7  lambda =  0.000850233109166719  loss:  2.66515627944236&quot;
## [1] &quot;step =  8  lambda =  0.000850233109166719  loss:  2.66515248345198&quot;
## [1] &quot;step =  9  lambda =  0.000850233109166719  loss:  2.66514916035004&quot;
## [1] &quot;step =  10  lambda =  0.000850233109166719  loss:  2.66514631004328&quot;
## [1] &quot;step =  11  lambda =  0.000850233109166719  loss:  2.6651439324277&quot;
## [1] &quot;step =  12  lambda =  0.000850233109166719  loss:  2.66514202740331&quot;
## [1] &quot;step =  13  lambda =  0.000850233109166719  loss:  2.66514059487839&quot;
## [1] &quot;step =  14  lambda =  0.000850233109166719  loss:  2.66513963476956&quot;
## [1] &quot;step =  15  lambda =  0.000850233109166719  loss:  2.66513914700024&quot;
## [1] &quot;step =  16  lambda =  0.000850233109166719  loss:  2.66513913149899&quot;
## [1] &quot;step =  17  lambda =  0.000850233109166719  loss:  2.66513958819793&quot;
## [1] &quot;step =  1  lambda =  0.000841773148378549  loss:  2.64618901414296&quot;
## [1] &quot;step =  2  lambda =  0.000841773148378549  loss:  2.64618287988865&quot;
## [1] &quot;step =  3  lambda =  0.000841773148378549  loss:  2.64617721267479&quot;
## [1] &quot;step =  4  lambda =  0.000841773148378549  loss:  2.64617201483569&quot;
## [1] &quot;step =  5  lambda =  0.000841773148378549  loss:  2.64616728844499&quot;
## [1] &quot;step =  6  lambda =  0.000841773148378549  loss:  2.64616303450657&quot;
## [1] &quot;step =  7  lambda =  0.000841773148378549  loss:  2.64615925340323&quot;
## [1] &quot;step =  8  lambda =  0.000841773148378549  loss:  2.646155945214&quot;
## [1] &quot;step =  9  lambda =  0.000841773148378549  loss:  2.64615310988448&quot;
## [1] &quot;step =  10  lambda =  0.000841773148378549  loss:  2.6461507473106&quot;
## [1] &quot;step =  11  lambda =  0.000841773148378549  loss:  2.64614885737645&quot;
## [1] &quot;step =  12  lambda =  0.000841773148378549  loss:  2.6461474399694&quot;
## [1] &quot;step =  13  lambda =  0.000841773148378549  loss:  2.64614649498473&quot;
## [1] &quot;step =  14  lambda =  0.000841773148378549  loss:  2.64614602232584&quot;
## [1] &quot;step =  15  lambda =  0.000841773148378549  loss:  2.64614602190281&quot;
## [1] &quot;step =  16  lambda =  0.000841773148378549  loss:  2.6461464936308&quot;
## [1] &quot;step =  1  lambda =  0.000833397365606696  loss:  2.62727901450527&quot;
## [1] &quot;step =  2  lambda =  0.000833397365606696  loss:  2.62727297631191&quot;
## [1] &quot;step =  3  lambda =  0.000833397365606696  loss:  2.62726740502969&quot;
## [1] &quot;step =  4  lambda =  0.000833397365606696  loss:  2.62726230296594&quot;
## [1] &quot;step =  5  lambda =  0.000833397365606696  loss:  2.62725767218144&quot;
## [1] &quot;step =  6  lambda =  0.000833397365606696  loss:  2.62725351367338&quot;
## [1] &quot;step =  7  lambda =  0.000833397365606696  loss:  2.62724982781838&quot;
## [1] &quot;step =  8  lambda =  0.000833397365606696  loss:  2.62724661468789&quot;
## [1] &quot;step =  9  lambda =  0.000833397365606696  loss:  2.6272438742183&quot;
## [1] &quot;step =  10  lambda =  0.000833397365606696  loss:  2.62724160629499&quot;
## [1] &quot;step =  11  lambda =  0.000833397365606696  loss:  2.62723981079057&quot;
## [1] &quot;step =  12  lambda =  0.000833397365606696  loss:  2.62723848758036&quot;
## [1] &quot;step =  13  lambda =  0.000833397365606696  loss:  2.62723763654724&quot;
## [1] &quot;step =  14  lambda =  0.000833397365606696  loss:  2.62723725758199&quot;
## [1] &quot;step =  15  lambda =  0.000833397365606696  loss:  2.62723735058202&quot;
## [1] &quot;step =  1  lambda =  0.000825104923265905  loss:  2.60846029350336&quot;
## [1] &quot;step =  2  lambda =  0.000825104923265905  loss:  2.60845395550815&quot;
## [1] &quot;step =  3  lambda =  0.000825104923265905  loss:  2.60844808417126&quot;
## [1] &quot;step =  4  lambda =  0.000825104923265905  loss:  2.6084426817709&quot;
## [1] &quot;step =  5  lambda =  0.000825104923265905  loss:  2.60843775035193&quot;
## [1] &quot;step =  6  lambda =  0.000825104923265905  loss:  2.6084332909029&quot;
## [1] &quot;step =  7  lambda =  0.000825104923265905  loss:  2.60842930379334&quot;
## [1] &quot;step =  8  lambda =  0.000825104923265905  loss:  2.60842578908685&quot;
## [1] &quot;step =  9  lambda =  0.000825104923265905  loss:  2.60842274671082&quot;
## [1] &quot;step =  10  lambda =  0.000825104923265905  loss:  2.60842017654056&quot;
## [1] &quot;step =  11  lambda =  0.000825104923265905  loss:  2.60841807843782&quot;
## [1] &quot;step =  12  lambda =  0.000825104923265905  loss:  2.60841645226655&quot;
## [1] &quot;step =  13  lambda =  0.000825104923265905  loss:  2.60841529789801&quot;
## [1] &quot;step =  14  lambda =  0.000825104923265905  loss:  2.60841461521115&quot;
## [1] &quot;step =  15  lambda =  0.000825104923265905  loss:  2.60841440409153&quot;
## [1] &quot;step =  16  lambda =  0.000825104923265905  loss:  2.60841466442971&quot;
## [1] &quot;step =  1  lambda =  0.000816894992105029  loss:  2.58972136450683&quot;
## [1] &quot;step =  2  lambda =  0.000816894992105029  loss:  2.58971527206458&quot;
## [1] &quot;step =  3  lambda =  0.000816894992105029  loss:  2.58970964571945&quot;
## [1] &quot;step =  4  lambda =  0.000816894992105029  loss:  2.58970448772386&quot;
## [1] &quot;step =  5  lambda =  0.000816894992105029  loss:  2.58969980010988&quot;
## [1] &quot;step =  6  lambda =  0.000816894992105029  loss:  2.58969558385997&quot;
## [1] &quot;step =  7  lambda =  0.000816894992105029  loss:  2.58969183933858&quot;
## [1] &quot;step =  8  lambda =  0.000816894992105029  loss:  2.58968856660315&quot;
## [1] &quot;step =  9  lambda =  0.000816894992105029  loss:  2.58968576557346&quot;
## [1] &quot;step =  10  lambda =  0.000816894992105029  loss:  2.58968343611595&quot;
## [1] &quot;step =  11  lambda =  0.000816894992105029  loss:  2.58968157808263&quot;
## [1] &quot;step =  12  lambda =  0.000816894992105029  loss:  2.58968019132716&quot;
## [1] &quot;step =  13  lambda =  0.000816894992105029  loss:  2.58967927571005&quot;
## [1] &quot;step =  14  lambda =  0.000816894992105029  loss:  2.58967883109944&quot;
## [1] &quot;step =  15  lambda =  0.000816894992105029  loss:  2.58967885736985&quot;
## [1] &quot;step =  1  lambda =  0.000808766751124111  loss:  2.57107644369616&quot;
## [1] &quot;step =  2  lambda =  0.000808766751124111  loss:  2.57107019346416&quot;
## [1] &quot;step =  3  lambda =  0.000808766751124111  loss:  2.57106440870057&quot;
## [1] &quot;step =  4  lambda =  0.000808766751124111  loss:  2.57105909162755&quot;
## [1] &quot;step =  5  lambda =  0.000808766751124111  loss:  2.57105424425991&quot;
## [1] &quot;step =  6  lambda =  0.000808766751124111  loss:  2.57104986757105&quot;
## [1] &quot;step =  7  lambda =  0.000808766751124111  loss:  2.57104596191865&quot;
## [1] &quot;step =  8  lambda =  0.000808766751124111  loss:  2.57104252735321&quot;
## [1] &quot;step =  9  lambda =  0.000808766751124111  loss:  2.57103956378662&quot;
## [1] &quot;step =  10  lambda =  0.000808766751124111  loss:  2.57103707107657&quot;
## [1] &quot;step =  11  lambda =  0.000808766751124111  loss:  2.57103504906562&quot;
## [1] &quot;step =  12  lambda =  0.000808766751124111  loss:  2.5710334975975&quot;
## [1] &quot;step =  13  lambda =  0.000808766751124111  loss:  2.57103241652253&quot;
## [1] &quot;step =  14  lambda =  0.000808766751124111  loss:  2.57103180569845&quot;
## [1] &quot;step =  15  lambda =  0.000808766751124111  loss:  2.57103166498937&quot;
## [1] &quot;step =  16  lambda =  0.000808766751124111  loss:  2.57103199426438&quot;
## [1] &quot;step =  1  lambda =  0.000800719387492281  loss:  2.55251392059435&quot;
## [1] &quot;step =  2  lambda =  0.000800719387492281  loss:  2.55250804893745&quot;
## [1] &quot;step =  3  lambda =  0.000800719387492281  loss:  2.55250264181326&quot;
## [1] &quot;step =  4  lambda =  0.000800719387492281  loss:  2.55249770141696&quot;
## [1] &quot;step =  5  lambda =  0.000800719387492281  loss:  2.55249322974901&quot;
## [1] &quot;step =  6  lambda =  0.000800719387492281  loss:  2.55248922777619&quot;
## [1] &quot;step =  7  lambda =  0.000800719387492281  loss:  2.55248569585132&quot;
## [1] &quot;step =  8  lambda =  0.000800719387492281  loss:  2.5524826340195&quot;
## [1] &quot;step =  9  lambda =  0.000800719387492281  loss:  2.55248004218606&quot;
## [1] &quot;step =  10  lambda =  0.000800719387492281  loss:  2.55247792020103&quot;
## [1] &quot;step =  11  lambda =  0.000800719387492281  loss:  2.55247626789853&quot;
## [1] &quot;step =  12  lambda =  0.000800719387492281  loss:  2.55247508511329&quot;
## [1] &quot;step =  13  lambda =  0.000800719387492281  loss:  2.55247437168628&quot;
## [1] &quot;step =  14  lambda =  0.000800719387492281  loss:  2.5524741274657&quot;
## [1] &quot;step =  15  lambda =  0.000800719387492281  loss:  2.55247435230602&quot;
## [1] &quot;step =  1  lambda =  0.000792752096466468  loss:  2.53404755651733&quot;
## [1] &quot;step =  2  lambda =  0.000792752096466468  loss:  2.53404165420318&quot;
## [1] &quot;step =  3  lambda =  0.000792752096466468  loss:  2.53403621545987&quot;
## [1] &quot;step =  4  lambda =  0.000792752096466468  loss:  2.53403124245125&quot;
## [1] &quot;step =  5  lambda =  0.000792752096466468  loss:  2.5340267371591&quot;
## [1] &quot;step =  6  lambda =  0.000792752096466468  loss:  2.53402270054055&quot;
## [1] &quot;step =  7  lambda =  0.000792752096466468  loss:  2.53401913294183&quot;
## [1] &quot;step =  8  lambda =  0.000792752096466468  loss:  2.5340160344016&quot;
## [1] &quot;step =  9  lambda =  0.000792752096466468  loss:  2.53401340481814&quot;
## [1] &quot;step =  10  lambda =  0.000792752096466468  loss:  2.53401124403376&quot;
## [1] &quot;step =  11  lambda =  0.000792752096466468  loss:  2.53400955187418&quot;
## [1] &quot;step =  12  lambda =  0.000792752096466468  loss:  2.53400832816532&quot;
## [1] &quot;step =  13  lambda =  0.000792752096466468  loss:  2.53400757273908&quot;
## [1] &quot;step =  14  lambda =  0.000792752096466468  loss:  2.53400728543442&quot;
## [1] &quot;step =  15  lambda =  0.000792752096466468  loss:  2.53400746609645&quot;
## [1] &quot;step =  1  lambda =  0.000784864081310932  loss:  2.51567208079942&quot;
## [1] &quot;step =  2  lambda =  0.000784864081310932  loss:  2.51566620688775&quot;
## [1] &quot;step =  3  lambda =  0.000784864081310932  loss:  2.51566079542571&quot;
## [1] &quot;step =  4  lambda =  0.000784864081310932  loss:  2.51565584854708&quot;
## [1] &quot;step =  5  lambda =  0.000784864081310932  loss:  2.51565136821545&quot;
## [1] &quot;step =  6  lambda =  0.000784864081310932  loss:  2.51564735537883&quot;
## [1] &quot;step =  7  lambda =  0.000784864081310932  loss:  2.51564381037737&quot;
## [1] &quot;step =  8  lambda =  0.000784864081310932  loss:  2.51564073324397&quot;
## [1] &quot;step =  9  lambda =  0.000784864081310932  loss:  2.51563812387056&quot;
## [1] &quot;step =  10  lambda =  0.000784864081310932  loss:  2.51563598209231&quot;
## [1] &quot;step =  11  lambda =  0.000784864081310932  loss:  2.51563430772725&quot;
## [1] &quot;step =  12  lambda =  0.000784864081310932  loss:  2.51563310059314&quot;
## [1] &quot;step =  13  lambda =  0.000784864081310932  loss:  2.51563236051346&quot;
## [1] &quot;step =  14  lambda =  0.000784864081310932  loss:  2.51563208731855&quot;
## [1] &quot;step =  15  lambda =  0.000784864081310932  loss:  2.51563228084483&quot;
## [1] &quot;step =  1  lambda =  0.000777054553217582  loss:  2.49738842311441&quot;
## [1] &quot;step =  2  lambda =  0.000777054553217582  loss:  2.49738263348865&quot;
## [1] &quot;step =  3  lambda =  0.000777054553217582  loss:  2.49737730504574&quot;
## [1] &quot;step =  4  lambda =  0.000777054553217582  loss:  2.4973724398891&quot;
## [1] &quot;step =  5  lambda =  0.000777054553217582  loss:  2.49736803996363&quot;
## [1] &quot;step =  6  lambda =  0.000777054553217582  loss:  2.49736410620807&quot;
## [1] &quot;step =  7  lambda =  0.000777054553217582  loss:  2.49736063895664&quot;
## [1] &quot;step =  8  lambda =  0.000777054553217582  loss:  2.49735763823676&quot;
## [1] &quot;step =  9  lambda =  0.000777054553217582  loss:  2.49735510393442&quot;
## [1] &quot;step =  10  lambda =  0.000777054553217582  loss:  2.49735303587819&quot;
## [1] &quot;step =  11  lambda =  0.000777054553217582  loss:  2.49735143387891&quot;
## [1] &quot;step =  12  lambda =  0.000777054553217582  loss:  2.49735029774673&quot;
## [1] &quot;step =  13  lambda =  0.000777054553217582  loss:  2.49734962729725&quot;
## [1] &quot;step =  14  lambda =  0.000777054553217582  loss:  2.49734942235274&quot;
## [1] &quot;step =  15  lambda =  0.000777054553217582  loss:  2.4973496827415&quot;
## [1] &quot;step =  1  lambda =  0.000769322731227101  loss:  2.47919745456696&quot;
## [1] &quot;step =  2  lambda =  0.000769322731227101  loss:  2.47919180209631&quot;
## [1] &quot;step =  3  lambda =  0.000769322731227101  loss:  2.47918660940466&quot;
## [1] &quot;step =  4  lambda =  0.000769322731227101  loss:  2.47918187856477&quot;
## [1] &quot;step =  5  lambda =  0.000769322731227101  loss:  2.47917761150236&quot;
## [1] &quot;step =  6  lambda =  0.000769322731227101  loss:  2.47917380914674&quot;
## [1] &quot;step =  7  lambda =  0.000769322731227101  loss:  2.47917047182618&quot;
## [1] &quot;step =  8  lambda =  0.000769322731227101  loss:  2.47916759956295&quot;
## [1] &quot;step =  9  lambda =  0.000769322731227101  loss:  2.47916519223743&quot;
## [1] &quot;step =  10  lambda =  0.000769322731227101  loss:  2.47916324967206&quot;
## [1] &quot;step =  11  lambda =  0.000769322731227101  loss:  2.47916177167093&quot;
## [1] &quot;step =  12  lambda =  0.000769322731227101  loss:  2.47916075803709&quot;
## [1] &quot;step =  13  lambda =  0.000769322731227101  loss:  2.47916020857873&quot;
## [1] &quot;step =  14  lambda =  0.000769322731227101  loss:  2.47916012311058&quot;
## [1] &quot;step =  15  lambda =  0.000769322731227101  loss:  2.47916050145325&quot;
## [1] &quot;step =  1  lambda =  0.000761667842150847  loss:  2.46109999017796&quot;
## [1] &quot;step =  2  lambda =  0.000761667842150847  loss:  2.46109452485338&quot;
## [1] &quot;step =  3  lambda =  0.000761667842150847  loss:  2.46108951777449&quot;
## [1] &quot;step =  4  lambda =  0.000761667842150847  loss:  2.46108497098308&quot;
## [1] &quot;step =  5  lambda =  0.000761667842150847  loss:  2.46108088638527&quot;
## [1] &quot;step =  6  lambda =  0.000761667842150847  loss:  2.46107726490063&quot;
## [1] &quot;step =  7  lambda =  0.000761667842150847  loss:  2.46107410685163&quot;
## [1] &quot;step =  8  lambda =  0.000761667842150847  loss:  2.46107141225552&quot;
## [1] &quot;step =  9  lambda =  0.000761667842150847  loss:  2.46106918098746&quot;
## [1] &quot;step =  10  lambda =  0.000761667842150847  loss:  2.46106741286409&quot;
## [1] &quot;step =  11  lambda =  0.000761667842150847  loss:  2.46106610768321&quot;
## [1] &quot;step =  12  lambda =  0.000761667842150847  loss:  2.46106526524119&quot;
## [1] &quot;step =  13  lambda =  0.000761667842150847  loss:  2.46106488533928&quot;
## [1] &quot;step =  14  lambda =  0.000761667842150847  loss:  2.46106496778506&quot;
## [1] &quot;step =  1  lambda =  0.000754089120493534  loss:  2.44310252260004&quot;
## [1] &quot;step =  2  lambda =  0.000754089120493534  loss:  2.44309682911908&quot;
## [1] &quot;step =  3  lambda =  0.000754089120493534  loss:  2.4430915924099&quot;
## [1] &quot;step =  4  lambda =  0.000754089120493534  loss:  2.44308681448072&quot;
## [1] &quot;step =  5  lambda =  0.000754089120493534  loss:  2.44308249721491&quot;
## [1] &quot;step =  6  lambda =  0.000754089120493534  loss:  2.44307864152014&quot;
## [1] &quot;step =  7  lambda =  0.000754089120493534  loss:  2.44307524771156&quot;
## [1] &quot;step =  8  lambda =  0.000754089120493534  loss:  2.44307231580057&quot;
## [1] &quot;step =  9  lambda =  0.000754089120493534  loss:  2.44306984565667&quot;
## [1] &quot;step =  10  lambda =  0.000754089120493534  loss:  2.44306783709048&quot;
## [1] &quot;step =  11  lambda =  0.000754089120493534  loss:  2.44306628989353&quot;
## [1] &quot;step =  12  lambda =  0.000754089120493534  loss:  2.44306520385557&quot;
## [1] &quot;step =  13  lambda =  0.000754089120493534  loss:  2.44306457877107&quot;
## [1] &quot;step =  14  lambda =  0.000754089120493534  loss:  2.4430644144407&quot;
## [1] &quot;step =  15  lambda =  0.000754089120493534  loss:  2.44306471067088&quot;
## [1] &quot;step =  1  lambda =  0.00074658580837668  loss:  2.42519404503596&quot;
## [1] &quot;step =  2  lambda =  0.00074658580837668  loss:  2.42518863241726&quot;
## [1] &quot;step =  3  lambda =  0.00074658580837668  loss:  2.42518367478905&quot;
## [1] &quot;step =  4  lambda =  0.00074658580837668  loss:  2.42517917413071&quot;
## [1] &quot;step =  5  lambda =  0.00074658580837668  loss:  2.42517513230693&quot;
## [1] &quot;step =  6  lambda =  0.00074658580837668  loss:  2.42517155021657&quot;
## [1] &quot;step =  7  lambda =  0.00074658580837668  loss:  2.42516842816992&quot;
## [1] &quot;step =  8  lambda =  0.00074658580837668  loss:  2.42516576617446&quot;
## [1] &quot;step =  9  lambda =  0.00074658580837668  loss:  2.42516356409554&quot;
## [1] &quot;step =  10  lambda =  0.00074658580837668  loss:  2.42516182173914&quot;
## [1] &quot;step =  11  lambda =  0.00074658580837668  loss:  2.42516053889157&quot;
## [1] &quot;step =  12  lambda =  0.00074658580837668  loss:  2.42515971533702&quot;
## [1] &quot;step =  13  lambda =  0.00074658580837668  loss:  2.42515935086406&quot;
## [1] &quot;step =  14  lambda =  0.00074658580837668  loss:  2.4251594452673&quot;
## [1] &quot;step =  1  lambda =  0.00073915715546282  loss:  2.40738674477695&quot;
## [1] &quot;step =  2  lambda =  0.00073915715546282  loss:  2.40738119606683&quot;
## [1] &quot;step =  3  lambda =  0.00073915715546282  loss:  2.40737610066046&quot;
## [1] &quot;step =  4  lambda =  0.00073915715546282  loss:  2.40737146050365&quot;
## [1] &quot;step =  5  lambda =  0.00073915715546282  loss:  2.40736727743788&quot;
## [1] &quot;step =  6  lambda =  0.00073915715546282  loss:  2.40736355234995&quot;
## [1] &quot;step =  7  lambda =  0.00073915715546282  loss:  2.40736028554309&quot;
## [1] &quot;step =  8  lambda =  0.00073915715546282  loss:  2.40735747701943&quot;
## [1] &quot;step =  9  lambda =  0.00073915715546282  loss:  2.4073551266393&quot;
## [1] &quot;step =  10  lambda =  0.00073915715546282  loss:  2.40735323420345&quot;
## [1] &quot;step =  11  lambda =  0.00073915715546282  loss:  2.40735179949274&quot;
## [1] &quot;step =  12  lambda =  0.00073915715546282  loss:  2.40735082228557&quot;
## [1] &quot;step =  13  lambda =  0.00073915715546282  loss:  2.40735030236459&quot;
## [1] &quot;step =  14  lambda =  0.00073915715546282  loss:  2.40735023951835&quot;
## [1] &quot;step =  15  lambda =  0.00073915715546282  loss:  2.40735063354098&quot;
## [1] &quot;step =  1  lambda =  0.000731802418880473  loss:  2.38966972524904&quot;
## [1] &quot;step =  2  lambda =  0.000731802418880473  loss:  2.38966454123259&quot;
## [1] &quot;step =  3  lambda =  0.000731802418880473  loss:  2.38965980852638&quot;
## [1] &quot;step =  4  lambda =  0.000731802418880473  loss:  2.38965552904718&quot;
## [1] &quot;step =  5  lambda =  0.000731802418880473  loss:  2.38965170461715&quot;
## [1] &quot;step =  6  lambda =  0.000731802418880473  loss:  2.38964833611402&quot;
## [1] &quot;step =  7  lambda =  0.000731802418880473  loss:  2.3896454238363&quot;
## [1] &quot;step =  8  lambda =  0.000731802418880473  loss:  2.38964296778262&quot;
## [1] &quot;step =  9  lambda =  0.000731802418880473  loss:  2.38964096780975&quot;
## [1] &quot;step =  10  lambda =  0.000731802418880473  loss:  2.38963942371449&quot;
## [1] &quot;step =  11  lambda =  0.000731802418880473  loss:  2.38963833527325&quot;
## [1] &quot;step =  12  lambda =  0.000731802418880473  loss:  2.38963770225963&quot;
## [1] &quot;step =  13  lambda =  0.000731802418880473  loss:  2.38963752445122&quot;
## [1] &quot;step =  14  lambda =  0.000731802418880473  loss:  2.3896378016313&quot;
## [1] &quot;step =  1  lambda =  0.000724520863149851  loss:  2.37205471251776&quot;
## [1] &quot;step =  2  lambda =  0.000724520863149851  loss:  2.37204947548118&quot;
## [1] &quot;step =  3  lambda =  0.000724520863149851  loss:  2.37204468787937&quot;
## [1] &quot;step =  4  lambda =  0.000724520863149851  loss:  2.37204035159552&quot;
## [1] &quot;step =  5  lambda =  0.000724520863149851  loss:  2.37203646842809&quot;
## [1] &quot;step =  6  lambda =  0.000724520863149851  loss:  2.37203303924254&quot;
## [1] &quot;step =  7  lambda =  0.000724520863149851  loss:  2.37203006433043&quot;
## [1] &quot;step =  8  lambda =  0.000724520863149851  loss:  2.37202754368537&quot;
## [1] &quot;step =  9  lambda =  0.000724520863149851  loss:  2.37202547715961&quot;
## [1] &quot;step =  10  lambda =  0.000724520863149851  loss:  2.37202386454535&quot;
## [1] &quot;step =  11  lambda =  0.000724520863149851  loss:  2.37202270561417&quot;
## [1] &quot;step =  12  lambda =  0.000724520863149851  loss:  2.37202200013459&quot;
## [1] &quot;step =  13  lambda =  0.000724520863149851  loss:  2.37202174787895&quot;
## [1] &quot;step =  14  lambda =  0.000724520863149851  loss:  2.37202194862515&quot;
## [1] &quot;step =  1  lambda =  0.000717311760109313  loss:  2.35453655674905&quot;
## [1] &quot;step =  2  lambda =  0.000717311760109313  loss:  2.35453130617223&quot;
## [1] &quot;step =  3  lambda =  0.000717311760109313  loss:  2.35452650305879&quot;
## [1] &quot;step =  4  lambda =  0.000717311760109313  loss:  2.35452214926066&quot;
## [1] &quot;step =  5  lambda =  0.000717311760109313  loss:  2.35451824655401&quot;
## [1] &quot;step =  6  lambda =  0.000717311760109313  loss:  2.3545147957931&quot;
## [1] &quot;step =  7  lambda =  0.000717311760109313  loss:  2.35451179726335&quot;
## [1] &quot;step =  8  lambda =  0.000717311760109313  loss:  2.35450925095412&quot;
## [1] &quot;step =  9  lambda =  0.000717311760109313  loss:  2.35450715671381&quot;
## [1] &quot;step =  10  lambda =  0.000717311760109313  loss:  2.35450551433057&quot;
## [1] &quot;step =  11  lambda =  0.000717311760109313  loss:  2.3545043235717&quot;
## [1] &quot;step =  12  lambda =  0.000717311760109313  loss:  2.35450358420118&quot;
## [1] &quot;step =  13  lambda =  0.000717311760109313  loss:  2.3545032959866&quot;
## [1] &quot;step =  14  lambda =  0.000717311760109313  loss:  2.35450345870095&quot;
## [1] &quot;step =  1  lambda =  0.000710174388842549  loss:  2.33711563267115&quot;
## [1] &quot;step =  2  lambda =  0.000710174388842549  loss:  2.33711040605723&quot;
## [1] &quot;step =  3  lambda =  0.000710174388842549  loss:  2.33710562485224&quot;
## [1] &quot;step =  4  lambda =  0.000710174388842549  loss:  2.33710129087699&quot;
## [1] &quot;step =  5  lambda =  0.000710174388842549  loss:  2.33709740588531&quot;
## [1] &quot;step =  6  lambda =  0.000710174388842549  loss:  2.33709397072026&quot;
## [1] &quot;step =  7  lambda =  0.000710174388842549  loss:  2.3370909856613&quot;
## [1] &quot;step =  8  lambda =  0.000710174388842549  loss:  2.33708845069376&quot;
## [1] &quot;step =  9  lambda =  0.000710174388842549  loss:  2.33708636566244&quot;
## [1] &quot;step =  10  lambda =  0.000710174388842549  loss:  2.33708473035186&quot;
## [1] &quot;step =  11  lambda =  0.000710174388842549  loss:  2.33708354452538&quot;
## [1] &quot;step =  12  lambda =  0.000710174388842549  loss:  2.33708280794278&quot;
## [1] &quot;step =  13  lambda =  0.000710174388842549  loss:  2.33708252036725&quot;
## [1] &quot;step =  14  lambda =  0.000710174388842549  loss:  2.3370826815673&quot;
## [1] &quot;step =  1  lambda =  0.000703108035606483  loss:  2.3197922808183&quot;
## [1] &quot;step =  2  lambda =  0.000703108035606483  loss:  2.31978711380585&quot;
## [1] &quot;step =  3  lambda =  0.000703108035606483  loss:  2.3197823900703&quot;
## [1] &quot;step =  4  lambda =  0.000703108035606483  loss:  2.31977811140143&quot;
## [1] &quot;step =  5  lambda =  0.000703108035606483  loss:  2.31977427953067&quot;
## [1] &quot;step =  6  lambda =  0.000703108035606483  loss:  2.31977089528982&quot;
## [1] &quot;step =  7  lambda =  0.000703108035606483  loss:  2.31976795895249&quot;
## [1] &quot;step =  8  lambda =  0.000703108035606483  loss:  2.31976547050016&quot;
## [1] &quot;step =  9  lambda =  0.000703108035606483  loss:  2.31976342977431&quot;
## [1] &quot;step =  10  lambda =  0.000703108035606483  loss:  2.31976183655606&quot;
## [1] &quot;step =  11  lambda =  0.000703108035606483  loss:  2.31976069060515&quot;
## [1] &quot;step =  12  lambda =  0.000703108035606483  loss:  2.31975999167752&quot;
## [1] &quot;step =  13  lambda =  0.000703108035606483  loss:  2.31975973953228&quot;
## [1] &quot;step =  14  lambda =  0.000703108035606483  loss:  2.31975993393374&quot;
## [1] &quot;step =  1  lambda =  0.000696111993759903  loss:  2.30256680899811&quot;
## [1] &quot;step =  2  lambda =  0.000696111993759903  loss:  2.30256173544269&quot;
## [1] &quot;step =  3  lambda =  0.000696111993759903  loss:  2.30255710295928&quot;
## [1] &quot;step =  4  lambda =  0.000696111993759903  loss:  2.30255291330683&quot;
## [1] &quot;step =  5  lambda =  0.000696111993759903  loss:  2.30254916819417&quot;
## [1] &quot;step =  6  lambda =  0.000696111993759903  loss:  2.30254586844185&quot;
## [1] &quot;step =  7  lambda =  0.000696111993759903  loss:  2.3025430143177&quot;
## [1] &quot;step =  8  lambda =  0.000696111993759903  loss:  2.30254060579948&quot;
## [1] &quot;step =  9  lambda =  0.000696111993759903  loss:  2.30253864272558&quot;
## [1] &quot;step =  10  lambda =  0.000696111993759903  loss:  2.30253712487399&quot;
## [1] &quot;step =  11  lambda =  0.000696111993759903  loss:  2.30253605200109&quot;
## [1] &quot;step =  12  lambda =  0.000696111993759903  loss:  2.30253542385926&quot;
## [1] &quot;step =  13  lambda =  0.000696111993759903  loss:  2.30253524020382&quot;
## [1] &quot;step =  14  lambda =  0.000696111993759903  loss:  2.30253550079522&quot;
## [1] &quot;step =  1  lambda =  0.000689185563692794  loss:  2.28543949356854&quot;
## [1] &quot;step =  2  lambda =  0.000689185563692794  loss:  2.28543454561645&quot;
## [1] &quot;step =  3  lambda =  0.000689185563692794  loss:  2.28543003646295&quot;
## [1] &quot;step =  4  lambda =  0.000689185563692794  loss:  2.28542596783626&quot;
## [1] &quot;step =  5  lambda =  0.000689185563692794  loss:  2.28542234142253&quot;
## [1] &quot;step =  6  lambda =  0.000689185563692794  loss:  2.28541915803103&quot;
## [1] &quot;step =  7  lambda =  0.000689185563692794  loss:  2.2854164179238&quot;
## [1] &quot;step =  8  lambda =  0.000689185563692794  loss:  2.28541412107506&quot;
## [1] &quot;step =  9  lambda =  0.000689185563692794  loss:  2.28541226732029&quot;
## [1] &quot;step =  10  lambda =  0.000689185563692794  loss:  2.28541085643457&quot;
## [1] &quot;step =  11  lambda =  0.000689185563692794  loss:  2.28540988817119&quot;
## [1] &quot;step =  12  lambda =  0.000689185563692794  loss:  2.28540936227919&quot;
## [1] &quot;step =  13  lambda =  0.000689185563692794  loss:  2.28540927851043&quot;
## [1] &quot;step =  14  lambda =  0.000689185563692794  loss:  2.28540963662166&quot;
## [1] &quot;step =  1  lambda =  0.000682328052756377  loss:  2.26841058061943&quot;
## [1] &quot;step =  2  lambda =  0.000682328052756377  loss:  2.26840578877551&quot;
## [1] &quot;step =  3  lambda =  0.000682328052756377  loss:  2.26840143339215&quot;
## [1] &quot;step =  4  lambda =  0.000682328052756377  loss:  2.26839751616691&quot;
## [1] &quot;step =  5  lambda =  0.000682328052756377  loss:  2.26839403876325&quot;
## [1] &quot;step =  6  lambda =  0.000682328052756377  loss:  2.26839100197902&quot;
## [1] &quot;step =  7  lambda =  0.000682328052756377  loss:  2.26838840607054&quot;
## [1] &quot;step =  8  lambda =  0.000682328052756377  loss:  2.26838625100863&quot;
## [1] &quot;step =  9  lambda =  0.000682328052756377  loss:  2.268384536626&quot;
## [1] &quot;step =  10  lambda =  0.000682328052756377  loss:  2.26838326269502&quot;
## [1] &quot;step =  11  lambda =  0.000682328052756377  loss:  2.26838242896613&quot;
## [1] &quot;step =  12  lambda =  0.000682328052756377  loss:  2.26838203518529&quot;
## [1] &quot;step =  13  lambda =  0.000682328052756377  loss:  2.26838208110107&quot;
## [1] &quot;step =  1  lambda =  0.000675538775193844  loss:  2.25148536598289&quot;
## [1] &quot;step =  2  lambda =  0.000675538775193844  loss:  2.25148031956019&quot;
## [1] &quot;step =  3  lambda =  0.000675538775193844  loss:  2.25147570743795&quot;
## [1] &quot;step =  4  lambda =  0.000675538775193844  loss:  2.25147153128067&quot;
## [1] &quot;step =  5  lambda =  0.000675538775193844  loss:  2.25146779272635&quot;
## [1] &quot;step =  6  lambda =  0.000675538775193844  loss:  2.25146449255929&quot;
## [1] &quot;step =  7  lambda =  0.000675538775193844  loss:  2.25146163102847&quot;
## [1] &quot;step =  8  lambda =  0.000675538775193844  loss:  2.25145920810007&quot;
## [1] &quot;step =  9  lambda =  0.000675538775193844  loss:  2.25145722360326&quot;
## [1] &quot;step =  10  lambda =  0.000675538775193844  loss:  2.25145567730716&quot;
## [1] &quot;step =  11  lambda =  0.000675538775193844  loss:  2.25145456895893&quot;
## [1] &quot;step =  12  lambda =  0.000675538775193844  loss:  2.25145389830114&quot;
## [1] &quot;step =  13  lambda =  0.000675538775193844  loss:  2.25145366507894&quot;
## [1] &quot;step =  14  lambda =  0.000675538775193844  loss:  2.25145386904216&quot;
## [1] &quot;step =  1  lambda =  0.000668817052071782  loss:  2.23465368771956&quot;
## [1] &quot;step =  2  lambda =  0.000668817052071782  loss:  2.23464885632656&quot;
## [1] &quot;step =  3  lambda =  0.000668817052071782  loss:  2.23464445676378&quot;
## [1] &quot;step =  4  lambda =  0.000668817052071782  loss:  2.23464049066845&quot;
## [1] &quot;step =  5  lambda =  0.000668817052071782  loss:  2.23463695965791&quot;
## [1] &quot;step =  6  lambda =  0.000668817052071782  loss:  2.23463386450668&quot;
## [1] &quot;step =  7  lambda =  0.000668817052071782  loss:  2.23463120545931&quot;
## [1] &quot;step =  8  lambda =  0.000668817052071782  loss:  2.23462898247964&quot;
## [1] &quot;step =  9  lambda =  0.000668817052071782  loss:  2.23462719539509&quot;
## [1] &quot;step =  10  lambda =  0.000668817052071782  loss:  2.23462584397298&quot;
## [1] &quot;step =  11  lambda =  0.000668817052071782  loss:  2.23462492795841&quot;
## [1] &quot;step =  12  lambda =  0.000668817052071782  loss:  2.23462444709168&quot;
## [1] &quot;step =  13  lambda =  0.000668817052071782  loss:  2.23462440111541&quot;
## [1] &quot;step =  14  lambda =  0.000668817052071782  loss:  2.23462478977675&quot;
## [1] &quot;step =  1  lambda =  0.000662162211212276  loss:  2.21792095283903&quot;
## [1] &quot;step =  2  lambda =  0.000662162211212276  loss:  2.21791636236331&quot;
## [1] &quot;step =  3  lambda =  0.000662162211212276  loss:  2.21791220120507&quot;
## [1] &quot;step =  4  lambda =  0.000662162211212276  loss:  2.21790847097163&quot;
## [1] &quot;step =  5  lambda =  0.000662162211212276  loss:  2.21790517325758&quot;
## [1] &quot;step =  6  lambda =  0.000662162211212276  loss:  2.21790230882615&quot;
## [1] &quot;step =  7  lambda =  0.000662162211212276  loss:  2.21789987791643&quot;
## [1] &quot;step =  8  lambda =  0.000662162211212276  loss:  2.21789788048927&quot;
## [1] &quot;step =  9  lambda =  0.000662162211212276  loss:  2.21789631636996&quot;
## [1] &quot;step =  10  lambda =  0.000662162211212276  loss:  2.2178951853238&quot;
## [1] &quot;step =  11  lambda =  0.000662162211212276  loss:  2.21789448709373&quot;
## [1] &quot;step =  12  lambda =  0.000662162211212276  loss:  2.21789422141777&quot;
## [1] &quot;step =  13  lambda =  0.000662162211212276  loss:  2.21789438803601&quot;
## [1] &quot;step =  1  lambda =  0.000655573587125696  loss:  2.20129208552613&quot;
## [1] &quot;step =  2  lambda =  0.000655573587125696  loss:  2.20128732834382&quot;
## [1] &quot;step =  3  lambda =  0.000655573587125696  loss:  2.20128299816454&quot;
## [1] &quot;step =  4  lambda =  0.000655573587125696  loss:  2.20127909656337&quot;
## [1] &quot;step =  5  lambda =  0.000655573587125696  loss:  2.2012756251096&quot;
## [1] &quot;step =  6  lambda =  0.000655573587125696  loss:  2.20127258455301&quot;
## [1] &quot;step =  7  lambda =  0.000655573587125696  loss:  2.20126997512558&quot;
## [1] &quot;step =  8  lambda =  0.000655573587125696  loss:  2.20126779678398&quot;
## [1] &quot;step =  9  lambda =  0.000655573587125696  loss:  2.20126604935043&quot;
## [1] &quot;step =  10  lambda =  0.000655573587125696  loss:  2.20126473258757&quot;
## [1] &quot;step =  11  lambda =  0.000655573587125696  loss:  2.20126384623574&quot;
## [1] &quot;step =  12  lambda =  0.000655573587125696  loss:  2.2012633900302&quot;
## [1] &quot;step =  13  lambda =  0.000655573587125696  loss:  2.20126336370831&quot;
## [1] &quot;step =  14  lambda =  0.000655573587125696  loss:  2.20126376701175&quot;
## [1] &quot;step =  1  lambda =  0.000649050520944141  loss:  2.18475734981666&quot;
## [1] &quot;step =  2  lambda =  0.000649050520944141  loss:  2.18475288402488&quot;
## [1] &quot;step =  3  lambda =  0.000649050520944141  loss:  2.18474884261303&quot;
## [1] &quot;step =  4  lambda =  0.000649050520944141  loss:  2.18474522712944&quot;
## [1] &quot;step =  5  lambda =  0.000649050520944141  loss:  2.18474203912266&quot;
## [1] &quot;step =  6  lambda =  0.000649050520944141  loss:  2.1847392793326&quot;
## [1] &quot;step =  7  lambda =  0.000649050520944141  loss:  2.18473694798697&quot;
## [1] &quot;step =  8  lambda =  0.000649050520944141  loss:  2.18473504504041&quot;
## [1] &quot;step =  9  lambda =  0.000649050520944141  loss:  2.18473357031383&quot;
## [1] &quot;step =  10  lambda =  0.000649050520944141  loss:  2.1847325235686&quot;
## [1] &quot;step =  11  lambda =  0.000649050520944141  loss:  2.1847319045436&quot;
## [1] &quot;step =  12  lambda =  0.000649050520944141  loss:  2.18473171297243&quot;
## [1] &quot;step =  13  lambda =  0.000649050520944141  loss:  2.18473194859055&quot;
## [1] &quot;step =  1  lambda =  0.000642592360355558  loss:  2.16832647861879&quot;
## [1] &quot;step =  2  lambda =  0.000642592360355558  loss:  2.16832189903024&quot;
## [1] &quot;step =  3  lambda =  0.000642592360355558  loss:  2.16831774141785&quot;
## [1] &quot;step =  4  lambda =  0.000642592360355558  loss:  2.16831400729843&quot;
## [1] &quot;step =  5  lambda =  0.000642592360355558  loss:  2.16831069819545&quot;
## [1] &quot;step =  6  lambda =  0.000642592360355558  loss:  2.16830781483554&quot;
## [1] &quot;step =  7  lambda =  0.000642592360355558  loss:  2.1683053574395&quot;
## [1] &quot;step =  8  lambda =  0.000642592360355558  loss:  2.16830332595808&quot;
## [1] &quot;step =  9  lambda =  0.000642592360355558  loss:  2.16830172020949&quot;
## [1] &quot;step =  10  lambda =  0.000642592360355558  loss:  2.16830053995281&quot;
## [1] &quot;step =  11  lambda =  0.000642592360355558  loss:  2.1682997849247&quot;
## [1] &quot;step =  12  lambda =  0.000642592360355558  loss:  2.16829945485649&quot;
## [1] &quot;step =  13  lambda =  0.000642592360355558  loss:  2.16829954948127&quot;
## [1] &quot;step =  1  lambda =  0.000636198459538506  loss:  2.15199469666672&quot;
## [1] &quot;step =  2  lambda =  0.000636198459538506  loss:  2.15199002925999&quot;
## [1] &quot;step =  3  lambda =  0.000636198459538506  loss:  2.15198578137391&quot;
## [1] &quot;step =  4  lambda =  0.000636198459538506  loss:  2.1519819544967&quot;
## [1] &quot;step =  5  lambda =  0.000636198459538506  loss:  2.15197855012881&quot;
## [1] &quot;step =  6  lambda =  0.000636198459538506  loss:  2.15197556898511&quot;
## [1] &quot;step =  7  lambda =  0.000636198459538506  loss:  2.15197301128065&quot;
## [1] &quot;step =  8  lambda =  0.000636198459538506  loss:  2.15197087696312&quot;
## [1] &quot;step =  9  lambda =  0.000636198459538506  loss:  2.15196916584872&quot;
## [1] &quot;step =  10  lambda =  0.000636198459538506  loss:  2.15196787769484&quot;
## [1] &quot;step =  11  lambda =  0.000636198459538506  loss:  2.15196701223641&quot;
## [1] &quot;step =  12  lambda =  0.000636198459538506  loss:  2.15196656920292&quot;
## [1] &quot;step =  13  lambda =  0.000636198459538506  loss:  2.1519665483255&quot;
## [1] &quot;step =  14  lambda =  0.000636198459538506  loss:  2.15196694933929&quot;
## [1] &quot;step =  1  lambda =  0.000629868179097574  loss:  2.13575721841028&quot;
## [1] &quot;step =  2  lambda =  0.000629868179097574  loss:  2.13575290995384&quot;
## [1] &quot;step =  3  lambda =  0.000629868179097574  loss:  2.13574901826614&quot;
## [1] &quot;step =  4  lambda =  0.000629868179097574  loss:  2.13574554480948&quot;
## [1] &quot;step =  5  lambda =  0.000629868179097574  loss:  2.13574249106378&quot;
## [1] &quot;step =  6  lambda =  0.000629868179097574  loss:  2.13573985773422&quot;
## [1] &quot;step =  7  lambda =  0.000629868179097574  loss:  2.13573764503181&quot;
## [1] &quot;step =  8  lambda =  0.000629868179097574  loss:  2.1357358529026&quot;
## [1] &quot;step =  9  lambda =  0.000629868179097574  loss:  2.13573448116199&quot;
## [1] &quot;step =  10  lambda =  0.000629868179097574  loss:  2.13573352956666&quot;
## [1] &quot;step =  11  lambda =  0.000629868179097574  loss:  2.13573299785069&quot;
## [1] &quot;step =  12  lambda =  0.000629868179097574  loss:  2.13573288574253&quot;
## [1] &quot;step =  13  lambda =  0.000629868179097574  loss:  2.13573319297206&quot;
## [1] &quot;step =  1  lambda =  0.000623600885999444  loss:  2.1196234606306&quot;
## [1] &quot;step =  2  lambda =  0.000623600885999444  loss:  2.11961911020806&quot;
## [1] &quot;step =  3  lambda =  0.000623600885999444  loss:  2.11961517403705&quot;
## [1] &quot;step =  4  lambda =  0.000623600885999444  loss:  2.11961165354931&quot;
## [1] &quot;step =  5  lambda =  0.000623600885999444  loss:  2.11960855020007&quot;
## [1] &quot;step =  6  lambda =  0.000623600885999444  loss:  2.11960586468143&quot;
## [1] &quot;step =  7  lambda =  0.000623600885999444  loss:  2.1196035971978&quot;
## [1] &quot;step =  8  lambda =  0.000623600885999444  loss:  2.11960174769166&quot;
## [1] &quot;step =  9  lambda =  0.000623600885999444  loss:  2.11960031597617&quot;
## [1] &quot;step =  10  lambda =  0.000623600885999444  loss:  2.1195993018062&quot;
## [1] &quot;step =  11  lambda =  0.000623600885999444  loss:  2.11959870491414&quot;
## [1] &quot;step =  12  lambda =  0.000623600885999444  loss:  2.1195985250267&quot;
## [1] &quot;step =  13  lambda =  0.000623600885999444  loss:  2.11959876187197&quot;
## [1] &quot;step =  1  lambda =  0.000617395953509583  loss:  2.10358868413437&quot;
## [1] &quot;step =  2  lambda =  0.000617395953509583  loss:  2.10358431446954&quot;
## [1] &quot;step =  3  lambda =  0.000617395953509583  loss:  2.10358035649531&quot;
## [1] &quot;step =  4  lambda =  0.000617395953509583  loss:  2.10357681161567&quot;
## [1] &quot;step =  5  lambda =  0.000617395953509583  loss:  2.10357368126319&quot;
## [1] &quot;step =  6  lambda =  0.000617395953509583  loss:  2.10357096611833&quot;
## [1] &quot;step =  7  lambda =  0.000617395953509583  loss:  2.10356866637996&quot;
## [1] &quot;step =  8  lambda =  0.000617395953509583  loss:  2.1035667819878&quot;
## [1] &quot;step =  9  lambda =  0.000617395953509583  loss:  2.1035653127534&quot;
## [1] &quot;step =  10  lambda =  0.000617395953509583  loss:  2.10356425843035&quot;
## [1] &quot;step =  11  lambda =  0.000617395953509583  loss:  2.1035636187498&quot;
## [1] &quot;step =  12  lambda =  0.000617395953509583  loss:  2.10356339343711&quot;
## [1] &quot;step =  13  lambda =  0.000617395953509583  loss:  2.10356358221891&quot;
## [1] &quot;step =  1  lambda =  0.000611252761129572  loss:  2.08765281100121&quot;
## [1] &quot;step =  2  lambda =  0.000611252761129572  loss:  2.08764844376188&quot;
## [1] &quot;step =  3  lambda =  0.000611252761129572  loss:  2.08764448562011&quot;
## [1] &quot;step =  4  lambda =  0.000611252761129572  loss:  2.08764093795261&quot;
## [1] &quot;step =  5  lambda =  0.000611252761129572  loss:  2.08763780216947&quot;
## [1] &quot;step =  6  lambda =  0.000611252761129572  loss:  2.08763507893971&quot;
## [1] &quot;step =  7  lambda =  0.000611252761129572  loss:  2.08763276845675&quot;
## [1] &quot;step =  8  lambda =  0.000611252761129572  loss:  2.08763087065778&quot;
## [1] &quot;step =  9  lambda =  0.000611252761129572  loss:  2.08762938535288&quot;
## [1] &quot;step =  10  lambda =  0.000611252761129572  loss:  2.08762831229457&quot;
## [1] &quot;step =  11  lambda =  0.000611252761129572  loss:  2.08762765121295&quot;
## [1] &quot;step =  12  lambda =  0.000611252761129572  loss:  2.08762740183222&quot;
## [1] &quot;step =  13  lambda =  0.000611252761129572  loss:  2.08762756387778&quot;
## [1] &quot;step =  1  lambda =  0.000605170694535053  loss:  2.07181574713958&quot;
## [1] &quot;step =  2  lambda =  0.000605170694535053  loss:  2.07181140301179&quot;
## [1] &quot;step =  3  lambda =  0.000605170694535053  loss:  2.07180746535943&quot;
## [1] &quot;step =  4  lambda =  0.000605170694535053  loss:  2.07180393553226&quot;
## [1] &quot;step =  5  lambda =  0.000605170694535053  loss:  2.07180081491805&quot;
## [1] &quot;step =  6  lambda =  0.000605170694535053  loss:  2.07179810417443&quot;
## [1] &quot;step =  7  lambda =  0.000605170694535053  loss:  2.07179580348955&quot;
## [1] &quot;step =  8  lambda =  0.000605170694535053  loss:  2.0717939127981&quot;
## [1] &quot;step =  9  lambda =  0.000605170694535053  loss:  2.07179243190889&quot;
## [1] &quot;step =  10  lambda =  0.000605170694535053  loss:  2.07179136057348&quot;
## [1] &quot;step =  11  lambda =  0.000605170694535053  loss:  2.07179069852108&quot;
## [1] &quot;step =  12  lambda =  0.000605170694535053  loss:  2.07179044547493&quot;
## [1] &quot;step =  13  lambda =  0.000605170694535053  loss:  2.07179060115932&quot;
## [1] &quot;step =  1  lambda =  0.000599149145514298  loss:  2.05607738304742&quot;
## [1] &quot;step =  2  lambda =  0.000599149145514298  loss:  2.05607308177617&quot;
## [1] &quot;step =  3  lambda =  0.000599149145514298  loss:  2.05606918433145&quot;
## [1] &quot;step =  4  lambda =  0.000599149145514298  loss:  2.0560656920364&quot;
## [1] &quot;step =  5  lambda =  0.000599149145514298  loss:  2.05606260625661&quot;
## [1] &quot;step =  6  lambda =  0.000599149145514298  loss:  2.05605992763845&quot;
## [1] &quot;step =  7  lambda =  0.000599149145514298  loss:  2.05605765636482&quot;
## [1] &quot;step =  8  lambda =  0.000599149145514298  loss:  2.05605579236807&quot;
## [1] &quot;step =  9  lambda =  0.000599149145514298  loss:  2.05605433545578&quot;
## [1] &quot;step =  10  lambda =  0.000599149145514298  loss:  2.05605328537874&quot;
## [1] &quot;step =  11  lambda =  0.000599149145514298  loss:  2.0560526418654&quot;
## [1] &quot;step =  12  lambda =  0.000599149145514298  loss:  2.05605240463815&quot;
## [1] &quot;step =  13  lambda =  0.000599149145514298  loss:  2.05605257342036&quot;
## [1] &quot;step =  1  lambda =  0.000593187511907387  loss:  2.0404375944095&quot;
## [1] &quot;step =  2  lambda =  0.000593187511907387  loss:  2.04043335483437&quot;
## [1] &quot;step =  3  lambda =  0.000593187511907387  loss:  2.04042951641221&quot;
## [1] &quot;step =  4  lambda =  0.000593187511907387  loss:  2.04042608043986&quot;
## [1] &quot;step =  5  lambda =  0.000593187511907387  loss:  2.04042304826091&quot;
## [1] &quot;step =  6  lambda =  0.000593187511907387  loss:  2.0404204205105&quot;
## [1] &quot;step =  7  lambda =  0.000593187511907387  loss:  2.04041819736639&quot;
## [1] &quot;step =  8  lambda =  0.000593187511907387  loss:  2.04041637875864&quot;
## [1] &quot;step =  9  lambda =  0.000593187511907387  loss:  2.04041496449374&quot;
## [1] &quot;step =  10  lambda =  0.000593187511907387  loss:  2.04041395432181&quot;
## [1] &quot;step =  11  lambda =  0.000593187511907387  loss:  2.04041334797066&quot;
## [1] &quot;step =  12  lambda =  0.000593187511907387  loss:  2.04041314516199&quot;
## [1] &quot;step =  13  lambda =  0.000593187511907387  loss:  2.04041334561836&quot;
## [1] &quot;step =  1  lambda =  0.000587285197545991  loss:  2.02489624264824&quot;
## [1] &quot;step =  2  lambda =  0.000587285197545991  loss:  2.02489208273643&quot;
## [1] &quot;step =  3  lambda =  0.000587285197545991  loss:  2.02488832128132&quot;
## [1] &quot;step =  4  lambda =  0.000587285197545991  loss:  2.02488495955377&quot;
## [1] &quot;step =  5  lambda =  0.000587285197545991  loss:  2.02488199887548&quot;
## [1] &quot;step =  6  lambda =  0.000587285197545991  loss:  2.02487943987045&quot;
## [1] &quot;step =  7  lambda =  0.000587285197545991  loss:  2.02487728271138&quot;
## [1] &quot;step =  8  lambda =  0.000587285197545991  loss:  2.02487552732611&quot;
## [1] &quot;step =  9  lambda =  0.000587285197545991  loss:  2.02487417352016&quot;
## [1] &quot;step =  10  lambda =  0.000587285197545991  loss:  2.02487322104303&quot;
## [1] &quot;step =  11  lambda =  0.000587285197545991  loss:  2.02487266962203&quot;
## [1] &quot;step =  12  lambda =  0.000587285197545991  loss:  2.0248725189783&quot;
## [1] &quot;step =  13  lambda =  0.000587285197545991  loss:  2.02487276883372&quot;
## [1] &quot;step =  1  lambda =  0.000581441612193756  loss:  2.00945317544234&quot;
## [1] &quot;step =  2  lambda =  0.000581441612193756  loss:  2.00944911231956&quot;
## [1] &quot;step =  3  lambda =  0.000581441612193756  loss:  2.00944544493623&quot;
## [1] &quot;step =  4  lambda =  0.000581441612193756  loss:  2.00944217453757&quot;
## [1] &quot;step =  5  lambda =  0.000581441612193756  loss:  2.00943930242356&quot;
## [1] &quot;step =  6  lambda =  0.000581441612193756  loss:  2.00943682920714&quot;
## [1] &quot;step =  7  lambda =  0.000581441612193756  loss:  2.00943475505598&quot;
## [1] &quot;step =  8  lambda =  0.000581441612193756  loss:  2.00943307989577&quot;
## [1] &quot;step =  9  lambda =  0.000581441612193756  loss:  2.00943180353111&quot;
## [1] &quot;step =  10  lambda =  0.000581441612193756  loss:  2.00943092571105&quot;
## [1] &quot;step =  11  lambda =  0.000581441612193756  loss:  2.00943044616249&quot;
## [1] &quot;step =  12  lambda =  0.000581441612193756  loss:  2.00943036460607&quot;
## [1] &quot;step =  13  lambda =  0.000581441612193756  loss:  2.00943068076313&quot;
## [1] &quot;step =  1  lambda =  0.000575656171487276  loss:  1.99410822721677&quot;
## [1] &quot;step =  2  lambda =  0.000575656171487276  loss:  1.99410427719603&quot;
## [1] &quot;step =  3  lambda =  0.000575656171487276  loss:  1.99410072017823&quot;
## [1] &quot;step =  4  lambda =  0.000575656171487276  loss:  1.9940975573832&quot;
## [1] &quot;step =  5  lambda =  0.000575656171487276  loss:  1.99409479008936&quot;
## [1] &quot;step =  6  lambda =  0.000575656171487276  loss:  1.99409241889867&quot;
## [1] &quot;step =  7  lambda =  0.000575656171487276  loss:  1.99409044397382&quot;
## [1] &quot;step =  8  lambda =  0.000575656171487276  loss:  1.99408886523842&quot;
## [1] &quot;step =  9  lambda =  0.000575656171487276  loss:  1.99408768249625&quot;
## [1] &quot;step =  10  lambda =  0.000575656171487276  loss:  1.99408689549596&quot;
## [1] &quot;step =  11  lambda =  0.000575656171487276  loss:  1.99408650396413&quot;
## [1] &quot;step =  12  lambda =  0.000575656171487276  loss:  1.99408650762105&quot;
## [1] &quot;step =  1  lambda =  0.00056992829687766  loss:  1.97886546077279&quot;
## [1] &quot;step =  2  lambda =  0.00056992829687766  loss:  1.97886124448795&quot;
## [1] &quot;step =  3  lambda =  0.00056992829687766  loss:  1.97885741872382&quot;
## [1] &quot;step =  4  lambda =  0.00056992829687766  loss:  1.97885398467288&quot;
## [1] &quot;step =  5  lambda =  0.00056992829687766  loss:  1.97885094358981&quot;
## [1] &quot;step =  6  lambda =  0.00056992829687766  loss:  1.97884829606358&quot;
## [1] &quot;step =  7  lambda =  0.00056992829687766  loss:  1.97884604225025&quot;
## [1] &quot;step =  8  lambda =  0.00056992829687766  loss:  1.97884418207005&quot;
## [1] &quot;step =  9  lambda =  0.00056992829687766  loss:  1.97884271532489&quot;
## [1] &quot;step =  10  lambda =  0.00056992829687766  loss:  1.97884164176224&quot;
## [1] &quot;step =  11  lambda =  0.00056992829687766  loss:  1.97884096110772&quot;
## [1] &quot;step =  12  lambda =  0.00056992829687766  loss:  1.97884067308075&quot;
## [1] &quot;step =  13  lambda =  0.00056992829687766  loss:  1.9788407774013&quot;
## [1] &quot;step =  1  lambda =  0.000564257415572674  loss:  1.96371607249545&quot;
## [1] &quot;step =  2  lambda =  0.000564257415572674  loss:  1.9637120025125&quot;
## [1] &quot;step =  3  lambda =  0.000564257415572674  loss:  1.96370832026447&quot;
## [1] &quot;step =  4  lambda =  0.000564257415572674  loss:  1.96370502692198&quot;
## [1] &quot;step =  5  lambda =  0.000564257415572674  loss:  1.96370212372077&quot;
## [1] &quot;step =  6  lambda =  0.000564257415572674  loss:  1.96369961124079&quot;
## [1] &quot;step =  7  lambda =  0.000564257415572674  loss:  1.96369748963458&quot;
## [1] &quot;step =  8  lambda =  0.000564257415572674  loss:  1.96369575882145&quot;
## [1] &quot;step =  9  lambda =  0.000564257415572674  loss:  1.96369441860339&quot;
## [1] &quot;step =  10  lambda =  0.000564257415572674  loss:  1.96369346872821&quot;
## [1] &quot;step =  11  lambda =  0.000564257415572674  loss:  1.96369290892185&quot;
## [1] &quot;step =  12  lambda =  0.000564257415572674  loss:  1.96369273890394&quot;
## [1] &quot;step =  13  lambda =  0.000564257415572674  loss:  1.9636929583945&quot;
## [1] &quot;step =  1  lambda =  0.000558642960479461  loss:  1.94866421719645&quot;
## [1] &quot;step =  2  lambda =  0.000558642960479461  loss:  1.94866030747389&quot;
## [1] &quot;step =  3  lambda =  0.000558642960479461  loss:  1.94865678269777&quot;
## [1] &quot;step =  4  lambda =  0.000558642960479461  loss:  1.94865364401455&quot;
## [1] &quot;step =  5  lambda =  0.000558642960479461  loss:  1.9486508926391&quot;
## [1] &quot;step =  6  lambda =  0.000558642960479461  loss:  1.94864852914075&quot;
## [1] &quot;step =  7  lambda =  0.000558642960479461  loss:  1.94864655366739&quot;
## [1] &quot;step =  8  lambda =  0.000558642960479461  loss:  1.94864496613659&quot;
## [1] &quot;step =  9  lambda =  0.000558642960479461  loss:  1.94864376634984&quot;
## [1] &quot;step =  10  lambda =  0.000558642960479461  loss:  1.94864295405491&quot;
## [1] &quot;step =  11  lambda =  0.000558642960479461  loss:  1.94864252897784&quot;
## [1] &quot;step =  12  lambda =  0.000558642960479461  loss:  1.94864249083825&quot;
## [1] &quot;step =  13  lambda =  0.000558642960479461  loss:  1.9486428393561&quot;
## [1] &quot;step =  1  lambda =  0.000553084370147834  loss:  1.93370968083619&quot;
## [1] &quot;step =  2  lambda =  0.000553084370147834  loss:  1.93370594467928&quot;
## [1] &quot;step =  3  lambda =  0.000553084370147834  loss:  1.93370259066839&quot;
## [1] &quot;step =  4  lambda =  0.000553084370147834  loss:  1.93369961992599&quot;
## [1] &quot;step =  5  lambda =  0.000553084370147834  loss:  1.93369703364613&quot;
## [1] &quot;step =  6  lambda =  0.000553084370147834  loss:  1.93369483238755&quot;
## [1] &quot;step =  7  lambda =  0.000553084370147834  loss:  1.93369301629347&quot;
## [1] &quot;step =  8  lambda =  0.000553084370147834  loss:  1.93369158527969&quot;
## [1] &quot;step =  9  lambda =  0.000553084370147834  loss:  1.9336905391472&quot;
## [1] &quot;step =  10  lambda =  0.000553084370147834  loss:  1.93368987764378&quot;
## [1] &quot;step =  11  lambda =  0.000553084370147834  loss:  1.93368960049554&quot;
## [1] &quot;step =  12  lambda =  0.000553084370147834  loss:  1.93368970742219&quot;
## [1] &quot;step =  1  lambda =  0.000547581088714126  loss:  1.9188561956069&quot;
## [1] &quot;step =  2  lambda =  0.000547581088714126  loss:  1.9188522618941&quot;
## [1] &quot;step =  3  lambda =  0.000547581088714126  loss:  1.91884870778881&quot;
## [1] &quot;step =  4  lambda =  0.000547581088714126  loss:  1.9188455343877&quot;
## [1] &quot;step =  5  lambda =  0.000547581088714126  loss:  1.91884274286202&quot;
## [1] &quot;step =  6  lambda =  0.000547581088714126  loss:  1.91884033375805&quot;
## [1] &quot;step =  7  lambda =  0.000547581088714126  loss:  1.91883830721275&quot;
## [1] &quot;step =  8  lambda =  0.000547581088714126  loss:  1.91883666313888&quot;
## [1] &quot;step =  9  lambda =  0.000547581088714126  loss:  1.91883540133592&quot;
## [1] &quot;step =  10  lambda =  0.000547581088714126  loss:  1.91883452155084&quot;
## [1] &quot;step =  11  lambda =  0.000547581088714126  loss:  1.91883402350919&quot;
## [1] &quot;step =  12  lambda =  0.000547581088714126  loss:  1.91883390693019&quot;
## [1] &quot;step =  13  lambda =  0.000547581088714126  loss:  1.91883417153331&quot;
## [1] &quot;step =  1  lambda =  0.000542132565845609  loss:  1.90409542547674&quot;
## [1] &quot;step =  2  lambda =  0.000542132565845609  loss:  1.90409169288763&quot;
## [1] &quot;step =  3  lambda =  0.000542132565845609  loss:  1.90408833706912&quot;
## [1] &quot;step =  4  lambda =  0.000542132565845609  loss:  1.90408535909716&quot;
## [1] &quot;step =  5  lambda =  0.000542132565845609  loss:  1.90408276012467&quot;
## [1] &quot;step =  6  lambda =  0.000542132565845609  loss:  1.90408054068915&quot;
## [1] &quot;step =  7  lambda =  0.000542132565845609  loss:  1.90407870092427&quot;
## [1] &quot;step =  8  lambda =  0.000542132565845609  loss:  1.90407724074205&quot;
## [1] &quot;step =  9  lambda =  0.000542132565845609  loss:  1.90407615994231&quot;
## [1] &quot;step =  10  lambda =  0.000542132565845609  loss:  1.90407545827264&quot;
## [1] &quot;step =  11  lambda =  0.000542132565845609  loss:  1.90407513545926&quot;
## [1] &quot;step =  12  lambda =  0.000542132565845609  loss:  1.90407519122194&quot;
## [1] &quot;step =  1  lambda =  0.000536738256685455  loss:  1.88943517777571&quot;
## [1] &quot;step =  2  lambda =  0.000536738256685455  loss:  1.88943127933085&quot;
## [1] &quot;step =  3  lambda =  0.000536738256685455  loss:  1.8894277550954&quot;
## [1] &quot;step =  4  lambda =  0.000536738256685455  loss:  1.88942460612038&quot;
## [1] &quot;step =  5  lambda =  0.000536738256685455  loss:  1.88942183353652&quot;
## [1] &quot;step =  6  lambda =  0.000536738256685455  loss:  1.88941943786926&quot;
## [1] &quot;step =  7  lambda =  0.000536738256685455  loss:  1.88941741924627&quot;
## [1] &quot;step =  8  lambda =  0.000536738256685455  loss:  1.88941577757676&quot;
## [1] &quot;step =  9  lambda =  0.000536738256685455  loss:  1.88941451265929&quot;
## [1] &quot;step =  10  lambda =  0.000536738256685455  loss:  1.88941362424085&quot;
## [1] &quot;step =  11  lambda =  0.000536738256685455  loss:  1.88941311204734&quot;
## [1] &quot;step =  12  lambda =  0.000536738256685455  loss:  1.88941297579828&quot;
## [1] &quot;step =  13  lambda =  0.000536738256685455  loss:  1.88941321521329&quot;
## [1] &quot;step =  1  lambda =  0.000531397621798253  loss:  1.87486714289354&quot;
## [1] &quot;step =  2  lambda =  0.000531397621798253  loss:  1.87486347050364&quot;
## [1] &quot;step =  3  lambda =  0.000531397621798253  loss:  1.87486016946708&quot;
## [1] &quot;step =  4  lambda =  0.000531397621798253  loss:  1.87485724081477&quot;
## [1] &quot;step =  5  lambda =  0.000531397621798253  loss:  1.87485468565949&quot;
## [1] &quot;step =  6  lambda =  0.000531397621798253  loss:  1.8748525045181&quot;
## [1] &quot;step =  7  lambda =  0.000531397621798253  loss:  1.87485069751509&quot;
## [1] &quot;step =  8  lambda =  0.000531397621798253  loss:  1.87484926455908&quot;
## [1] &quot;step =  9  lambda =  0.000531397621798253  loss:  1.87484820544909&quot;
## [1] &quot;step =  10  lambda =  0.000531397621798253  loss:  1.87484751993294&quot;
## [1] &quot;step =  11  lambda =  0.000531397621798253  loss:  1.87484720773732&quot;
## [1] &quot;step =  12  lambda =  0.000531397621798253  loss:  1.87484726858248&quot;
## [1] &quot;step =  1  lambda =  0.000526110127116064  loss:  1.86039900950872&quot;
## [1] &quot;step =  2  lambda =  0.000526110127116064  loss:  1.86039520051502&quot;
## [1] &quot;step =  3  lambda =  0.000526110127116064  loss:  1.86039176029658&quot;
## [1] &quot;step =  4  lambda =  0.000526110127116064  loss:  1.86038868986019&quot;
## [1] &quot;step =  5  lambda =  0.000526110127116064  loss:  1.86038599029696&quot;
## [1] &quot;step =  6  lambda =  0.000526110127116064  loss:  1.86038366211191&quot;
## [1] &quot;step =  7  lambda =  0.000526110127116064  loss:  1.86038170542378&quot;
## [1] &quot;step =  8  lambda =  0.000526110127116064  loss:  1.86038012013853&quot;
## [1] &quot;step =  9  lambda =  0.000526110127116064  loss:  1.86037890605408&quot;
## [1] &quot;step =  10  lambda =  0.000526110127116064  loss:  1.86037806291779&quot;
## [1] &quot;step =  11  lambda =  0.000526110127116064  loss:  1.86037759045624&quot;
## [1] &quot;step =  12  lambda =  0.000526110127116064  loss:  1.86037748838955&quot;
## [1] &quot;step =  13  lambda =  0.000526110127116064  loss:  1.86037775643791&quot;
## [1] &quot;step =  1  lambda =  0.000520875243885012  loss:  1.84602260690215&quot;
## [1] &quot;step =  2  lambda =  0.000520875243885012  loss:  1.84601904650122&quot;
## [1] &quot;step =  3  lambda =  0.000520875243885012  loss:  1.84601585200555&quot;
## [1] &quot;step =  4  lambda =  0.000520875243885012  loss:  1.84601302440243&quot;
## [1] &quot;step =  5  lambda =  0.000520875243885012  loss:  1.84601056476537&quot;
## [1] &quot;step =  6  lambda =  0.000520875243885012  loss:  1.84600847359101&quot;
## [1] &quot;step =  7  lambda =  0.000520875243885012  loss:  1.84600675099498&quot;
## [1] &quot;step =  8  lambda =  0.000520875243885012  loss:  1.84600539688276&quot;
## [1] &quot;step =  9  lambda =  0.000520875243885012  loss:  1.84600441105283&quot;
## [1] &quot;step =  10  lambda =  0.000520875243885012  loss:  1.84600379325349&quot;
## [1] &quot;step =  11  lambda =  0.000520875243885012  loss:  1.84600354321228&quot;
## [1] &quot;step =  12  lambda =  0.000520875243885012  loss:  1.8460036606502&quot;
## [1] &quot;step =  1  lambda =  0.000515692448612414  loss:  1.83174540041746&quot;
## [1] &quot;step =  2  lambda =  0.000515692448612414  loss:  1.83174173038319&quot;
## [1] &quot;step =  3  lambda =  0.000515692448612414  loss:  1.83173842366372&quot;
## [1] &quot;step =  4  lambda =  0.000515692448612414  loss:  1.83173548122293&quot;
## [1] &quot;step =  5  lambda =  0.000515692448612414  loss:  1.83173290411317&quot;
## [1] &quot;step =  6  lambda =  0.000515692448612414  loss:  1.83173069281954&quot;
## [1] &quot;step =  7  lambda =  0.000515692448612414  loss:  1.83172884745208&quot;
## [1] &quot;step =  8  lambda =  0.000515692448612414  loss:  1.83172736791375&quot;
## [1] &quot;step =  9  lambda =  0.000515692448612414  loss:  1.83172625400207&quot;
## [1] &quot;step =  10  lambda =  0.000515692448612414  loss:  1.83172550546504&quot;
## [1] &quot;step =  11  lambda =  0.000515692448612414  loss:  1.83172512203017&quot;
## [1] &quot;step =  12  lambda =  0.000515692448612414  loss:  1.83172510341855&quot;
## [1] &quot;step =  13  lambda =  0.000515692448612414  loss:  1.83172544935117&quot;
## [1] &quot;step =  1  lambda =  0.000510561223014422  loss:  1.81755946083752&quot;
## [1] &quot;step =  2  lambda =  0.000510561223014422  loss:  1.81755605970416&quot;
## [1] &quot;step =  3  lambda =  0.000510561223014422  loss:  1.81755301900615&quot;
## [1] &quot;step =  4  lambda =  0.000510561223014422  loss:  1.81755033968843&quot;
## [1] &quot;step =  5  lambda =  0.000510561223014422  loss:  1.8175480227861&quot;
## [1] &quot;step =  6  lambda =  0.000510561223014422  loss:  1.81754606877602&quot;
## [1] &quot;step =  7  lambda =  0.000510561223014422  loss:  1.81754447776524&quot;
## [1] &quot;step =  8  lambda =  0.000510561223014422  loss:  1.81754324965632&quot;
## [1] &quot;step =  9  lambda =  0.000510561223014422  loss:  1.81754238424741&quot;
## [1] &quot;step =  10  lambda =  0.000510561223014422  loss:  1.81754188128756&quot;
## [1] &quot;step =  11  lambda =  0.000510561223014422  loss:  1.81754174050536&quot;
## [1] &quot;step =  12  lambda =  0.000510561223014422  loss:  1.81754196162287&quot;
## [1] &quot;step =  1  lambda =  0.0005054810539642  loss:  1.80347193454801&quot;
## [1] &quot;step =  2  lambda =  0.0005054810539642  loss:  1.80346844861586&quot;
## [1] &quot;step =  3  lambda =  0.0005054810539642  loss:  1.80346532051999&quot;
## [1] &quot;step =  4  lambda =  0.0005054810539642  loss:  1.80346255118274&quot;
## [1] &quot;step =  5  lambda =  0.0005054810539642  loss:  1.8034601416185&quot;
## [1] &quot;step =  6  lambda =  0.0005054810539642  loss:  1.80345809229288&quot;
## [1] &quot;step =  7  lambda =  0.0005054810539642  loss:  1.80345640330748&quot;
## [1] &quot;step =  8  lambda =  0.0005054810539642  loss:  1.80345507456248&quot;
## [1] &quot;step =  9  lambda =  0.0005054810539642  loss:  1.80345410585519&quot;
## [1] &quot;step =  10  lambda =  0.0005054810539642  loss:  1.80345349693448&quot;
## [1] &quot;step =  11  lambda =  0.0005054810539642  loss:  1.80345324752902&quot;
## [1] &quot;step =  12  lambda =  0.0005054810539642  loss:  1.8034533573611&quot;
## [1] &quot;step =  1  lambda =  0.000500451433440611  loss:  1.78947880514764&quot;
## [1] &quot;step =  2  lambda =  0.000500451433440611  loss:  1.78947524746567&quot;
## [1] &quot;step =  3  lambda =  0.000500451433440611  loss:  1.7894720450069&quot;
## [1] &quot;step =  4  lambda =  0.000500451433440611  loss:  1.7894691986735&quot;
## [1] &quot;step =  5  lambda =  0.000500451433440611  loss:  1.78946670946117&quot;
## [1] &quot;step =  6  lambda =  0.000500451433440611  loss:  1.7894645778258&quot;
## [1] &quot;step =  7  lambda =  0.000500451433440611  loss:  1.78946280386468&quot;
## [1] &quot;step =  8  lambda =  0.000500451433440611  loss:  1.78946138747644&quot;
## [1] &quot;step =  9  lambda =  0.000500451433440611  loss:  1.78946032845814&quot;
## [1] &quot;step =  10  lambda =  0.000500451433440611  loss:  1.78945962655897&quot;
## [1] &quot;step =  11  lambda =  0.000500451433440611  loss:  1.78945928150809&quot;
## [1] &quot;step =  12  lambda =  0.000500451433440611  loss:  1.78945929302829&quot;
## [1] &quot;step =  1  lambda =  0.00049547185847741  loss:  1.77557972882512&quot;
## [1] &quot;step =  2  lambda =  0.00049547185847741  loss:  1.77557611190349&quot;
## [1] &quot;step =  3  lambda =  0.00049547185847741  loss:  1.7755728475879&quot;
## [1] &quot;step =  4  lambda =  0.00049547185847741  loss:  1.77556993676076&quot;
## [1] &quot;step =  5  lambda =  0.00049547185847741  loss:  1.7755673803995&quot;
## [1] &quot;step =  6  lambda =  0.00049547185847741  loss:  1.77556517895047&quot;
## [1] &quot;step =  7  lambda =  0.00049547185847741  loss:  1.7755633325068&quot;
## [1] &quot;step =  8  lambda =  0.00049547185847741  loss:  1.7755618409657&quot;
## [1] &quot;step =  9  lambda =  0.00049547185847741  loss:  1.77556070412416&quot;
## [1] &quot;step =  10  lambda =  0.00049547185847741  loss:  1.77555992173176&quot;
## [1] &quot;step =  11  lambda =  0.00049547185847741  loss:  1.77555949351826&quot;
## [1] &quot;step =  12  lambda =  0.00049547185847741  loss:  1.77555941920709&quot;
## [1] &quot;step =  13  lambda =  0.00049547185847741  loss:  1.77555969852136&quot;
## [1] &quot;step =  1  lambda =  0.000490541831112951  loss:  1.76177067246133&quot;
## [1] &quot;step =  2  lambda =  0.000490541831112951  loss:  1.76176736185779&quot;
## [1] &quot;step =  3  lambda =  0.000490541831112951  loss:  1.76176440096961&quot;
## [1] &quot;step =  4  lambda =  0.000490541831112951  loss:  1.7617617906616&quot;
## [1] &quot;step =  5  lambda =  0.000490541831112951  loss:  1.76175953189481&quot;
## [1] &quot;step =  6  lambda =  0.000490541831112951  loss:  1.76175762510784&quot;
## [1] &quot;step =  7  lambda =  0.000490541831112951  loss:  1.76175607039116&quot;
## [1] &quot;step =  8  lambda =  0.000490541831112951  loss:  1.76175486764182&quot;
## [1] &quot;step =  9  lambda =  0.000490541831112951  loss:  1.76175401665771&quot;
## [1] &quot;step =  10  lambda =  0.000490541831112951  loss:  1.76175351718972&quot;
## [1] &quot;step =  11  lambda =  0.000490541831112951  loss:  1.76175336896898&quot;
## [1] &quot;step =  12  lambda =  0.000490541831112951  loss:  1.76175357172019&quot;
## [1] &quot;step =  1  lambda =  0.000485660858340389  loss:  1.74805860074325&quot;
## [1] &quot;step =  2  lambda =  0.000485660858340389  loss:  1.74805525175328&quot;
## [1] &quot;step =  3  lambda =  0.000485660858340389  loss:  1.74805224987051&quot;
## [1] &quot;step =  4  lambda =  0.000485660858340389  loss:  1.74804959593863&quot;
## [1] &quot;step =  5  lambda =  0.000485660858340389  loss:  1.74804729089905&quot;
## [1] &quot;step =  6  lambda =  0.000485660858340389  loss:  1.74804533517972&quot;
## [1] &quot;step =  7  lambda =  0.000485660858340389  loss:  1.74804372886605&quot;
## [1] &quot;step =  8  lambda =  0.000485660858340389  loss:  1.748042471853&quot;
## [1] &quot;step =  9  lambda =  0.000485660858340389  loss:  1.74804156393786&quot;
## [1] &quot;step =  10  lambda =  0.000485660858340389  loss:  1.7480410048716&quot;
## [1] &quot;step =  11  lambda =  0.000485660858340389  loss:  1.74804079438573&quot;
## [1] &quot;step =  12  lambda =  0.000485660858340389  loss:  1.7480409322054&quot;
## [1] &quot;step =  1  lambda =  0.00048082845205838  loss:  1.73443952855348&quot;
## [1] &quot;step =  2  lambda =  0.00048082845205838  loss:  1.73443615235455&quot;
## [1] &quot;step =  3  lambda =  0.00048082845205838  loss:  1.73443312064484&quot;
## [1] &quot;step =  4  lambda =  0.00048082845205838  loss:  1.73443043424917&quot;
## [1] &quot;step =  5  lambda =  0.00048082845205838  loss:  1.73442809409121&quot;
## [1] &quot;step =  6  lambda =  0.00048082845205838  loss:  1.73442610058967&quot;
## [1] &quot;step =  7  lambda =  0.00048082845205838  loss:  1.73442445382592&quot;
## [1] &quot;step =  8  lambda =  0.00048082845205838  loss:  1.73442315369362&quot;
## [1] &quot;step =  9  lambda =  0.00048082845205838  loss:  1.73442219999003&quot;
## [1] &quot;step =  10  lambda =  0.00048082845205838  loss:  1.73442159246665&quot;
## [1] &quot;step =  11  lambda =  0.00048082845205838  loss:  1.73442133085567&quot;
## [1] &quot;step =  12  lambda =  0.00048082845205838  loss:  1.734421414883&quot;
## [1] &quot;step =  1  lambda =  0.000476044129022269  loss:  1.72091309077931&quot;
## [1] &quot;step =  2  lambda =  0.000476044129022269  loss:  1.72090969807246&quot;
## [1] &quot;step =  3  lambda =  0.000476044129022269  loss:  1.72090664723608&quot;
## [1] &quot;step =  4  lambda =  0.000476044129022269  loss:  1.72090393907657&quot;
## [1] &quot;step =  5  lambda =  0.000476044129022269  loss:  1.72090157450018&quot;
## [1] &quot;step =  6  lambda =  0.000476044129022269  loss:  1.72089955391658&quot;
## [1] &quot;step =  7  lambda =  0.000476044129022269  loss:  1.72089787740326&quot;
## [1] &quot;step =  8  lambda =  0.000476044129022269  loss:  1.72089654485268&quot;
## [1] &quot;step =  9  lambda =  0.000476044129022269  loss:  1.72089555606219&quot;
## [1] &quot;step =  10  lambda =  0.000476044129022269  loss:  1.72089491078389&quot;
## [1] &quot;step =  11  lambda =  0.000476044129022269  loss:  1.72089460875078&quot;
## [1] &quot;step =  12  lambda =  0.000476044129022269  loss:  1.7208946496896&quot;
## [1] &quot;step =  1  lambda =  0.000471307410795765  loss:  1.70747891739428&quot;
## [1] &quot;step =  2  lambda =  0.000471307410795765  loss:  1.7074755184518&quot;
## [1] &quot;step =  3  lambda =  0.000471307410795765  loss:  1.70747245876171&quot;
## [1] &quot;step =  4  lambda =  0.000471307410795765  loss:  1.70746973911233&quot;
## [1] &quot;step =  5  lambda =  0.000471307410795765  loss:  1.70746736039272&quot;
## [1] &quot;step =  6  lambda =  0.000471307410795765  loss:  1.70746532300369&quot;
## [1] &quot;step =  7  lambda =  0.000471307410795765  loss:  1.7074636270189&quot;
## [1] &quot;step =  8  lambda =  0.000471307410795765  loss:  1.70746227232973&quot;
## [1] &quot;step =  9  lambda =  0.000471307410795765  loss:  1.70746125873363&quot;
## [1] &quot;step =  10  lambda =  0.000471307410795765  loss:  1.70746058598339&quot;
## [1] &quot;step =  11  lambda =  0.000471307410795765  loss:  1.70746025381286&quot;
## [1] &quot;step =  12  lambda =  0.000471307410795765  loss:  1.70746026194968&quot;
## [1] &quot;step =  1  lambda =  0.000466617823703098  loss:  1.69413663381619&quot;
## [1] &quot;step =  2  lambda =  0.000466617823703098  loss:  1.6941332384991&quot;
## [1] &quot;step =  3  lambda =  0.000466617823703098  loss:  1.69413017981789&quot;
## [1] &quot;step =  4  lambda =  0.000466617823703098  loss:  1.69412745854308&quot;
## [1] &quot;step =  5  lambda =  0.000466617823703098  loss:  1.69412507554681&quot;
## [1] &quot;step =  6  lambda =  0.000466617823703098  loss:  1.69412303122112&quot;
## [1] &quot;step =  7  lambda =  0.000466617823703098  loss:  1.69412132563598&quot;
## [1] &quot;step =  8  lambda =  0.000466617823703098  loss:  1.69411995868166&quot;
## [1] &quot;step =  9  lambda =  0.000466617823703098  loss:  1.69411893015583&quot;
## [1] &quot;step =  10  lambda =  0.000466617823703098  loss:  1.69411823981197&quot;
## [1] &quot;step =  11  lambda =  0.000466617823703098  loss:  1.69411788738488&quot;
## [1] &quot;step =  12  lambda =  0.000466617823703098  loss:  1.69411787260311&quot;
## [1] &quot;step =  13  lambda =  0.000466617823703098  loss:  1.69411819519467&quot;
## [1] &quot;step =  1  lambda =  0.000461974898781651  loss:  1.68088246145294&quot;
## [1] &quot;step =  2  lambda =  0.000461974898781651  loss:  1.68087941648752&quot;
## [1] &quot;step =  3  lambda =  0.000461974898781651  loss:  1.68087670527703&quot;
## [1] &quot;step =  4  lambda =  0.000461974898781651  loss:  1.68087432857607&quot;
## [1] &quot;step =  5  lambda =  0.000461974898781651  loss:  1.68087228724158&quot;
## [1] &quot;step =  6  lambda =  0.000461974898781651  loss:  1.6808705816584&quot;
## [1] &quot;step =  7  lambda =  0.000461974898781651  loss:  1.68086921189412&quot;
## [1] &quot;step =  8  lambda =  0.000461974898781651  loss:  1.68086817783907&quot;
## [1] &quot;step =  9  lambda =  0.000461974898781651  loss:  1.68086747929201&quot;
## [1] &quot;step =  10  lambda =  0.000461974898781651  loss:  1.68086711600797&quot;
## [1] &quot;step =  11  lambda =  0.000461974898781651  loss:  1.68086708772333&quot;
## [1] &quot;step =  12  lambda =  0.000461974898781651  loss:  1.68086739416817&quot;
## [1] &quot;step =  1  lambda =  0.000457378171735063  loss:  1.66772282850991&quot;
## [1] &quot;step =  2  lambda =  0.000457378171735063  loss:  1.66771980300096&quot;
## [1] &quot;step =  3  lambda =  0.000457378171735063  loss:  1.66771710864747&quot;
## [1] &quot;step =  4  lambda =  0.000457378171735063  loss:  1.66771474618496&quot;
## [1] &quot;step =  5  lambda =  0.000457378171735063  loss:  1.66771271645227&quot;
## [1] &quot;step =  6  lambda =  0.000457378171735063  loss:  1.66771101982444&quot;
## [1] &quot;step =  7  lambda =  0.000457378171735063  loss:  1.66770965636449&quot;
## [1] &quot;step =  8  lambda =  0.000457378171735063  loss:  1.66770862596099&quot;
## [1] &quot;step =  9  lambda =  0.000457378171735063  loss:  1.66770792841242&quot;
## [1] &quot;step =  10  lambda =  0.000457378171735063  loss:  1.66770756347415&quot;
## [1] &quot;step =  11  lambda =  0.000457378171735063  loss:  1.6677075308832&quot;
## [1] &quot;step =  12  lambda =  0.000457378171735063  loss:  1.66770783037041&quot;
## [1] &quot;step =  1  lambda =  0.000452827182886797  loss:  1.65465394533039&quot;
## [1] &quot;step =  2  lambda =  0.000452827182886797  loss:  1.65465094804872&quot;
## [1] &quot;step =  3  lambda =  0.000452827182886797  loss:  1.65464827931799&quot;
## [1] &quot;step =  4  lambda =  0.000452827182886797  loss:  1.65464593985667&quot;
## [1] &quot;step =  5  lambda =  0.000452827182886797  loss:  1.65464393048723&quot;
## [1] &quot;step =  6  lambda =  0.000452827182886797  loss:  1.65464225157615&quot;
## [1] &quot;step =  7  lambda =  0.000452827182886797  loss:  1.65464090318286&quot;
## [1] &quot;step =  8  lambda =  0.000452827182886797  loss:  1.65463988519486&quot;
## [1] &quot;step =  9  lambda =  0.000452827182886797  loss:  1.65463919741083&quot;
## [1] &quot;step =  10  lambda =  0.000452827182886797  loss:  1.65463883958689&quot;
## [1] &quot;step =  11  lambda =  0.000452827182886797  loss:  1.65463881146101&quot;
## [1] &quot;step =  12  lambda =  0.000452827182886797  loss:  1.65463911276502&quot;
## [1] &quot;step =  1  lambda =  0.000448321477134178  loss:  1.64167542119292&quot;
## [1] &quot;step =  2  lambda =  0.000448321477134178  loss:  1.64167246050881&quot;
## [1] &quot;step =  3  lambda =  0.000448321477134178  loss:  1.64166982577407&quot;
## [1] &quot;step =  4  lambda =  0.000448321477134178  loss:  1.64166751769058&quot;
## [1] &quot;step =  5  lambda =  0.000448321477134178  loss:  1.64166553706473&quot;
## [1] &quot;step =  6  lambda =  0.000448321477134178  loss:  1.64166388425464&quot;
## [1] &quot;step =  7  lambda =  0.000448321477134178  loss:  1.64166255931624&quot;
## [1] &quot;step =  8  lambda =  0.000448321477134178  loss:  1.64166156213609&quot;
## [1] &quot;step =  9  lambda =  0.000448321477134178  loss:  1.64166089251317&quot;
## [1] &quot;step =  10  lambda =  0.000448321477134178  loss:  1.64166055020441&quot;
## [1] &quot;step =  11  lambda =  0.000448321477134178  loss:  1.64166053494879&quot;
## [1] &quot;step =  12  lambda =  0.000448321477134178  loss:  1.6416608464792&quot;
## [1] &quot;step =  1  lambda =  0.000443860603902874  loss:  1.62878686158717&quot;
## [1] &quot;step =  2  lambda =  0.000443860603902874  loss:  1.62878394551193&quot;
## [1] &quot;step =  3  lambda =  0.000443860603902874  loss:  1.62878135278866&quot;
## [1] &quot;step =  4  lambda =  0.000443860603902874  loss:  1.62877908410292&quot;
## [1] &quot;step =  5  lambda =  0.000443860603902874  loss:  1.62877714024526&quot;
## [1] &quot;step =  6  lambda =  0.000443860603902874  loss:  1.62877552156559&quot;
## [1] &quot;step =  7  lambda =  0.000443860603902874  loss:  1.62877422811643&quot;
## [1] &quot;step =  8  lambda =  0.000443860603902874  loss:  1.62877325978345&quot;
## [1] &quot;step =  9  lambda =  0.000443860603902874  loss:  1.62877261636593&quot;
## [1] &quot;step =  10  lambda =  0.000443860603902874  loss:  1.6287722976217&quot;
## [1] &quot;step =  11  lambda =  0.000443860603902874  loss:  1.62877230329078&quot;
## [1] &quot;step =  1  lambda =  0.000439444117101845  loss:  1.61599107233661&quot;
## [1] &quot;step =  2  lambda =  0.000439444117101845  loss:  1.61598788451384&quot;
## [1] &quot;step =  3  lambda =  0.000439444117101845  loss:  1.61598501771442&quot;
## [1] &quot;step =  4  lambda =  0.000439444117101845  loss:  1.6159824726065&quot;
## [1] &quot;step =  5  lambda =  0.000439444117101845  loss:  1.61598024996338&quot;
## [1] &quot;step =  6  lambda =  0.000439444117101845  loss:  1.61597835012527&quot;
## [1] &quot;step =  7  lambda =  0.000439444117101845  loss:  1.61597677313988&quot;
## [1] &quot;step =  8  lambda =  0.000439444117101845  loss:  1.61597551889082&quot;
## [1] &quot;step =  9  lambda =  0.000439444117101845  loss:  1.61597458717669&quot;
## [1] &quot;step =  10  lambda =  0.000439444117101845  loss:  1.61597397775539&quot;
## [1] &quot;step =  11  lambda =  0.000439444117101845  loss:  1.61597369036734&quot;
## [1] &quot;step =  12  lambda =  0.000439444117101845  loss:  1.61597372474704&quot;
## [1] &quot;step =  1  lambda =  0.000435071575078732  loss:  1.60328119151529&quot;
## [1] &quot;step =  2  lambda =  0.000435071575078732  loss:  1.60327806599514&quot;
## [1] &quot;step =  3  lambda =  0.000435071575078732  loss:  1.60327525889836&quot;
## [1] &quot;step =  4  lambda =  0.000435071575078732  loss:  1.60327277087909&quot;
## [1] &quot;step =  5  lambda =  0.000435071575078732  loss:  1.60327060269697&quot;
## [1] &quot;step =  6  lambda =  0.000435071575078732  loss:  1.60326875468575&quot;
## [1] &quot;step =  7  lambda =  0.000435071575078732  loss:  1.60326722689103&quot;
## [1] &quot;step =  8  lambda =  0.000435071575078732  loss:  1.60326601919655&quot;
## [1] &quot;step =  9  lambda =  0.000435071575078732  loss:  1.60326513140202&quot;
## [1] &quot;step =  10  lambda =  0.000435071575078732  loss:  1.60326456326681&quot;
## [1] &quot;step =  11  lambda =  0.000435071575078732  loss:  1.60326431453287&quot;
## [1] &quot;step =  12  lambda =  0.000435071575078732  loss:  1.60326438493622&quot;
## [1] &quot;step =  1  lambda =  0.000430742540575688  loss:  1.59066006441547&quot;
## [1] &quot;step =  2  lambda =  0.000430742540575688  loss:  1.59065700813824&quot;
## [1] &quot;step =  3  lambda =  0.000430742540575688  loss:  1.59065426769978&quot;
## [1] &quot;step =  4  lambda =  0.000430742540575688  loss:  1.59065184373897&quot;
## [1] &quot;step =  5  lambda =  0.000430742540575688  loss:  1.59064973700043&quot;
## [1] &quot;step =  6  lambda =  0.000430742540575688  loss:  1.59064794781023&quot;
## [1] &quot;step =  7  lambda =  0.000430742540575688  loss:  1.59064647621086&quot;
## [1] &quot;step =  8  lambda =  0.000430742540575688  loss:  1.59064532208541&quot;
## [1] &quot;step =  9  lambda =  0.000430742540575688  loss:  1.59064448523416&quot;
## [1] &quot;step =  10  lambda =  0.000430742540575688  loss:  1.59064396541749&quot;
## [1] &quot;step =  11  lambda =  0.000430742540575688  loss:  1.59064376237862&quot;
## [1] &quot;step =  12  lambda =  0.000430742540575688  loss:  1.59064387585484&quot;
## [1] &quot;step =  1  lambda =  0.000426456580685654  loss:  1.57812728357655&quot;
## [1] &quot;step =  2  lambda =  0.000426456580685654  loss:  1.57812430321512&quot;
## [1] &quot;step =  3  lambda =  0.000426456580685654  loss:  1.57812163611544&quot;
## [1] &quot;step =  4  lambda =  0.000426456580685654  loss:  1.57811928290131&quot;
## [1] &quot;step =  5  lambda =  0.000426456580685654  loss:  1.57811724430245&quot;
## [1] &quot;step =  6  lambda =  0.000426456580685654  loss:  1.57811552063728&quot;
## [1] &quot;step =  7  lambda =  0.000426456580685654  loss:  1.57811411194518&quot;
## [1] &quot;step =  8  lambda =  0.000426456580685654  loss:  1.57811301810859&quot;
## [1] &quot;step =  9  lambda =  0.000426456580685654  loss:  1.57811223892826&quot;
## [1] &quot;step =  10  lambda =  0.000426456580685654  loss:  1.57811177416565&quot;
## [1] &quot;step =  11  lambda =  0.000426456580685654  loss:  1.57811162356519&quot;
## [1] &quot;step =  12  lambda =  0.000426456580685654  loss:  1.57811178686544&quot;
## [1] &quot;step =  1  lambda =  0.00042221326680907  loss:  1.5656824389506&quot;
## [1] &quot;step =  2  lambda =  0.00042221326680907  loss:  1.56567954088399&quot;
## [1] &quot;step =  3  lambda =  0.00042221326680907  loss:  1.56567695350953&quot;
## [1] &quot;step =  4  lambda =  0.00042221326680907  loss:  1.56567467743613&quot;
## [1] &quot;step =  5  lambda =  0.00042221326680907  loss:  1.56567271337886&quot;
## [1] &quot;step =  6  lambda =  0.00042221326680907  loss:  1.56567106164853&quot;
## [1] &quot;step =  7  lambda =  0.00042221326680907  loss:  1.5656697222815&quot;
## [1] &quot;step =  8  lambda =  0.00042221326680907  loss:  1.56566869515952&quot;
## [1] &quot;step =  9  lambda =  0.00042221326680907  loss:  1.56566798008391&quot;
## [1] &quot;step =  10  lambda =  0.00042221326680907  loss:  1.56566757681715&quot;
## [1] &quot;step =  11  lambda =  0.00042221326680907  loss:  1.56566748510491&quot;
## [1] &quot;step =  12  lambda =  0.00042221326680907  loss:  1.56566770468707&quot;
## [1] &quot;step =  1  lambda =  0.000418012174611013  loss:  1.55332511788967&quot;
## [1] &quot;step =  2  lambda =  0.000418012174611013  loss:  1.55332230820825&quot;
## [1] &quot;step =  3  lambda =  0.000418012174611013  loss:  1.55331980665698&quot;
## [1] &quot;step =  4  lambda =  0.000418012174611013  loss:  1.55331761383019&quot;
## [1] &quot;step =  5  lambda =  0.000418012174611013  loss:  1.55331573042847&quot;
## [1] &quot;step =  6  lambda =  0.000418012174611013  loss:  1.55331415675517&quot;
## [1] &quot;step =  7  lambda =  0.000418012174611013  loss:  1.55331289284364&quot;
## [1] &quot;step =  8  lambda =  0.000418012174611013  loss:  1.55331193857501&quot;
## [1] &quot;step =  9  lambda =  0.000418012174611013  loss:  1.55331129375113&quot;
## [1] &quot;step =  10  lambda =  0.000418012174611013  loss:  1.55331095813555&quot;
## [1] &quot;step =  11  lambda =  0.000418012174611013  loss:  1.55331093147521&quot;
## [1] &quot;step =  12  lambda =  0.000418012174611013  loss:  1.55331121351128&quot;
## [1] &quot;step =  1  lambda =  0.000413852883978762  loss:  1.54105490526008&quot;
## [1] &quot;step =  2  lambda =  0.000413852883978762  loss:  1.54105218977293&quot;
## [1] &quot;step =  3  lambda =  0.000413852883978762  loss:  1.54104977986188&quot;
## [1] &quot;step =  4  lambda =  0.000413852883978762  loss:  1.54104767610694&quot;
## [1] &quot;step =  5  lambda =  0.000413852883978762  loss:  1.54104587919439&quot;
## [1] &quot;step =  6  lambda =  0.000413852883978762  loss:  1.54104438942026&quot;
## [1] &quot;step =  7  lambda =  0.000413852883978762  loss:  1.54104320681491&quot;
## [1] &quot;step =  8  lambda =  0.000413852883978762  loss:  1.54104233125889&quot;
## [1] &quot;step =  9  lambda =  0.000413852883978762  loss:  1.54104176255461&quot;
## [1] &quot;step =  10  lambda =  0.000413852883978762  loss:  1.54104150046669&quot;
## [1] &quot;step =  11  lambda =  0.000413852883978762  loss:  1.54104154474335&quot;
## [1] &quot;step =  1  lambda =  0.000409734978979787  loss:  1.52887431943231&quot;
## [1] &quot;step =  2  lambda =  0.000409734978979787  loss:  1.52887139770425&quot;
## [1] &quot;step =  3  lambda =  0.000409734978979787  loss:  1.52886877926232&quot;
## [1] &quot;step =  4  lambda =  0.000409734978979787  loss:  1.52886646467128&quot;
## [1] &quot;step =  5  lambda =  0.000409734978979787  loss:  1.52886445460196&quot;
## [1] &quot;step =  6  lambda =  0.000409734978979787  loss:  1.52886274934168&quot;
## [1] &quot;step =  7  lambda =  0.000409734978979787  loss:  1.52886134891662&quot;
## [1] &quot;step =  8  lambda =  0.000409734978979787  loss:  1.52886025320563&quot;
## [1] &quot;step =  9  lambda =  0.000409734978979787  loss:  1.52885946201078&quot;
## [1] &quot;step =  10  lambda =  0.000409734978979787  loss:  1.52885897509698&quot;
## [1] &quot;step =  11  lambda =  0.000409734978979787  loss:  1.52885879221308&quot;
## [1] &quot;step =  12  lambda =  0.000409734978979787  loss:  1.52885891310232&quot;
## [1] &quot;step =  1  lambda =  0.000405658047820157  loss:  1.51677696939447&quot;
## [1] &quot;step =  2  lambda =  0.000405658047820157  loss:  1.51677415529806&quot;
## [1] &quot;step =  3  lambda =  0.000405658047820157  loss:  1.51677164193889&quot;
## [1] &quot;step =  4  lambda =  0.000405658047820157  loss:  1.51676942986939&quot;
## [1] &quot;step =  5  lambda =  0.000405658047820157  loss:  1.51676751974803&quot;
## [1] &quot;step =  6  lambda =  0.000405658047820157  loss:  1.51676591185627&quot;
## [1] &quot;step =  7  lambda =  0.000405658047820157  loss:  1.51676460621853&quot;
## [1] &quot;step =  8  lambda =  0.000405658047820157  loss:  1.51676360271391&quot;
## [1] &quot;step =  9  lambda =  0.000405658047820157  loss:  1.5167629011457&quot;
## [1] &quot;step =  10  lambda =  0.000405658047820157  loss:  1.51676250128043&quot;
## [1] &quot;step =  11  lambda =  0.000405658047820157  loss:  1.5167624028686&quot;
## [1] &quot;step =  12  lambda =  0.000405658047820157  loss:  1.51676260565508&quot;
## [1] &quot;step =  1  lambda =  0.000401621682803358  loss:  1.50476546361353&quot;
## [1] &quot;step =  2  lambda =  0.000401621682803358  loss:  1.50476276209243&quot;
## [1] &quot;step =  3  lambda =  0.000401621682803358  loss:  1.50476035877685&quot;
## [1] &quot;step =  4  lambda =  0.000401621682803358  loss:  1.50475825420579&quot;
## [1] &quot;step =  5  lambda =  0.000401621682803358  loss:  1.50475644902418&quot;
## [1] &quot;step =  6  lambda =  0.000401621682803358  loss:  1.50475494350661&quot;
## [1] &quot;step =  7  lambda =  0.000401621682803358  loss:  1.50475373767476&quot;
## [1] &quot;step =  8  lambda =  0.000401621682803358  loss:  1.50475283140734&quot;
## [1] &quot;step =  9  lambda =  0.000401621682803358  loss:  1.50475222450834&quot;
## [1] &quot;step =  10  lambda =  0.000401621682803358  loss:  1.50475191674546&quot;
## [1] &quot;step =  11  lambda =  0.000401621682803358  loss:  1.5047519078706&quot;
## [1] &quot;step =  12  lambda =  0.000401621682803358  loss:  1.50475219763008&quot;
## [1] &quot;step =  1  lambda =  0.000397625480289526  loss:  1.49283937814237&quot;
## [1] &quot;step =  2  lambda =  0.000397625480289526  loss:  1.49283679392824&quot;
## [1] &quot;step =  3  lambda =  0.000397625480289526  loss:  1.492834505398&quot;
## [1] &quot;step =  4  lambda =  0.000397625480289526  loss:  1.49283251307739&quot;
## [1] &quot;step =  5  lambda =  0.000397625480289526  loss:  1.49283081759793&quot;
## [1] &quot;step =  6  lambda =  0.000397625480289526  loss:  1.49282941922732&quot;
## [1] &quot;step =  7  lambda =  0.000397625480289526  loss:  1.49282831798456&quot;
## [1] &quot;step =  8  lambda =  0.000397625480289526  loss:  1.49282751374789&quot;
## [1] &quot;step =  9  lambda =  0.000397625480289526  loss:  1.49282700632196&quot;
## [1] &quot;step =  10  lambda =  0.000397625480289526  loss:  1.49282679547565&quot;
## [1] &quot;step =  11  lambda =  0.000397625480289526  loss:  1.49282688096223&quot;
## [1] &quot;step =  1  lambda =  0.000393669040655078  loss:  1.48100105886488&quot;
## [1] &quot;step =  2  lambda =  0.000393669040655078  loss:  1.48099830051318&quot;
## [1] &quot;step =  3  lambda =  0.000393669040655078  loss:  1.48099583558433&quot;
## [1] &quot;step =  4  lambda =  0.000393669040655078  loss:  1.48099366458996&quot;
## [1] &quot;step =  5  lambda =  0.000393669040655078  loss:  1.4809917881471&quot;
## [1] &quot;step =  6  lambda =  0.000393669040655078  loss:  1.48099020651532&quot;
## [1] &quot;step =  7  lambda =  0.000393669040655078  loss:  1.48098891970971&quot;
## [1] &quot;step =  8  lambda =  0.000393669040655078  loss:  1.48098792760706&quot;
## [1] &quot;step =  9  lambda =  0.000393669040655078  loss:  1.48098723001179&quot;
## [1] &quot;step =  10  lambda =  0.000393669040655078  loss:  1.48098682669321&quot;
## [1] &quot;step =  11  lambda =  0.000393669040655078  loss:  1.48098671740532&quot;
## [1] &quot;step =  12  lambda =  0.000393669040655078  loss:  1.48098690189671&quot;
## [1] &quot;step =  1  lambda =  0.000389751968252755  loss:  1.4692444142027&quot;
## [1] &quot;step =  2  lambda =  0.000389751968252755  loss:  1.46924178450457&quot;
## [1] &quot;step =  3  lambda =  0.000389751968252755  loss:  1.46923944571645&quot;
## [1] &quot;step =  4  lambda =  0.000389751968252755  loss:  1.46923739833853&quot;
## [1] &quot;step =  5  lambda =  0.000389751968252755  loss:  1.46923564297618&quot;
## [1] &quot;step =  6  lambda =  0.000389751968252755  loss:  1.46923417988348&quot;
## [1] &quot;step =  7  lambda =  0.000389751968252755  loss:  1.46923300907394&quot;
## [1] &quot;step =  8  lambda =  0.000389751968252755  loss:  1.46923213042466&quot;
## [1] &quot;step =  9  lambda =  0.000389751968252755  loss:  1.46923154374135&quot;
## [1] &quot;step =  10  lambda =  0.000389751968252755  loss:  1.46923124879497&quot;
## [1] &quot;step =  11  lambda =  0.000389751968252755  loss:  1.46923124534122&quot;
## [1] &quot;step =  12  lambda =  0.000389751968252755  loss:  1.4692315331304&quot;
## [1] &quot;step =  1  lambda =  0.000385873871372051  loss:  1.45757190377901&quot;
## [1] &quot;step =  2  lambda =  0.000385873871372051  loss:  1.45756940670309&quot;
## [1] &quot;step =  3  lambda =  0.000385873871372051  loss:  1.45756719804223&quot;
## [1] &quot;step =  4  lambda =  0.000385873871372051  loss:  1.45756527828417&quot;
## [1] &quot;step =  5  lambda =  0.000385873871372051  loss:  1.45756364802161&quot;
## [1] &quot;step =  6  lambda =  0.000385873871372051  loss:  1.45756230750216&quot;
## [1] &quot;step =  7  lambda =  0.000385873871372051  loss:  1.45756125673683&quot;
## [1] &quot;step =  8  lambda =  0.000385873871372051  loss:  1.45756049560244&quot;
## [1] &quot;step =  9  lambda =  0.000385873871372051  loss:  1.45756002390548&quot;
## [1] &quot;step =  10  lambda =  0.000385873871372051  loss:  1.45755984141815&quot;
## [1] &quot;step =  11  lambda =  0.000385873871372051  loss:  1.45755994789763&quot;
## [1] &quot;step =  1  lambda =  0.000382034362200047  loss:  1.44598575871019&quot;
## [1] &quot;step =  2  lambda =  0.000382034362200047  loss:  1.44598310946679&quot;
## [1] &quot;step =  3  lambda =  0.000382034362200047  loss:  1.44598074640165&quot;
## [1] &quot;step =  4  lambda =  0.000382034362200047  loss:  1.44597866998923&quot;
## [1] &quot;step =  5  lambda =  0.000382034362200047  loss:  1.44597688080844&quot;
## [1] &quot;step =  6  lambda =  0.000382034362200047  loss:  1.44597537909917&quot;
## [1] &quot;step =  7  lambda =  0.000382034362200047  loss:  1.44597416486879&quot;
## [1] &quot;step =  8  lambda =  0.000382034362200047  loss:  1.4459732379928&quot;
## [1] &quot;step =  9  lambda =  0.000382034362200047  loss:  1.44597259827757&quot;
## [1] &quot;step =  10  lambda =  0.000382034362200047  loss:  1.44597224549582&quot;
## [1] &quot;step =  11  lambda =  0.000382034362200047  loss:  1.44597217940554&quot;
## [1] &quot;step =  12  lambda =  0.000382034362200047  loss:  1.44597239975947&quot;
## [1] &quot;step =  1  lambda =  0.000378233056782626  loss:  1.43448008819499&quot;
## [1] &quot;step =  2  lambda =  0.000378233056782626  loss:  1.43447758149145&quot;
## [1] &quot;step =  3  lambda =  0.000378233056782626  loss:  1.43447535848299&quot;
## [1] &quot;step =  4  lambda =  0.000378233056782626  loss:  1.43447341963326&quot;
## [1] &quot;step =  5  lambda =  0.000378233056782626  loss:  1.43447176551004&quot;
## [1] &quot;step =  6  lambda =  0.000378233056782626  loss:  1.43447039634804&quot;
## [1] &quot;step =  7  lambda =  0.000378233056782626  loss:  1.43446931215311&quot;
## [1] &quot;step =  8  lambda =  0.000378233056782626  loss:  1.43446851280116&quot;
## [1] &quot;step =  9  lambda =  0.000378233056782626  loss:  1.43446799809987&quot;
## [1] &quot;step =  10  lambda =  0.000378233056782626  loss:  1.43446776782364&quot;
## [1] &quot;step =  11  lambda =  0.000378233056782626  loss:  1.43446782173219&quot;
## [1] &quot;step =  1  lambda =  0.000374469574986078  loss:  1.42305990959583&quot;
## [1] &quot;step =  2  lambda =  0.000374469574986078  loss:  1.42305726494668&quot;
## [1] &quot;step =  3  lambda =  0.000374469574986078  loss:  1.42305490177202&quot;
## [1] &quot;step =  4  lambda =  0.000374469574986078  loss:  1.42305282052282&quot;
## [1] &quot;step =  5  lambda =  0.000374469574986078  loss:  1.42305102175363&quot;
## [1] &quot;step =  6  lambda =  0.000374469574986078  loss:  1.42304950569175&quot;
## [1] &quot;step =  7  lambda =  0.000374469574986078  loss:  1.42304827233962&quot;
## [1] &quot;step =  8  lambda =  0.000374469574986078  loss:  1.42304732157195&quot;
## [1] &quot;step =  9  lambda =  0.000374469574986078  loss:  1.42304665319643&quot;
## [1] &quot;step =  10  lambda =  0.000374469574986078  loss:  1.42304626698805&quot;
## [1] &quot;step =  11  lambda =  0.000374469574986078  loss:  1.42304616270744&quot;
## [1] &quot;step =  12  lambda =  0.000374469574986078  loss:  1.42304634011008&quot;
## [1] &quot;step =  1  lambda =  0.000370743540459088  loss:  1.41171933034134&quot;
## [1] &quot;step =  2  lambda =  0.000370743540459088  loss:  1.41171683721616&quot;
## [1] &quot;step =  3  lambda =  0.000370743540459088  loss:  1.41171462310252&quot;
## [1] &quot;step =  4  lambda =  0.000370743540459088  loss:  1.41171268844102&quot;
## [1] &quot;step =  5  lambda =  0.000370743540459088  loss:  1.41171103377547&quot;
## [1] &quot;step =  6  lambda =  0.000370743540459088  loss:  1.41170965932814&quot;
## [1] &quot;step =  7  lambda =  0.000370743540459088  loss:  1.41170856510009&quot;
## [1] &quot;step =  8  lambda =  0.000370743540459088  loss:  1.41170775096648&quot;
## [1] &quot;step =  9  lambda =  0.000370743540459088  loss:  1.41170721673634&quot;
## [1] &quot;step =  10  lambda =  0.000370743540459088  loss:  1.41170696218636&quot;
## [1] &quot;step =  11  lambda =  0.000370743540459088  loss:  1.41170698707895&quot;
## [1] &quot;step =  1  lambda =  0.000367054580595098  loss:  1.40046334289094&quot;
## [1] &quot;step =  2  lambda =  0.000367054580595098  loss:  1.40046072512931&quot;
## [1] &quot;step =  3  lambda =  0.000367054580595098  loss:  1.40045838417659&quot;
## [1] &quot;step =  4  lambda =  0.000367054580595098  loss:  1.4004563204612&quot;
## [1] &quot;step =  5  lambda =  0.000367054580595098  loss:  1.40045453451414&quot;
## [1] &quot;step =  6  lambda =  0.000367054580595098  loss:  1.40045302655059&quot;
## [1] &quot;step =  7  lambda =  0.000367054580595098  loss:  1.40045179656829&quot;
## [1] &quot;step =  8  lambda =  0.000367054580595098  loss:  1.40045084444132&quot;
## [1] &quot;step =  9  lambda =  0.000367054580595098  loss:  1.40045016997877&quot;
## [1] &quot;step =  10  lambda =  0.000367054580595098  loss:  1.40044977295796&quot;
## [1] &quot;step =  11  lambda =  0.000367054580595098  loss:  1.40044965314227&quot;
## [1] &quot;step =  12  lambda =  0.000367054580595098  loss:  1.40044981029&quot;
## [1] &quot;step =  1  lambda =  0.000363402326495048  loss:  1.38928609887829&quot;
## [1] &quot;step =  2  lambda =  0.000363402326495048  loss:  1.38928364078195&quot;
## [1] &quot;step =  3  lambda =  0.000363402326495048  loss:  1.38928145705308&quot;
## [1] &quot;step =  4  lambda =  0.000363402326495048  loss:  1.3892795481101&quot;
## [1] &quot;step =  5  lambda =  0.000363402326495048  loss:  1.38927791447361&quot;
## [1] &quot;step =  6  lambda =  0.000363402326495048  loss:  1.38927655635396&quot;
## [1] &quot;step =  7  lambda =  0.000363402326495048  loss:  1.38927547374756&quot;
## [1] &quot;step =  8  lambda =  0.000363402326495048  loss:  1.38927466652898&quot;
## [1] &quot;step =  9  lambda =  0.000363402326495048  loss:  1.38927413450869&quot;
## [1] &quot;step =  10  lambda =  0.000363402326495048  loss:  1.38927387746572&quot;
## [1] &quot;step =  11  lambda =  0.000363402326495048  loss:  1.38927389516523&quot;
## [1] &quot;step =  1  lambda =  0.000359786412930483  loss:  1.3781925225421&quot;
## [1] &quot;step =  2  lambda =  0.000359786412930483  loss:  1.3781899522602&quot;
## [1] &quot;step =  3  lambda =  0.000359786412930483  loss:  1.37818765416206&quot;
## [1] &quot;step =  4  lambda =  0.000359786412930483  loss:  1.37818562865444&quot;
## [1] &quot;step =  5  lambda =  0.000359786412930483  loss:  1.37818387624555&quot;
## [1] &quot;step =  6  lambda =  0.000359786412930483  loss:  1.37818239713887&quot;
## [1] &quot;step =  7  lambda =  0.000359786412930483  loss:  1.37818119132767&quot;
## [1] &quot;step =  8  lambda =  0.000359786412930483  loss:  1.3781802586855&quot;
## [1] &quot;step =  9  lambda =  0.000359786412930483  loss:  1.37817959902293&quot;
## [1] &quot;step =  10  lambda =  0.000359786412930483  loss:  1.37817921211971&quot;
## [1] &quot;step =  11  lambda =  0.000359786412930483  loss:  1.37817909774193&quot;
## [1] &quot;step =  12  lambda =  0.000359786412930483  loss:  1.37817925565081&quot;
## [1] &quot;step =  1  lambda =  0.000356206478307034  loss:  1.36717685206747&quot;
## [1] &quot;step =  2  lambda =  0.000356206478307034  loss:  1.36717444880051&quot;
## [1] &quot;step =  3  lambda =  0.000356206478307034  loss:  1.3671723152983&quot;
## [1] &quot;step =  4  lambda =  0.000356206478307034  loss:  1.36717045195796&quot;
## [1] &quot;step =  5  lambda =  0.000356206478307034  loss:  1.36716885927767&quot;
## [1] &quot;step =  6  lambda =  0.000356206478307034  loss:  1.36716753745622&quot;
## [1] &quot;step =  7  lambda =  0.000356206478307034  loss:  1.36716648648564&quot;
## [1] &quot;step =  8  lambda =  0.000356206478307034  loss:  1.36716570624001&quot;
## [1] &quot;step =  9  lambda =  0.000356206478307034  loss:  1.36716519653125&quot;
## [1] &quot;step =  10  lambda =  0.000356206478307034  loss:  1.36716495714085&quot;
## [1] &quot;step =  11  lambda =  0.000356206478307034  loss:  1.3671649878367&quot;
## [1] &quot;step =  1  lambda =  0.000352662164628256  loss:  1.35624390346137&quot;
## [1] &quot;step =  2  lambda =  0.000352662164628256  loss:  1.35624139964772&quot;
## [1] &quot;step =  3  lambda =  0.000352662164628256  loss:  1.35623916343488&quot;
## [1] &quot;step =  4  lambda =  0.000352662164628256  loss:  1.35623719520874&quot;
## [1] &quot;step =  5  lambda =  0.000352662164628256  loss:  1.35623549545553&quot;
## [1] &quot;step =  6  lambda =  0.000352662164628256  loss:  1.35623406436742&quot;
## [1] &quot;step =  7  lambda =  0.000352662164628256  loss:  1.35623290193339&quot;
## [1] &quot;step =  8  lambda =  0.000352662164628256  loss:  1.35623200802659&quot;
## [1] &quot;step =  9  lambda =  0.000352662164628256  loss:  1.35623138245913&quot;
## [1] &quot;step =  10  lambda =  0.000352662164628256  loss:  1.35623102501316&quot;
## [1] &quot;step =  11  lambda =  0.000352662164628256  loss:  1.35623093545763&quot;
## [1] &quot;step =  12  lambda =  0.000352662164628256  loss:  1.35623111355664&quot;
## [1] &quot;step =  1  lambda =  0.000349153117459826  loss:  1.34538804051864&quot;
## [1] &quot;step =  2  lambda =  0.000349153117459826  loss:  1.3453857103237&quot;
## [1] &quot;step =  3  lambda =  0.000349153117459826  loss:  1.34538364533375&quot;
## [1] &quot;step =  4  lambda =  0.000349153117459826  loss:  1.34538184592543&quot;
## [1] &quot;step =  5  lambda =  0.000349153117459826  loss:  1.34538031257524&quot;
## [1] &quot;step =  6  lambda =  0.000349153117459826  loss:  1.34537904547084&quot;
## [1] &quot;step =  7  lambda =  0.000349153117459826  loss:  1.34537804460001&quot;
## [1] &quot;step =  8  lambda =  0.000349153117459826  loss:  1.34537730983647&quot;
## [1] &quot;step =  9  lambda =  0.000349153117459826  loss:  1.34537684099369&quot;
## [1] &quot;step =  10  lambda =  0.000349153117459826  loss:  1.34537663785557&quot;
## [1] &quot;step =  11  lambda =  0.000349153117459826  loss:  1.34537670019284&quot;
## [1] &quot;step =  1  lambda =  0.000345678985894105  loss:  1.33461393413969&quot;
## [1] &quot;step =  2  lambda =  0.000345678985894105  loss:  1.3346115142672&quot;
## [1] &quot;step =  3  lambda =  0.000345678985894105  loss:  1.33460935745616&quot;
## [1] &quot;step =  4  lambda =  0.000345678985894105  loss:  1.33460746407247&quot;
## [1] &quot;step =  5  lambda =  0.000345678985894105  loss:  1.33460583458105&quot;
## [1] &quot;step =  6  lambda =  0.000345678985894105  loss:  1.33460446916316&quot;
## [1] &quot;step =  7  lambda =  0.000345678985894105  loss:  1.33460336780371&quot;
## [1] &quot;step =  8  lambda =  0.000345678985894105  loss:  1.33460253037552&quot;
## [1] &quot;step =  9  lambda =  0.000345678985894105  loss:  1.33460195669227&quot;
## [1] &quot;step =  10  lambda =  0.000345678985894105  loss:  1.33460164653861&quot;
## [1] &quot;step =  11  lambda =  0.000345678985894105  loss:  1.3346015996863&quot;
## [1] &quot;step =  12  lambda =  0.000345678985894105  loss:  1.3346018159024&quot;
## [1] &quot;step =  1  lambda =  0.000342239422515039  loss:  1.32391610968653&quot;
## [1] &quot;step =  2  lambda =  0.000342239422515039  loss:  1.32391386933252&quot;
## [1] &quot;step =  3  lambda =  0.000342239422515039  loss:  1.32391188966797&quot;
## [1] &quot;step =  4  lambda =  0.000342239422515039  loss:  1.3239101710499&quot;
## [1] &quot;step =  5  lambda =  0.000342239422515039  loss:  1.3239087139338&quot;
## [1] &quot;step =  6  lambda =  0.000342239422515039  loss:  1.32390751849658&quot;
## [1] &quot;step =  7  lambda =  0.000342239422515039  loss:  1.32390658472202&quot;
## [1] &quot;step =  8  lambda =  0.000342239422515039  loss:  1.32390591248353&quot;
## [1] &quot;step =  9  lambda =  0.000342239422515039  loss:  1.32390550159616&quot;
## [1] &quot;step =  10  lambda =  0.000342239422515039  loss:  1.32390535184629&quot;
## [1] &quot;step =  11  lambda =  0.000342239422515039  loss:  1.32390546300748&quot;
## [1] &quot;step =  1  lambda =  0.000338834083363426  loss:  1.31329905904658&quot;
## [1] &quot;step =  2  lambda =  0.000338834083363426  loss:  1.31329673915342&quot;
## [1] &quot;step =  3  lambda =  0.000338834083363426  loss:  1.31329467782718&quot;
## [1] &quot;step =  4  lambda =  0.000338834083363426  loss:  1.31329287541455&quot;
## [1] &quot;step =  5  lambda =  0.000338834083363426  loss:  1.31329133235985&quot;
## [1] &quot;step =  6  lambda =  0.000338834083363426  loss:  1.31329004883382&quot;
## [1] &quot;step =  7  lambda =  0.000338834083363426  loss:  1.31328902481747&quot;
## [1] &quot;step =  8  lambda =  0.000338834083363426  loss:  1.31328826018338&quot;
## [1] &quot;step =  9  lambda =  0.000338834083363426  loss:  1.31328775474687&quot;
## [1] &quot;step =  10  lambda =  0.000338834083363426  loss:  1.31328750829508&quot;
## [1] &quot;step =  11  lambda =  0.000338834083363426  loss:  1.31328752060261&quot;
## [1] &quot;step =  1  lambda =  0.000335462627902512  loss:  1.30275990555483&quot;
## [1] &quot;step =  2  lambda =  0.000335462627902512  loss:  1.30275751203875&quot;
## [1] &quot;step =  3  lambda =  0.000335462627902512  loss:  1.30275537497096&quot;
## [1] &quot;step =  4  lambda =  0.000335462627902512  loss:  1.30275349468885&quot;
## [1] &quot;step =  5  lambda =  0.000335462627902512  loss:  1.30275187162669&quot;
## [1] &quot;step =  6  lambda =  0.000335462627902512  loss:  1.30275050595&quot;
## [1] &quot;step =  7  lambda =  0.000335462627902512  loss:  1.30274939763777&quot;
## [1] &quot;step =  8  lambda =  0.000335462627902512  loss:  1.30274854656237&quot;
## [1] &quot;step =  9  lambda =  0.000335462627902512  loss:  1.30274795253974&quot;
## [1] &quot;step =  10  lambda =  0.000335462627902512  loss:  1.30274761535814&quot;
## [1] &quot;step =  11  lambda =  0.000335462627902512  loss:  1.30274753479346&quot;
## [1] &quot;step =  12  lambda =  0.000335462627902512  loss:  1.30274771061705&quot;
## [1] &quot;step =  1  lambda =  0.000332124718983941  loss:  1.29229572641307&quot;
## [1] &quot;step =  2  lambda =  0.000332124718983941  loss:  1.2922935212135&quot;
## [1] &quot;step =  3  lambda =  0.000332124718983941  loss:  1.29229157012517&quot;
## [1] &quot;step =  4  lambda =  0.000332124718983941  loss:  1.29228987347719&quot;
## [1] &quot;step =  5  lambda =  0.000332124718983941  loss:  1.292288431695&quot;
## [1] &quot;step =  6  lambda =  0.000332124718983941  loss:  1.29228724494006&quot;
## [1] &quot;step =  7  lambda =  0.000332124718983941  loss:  1.29228631319038&quot;
## [1] &quot;step =  8  lambda =  0.000332124718983941  loss:  1.292285636319&quot;
## [1] &quot;step =  9  lambda =  0.000332124718983941  loss:  1.29228521414333&quot;
## [1] &quot;step =  10  lambda =  0.000332124718983941  loss:  1.29228504645337&quot;
## [1] &quot;step =  11  lambda =  0.000332124718983941  loss:  1.29228513302686&quot;
## [1] &quot;step =  1  lambda =  0.00032882002281404  loss:  1.28191094477462&quot;
## [1] &quot;step =  2  lambda =  0.00032882002281404  loss:  1.28190867501281&quot;
## [1] &quot;step =  3  lambda =  0.00032882002281404  loss:  1.28190665727107&quot;
## [1] &quot;step =  4  lambda =  0.00032882002281404  loss:  1.28190489186882&quot;
## [1] &quot;step =  5  lambda =  0.00032882002281404  loss:  1.28190337922091&quot;
## [1] &quot;step =  6  lambda =  0.00032882002281404  loss:  1.28190211948298&quot;
## [1] &quot;step =  7  lambda =  0.00032882002281404  loss:  1.28190111263049&quot;
## [1] &quot;step =  8  lambda =  0.00032882002281404  loss:  1.28190035853576&quot;
## [1] &quot;step =  9  lambda =  0.00032882002281404  loss:  1.28189985701652&quot;
## [1] &quot;step =  10  lambda =  0.00032882002281404  loss:  1.2818996078636&quot;
## [1] &quot;step =  11  lambda =  0.00032882002281404  loss:  1.28189961085581&quot;
## [1] &quot;step =  1  lambda =  0.000325548208920438  loss:  1.27160271159294&quot;
## [1] &quot;step =  2  lambda =  0.000325548208920438  loss:  1.27160038266295&quot;
## [1] &quot;step =  3  lambda =  0.000325548208920438  loss:  1.27159830366663&quot;
## [1] &quot;step =  4  lambda =  0.000325548208920438  loss:  1.27159647491466&quot;
## [1] &quot;step =  5  lambda =  0.000325548208920438  loss:  1.27159489681236&quot;
## [1] &quot;step =  6  lambda =  0.000325548208920438  loss:  1.27159356951044&quot;
## [1] &quot;step =  7  lambda =  0.000325548208920438  loss:  1.27159249298249&quot;
## [1] &quot;step =  8  lambda =  0.000325548208920438  loss:  1.27159166710067&quot;
## [1] &quot;step =  9  lambda =  0.000325548208920438  loss:  1.27159109168341&quot;
## [1] &quot;step =  10  lambda =  0.000325548208920438  loss:  1.27159076652268&quot;
## [1] &quot;step =  11  lambda =  0.000325548208920438  loss:  1.27159069139859&quot;
## [1] &quot;step =  12  lambda =  0.000325548208920438  loss:  1.2715908660869&quot;
## [1] &quot;step =  1  lambda =  0.000322308950119019  loss:  1.26136818434036&quot;
## [1] &quot;step =  2  lambda =  0.000322308950119019  loss:  1.26136605109834&quot;
## [1] &quot;step =  3  lambda =  0.000322308950119019  loss:  1.26136416548995&quot;
## [1] &quot;step =  4  lambda =  0.000322308950119019  loss:  1.2613625278181&quot;
## [1] &quot;step =  5  lambda =  0.000322308950119019  loss:  1.26136113847974&quot;
## [1] &quot;step =  6  lambda =  0.000322308950119019  loss:  1.26135999762171&quot;
## [1] &quot;step =  7  lambda =  0.000322308950119019  loss:  1.26135910521677&quot;
## [1] &quot;step =  8  lambda =  0.000322308950119019  loss:  1.26135846113777&quot;
## [1] &quot;step =  9  lambda =  0.000322308950119019  loss:  1.26135806520459&quot;
## [1] &quot;step =  10  lambda =  0.000322308950119019  loss:  1.26135791721094&quot;
## [1] &quot;step =  11  lambda =  0.000322308950119019  loss:  1.26135801693882&quot;
## [1] &quot;step =  1  lambda =  0.000319101922481203  loss:  1.25121164322302&quot;
## [1] &quot;step =  2  lambda =  0.000319101922481203  loss:  1.25120945890977&quot;
## [1] &quot;step =  3  lambda =  0.000319101922481203  loss:  1.25120752017172&quot;
## [1] &quot;step =  4  lambda =  0.000319101922481203  loss:  1.2512058273027&quot;
## [1] &quot;step =  5  lambda =  0.000319101922481203  loss:  1.25120416472327&quot;
## [1] &quot;step =  6  lambda =  0.000319101922481203  loss:  1.25120297999671&quot;
## [1] &quot;step =  7  lambda =  0.000319101922481203  loss:  1.25120203018171&quot;
## [1] &quot;step =  8  lambda =  0.000319101922481203  loss:  1.25120130398999&quot;
## [1] &quot;step =  9  lambda =  0.000319101922481203  loss:  1.25120080882753&quot;
## [1] &quot;step =  10  lambda =  0.000319101922481203  loss:  1.25120055128025&quot;
## [1] &quot;step =  11  lambda =  0.000319101922481203  loss:  1.25120053555472&quot;
## [1] &quot;step =  12  lambda =  0.000319101922481203  loss:  1.25120076404865&quot;
## [1] &quot;step =  1  lambda =  0.000315926805301555  loss:  1.24112769213165&quot;
## [1] &quot;step =  2  lambda =  0.000315926805301555  loss:  1.24112566089876&quot;
## [1] &quot;step =  3  lambda =  0.000315926805301555  loss:  1.24112387399865&quot;
## [1] &quot;step =  4  lambda =  0.000315926805301555  loss:  1.24112233209658&quot;
## [1] &quot;step =  5  lambda =  0.000315926805301555  loss:  1.24112103552055&quot;
## [1] &quot;step =  6  lambda =  0.000315926805301555  loss:  1.24111998424001&quot;
## [1] &quot;step =  7  lambda =  0.000315926805301555  loss:  1.24111917802557&quot;
## [1] &quot;step =  8  lambda =  0.000315926805301555  loss:  1.24111861655606&quot;
## [1] &quot;step =  9  lambda =  0.000315926805301555  loss:  1.24111829947977&quot;
## [1] &quot;step =  10  lambda =  0.000315926805301555  loss:  1.24111822644649&quot;
## [1] &quot;step =  11  lambda =  0.000315926805301555  loss:  1.24111839712235&quot;
## [1] &quot;step =  1  lambda =  0.000312783281065711  loss:  1.23112066688204&quot;
## [1] &quot;step =  2  lambda =  0.000312783281065711  loss:  1.23111860108805&quot;
## [1] &quot;step =  3  lambda =  0.000312783281065711  loss:  1.23111677686097&quot;
## [1] &quot;step =  4  lambda =  0.000312783281065711  loss:  1.23111519453789&quot;
## [1] &quot;step =  5  lambda =  0.000312783281065711  loss:  1.23111385437998&quot;
## [1] &quot;step =  6  lambda =  0.000312783281065711  loss:  1.23111275643201&quot;
## [1] &quot;step =  7  lambda =  0.000312783281065711  loss:  1.23111190060669&quot;
## [1] &quot;step =  8  lambda =  0.000312783281065711  loss:  1.23111128674737&quot;
## [1] &quot;step =  9  lambda =  0.000312783281065711  loss:  1.23111091466462&quot;
## [1] &quot;step =  10  lambda =  0.000312783281065711  loss:  1.23111078415578&quot;
## [1] &quot;step =  11  lambda =  0.000312783281065711  loss:  1.23111089501454&quot;
## [1] &quot;step =  1  lambda =  0.000309671035418626  loss:  1.22118800919442&quot;
## [1] &quot;step =  2  lambda =  0.000309671035418626  loss:  1.22118590644763&quot;
## [1] &quot;step =  3  lambda =  0.000309671035418626  loss:  1.22118404314477&quot;
## [1] &quot;step =  4  lambda =  0.000309671035418626  loss:  1.22118241968059&quot;
## [1] &quot;step =  5  lambda =  0.000309671035418626  loss:  1.22118103636039&quot;
## [1] &quot;step =  6  lambda =  0.000309671035418626  loss:  1.22117989326414&quot;
## [1] &quot;step =  7  lambda =  0.000309671035418626  loss:  1.22117899033155&quot;
## [1] &quot;step =  8  lambda =  0.000309671035418626  loss:  1.22117832742566&quot;
## [1] &quot;step =  9  lambda =  0.000309671035418626  loss:  1.22117790437047&quot;
## [1] &quot;step =  10  lambda =  0.000309671035418626  loss:  1.22117772097163&quot;
## [1] &quot;step =  11  lambda =  0.000309671035418626  loss:  1.22117777702699&quot;
## [1] &quot;step =  1  lambda =  0.000306589757133144  loss:  1.21132924095586&quot;
## [1] &quot;step =  2  lambda =  0.000306589757133144  loss:  1.21132710633415&quot;
## [1] &quot;step =  3  lambda =  0.000306589757133144  loss:  1.21132520926714&quot;
## [1] &quot;step =  4  lambda =  0.000306589757133144  loss:  1.21132355013599&quot;
## [1] &quot;step =  5  lambda =  0.000306589757133144  loss:  1.21132212923286&quot;
## [1] &quot;step =  6  lambda =  0.000306589757133144  loss:  1.21132094662745&quot;
## [1] &quot;step =  7  lambda =  0.000306589757133144  loss:  1.21132000225089&quot;
## [1] &quot;step =  8  lambda =  0.000306589757133144  loss:  1.21131929595852&quot;
## [1] &quot;step =  9  lambda =  0.000306589757133144  loss:  1.211318827567&quot;
## [1] &quot;step =  10  lambda =  0.000306589757133144  loss:  1.21131859687483&quot;
## [1] &quot;step =  11  lambda =  0.000306589757133144  loss:  1.21131860367287&quot;
## [1] &quot;step =  1  lambda =  0.000303539138078867  loss:  1.20154392501649&quot;
## [1] &quot;step =  2  lambda =  0.000303539138078867  loss:  1.20154176375461&quot;
## [1] &quot;step =  3  lambda =  0.000303539138078867  loss:  1.20153983809495&quot;
## [1] &quot;step =  4  lambda =  0.000303539138078867  loss:  1.20153814840225&quot;
## [1] &quot;step =  5  lambda =  0.000303539138078867  loss:  1.20153669495422&quot;
## [1] &quot;step =  6  lambda =  0.000303539138078867  loss:  1.20153547781029&quot;
## [1] &quot;step =  7  lambda =  0.000303539138078867  loss:  1.20153449689394&quot;
## [1] &quot;step =  8  lambda =  0.000303539138078867  loss:  1.20153375205438&quot;
## [1] &quot;step =  9  lambda =  0.000303539138078867  loss:  1.20153324310316&quot;
## [1] &quot;step =  10  lambda =  0.000303539138078867  loss:  1.20153296983422&quot;
## [1] &quot;step =  11  lambda =  0.000303539138078867  loss:  1.20153293203433&quot;
## [1] &quot;step =  12  lambda =  0.000303539138078867  loss:  1.201533129488&quot;
## [1] &quot;step =  1  lambda =  0.000300518873191348  loss:  1.19182942940015&quot;
## [1] &quot;step =  2  lambda =  0.000300518873191348  loss:  1.19182748095133&quot;
## [1] &quot;step =  3  lambda =  0.000300518873191348  loss:  1.19182576588726&quot;
## [1] &quot;step =  4  lambda =  0.000300518873191348  loss:  1.19182428455998&quot;
## [1] &quot;step =  5  lambda =  0.000300518873191348  loss:  1.19182303723669&quot;
## [1] &quot;step =  6  lambda =  0.000300518873191348  loss:  1.19182202397029&quot;
## [1] &quot;step =  7  lambda =  0.000300518873191348  loss:  1.19182124468028&quot;
## [1] &quot;step =  8  lambda =  0.000300518873191348  loss:  1.19182069921332&quot;
## [1] &quot;step =  9  lambda =  0.000300518873191348  loss:  1.19182038737918&quot;
## [1] &quot;step =  10  lambda =  0.000300518873191348  loss:  1.19182030897055&quot;
## [1] &quot;step =  11  lambda =  0.000300518873191348  loss:  1.19182046377324&quot;
## [1] &quot;step =  1  lambda =  0.000297528660441581  loss:  1.18218966792559&quot;
## [1] &quot;step =  2  lambda =  0.000297528660441581  loss:  1.18218769887832&quot;
## [1] &quot;step =  3  lambda =  0.000297528660441581  loss:  1.18218596119606&quot;
## [1] &quot;step =  4  lambda =  0.000297528660441581  loss:  1.18218445521998&quot;
## [1] &quot;step =  5  lambda =  0.000297528660441581  loss:  1.18218318120834&quot;
## [1] &quot;step =  6  lambda =  0.000297528660441581  loss:  1.18218213920917&quot;
## [1] &quot;step =  7  lambda =  0.000297528660441581  loss:  1.1821813291397&quot;
## [1] &quot;step =  8  lambda =  0.000297528660441581  loss:  1.18218075084575&quot;
## [1] &quot;step =  9  lambda =  0.000297528660441581  loss:  1.18218040413714&quot;
## [1] &quot;step =  10  lambda =  0.000297528660441581  loss:  1.18218028880701&quot;
## [1] &quot;step =  11  lambda =  0.000297528660441581  loss:  1.18218040464192&quot;
## [1] &quot;step =  1  lambda =  0.0002945682008058  loss:  1.17262202763622&quot;
## [1] &quot;step =  2  lambda =  0.0002945682008058  loss:  1.17262004142806&quot;
## [1] &quot;step =  3  lambda =  0.0002945682008058  loss:  1.17261828456435&quot;
## [1] &quot;step =  4  lambda =  0.0002945682008058  loss:  1.17261675737781&quot;
## [1] &quot;step =  5  lambda =  0.0002945682008058  loss:  1.17261546011994&quot;
## [1] &quot;step =  6  lambda =  0.0002945682008058  loss:  1.17261439283576&quot;
## [1] &quot;step =  7  lambda =  0.0002945682008058  loss:  1.17261355544176&quot;
## [1] &quot;step =  8  lambda =  0.0002945682008058  loss:  1.17261294778431&quot;
## [1] &quot;step =  9  lambda =  0.0002945682008058  loss:  1.17261256967438&quot;
## [1] &quot;step =  10  lambda =  0.0002945682008058  loss:  1.17261242090665&quot;
## [1] &quot;step =  11  lambda =  0.0002945682008058  loss:  1.17261250126931&quot;
## [1] &quot;step =  1  lambda =  0.000291637198235574  loss:  1.16312606009564&quot;
## [1] &quot;step =  2  lambda =  0.000291637198235574  loss:  1.16312405999229&quot;
## [1] &quot;step =  3  lambda =  0.000291637198235574  loss:  1.1631222872256&quot;
## [1] &quot;step =  4  lambda =  0.000291637198235574  loss:  1.16312074212063&quot;
## [1] &quot;step =  5  lambda =  0.000291637198235574  loss:  1.16311942492289&quot;
## [1] &quot;step =  6  lambda =  0.000291637198235574  loss:  1.16311833567502&quot;
## [1] &quot;step =  7  lambda =  0.000291637198235574  loss:  1.16311747429334&quot;
## [1] &quot;step =  8  lambda =  0.000291637198235574  loss:  1.16311684062527&quot;
## [1] &quot;step =  9  lambda =  0.000291637198235574  loss:  1.16311643448338&quot;
## [1] &quot;step =  10  lambda =  0.000291637198235574  loss:  1.16311625566425&quot;
## [1] &quot;step =  11  lambda =  0.000291637198235574  loss:  1.1631163039581&quot;
## [1] &quot;step =  1  lambda =  0.000288735359628203  loss:  1.1537013179819&quot;
## [1] &quot;step =  2  lambda =  0.000288735359628203  loss:  1.15369930716327&quot;
## [1] &quot;step =  3  lambda =  0.000288735359628203  loss:  1.15369752169088&quot;
## [1] &quot;step =  4  lambda =  0.000288735359628203  loss:  1.15369596188255&quot;
## [1] &quot;step =  5  lambda =  0.000288735359628203  loss:  1.15369462797811&quot;
## [1] &quot;step =  6  lambda =  0.000288735359628203  loss:  1.1536935200181&quot;
## [1] &quot;step =  7  lambda =  0.000288735359628203  loss:  1.15369263791884&quot;
## [1] &quot;step =  8  lambda =  0.000288735359628203  loss:  1.15369198152891&quot;
## [1] &quot;step =  9  lambda =  0.000288735359628203  loss:  1.15369155066265&quot;
## [1] &quot;step =  10  lambda =  0.000288735359628203  loss:  1.15369134511861&quot;
## [1] &quot;step =  11  lambda =  0.000288735359628203  loss:  1.15369136468909&quot;
## [1] &quot;step =  1  lambda =  0.000285862394797409  loss:  1.14434735564198&quot;
## [1] &quot;step =  2  lambda =  0.000285862394797409  loss:  1.14434533723306&quot;
## [1] &quot;step =  3  lambda =  0.000285862394797409  loss:  1.1443435421986&quot;
## [1] &quot;step =  4  lambda =  0.000285862394797409  loss:  1.14434197084932&quot;
## [1] &quot;step =  5  lambda =  0.000285862394797409  loss:  1.14434062341951&quot;
## [1] &quot;step =  6  lambda =  0.000285862394797409  loss:  1.14433949994762&quot;
## [1] &quot;step =  7  lambda =  0.000285862394797409  loss:  1.14433860035&quot;
## [1] &quot;step =  8  lambda =  0.000285862394797409  loss:  1.14433792447636&quot;
## [1] &quot;step =  9  lambda =  0.000285862394797409  loss:  1.14433747214274&quot;
## [1] &quot;step =  10  lambda =  0.000285862394797409  loss:  1.14433724314962&quot;
## [1] &quot;step =  11  lambda =  0.000285862394797409  loss:  1.14433723729129&quot;
## [1] &quot;step =  12  lambda =  0.000285862394797409  loss:  1.1443374543604&quot;
## [1] &quot;step =  1  lambda =  0.000283018016444314  loss:  1.13506169928677&quot;
## [1] &quot;step =  2  lambda =  0.000283018016444314  loss:  1.13505989913506&quot;
## [1] &quot;step =  3  lambda =  0.000283018016444314  loss:  1.13505832019893&quot;
## [1] &quot;step =  4  lambda =  0.000283018016444314  loss:  1.13505696278269&quot;
## [1] &quot;step =  5  lambda =  0.000283018016444314  loss:  1.13505582711581&quot;
## [1] &quot;step =  6  lambda =  0.000283018016444314  loss:  1.13505491323524&quot;
## [1] &quot;step =  7  lambda =  0.000283018016444314  loss:  1.13505422105791&quot;
## [1] &quot;step =  8  lambda =  0.000283018016444314  loss:  1.13505375043511&quot;
## [1] &quot;step =  9  lambda =  0.000283018016444314  loss:  1.13505350118491&quot;
## [1] &quot;step =  10  lambda =  0.000283018016444314  loss:  1.13505347311&quot;
## [1] &quot;step =  11  lambda =  0.000283018016444314  loss:  1.13505366600694&quot;
## [1] &quot;step =  1  lambda =  0.000280201940128712  loss:  1.12584796125675&quot;
## [1] &quot;step =  2  lambda =  0.000280201940128712  loss:  1.12584615763396&quot;
## [1] &quot;step =  3  lambda =  0.000280201940128712  loss:  1.12584457329616&quot;
## [1] &quot;step =  4  lambda =  0.000280201940128712  loss:  1.12584320853988&quot;
## [1] &quot;step =  5  lambda =  0.000280201940128712  loss:  1.12584206358837&quot;
## [1] &quot;step =  6  lambda =  0.000280201940128712  loss:  1.12584113847586&quot;
## [1] &quot;step =  7  lambda =  0.000280201940128712  loss:  1.12584043311876&quot;
## [1] &quot;step =  8  lambda =  0.000280201940128712  loss:  1.12583994736903&quot;
## [1] &quot;step =  9  lambda =  0.000280201940128712  loss:  1.12583968104602&quot;
## [1] &quot;step =  10  lambda =  0.000280201940128712  loss:  1.125839633954&quot;
## [1] &quot;step =  11  lambda =  0.000280201940128712  loss:  1.12583980589123&quot;
## [1] &quot;step =  1  lambda =  0.000277413884240626  loss:  1.11670368002513&quot;
## [1] &quot;step =  2  lambda =  0.000277413884240626  loss:  1.11670187592318&quot;
## [1] &quot;step =  3  lambda =  0.000277413884240626  loss:  1.11670028918844&quot;
## [1] &quot;step =  4  lambda =  0.000277413884240626  loss:  1.11669892011041&quot;
## [1] &quot;step =  5  lambda =  0.000277413884240626  loss:  1.11669776890676&quot;
## [1] &quot;step =  6  lambda =  0.000277413884240626  loss:  1.11669683560944&quot;
## [1] &quot;step =  7  lambda =  0.000277413884240626  loss:  1.11669612013456&quot;
## [1] &quot;step =  8  lambda =  0.000277413884240626  loss:  1.11669562233489&quot;
## [1] &quot;step =  9  lambda =  0.000277413884240626  loss:  1.11669534203109&quot;
## [1] &quot;step =  10  lambda =  0.000277413884240626  loss:  1.11669527902901&quot;
## [1] &quot;step =  11  lambda =  0.000277413884240626  loss:  1.11669543312849&quot;
## [1] &quot;step =  1  lambda =  0.000274653569972143  loss:  1.1076284170632&quot;
## [1] &quot;step =  2  lambda =  0.000274653569972143  loss:  1.10762661538771&quot;
## [1] &quot;step =  3  lambda =  0.000274653569972143  loss:  1.1076250291761&quot;
## [1] &quot;step =  4  lambda =  0.000274653569972143  loss:  1.10762365871088&quot;
## [1] &quot;step =  5  lambda =  0.000274653569972143  loss:  1.10762250420416&quot;
## [1] &quot;step =  6  lambda =  0.000274653569972143  loss:  1.10762156568558&quot;
## [1] &quot;step =  7  lambda =  0.000274653569972143  loss:  1.10762084307093&quot;
## [1] &quot;step =  8  lambda =  0.000274653569972143  loss:  1.10762033621369&quot;
## [1] &quot;step =  9  lambda =  0.000274653569972143  loss:  1.10762004493579&quot;
## [1] &quot;step =  10  lambda =  0.000274653569972143  loss:  1.10761996904452&quot;
## [1] &quot;step =  11  lambda =  0.000274653569972143  loss:  1.10762010834133&quot;
## [1] &quot;step =  1  lambda =  0.000271920721289535  loss:  1.09862173534789&quot;
## [1] &quot;step =  2  lambda =  0.000271920721289535  loss:  1.09861993891906&quot;
## [1] &quot;step =  3  lambda =  0.000271920721289535  loss:  1.09861835606449&quot;
## [1] &quot;step =  4  lambda =  0.000271920721289535  loss:  1.09861698705979&quot;
## [1] &quot;step =  5  lambda =  0.000271920721289535  loss:  1.09861583211155&quot;
## [1] &quot;step =  6  lambda =  0.000271920721289535  loss:  1.09861489124712&quot;
## [1] &quot;step =  7  lambda =  0.000271920721289535  loss:  1.09861416438189&quot;
## [1] &quot;step =  8  lambda =  0.000271920721289535  loss:  1.09861365137006&quot;
## [1] &quot;step =  9  lambda =  0.000271920721289535  loss:  1.09861335203475&quot;
## [1] &quot;step =  10  lambda =  0.000271920721289535  loss:  1.09861326618468&quot;
## [1] &quot;step =  11  lambda =  0.000271920721289535  loss:  1.09861339362282&quot;
## [1] &quot;step =  1  lambda =  0.000269215064905658  loss:  1.0896831993262&quot;
## [1] &quot;step =  2  lambda =  0.000269215064905658  loss:  1.08968141087492&quot;
## [1] &quot;step =  3  lambda =  0.000269215064905658  loss:  1.08967983412151&quot;
## [1] &quot;step =  4  lambda =  0.000269215064905658  loss:  1.08967846933473&quot;
## [1] &quot;step =  5  lambda =  0.000269215064905658  loss:  1.08967731671573&quot;
## [1] &quot;step =  6  lambda =  0.000269215064905658  loss:  1.08967637628956&quot;
## [1] &quot;step =  7  lambda =  0.000269215064905658  loss:  1.08967564797125&quot;
## [1] &quot;step =  8  lambda =  0.000269215064905658  loss:  1.08967513161563&quot;
## [1] &quot;step =  9  lambda =  0.000269215064905658  loss:  1.089674827047&quot;
## [1] &quot;step =  10  lambda =  0.000269215064905658  loss:  1.08967473407546&quot;
## [1] &quot;step =  11  lambda =  0.000269215064905658  loss:  1.08967485250546&quot;
## [1] &quot;step =  1  lambda =  0.000266536330252618  loss:  1.08081237488459&quot;
## [1] &quot;step =  2  lambda =  0.000266536330252618  loss:  1.08081059705025&quot;
## [1] &quot;step =  3  lambda =  0.000266536330252618  loss:  1.08080902905018&quot;
## [1] &quot;step =  4  lambda =  0.000266536330252618  loss:  1.08080767114643&quot;
## [1] &quot;step =  5  lambda =  0.000266536330252618  loss:  1.08080652353479&quot;
## [1] &quot;step =  6  lambda =  0.000266536330252618  loss:  1.08080558623802&quot;
## [1] &quot;step =  7  lambda =  0.000266536330252618  loss:  1.08080485917076&quot;
## [1] &quot;step =  8  lambda =  0.000266536330252618  loss:  1.08080434218849&quot;
## [1] &quot;step =  9  lambda =  0.000266536330252618  loss:  1.08080403511664&quot;
## [1] &quot;step =  10  lambda =  0.000266536330252618  loss:  1.08080393776668&quot;
## [1] &quot;step =  11  lambda =  0.000266536330252618  loss:  1.0808040499445&quot;
## [1] &quot;step =  1  lambda =  0.000263884249454718  loss:  1.07200882933243&quot;
## [1] &quot;step =  2  lambda =  0.000263884249454718  loss:  1.07200706466183&quot;
## [1] &quot;step =  3  lambda =  0.000263884249454718  loss:  1.07200550797448&quot;
## [1] &quot;step =  4  lambda =  0.000263884249454718  loss:  1.07200415952581&quot;
## [1] &quot;step =  5  lambda =  0.000263884249454718  loss:  1.07200301950631&quot;
## [1] &quot;step =  6  lambda =  0.000263884249454718  loss:  1.07200208793649&quot;
## [1] &quot;step =  7  lambda =  0.000263884249454718  loss:  1.07200136473063&quot;
## [1] &quot;step =  8  lambda =  0.000263884249454718  loss:  1.0720008497448&quot;
## [1] &quot;step =  9  lambda =  0.000263884249454718  loss:  1.07200054280558&quot;
## [1] &quot;step =  10  lambda =  0.000263884249454718  loss:  1.07200044372577&quot;
## [1] &quot;step =  11  lambda =  0.000263884249454718  loss:  1.07200055231269&quot;
## [1] &quot;step =  1  lambda =  0.000261258557301668  loss:  1.06327213139667&quot;
## [1] &quot;step =  2  lambda =  0.000261258557301668  loss:  1.0632703823441&quot;
## [1] &quot;step =  3  lambda =  0.000261258557301668  loss:  1.06326883943612&quot;
## [1] &quot;step =  4  lambda =  0.000261258557301668  loss:  1.06326750292166&quot;
## [1] &quot;step =  5  lambda =  0.000261258557301668  loss:  1.06326637298601&quot;
## [1] &quot;step =  6  lambda =  0.000261258557301668  loss:  1.06326544964745&quot;
## [1] &quot;step =  7  lambda =  0.000261258557301668  loss:  1.06326473281991&quot;
## [1] &quot;step =  8  lambda =  0.000261258557301668  loss:  1.06326422236007&quot;
## [1] &quot;step =  9  lambda =  0.000261258557301668  loss:  1.06326391809562&quot;
## [1] &quot;step =  10  lambda =  0.000261258557301668  loss:  1.0632638198407&quot;
## [1] &quot;step =  11  lambda =  0.000261258557301668  loss:  1.06326392740402&quot;
## [1] &quot;step =  1  lambda =  0.000258658991222064  loss:  1.05460185122546&quot;
## [1] &quot;step =  2  lambda =  0.000258658991222064  loss:  1.05460012015339&quot;
## [1] &quot;step =  3  lambda =  0.000258658991222064  loss:  1.05459859339952&quot;
## [1] &quot;step =  4  lambda =  0.000258658991222064  loss:  1.05459727120635&quot;
## [1] &quot;step =  5  lambda =  0.000258658991222064  loss:  1.05459615375409&quot;
## [1] &quot;step =  6  lambda =  0.000258658991222064  loss:  1.05459524105884&quot;
## [1] &quot;step =  7  lambda =  0.000258658991222064  loss:  1.05459453303417&quot;
## [1] &quot;step =  8  lambda =  0.000258658991222064  loss:  1.05459402953739&quot;
## [1] &quot;step =  9  lambda =  0.000258658991222064  loss:  1.05459373039727&quot;
## [1] &quot;step =  10  lambda =  0.000258658991222064  loss:  1.05459363542928&quot;
## [1] &quot;step =  11  lambda =  0.000258658991222064  loss:  1.05459374444354&quot;
## [1] &quot;step =  1  lambda =  0.000256085291257132  loss:  1.04599756039783&quot;
## [1] &quot;step =  2  lambda =  0.000256085291257132  loss:  1.04599584957816&quot;
## [1] &quot;step =  3  lambda =  0.000256085291257132  loss:  1.04599434126245&quot;
## [1] &quot;step =  4  lambda =  0.000256085291257132  loss:  1.04599303568695&quot;
## [1] &quot;step =  5  lambda =  0.000256085291257132  loss:  1.04599193302681&quot;
## [1] &quot;step =  6  lambda =  0.000256085291257132  loss:  1.04599103329602&quot;
## [1] &quot;step =  7  lambda =  0.000256085291257132  loss:  1.0459903364078&quot;
## [1] &quot;step =  8  lambda =  0.000256085291257132  loss:  1.0459898422201&quot;
## [1] &quot;step =  9  lambda =  0.000256085291257132  loss:  1.04598955056278&quot;
## [1] &quot;step =  10  lambda =  0.000256085291257132  loss:  1.04598946125261&quot;
## [1] &quot;step =  11  lambda =  0.000256085291257132  loss:  1.04598957410112&quot;
## [1] &quot;step =  1  lambda =  0.00025353720003473  loss:  1.03745883193719&quot;
## [1] &quot;step =  2  lambda =  0.00025353720003473  loss:  1.0374571435528&quot;
## [1] &quot;step =  3  lambda =  0.00025353720003473  loss:  1.03745565587024&quot;
## [1] &quot;step =  4  lambda =  0.00025353720003473  loss:  1.03745436911962&quot;
## [1] &quot;step =  5  lambda =  0.00025353720003473  loss:  1.03745328347113&quot;
## [1] &quot;step =  6  lambda =  0.00025353720003473  loss:  1.03745239893671&quot;
## [1] &quot;step =  7  lambda =  0.00025353720003473  loss:  1.03745171542923&quot;
## [1] &quot;step =  8  lambda =  0.00025353720003473  loss:  1.03745123280729&quot;
## [1] &quot;step =  9  lambda =  0.00025353720003473  loss:  1.03745095090181&quot;
## [1] &quot;step =  10  lambda =  0.00025353720003473  loss:  1.03745086953089&quot;
## [1] &quot;step =  11  lambda =  0.00025353720003473  loss:  1.03745098850744&quot;
## [1] &quot;step =  1  lambda =  0.000251014462743614  loss:  1.02898524032724&quot;
## [1] &quot;step =  2  lambda =  0.000251014462743614  loss:  1.0289835764737&quot;
## [1] &quot;step =  3  lambda =  0.000251014462743614  loss:  1.02898211153194&quot;
## [1] &quot;step =  4  lambda =  0.000251014462743614  loss:  1.02898084572602&quot;
## [1] &quot;step =  5  lambda =  0.000251014462743614  loss:  1.0289797792213&quot;
## [1] &quot;step =  6  lambda =  0.000251014462743614  loss:  1.02897891202768&quot;
## [1] &quot;step =  7  lambda =  0.000251014462743614  loss:  1.02897824405773&quot;
## [1] &quot;step =  8  lambda =  0.000251014462743614  loss:  1.02897777517067&quot;
## [1] &quot;step =  9  lambda =  0.000251014462743614  loss:  1.02897750519852&quot;
## [1] &quot;step =  10  lambda =  0.000251014462743614  loss:  1.02897743396067&quot;
## [1] &quot;step =  11  lambda =  0.000251014462743614  loss:  1.02897756127141&quot;
## [1] &quot;step =  1  lambda =  0.000248516827107952  loss:  1.02057636152903&quot;
## [1] &quot;step =  2  lambda =  0.000248516827107952  loss:  1.02057472421645&quot;
## [1] &quot;step =  3  lambda =  0.000248516827107952  loss:  1.02057328403763&quot;
## [1] &quot;step =  4  lambda =  0.000248516827107952  loss:  1.02057204121072&quot;
## [1] &quot;step =  5  lambda =  0.000248516827107952  loss:  1.02057099589633&quot;
## [1] &quot;step =  6  lambda =  0.000248516827107952  loss:  1.02057014810235&quot;
## [1] &quot;step =  7  lambda =  0.000248516827107952  loss:  1.02056949774109&quot;
## [1] &quot;step =  8  lambda =  0.000248516827107952  loss:  1.02056904467237&quot;
## [1] &quot;step =  9  lambda =  0.000248516827107952  loss:  1.02056878872933&quot;
## [1] &quot;step =  10  lambda =  0.000248516827107952  loss:  1.02056872973264&quot;
## [1] &quot;step =  11  lambda =  0.000248516827107952  loss:  1.02056886749797&quot;
## [1] &quot;step =  1  lambda =  0.000246044043362099  loss:  1.01223177299866&quot;
## [1] &quot;step =  2  lambda =  0.000246044043362099  loss:  1.01223016415357&quot;
## [1] &quot;step =  3  lambda =  0.000246044043362099  loss:  1.01222875067625&quot;
## [1] &quot;step =  4  lambda =  0.000246044043362099  loss:  1.01222753277901&quot;
## [1] &quot;step =  5  lambda =  0.000246044043362099  loss:  1.01222651061783&quot;
## [1] &quot;step =  6  lambda =  0.000246044043362099  loss:  1.01222568419865&quot;
## [1] &quot;step =  7  lambda =  0.000246044043362099  loss:  1.01222505343351&quot;
## [1] &quot;step =  8  lambda =  0.000246044043362099  loss:  1.01222461818287&quot;
## [1] &quot;step =  9  lambda =  0.000246044043362099  loss:  1.01222437828094&quot;
## [1] &quot;step =  10  lambda =  0.000246044043362099  loss:  1.0122243335497&quot;
## [1] &quot;step =  11  lambda =  0.000246044043362099  loss:  1.01222448380618&quot;
## [1] &quot;step =  1  lambda =  0.000243595864225619  loss:  1.00395105370512&quot;
## [1] &quot;step =  2  lambda =  0.000243595864225619  loss:  1.00394947517236&quot;
## [1] &quot;step =  3  lambda =  0.000243595864225619  loss:  1.00394809025338&quot;
## [1] &quot;step =  4  lambda =  0.000243595864225619  loss:  1.00394689915477&quot;
## [1] &quot;step =  5  lambda =  0.000243595864225619  loss:  1.00394590202794&quot;
## [1] &quot;step =  6  lambda =  0.000243595864225619  loss:  1.00394509887693&quot;
## [1] &quot;step =  7  lambda =  0.000243595864225619  loss:  1.00394448961354&quot;
## [1] &quot;step =  8  lambda =  0.000243595864225619  loss:  1.00394407409887&quot;
## [1] &quot;step =  9  lambda =  0.000243595864225619  loss:  1.00394385216821&quot;
## [1] &quot;step =  10  lambda =  0.000243595864225619  loss:  1.00394382364483&quot;
## [1] &quot;step =  11  lambda =  0.000243595864225619  loss:  1.00394398834712&quot;
## [1] &quot;step =  1  lambda =  0.000241172044878559  loss:  0.99573378414796&quot;
## [1] &quot;step =  2  lambda =  0.000241172044878559  loss:  0.995732237692617&quot;
## [1] &quot;step =  3  lambda =  0.000241172044878559  loss:  0.995730883109007&quot;
## [1] &quot;step =  4  lambda =  0.000241172044878559  loss:  0.995729720598149&quot;
## [1] &quot;step =  5  lambda =  0.000241172044878559  loss:  0.995728750306958&quot;
## [1] &quot;step =  6  lambda =  0.000241172044878559  loss:  0.995727972237637&quot;
## [1] &quot;step =  7  lambda =  0.000241172044878559  loss:  0.995727386301731&quot;
## [1] &quot;step =  8  lambda =  0.000241172044878559  loss:  0.995726992360995&quot;
## [1] &quot;step =  9  lambda =  0.000241172044878559  loss:  0.995726790251817&quot;
## [1] &quot;step =  10  lambda =  0.000241172044878559  loss:  0.995726779798727&quot;
## [1] &quot;step =  11  lambda =  0.000241172044878559  loss:  0.995726960821478&quot;
## [1] &quot;step =  1  lambda =  0.000238772342936964  loss:  0.987579546374705&quot;
## [1] &quot;step =  2  lambda =  0.000238772342936964  loss:  0.987578033683975&quot;
## [1] &quot;step =  3  lambda =  0.000238772342936964  loss:  0.987576711134878&quot;
## [1] &quot;step =  4  lambda =  0.000238772342936964  loss:  0.987575578922962&quot;
## [1] &quot;step =  5  lambda =  0.000238772342936964  loss:  0.987574637190749&quot;
## [1] &quot;step =  6  lambda =  0.000238772342936964  loss:  0.987573885938626&quot;
## [1] &quot;step =  7  lambda =  0.000238772342936964  loss:  0.987573325077937&quot;
## [1] &quot;step =  8  lambda =  0.000238772342936964  loss:  0.98757295447108&quot;
## [1] &quot;step =  9  lambda =  0.000238772342936964  loss:  0.987572773955515&quot;
## [1] &quot;step =  10  lambda =  0.000238772342936964  loss:  0.987572783357063&quot;
## [1] &quot;step =  1  lambda =  0.000236396518428641  loss:  0.979489595915496&quot;
## [1] &quot;step =  2  lambda =  0.000236396518428641  loss:  0.979487929008905&quot;
## [1] &quot;step =  3  lambda =  0.000236396518428641  loss:  0.979486450705258&quot;
## [1] &quot;step =  4  lambda =  0.000236396518428641  loss:  0.979485161194402&quot;
## [1] &quot;step =  5  lambda =  0.000236396518428641  loss:  0.979484060614017&quot;
## [1] &quot;step =  6  lambda =  0.000236396518428641  loss:  0.979483148962128&quot;
## [1] &quot;step =  7  lambda =  0.000236396518428641  loss:  0.979482426149333&quot;
## [1] &quot;step =  8  lambda =  0.000236396518428641  loss:  0.979481892038206&quot;
## [1] &quot;step =  9  lambda =  0.000236396518428641  loss:  0.979481546466886&quot;
## [1] &quot;step =  10  lambda =  0.000236396518428641  loss:  0.979481389262116&quot;
## [1] &quot;step =  11  lambda =  0.000236396518428641  loss:  0.97948142024604&quot;
## [1] &quot;step =  1  lambda =  0.000234044333769158  loss:  0.9714601383487&quot;
## [1] &quot;step =  2  lambda =  0.000234044333769158  loss:  0.971458510096498&quot;
## [1] &quot;step =  3  lambda =  0.000234044333769158  loss:  0.97145706873623&quot;
## [1] &quot;step =  4  lambda =  0.000234044333769158  loss:  0.971455814452904&quot;
## [1] &quot;step =  5  lambda =  0.000234044333769158  loss:  0.97145474738052&quot;
## [1] &quot;step =  6  lambda =  0.000234044333769158  loss:  0.971453867515866&quot;
## [1] &quot;step =  7  lambda =  0.000234044333769158  loss:  0.971453174769747&quot;
## [1] &quot;step =  8  lambda =  0.000234044333769158  loss:  0.971452669005685&quot;
## [1] &quot;step =  9  lambda =  0.000234044333769158  loss:  0.971452350063095&quot;
## [1] &quot;step =  10  lambda =  0.000234044333769158  loss:  0.971452217770119&quot;
## [1] &quot;step =  11  lambda =  0.000234044333769158  loss:  0.971452271950343&quot;
## [1] &quot;step =  1  lambda =  0.00023171555373809  loss:  0.963492466674927&quot;
## [1] &quot;step =  2  lambda =  0.00023171555373809  loss:  0.963490878512034&quot;
## [1] &quot;step =  3  lambda =  0.00023171555373809  loss:  0.963489475544616&quot;
## [1] &quot;step =  4  lambda =  0.00023171555373809  loss:  0.963488257952546&quot;
## [1] &quot;step =  5  lambda =  0.00023171555373809  loss:  0.963487225865717&quot;
## [1] &quot;step =  6  lambda =  0.00023171555373809  loss:  0.963486379279276&quot;
## [1] &quot;step =  7  lambda =  0.00023171555373809  loss:  0.963485718103889&quot;
## [1] &quot;step =  8  lambda =  0.00023171555373809  loss:  0.963485242203783&quot;
## [1] &quot;step =  9  lambda =  0.00023171555373809  loss:  0.963484951419465&quot;
## [1] &quot;step =  10  lambda =  0.00023171555373809  loss:  0.963484845580382&quot;
## [1] &quot;step =  11  lambda =  0.00023171555373809  loss:  0.963484924511478&quot;
## [1] &quot;step =  1  lambda =  0.000229409945455492  loss:  0.955586169270656&quot;
## [1] &quot;step =  2  lambda =  0.000229409945455492  loss:  0.955584622572689&quot;
## [1] &quot;step =  3  lambda =  0.000229409945455492  loss:  0.955583259385907&quot;
## [1] &quot;step =  4  lambda =  0.000229409945455492  loss:  0.955582079885143&quot;
## [1] &quot;step =  5  lambda =  0.000229409945455492  loss:  0.955581084196258&quot;
## [1] &quot;step =  6  lambda =  0.000229409945455492  loss:  0.95558027231278&quot;
## [1] &quot;step =  7  lambda =  0.000229409945455492  loss:  0.95557964414527&quot;
## [1] &quot;step =  8  lambda =  0.000229409945455492  loss:  0.955579199558625&quot;
## [1] &quot;step =  9  lambda =  0.000229409945455492  loss:  0.955578938394466&quot;
## [1] &quot;step =  10  lambda =  0.000229409945455492  loss:  0.955578860483514&quot;
## [1] &quot;step =  11  lambda =  0.000229409945455492  loss:  0.955578965652066&quot;
## [1] &quot;step =  1  lambda =  0.000227127278358615  loss:  0.947740836191792&quot;
## [1] &quot;step =  2  lambda =  0.000227127278358615  loss:  0.947739332268321&quot;
## [1] &quot;step =  3  lambda =  0.000227127278358615  loss:  0.947738010183801&quot;
## [1] &quot;step =  4  lambda =  0.000227127278358615  loss:  0.947736870108118&quot;
## [1] &quot;step =  5  lambda =  0.000227127278358615  loss:  0.947735912163174&quot;
## [1] &quot;step =  6  lambda =  0.000227127278358615  loss:  0.947735136340921&quot;
## [1] &quot;step =  7  lambda =  0.000227127278358615  loss:  0.94773454255179&quot;
## [1] &quot;step =  8  lambda =  0.000227127278358615  loss:  0.947734130661389&quot;
## [1] &quot;step =  9  lambda =  0.000227127278358615  loss:  0.947733900512406&quot;
## [1] &quot;step =  10  lambda =  0.000227127278358615  loss:  0.947733851936835&quot;
## [1] &quot;step =  11  lambda =  0.000227127278358615  loss:  0.947733984762315&quot;
## [1] &quot;step =  1  lambda =  0.000224867324178848  loss:  0.939956059159237&quot;
## [1] &quot;step =  2  lambda =  0.000224867324178848  loss:  0.939954599254622&quot;
## [1] &quot;step =  3  lambda =  0.000224867324178848  loss:  0.939953319528674&quot;
## [1] &quot;step =  4  lambda =  0.000224867324178848  loss:  0.939952220146428&quot;
## [1] &quot;step =  5  lambda =  0.000224867324178848  loss:  0.939951301225893&quot;
## [1] &quot;step =  6  lambda =  0.000224867324178848  loss:  0.939950562757484&quot;
## [1] &quot;step =  7  lambda =  0.000224867324178848  loss:  0.939950004651527&quot;
## [1] &quot;step =  8  lambda =  0.000224867324178848  loss:  0.939949626774313&quot;
## [1] &quot;step =  9  lambda =  0.000224867324178848  loss:  0.93994942896961&quot;
## [1] &quot;step =  10  lambda =  0.000224867324178848  loss:  0.939949411070676&quot;
## [1] &quot;step =  11  lambda =  0.000224867324178848  loss:  0.939949572906476&quot;
## [1] &quot;step =  1  lambda =  0.000222629856918889  loss:  0.932231431564915&quot;
## [1] &quot;step =  2  lambda =  0.000222629856918889  loss:  0.932230016859339&quot;
## [1] &quot;step =  3  lambda =  0.000222629856918889  loss:  0.932228780684018&quot;
## [1] &quot;step =  4  lambda =  0.000222629856918889  loss:  0.932227723199224&quot;
## [1] &quot;step =  5  lambda =  0.000222629856918889  loss:  0.932226844519171&quot;
## [1] &quot;step =  6  lambda =  0.000222629856918889  loss:  0.932226144632752&quot;
## [1] &quot;step =  7  lambda =  0.000222629856918889  loss:  0.932225623450206&quot;
## [1] &quot;step =  8  lambda =  0.000222629856918889  loss:  0.932225280838509&quot;
## [1] &quot;step =  9  lambda =  0.000222629856918889  loss:  0.932225116642511&quot;
## [1] &quot;step =  10  lambda =  0.000222629856918889  loss:  0.93222513069671&quot;
## [1] &quot;step =  1  lambda =  0.000220414652830147  loss:  0.924568099190368&quot;
## [1] &quot;step =  2  lambda =  0.000220414652830147  loss:  0.924566552863344&quot;
## [1] &quot;step =  3  lambda =  0.000220414652830147  loss:  0.924565183600208&quot;
## [1] &quot;step =  4  lambda =  0.000220414652830147  loss:  0.924563991556289&quot;
## [1] &quot;step =  5  lambda =  0.000220414652830147  loss:  0.924562976841589&quot;
## [1] &quot;step =  6  lambda =  0.000220414652830147  loss:  0.924562139442981&quot;
## [1] &quot;step =  7  lambda =  0.000220414652830147  loss:  0.924561479270143&quot;
## [1] &quot;step =  8  lambda =  0.000220414652830147  loss:  0.924560996190291&quot;
## [1] &quot;step =  9  lambda =  0.000220414652830147  loss:  0.92456069004898&quot;
## [1] &quot;step =  10  lambda =  0.000220414652830147  loss:  0.924560560681627&quot;
## [1] &quot;step =  11  lambda =  0.000220414652830147  loss:  0.924560607919557&quot;
## [1] &quot;step =  1  lambda =  0.000218221490390368  loss:  0.91696251083991&quot;
## [1] &quot;step =  2  lambda =  0.000218221490390368  loss:  0.916961013541424&quot;
## [1] &quot;step =  3  lambda =  0.000218221490390368  loss:  0.916959691677675&quot;
## [1] &quot;step =  4  lambda =  0.000218221490390368  loss:  0.916958545399782&quot;
## [1] &quot;step =  5  lambda =  0.000218221490390368  loss:  0.916957574814555&quot;
## [1] &quot;step =  6  lambda =  0.000218221490390368  loss:  0.916956779907869&quot;
## [1] &quot;step =  7  lambda =  0.000218221490390368  loss:  0.916956160589683&quot;
## [1] &quot;step =  8  lambda =  0.000218221490390368  loss:  0.916955716728159&quot;
## [1] &quot;step =  9  lambda =  0.000218221490390368  loss:  0.916955448170082&quot;
## [1] &quot;step =  10  lambda =  0.000218221490390368  loss:  0.916955354752244&quot;
## [1] &quot;step =  11  lambda =  0.000218221490390368  loss:  0.91695543630735&quot;
## [1] &quot;step =  1  lambda =  0.000216050150281479  loss:  0.909415861205891&quot;
## [1] &quot;step =  2  lambda =  0.000216050150281479  loss:  0.909414413903834&quot;
## [1] &quot;step =  3  lambda =  0.000216050150281479  loss:  0.90941314042143&quot;
## [1] &quot;step =  4  lambda =  0.000216050150281479  loss:  0.909412040905346&quot;
## [1] &quot;step =  5  lambda =  0.000216050150281479  loss:  0.909411115458847&quot;
## [1] &quot;step =  6  lambda =  0.000216050150281479  loss:  0.909410364066419&quot;
## [1] &quot;step =  7  lambda =  0.000216050150281479  loss:  0.909409786638014&quot;
## [1] &quot;step =  8  lambda =  0.000216050150281479  loss:  0.909409383042502&quot;
## [1] &quot;step =  9  lambda =  0.000216050150281479  loss:  0.909409153127766&quot;
## [1] &quot;step =  10  lambda =  0.000216050150281479  loss:  0.909409096731852&quot;
## [1] &quot;step =  11  lambda =  0.000216050150281479  loss:  0.909409213688793&quot;
## [1] &quot;step =  1  lambda =  0.000213900415367661  loss:  0.901927750427882&quot;
## [1] &quot;step =  2  lambda =  0.000213900415367661  loss:  0.901926354041466&quot;
## [1] &quot;step =  3  lambda =  0.000213900415367661  loss:  0.901925129871567&quot;
## [1] &quot;step =  4  lambda =  0.000213900415367661  loss:  0.901924078060493&quot;
## [1] &quot;step =  5  lambda =  0.000213900415367661  loss:  0.901923198708003&quot;
## [1] &quot;step =  6  lambda =  0.000213900415367661  loss:  0.901922491797254&quot;
## [1] &quot;step =  7  lambda =  0.000213900415367661  loss:  0.901921957238173&quot;
## [1] &quot;step =  8  lambda =  0.000213900415367661  loss:  0.901921594900354&quot;
## [1] &quot;step =  9  lambda =  0.000213900415367661  loss:  0.90192140463275&quot;
## [1] &quot;step =  10  lambda =  0.000213900415367661  loss:  0.901921386274656&quot;
## [1] &quot;step =  11  lambda =  0.000213900415367661  loss:  0.901921539661406&quot;
## [1] &quot;step =  1  lambda =  0.000211772070673631  loss:  0.894497780411252&quot;
## [1] &quot;step =  2  lambda =  0.000211772070673631  loss:  0.894496435804808&quot;
## [1] &quot;step =  3  lambda =  0.000211772070673631  loss:  0.894495261823573&quot;
## [1] &quot;step =  4  lambda =  0.000211772070673631  loss:  0.894494258605575&quot;
## [1] &quot;step =  5  lambda =  0.000211772070673631  loss:  0.894493426247138&quot;
## [1] &quot;step =  6  lambda =  0.000211772070673631  loss:  0.894492764730107&quot;
## [1] &quot;step =  7  lambda =  0.000211772070673631  loss:  0.8944922739644&quot;
## [1] &quot;step =  8  lambda =  0.000211772070673631  loss:  0.894491953820319&quot;
## [1] &quot;step =  9  lambda =  0.000211772070673631  loss:  0.894491804147888&quot;
## [1] &quot;step =  10  lambda =  0.000211772070673631  loss:  0.894491824787626&quot;
## [1] &quot;step =  1  lambda =  0.000209664903363145  loss:  0.887127020818939&quot;
## [1] &quot;step =  2  lambda =  0.000209664903363145  loss:  0.887125558793481&quot;
## [1] &quot;step =  3  lambda =  0.000209664903363145  loss:  0.887124265977484&quot;
## [1] &quot;step =  4  lambda =  0.000209664903363145  loss:  0.88712314250451&quot;
## [1] &quot;step =  5  lambda =  0.000209664903363145  loss:  0.887122188467091&quot;
## [1] &quot;step =  6  lambda =  0.000209664903363145  loss:  0.887121403845274&quot;
## [1] &quot;step =  7  lambda =  0.000209664903363145  loss:  0.887120788548524&quot;
## [1] &quot;step =  8  lambda =  0.000209664903363145  loss:  0.887120342447441&quot;
## [1] &quot;step =  9  lambda =  0.000209664903363145  loss:  0.887120065392755&quot;
## [1] &quot;step =  10  lambda =  0.000209664903363145  loss:  0.887119957225905&quot;
## [1] &quot;step =  11  lambda =  0.000209664903363145  loss:  0.887120017784551&quot;
## [1] &quot;step =  1  lambda =  0.000207578702717718  loss:  0.879812092350712&quot;
## [1] &quot;step =  2  lambda =  0.000207578702717718  loss:  0.879810685269282&quot;
## [1] &quot;step =  3  lambda =  0.000207578702717718  loss:  0.879809445825331&quot;
## [1] &quot;step =  4  lambda =  0.000207578702717718  loss:  0.879808374148614&quot;
## [1] &quot;step =  5  lambda =  0.000207578702717718  loss:  0.879807470328806&quot;
## [1] &quot;step =  6  lambda =  0.000207578702717718  loss:  0.879806734345092&quot;
## [1] &quot;step =  7  lambda =  0.000207578702717718  loss:  0.87980616610728&quot;
## [1] &quot;step =  8  lambda =  0.000207578702717718  loss:  0.879805765486893&quot;
## [1] &quot;step =  9  lambda =  0.000207578702717718  loss:  0.879805532335897&quot;
## [1] &quot;step =  10  lambda =  0.000207578702717718  loss:  0.87980546649705&quot;
## [1] &quot;step =  11  lambda =  0.000207578702717718  loss:  0.87980556780937&quot;
## [1] &quot;step =  1  lambda =  0.000205513260115544  loss:  0.872554120154287&quot;
## [1] &quot;step =  2  lambda =  0.000205513260115544  loss:  0.872552768695092&quot;
## [1] &quot;step =  3  lambda =  0.000205513260115544  loss:  0.872551583315119&quot;
## [1] &quot;step =  4  lambda =  0.000205513260115544  loss:  0.87255056414012&quot;
## [1] &quot;step =  5  lambda =  0.000205513260115544  loss:  0.872549711256567&quot;
## [1] &quot;step =  6  lambda =  0.000205513260115544  loss:  0.872549024642466&quot;
## [1] &quot;step =  7  lambda =  0.000205513260115544  loss:  0.872548504207657&quot;
## [1] &quot;step =  8  lambda =  0.000205513260115544  loss:  0.872548149824406&quot;
## [1] &quot;step =  9  lambda =  0.000205513260115544  loss:  0.872547961345752&quot;
## [1] &quot;step =  10  lambda =  0.000205513260115544  loss:  0.872547938615686&quot;
## [1] &quot;step =  11  lambda =  0.000205513260115544  loss:  0.872548081474525&quot;
## [1] &quot;step =  1  lambda =  0.000203468369010644  loss:  0.865352713130341&quot;
## [1] &quot;step =  2  lambda =  0.000203468369010644  loss:  0.865351417929827&quot;
## [1] &quot;step =  3  lambda =  0.000203468369010644  loss:  0.865350287262062&quot;
## [1] &quot;step =  4  lambda =  0.000203468369010644  loss:  0.865349321248867&quot;
## [1] &quot;step =  5  lambda =  0.000203468369010644  loss:  0.865348519973578&quot;
## [1] &quot;step =  6  lambda =  0.000203468369010644  loss:  0.865347883413017&quot;
## [1] &quot;step =  7  lambda =  0.000203468369010644  loss:  0.8653474114771&quot;
## [1] &quot;step =  8  lambda =  0.000203468369010644  loss:  0.865347104038817&quot;
## [1] &quot;step =  9  lambda =  0.000203468369010644  loss:  0.865346960952257&quot;
## [1] &quot;step =  10  lambda =  0.000203468369010644  loss:  0.865346982062645&quot;
## [1] &quot;step =  1  lambda =  0.000201443824912203  loss:  0.858208887937021&quot;
## [1] &quot;step =  2  lambda =  0.000201443824912203  loss:  0.858207485682706&quot;
## [1] &quot;step =  3  lambda =  0.000201443824912203  loss:  0.85820624658518&quot;
## [1] &quot;step =  4  lambda =  0.000201443824912203  loss:  0.858205170762169&quot;
## [1] &quot;step =  5  lambda =  0.000201443824912203  loss:  0.858204258293505&quot;
## [1] &quot;step =  6  lambda =  0.000201443824912203  loss:  0.858203509154408&quot;
## [1] &quot;step =  7  lambda =  0.000201443824912203  loss:  0.858202923254403&quot;
## [1] &quot;step =  8  lambda =  0.000201443824912203  loss:  0.858202500466816&quot;
## [1] &quot;step =  9  lambda =  0.000201443824912203  loss:  0.85820224064646&quot;
## [1] &quot;step =  10  lambda =  0.000201443824912203  loss:  0.85820214363947&quot;
## [1] &quot;step =  11  lambda =  0.000201443824912203  loss:  0.858202209288446&quot;
## [1] &quot;step =  1  lambda =  0.000199439425364123  loss:  0.851119388339949&quot;
## [1] &quot;step =  2  lambda =  0.000199439425364123  loss:  0.851118045025742&quot;
## [1] &quot;step =  3  lambda =  0.000199439425364123  loss:  0.85111686334122&quot;
## [1] &quot;step =  4  lambda =  0.000199439425364123  loss:  0.851115843400615&quot;
## [1] &quot;step =  5  lambda =  0.000199439425364123  loss:  0.851114985281149&quot;
## [1] &quot;step =  6  lambda =  0.000199439425364123  loss:  0.851114288957288&quot;
## [1] &quot;step =  7  lambda =  0.000199439425364123  loss:  0.851113754338922&quot;
## [1] &quot;step =  8  lambda =  0.000199439425364123  loss:  0.851113381300325&quot;
## [1] &quot;step =  9  lambda =  0.000199439425364123  loss:  0.851113169697501&quot;
## [1] &quot;step =  10  lambda =  0.000199439425364123  loss:  0.851113119377889&quot;
## [1] &quot;step =  11  lambda =  0.000199439425364123  loss:  0.851113230185424&quot;
## [1] &quot;step =  1  lambda =  0.000197454969924779  loss:  0.844085290952082&quot;
## [1] &quot;step =  2  lambda =  0.000197454969924779  loss:  0.84408400704824&quot;
## [1] &quot;step =  3  lambda =  0.000197454969924779  loss:  0.844082883260446&quot;
## [1] &quot;step =  4  lambda =  0.000197454969924779  loss:  0.844081919699257&quot;
## [1] &quot;step =  5  lambda =  0.000197454969924779  loss:  0.844081116438961&quot;
## [1] &quot;step =  6  lambda =  0.000197454969924779  loss:  0.844080473452964&quot;
## [1] &quot;step =  7  lambda =  0.000197454969924779  loss:  0.844079990651265&quot;
## [1] &quot;step =  8  lambda =  0.000197454969924779  loss:  0.844079667908865&quot;
## [1] &quot;step =  9  lambda =  0.000197454969924779  loss:  0.844079505082851&quot;
## [1] &quot;step =  10  lambda =  0.000197454969924779  loss:  0.844079502021863&quot;
## [1] &quot;step =  11  lambda =  0.000197454969924779  loss:  0.844079658571111&quot;
## [1] &quot;step =  1  lambda =  0.000195490260146975  loss:  0.837106211858127&quot;
## [1] &quot;step =  2  lambda =  0.000195490260146975  loss:  0.837104987798282&quot;
## [1] &quot;step =  3  lambda =  0.000195490260146975  loss:  0.837103922352496&quot;
## [1] &quot;step =  4  lambda =  0.000195490260146975  loss:  0.83710301562771&quot;
## [1] &quot;step =  5  lambda =  0.000195490260146975  loss:  0.837102267695331&quot;
## [1] &quot;step =  6  lambda =  0.000195490260146975  loss:  0.837101678527732&quot;
## [1] &quot;step =  7  lambda =  0.000195490260146975  loss:  0.837101248035018&quot;
## [1] &quot;step =  8  lambda =  0.000195490260146975  loss:  0.837100976092926&quot;
## [1] &quot;step =  9  lambda =  0.000195490260146975  loss:  0.837100862559588&quot;
## [1] &quot;step =  10  lambda =  0.000195490260146975  loss:  0.837100907284851&quot;
## [1] &quot;step =  1  lambda =  0.000193545099558094  loss:  0.830183094200093&quot;
## [1] &quot;step =  2  lambda =  0.000193545099558094  loss:  0.830181772405169&quot;
## [1] &quot;step =  3  lambda =  0.000193545099558094  loss:  0.830180607887778&quot;
## [1] &quot;step =  4  lambda =  0.000193545099558094  loss:  0.830179600751093&quot;
## [1] &quot;step =  5  lambda =  0.000193545099558094  loss:  0.830178751063304&quot;
## [1] &quot;step =  6  lambda =  0.000193545099558094  loss:  0.83017805879533&quot;
## [1] &quot;step =  7  lambda =  0.000193545099558094  loss:  0.83017752385697&quot;
## [1] &quot;step =  8  lambda =  0.000193545099558094  loss:  0.830177146124326&quot;
## [1] &quot;step =  9  lambda =  0.000193545099558094  loss:  0.830176925456248&quot;
## [1] &quot;step =  10  lambda =  0.000193545099558094  loss:  0.830176861703492&quot;
## [1] &quot;step =  11  lambda =  0.000193545099558094  loss:  0.83017695471351&quot;
## [1] &quot;step =  1  lambda =  0.000191619293640457  loss:  0.823312847144962&quot;
## [1] &quot;step =  2  lambda =  0.000191619293640457  loss:  0.823311587440085&quot;
## [1] &quot;step =  3  lambda =  0.000191619293640457  loss:  0.823310483530009&quot;
## [1] &quot;step =  4  lambda =  0.000191619293640457  loss:  0.823309535514713&quot;
## [1] &quot;step =  5  lambda =  0.000191619293640457  loss:  0.82330874345999&quot;
## [1] &quot;step =  6  lambda =  0.000191619293640457  loss:  0.823308107336108&quot;
## [1] &quot;step =  7  lambda =  0.000191619293640457  loss:  0.823307627053268&quot;
## [1] &quot;step =  8  lambda =  0.000191619293640457  loss:  0.823307302488496&quot;
## [1] &quot;step =  9  lambda =  0.000191619293640457  loss:  0.823307133501829&quot;
## [1] &quot;step =  10  lambda =  0.000191619293640457  loss:  0.823307119945298&quot;
## [1] &quot;step =  11  lambda =  0.000191619293640457  loss:  0.823307261667656&quot;
## [1] &quot;step =  1  lambda =  0.000189712649811868  loss:  0.816496477352592&quot;
## [1] &quot;step =  2  lambda =  0.000189712649811868  loss:  0.816495280022148&quot;
## [1] &quot;step =  3  lambda =  0.000189712649811868  loss:  0.816494237016957&quot;
## [1] &quot;step =  4  lambda =  0.000189712649811868  loss:  0.816493348433628&quot;
## [1] &quot;step =  5  lambda =  0.000189712649811868  loss:  0.816492614335264&quot;
## [1] &quot;step =  6  lambda =  0.000189712649811868  loss:  0.816492034691203&quot;
## [1] &quot;step =  7  lambda =  0.000189712649811868  loss:  0.816491609411785&quot;
## [1] &quot;step =  8  lambda =  0.000189712649811868  loss:  0.816491338374796&quot;
## [1] &quot;step =  9  lambda =  0.000189712649811868  loss:  0.816491221441312&quot;
## [1] &quot;step =  10  lambda =  0.000189712649811868  loss:  0.816491258464559&quot;
## [1] &quot;step =  1  lambda =  0.000187824977406354  loss:  0.809734899815124&quot;
## [1] &quot;step =  2  lambda =  0.000187824977406354  loss:  0.809733611437192&quot;
## [1] &quot;step =  3  lambda =  0.000187824977406354  loss:  0.809732476076834&quot;
## [1] &quot;step =  4  lambda =  0.000187824977406354  loss:  0.809731493827127&quot;
## [1] &quot;step =  5  lambda =  0.000187824977406354  loss:  0.809730664748168&quot;
## [1] &quot;step =  6  lambda =  0.000187824977406354  loss:  0.809729988807952&quot;
## [1] &quot;step =  7  lambda =  0.000187824977406354  loss:  0.809729465916578&quot;
## [1] &quot;step =  8  lambda =  0.000187824977406354  loss:  0.809729095952214&quot;
## [1] &quot;step =  9  lambda =  0.000187824977406354  loss:  0.809728878776669&quot;
## [1] &quot;step =  10  lambda =  0.000187824977406354  loss:  0.809728814244073&quot;
## [1] &quot;step =  11  lambda =  0.000187824977406354  loss:  0.809728902205413&quot;
## [1] &quot;step =  1  lambda =  0.000185956087655102  loss:  0.803025094082327&quot;
## [1] &quot;step =  2  lambda =  0.000185956087655102  loss:  0.803023870017539&quot;
## [1] &quot;step =  3  lambda =  0.000185956087655102  loss:  0.803022797520091&quot;
## [1] &quot;step =  4  lambda =  0.000185956087655102  loss:  0.803021876680065&quot;
## [1] &quot;step =  5  lambda =  0.000185956087655102  loss:  0.803021107555333&quot;
## [1] &quot;step =  6  lambda =  0.000185956087655102  loss:  0.803020490113314&quot;
## [1] &quot;step =  7  lambda =  0.000185956087655102  loss:  0.803020024264525&quot;
## [1] &quot;step =  8  lambda =  0.000185956087655102  loss:  0.803019709888067&quot;
## [1] &quot;step =  9  lambda =  0.000185956087655102  loss:  0.803019546846922&quot;
## [1] &quot;step =  10  lambda =  0.000185956087655102  loss:  0.803019534996468&quot;
## [1] &quot;step =  11  lambda =  0.000185956087655102  loss:  0.803019674188985&quot;
## [1] &quot;step =  1  lambda =  0.000184105793667579  loss:  0.796368041200489&quot;
## [1] &quot;step =  2  lambda =  0.000184105793667579  loss:  0.796366881603472&quot;
## [1] &quot;step =  3  lambda =  0.000184105793667579  loss:  0.796365872136567&quot;
## [1] &quot;step =  4  lambda =  0.000184105793667579  loss:  0.796365012886703&quot;
## [1] &quot;step =  5  lambda =  0.000184105793667579  loss:  0.796364303909243&quot;
## [1] &quot;step =  6  lambda =  0.000184105793667579  loss:  0.796363745170751&quot;
## [1] &quot;step =  7  lambda =  0.000184105793667579  loss:  0.796363336581939&quot;
## [1] &quot;step =  8  lambda =  0.000184105793667579  loss:  0.796363078022654&quot;
## [1] &quot;step =  9  lambda =  0.000184105793667579  loss:  0.796362969356926&quot;
## [1] &quot;step =  10  lambda =  0.000184105793667579  loss:  0.796363010441302&quot;
## [1] &quot;step =  1  lambda =  0.000182273910412845  loss:  0.789764617626748&quot;
## [1] &quot;step =  2  lambda =  0.000182273910412845  loss:  0.789763373150356&quot;
## [1] &quot;step =  3  lambda =  0.000182273910412845  loss:  0.789762277525084&quot;
## [1] &quot;step =  4  lambda =  0.000182273910412845  loss:  0.789761330834551&quot;
## [1] &quot;step =  5  lambda =  0.000182273910412845  loss:  0.789760533131304&quot;
## [1] &quot;step =  6  lambda =  0.000182273910412845  loss:  0.78975988438068&quot;
## [1] &quot;step =  7  lambda =  0.000182273910412845  loss:  0.789759384493185&quot;
## [1] &quot;step =  8  lambda =  0.000182273910412845  loss:  0.78975903334908&quot;
## [1] &quot;step =  9  lambda =  0.000182273910412845  loss:  0.789758830813114&quot;
## [1] &quot;step =  10  lambda =  0.000182273910412845  loss:  0.789758776742745&quot;
## [1] &quot;step =  11  lambda =  0.000182273910412845  loss:  0.789758870992454&quot;
## [1] &quot;step =  1  lambda =  0.000180460254701048  loss:  0.783211894723717&quot;
## [1] &quot;step =  2  lambda =  0.000180460254701048  loss:  0.78321071637173&quot;
## [1] &quot;step =  3  lambda =  0.000180460254701048  loss:  0.783209685452867&quot;
## [1] &quot;step =  4  lambda =  0.000180460254701048  loss:  0.783208802047946&quot;
## [1] &quot;step =  5  lambda =  0.000180460254701048  loss:  0.783208066207433&quot;
## [1] &quot;step =  6  lambda =  0.000180460254701048  loss:  0.783207477896158&quot;
## [1] &quot;step =  7  lambda =  0.000180460254701048  loss:  0.783207037025072&quot;
## [1] &quot;step =  8  lambda =  0.000180460254701048  loss:  0.783206743475358&quot;
## [1] &quot;step =  9  lambda =  0.000180460254701048  loss:  0.783206597112912&quot;
## [1] &quot;step =  10  lambda =  0.000180460254701048  loss:  0.783206597796446&quot;
## [1] &quot;step =  1  lambda =  0.000178664645165106  loss:  0.776712078965321&quot;
## [1] &quot;step =  2  lambda =  0.000178664645165106  loss:  0.776710820001321&quot;
## [1] &quot;step =  3  lambda =  0.000178664645165106  loss:  0.776709707209848&quot;
## [1] &quot;step =  4  lambda =  0.000178664645165106  loss:  0.776708740668573&quot;
## [1] &quot;step =  5  lambda =  0.000178664645165106  loss:  0.776707920425279&quot;
## [1] &quot;step =  6  lambda =  0.000178664645165106  loss:  0.776707246443631&quot;
## [1] &quot;step =  7  lambda =  0.000178664645165106  loss:  0.776706718634424&quot;
## [1] &quot;step =  8  lambda =  0.000178664645165106  loss:  0.776706336879263&quot;
## [1] &quot;step =  9  lambda =  0.000178664645165106  loss:  0.776706101044792&quot;
## [1] &quot;step =  10  lambda =  0.000178664645165106  loss:  0.776706010990606&quot;
## [1] &quot;step =  11  lambda =  0.000178664645165106  loss:  0.776706066573437&quot;
## [1] &quot;step =  1  lambda =  0.000176886902242567  loss:  0.770262215144894&quot;
## [1] &quot;step =  2  lambda =  0.000176886902242567  loss:  0.770261023769202&quot;
## [1] &quot;step =  3  lambda =  0.000176886902242567  loss:  0.770259977168793&quot;
## [1] &quot;step =  4  lambda =  0.000176886902242567  loss:  0.770259075418667&quot;
## [1] &quot;step =  5  lambda =  0.000176886902242567  loss:  0.770258318564628&quot;
## [1] &quot;step =  6  lambda =  0.000176886902242567  loss:  0.770257706569892&quot;
## [1] &quot;step =  7  lambda =  0.000176886902242567  loss:  0.770257239345713&quot;
## [1] &quot;step =  8  lambda =  0.000176886902242567  loss:  0.770256916774627&quot;
## [1] &quot;step =  9  lambda =  0.000176886902242567  loss:  0.770256738724418&quot;
## [1] &quot;step =  10  lambda =  0.000176886902242567  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000175126848157658  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000173384306903506  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00017165910422453  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000169951067599028  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000168260026221911  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000166585810987634  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000164928254473277  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000163287190921808  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000161662456225505  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000160053887909543  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000158461325115751  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000156884608586522  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00015532358064889  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000153778085198759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000152247967685297  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000150733075095477  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000149233255938777  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000147748360232034  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000146278239484437  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000144822746682688  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000143381736276293  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000141955064163011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000140542587674442  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000139144165561759  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000137759657981586  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000136388926482011  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000135031833988743  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.0001336882447914  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000132358024529944  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000131041040181239  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000129737160045754  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000128446253734388  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000127168192155434  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012590284750167  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000124650093237575  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012340980408668  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000122181856019035  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00012096612623881  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000119762493172015  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000118570836454339  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000117391036919118  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000116222976585415  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000115066538646224  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000113921607456786  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000112788068523029  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000111665808490115  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000110554715131105  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000109454677335737  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000108365585099315  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000107287329511708  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000106219802746459  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000105162898050001  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000104116509730984  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000103080533149705  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000102054864707641  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.000101039401837093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0.00010003404299093  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.9038687632427e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.80532362252201e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.70775902233471e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.61116520613947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.51553251447417e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.42085138398996e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.32711234649488e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.23430602800707e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.14242314781733e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  9.05145451756109e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.96139104029952e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.87222370960982e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.78394360868463e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.69654190944029e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.61000987163404e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.52433884198997e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.43952025333374e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.3555456237358e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.27240655566322e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1900947351399e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.1086019309152e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  8.02791999364078e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.94804085505568e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.86895652717947e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.79065910151345e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.71314074824984e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.63639371548869e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.56041032846277e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.48518298877006e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.4107041736139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.33696643505071e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.26396239924518e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.1916847657329e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.12012630669027e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  7.04927986621178e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.97913835959434e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.90969477262882e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.84094216089865e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.77287364908539e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.70548243028111e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.63876176530778e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.5727049820433e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.50730547475429e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.44255670343554e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.37845219315595e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.31498553341106e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.25215037748203e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.18994044180088e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.12834950532221e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.06737140890104e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  6.00700005467694e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.94722940546415e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.88805348414794e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.82946637308688e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.77146221352103e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.71403520498611e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.65717960473339e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.60088972715548e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.54515994321769e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.48998467989523e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.43535841961575e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.38127569970771e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.32773111185406e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.27471930155139e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.22223496757448e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.1702728614462e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.11882778691264e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.06789459942348e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  5.01746820561753e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.96754356281337e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.91811567850513e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.86917960986318e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.82073046323988e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.7727633936802e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.72527360443719e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.67825634649237e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.63170691808076e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.58562066422073e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  4.53999297624849e-05  loss:  NaN&quot;
## [1] &quot;step =  1  lambda =  0  loss:  NaN&quot;</code></pre>
<pre class="r"><code># the result is a list consist of lambda list &quot;lambda&quot; and scores for each lambda &quot;performance&quot;
lambda_result</code></pre>
<pre><code>## $lambda
##    [1] 4.539993e-05 4.585621e-05 4.631707e-05 4.678256e-05 4.725274e-05
##    [6] 4.772763e-05 4.820730e-05 4.869180e-05 4.918116e-05 4.967544e-05
##   [11] 5.017468e-05 5.067895e-05 5.118828e-05 5.170273e-05 5.222235e-05
##   [16] 5.274719e-05 5.327731e-05 5.381276e-05 5.435358e-05 5.489985e-05
##   [21] 5.545160e-05 5.600890e-05 5.657180e-05 5.714035e-05 5.771462e-05
##   [26] 5.829466e-05 5.888053e-05 5.947229e-05 6.007000e-05 6.067371e-05
##   [31] 6.128350e-05 6.189940e-05 6.252150e-05 6.314986e-05 6.378452e-05
##   [36] 6.442557e-05 6.507305e-05 6.572705e-05 6.638762e-05 6.705482e-05
##   [41] 6.772874e-05 6.840942e-05 6.909695e-05 6.979138e-05 7.049280e-05
##   [46] 7.120126e-05 7.191685e-05 7.263962e-05 7.336966e-05 7.410704e-05
##   [51] 7.485183e-05 7.560410e-05 7.636394e-05 7.713141e-05 7.790659e-05
##   [56] 7.868957e-05 7.948041e-05 8.027920e-05 8.108602e-05 8.190095e-05
##   [61] 8.272407e-05 8.355546e-05 8.439520e-05 8.524339e-05 8.610010e-05
##   [66] 8.696542e-05 8.783944e-05 8.872224e-05 8.961391e-05 9.051455e-05
##   [71] 9.142423e-05 9.234306e-05 9.327112e-05 9.420851e-05 9.515533e-05
##   [76] 9.611165e-05 9.707759e-05 9.805324e-05 9.903869e-05 1.000340e-04
##   [81] 1.010394e-04 1.020549e-04 1.030805e-04 1.041165e-04 1.051629e-04
##   [86] 1.062198e-04 1.072873e-04 1.083656e-04 1.094547e-04 1.105547e-04
##   [91] 1.116658e-04 1.127881e-04 1.139216e-04 1.150665e-04 1.162230e-04
##   [96] 1.173910e-04 1.185708e-04 1.197625e-04 1.209661e-04 1.221819e-04
##  [101] 1.234098e-04 1.246501e-04 1.259028e-04 1.271682e-04 1.284463e-04
##  [106] 1.297372e-04 1.310410e-04 1.323580e-04 1.336882e-04 1.350318e-04
##  [111] 1.363889e-04 1.377597e-04 1.391442e-04 1.405426e-04 1.419551e-04
##  [116] 1.433817e-04 1.448227e-04 1.462782e-04 1.477484e-04 1.492333e-04
##  [121] 1.507331e-04 1.522480e-04 1.537781e-04 1.553236e-04 1.568846e-04
##  [126] 1.584613e-04 1.600539e-04 1.616625e-04 1.632872e-04 1.649283e-04
##  [131] 1.665858e-04 1.682600e-04 1.699511e-04 1.716591e-04 1.733843e-04
##  [136] 1.751268e-04 1.768869e-04 1.786646e-04 1.804603e-04 1.822739e-04
##  [141] 1.841058e-04 1.859561e-04 1.878250e-04 1.897126e-04 1.916193e-04
##  [146] 1.935451e-04 1.954903e-04 1.974550e-04 1.994394e-04 2.014438e-04
##  [151] 2.034684e-04 2.055133e-04 2.075787e-04 2.096649e-04 2.117721e-04
##  [156] 2.139004e-04 2.160502e-04 2.182215e-04 2.204147e-04 2.226299e-04
##  [161] 2.248673e-04 2.271273e-04 2.294099e-04 2.317156e-04 2.340443e-04
##  [166] 2.363965e-04 2.387723e-04 2.411720e-04 2.435959e-04 2.460440e-04
##  [171] 2.485168e-04 2.510145e-04 2.535372e-04 2.560853e-04 2.586590e-04
##  [176] 2.612586e-04 2.638842e-04 2.665363e-04 2.692151e-04 2.719207e-04
##  [181] 2.746536e-04 2.774139e-04 2.802019e-04 2.830180e-04 2.858624e-04
##  [186] 2.887354e-04 2.916372e-04 2.945682e-04 2.975287e-04 3.005189e-04
##  [191] 3.035391e-04 3.065898e-04 3.096710e-04 3.127833e-04 3.159268e-04
##  [196] 3.191019e-04 3.223090e-04 3.255482e-04 3.288200e-04 3.321247e-04
##  [201] 3.354626e-04 3.388341e-04 3.422394e-04 3.456790e-04 3.491531e-04
##  [206] 3.526622e-04 3.562065e-04 3.597864e-04 3.634023e-04 3.670546e-04
##  [211] 3.707435e-04 3.744696e-04 3.782331e-04 3.820344e-04 3.858739e-04
##  [216] 3.897520e-04 3.936690e-04 3.976255e-04 4.016217e-04 4.056580e-04
##  [221] 4.097350e-04 4.138529e-04 4.180122e-04 4.222133e-04 4.264566e-04
##  [226] 4.307425e-04 4.350716e-04 4.394441e-04 4.438606e-04 4.483215e-04
##  [231] 4.528272e-04 4.573782e-04 4.619749e-04 4.666178e-04 4.713074e-04
##  [236] 4.760441e-04 4.808285e-04 4.856609e-04 4.905418e-04 4.954719e-04
##  [241] 5.004514e-04 5.054811e-04 5.105612e-04 5.156924e-04 5.208752e-04
##  [246] 5.261101e-04 5.313976e-04 5.367383e-04 5.421326e-04 5.475811e-04
##  [251] 5.530844e-04 5.586430e-04 5.642574e-04 5.699283e-04 5.756562e-04
##  [256] 5.814416e-04 5.872852e-04 5.931875e-04 5.991491e-04 6.051707e-04
##  [261] 6.112528e-04 6.173960e-04 6.236009e-04 6.298682e-04 6.361985e-04
##  [266] 6.425924e-04 6.490505e-04 6.555736e-04 6.621622e-04 6.688171e-04
##  [271] 6.755388e-04 6.823281e-04 6.891856e-04 6.961120e-04 7.031080e-04
##  [276] 7.101744e-04 7.173118e-04 7.245209e-04 7.318024e-04 7.391572e-04
##  [281] 7.465858e-04 7.540891e-04 7.616678e-04 7.693227e-04 7.770546e-04
##  [286] 7.848641e-04 7.927521e-04 8.007194e-04 8.087668e-04 8.168950e-04
##  [291] 8.251049e-04 8.333974e-04 8.417731e-04 8.502331e-04 8.587781e-04
##  [296] 8.674090e-04 8.761266e-04 8.849318e-04 8.938255e-04 9.028086e-04
##  [301] 9.118820e-04 9.210465e-04 9.303032e-04 9.396529e-04 9.490966e-04
##  [306] 9.586352e-04 9.682696e-04 9.780009e-04 9.878299e-04 9.977578e-04
##  [311] 1.007785e-03 1.017914e-03 1.028144e-03 1.038477e-03 1.048914e-03
##  [316] 1.059456e-03 1.070103e-03 1.080858e-03 1.091721e-03 1.102693e-03
##  [321] 1.113775e-03 1.124969e-03 1.136275e-03 1.147695e-03 1.159229e-03
##  [326] 1.170880e-03 1.182647e-03 1.194533e-03 1.206538e-03 1.218664e-03
##  [331] 1.230912e-03 1.243283e-03 1.255778e-03 1.268399e-03 1.281146e-03
##  [336] 1.294022e-03 1.307027e-03 1.320163e-03 1.333431e-03 1.346832e-03
##  [341] 1.360368e-03 1.374040e-03 1.387849e-03 1.401797e-03 1.415886e-03
##  [346] 1.430116e-03 1.444488e-03 1.459006e-03 1.473669e-03 1.488480e-03
##  [351] 1.503439e-03 1.518549e-03 1.533811e-03 1.549226e-03 1.564796e-03
##  [356] 1.580522e-03 1.596407e-03 1.612451e-03 1.628656e-03 1.645025e-03
##  [361] 1.661557e-03 1.678256e-03 1.695123e-03 1.712159e-03 1.729367e-03
##  [366] 1.746747e-03 1.764302e-03 1.782034e-03 1.799944e-03 1.818033e-03
##  [371] 1.836305e-03 1.854760e-03 1.873401e-03 1.892229e-03 1.911246e-03
##  [376] 1.930454e-03 1.949856e-03 1.969452e-03 1.989245e-03 2.009237e-03
##  [381] 2.029431e-03 2.049827e-03 2.070428e-03 2.091236e-03 2.112253e-03
##  [386] 2.133482e-03 2.154924e-03 2.176581e-03 2.198456e-03 2.220551e-03
##  [391] 2.242868e-03 2.265409e-03 2.288177e-03 2.311173e-03 2.334401e-03
##  [396] 2.357862e-03 2.381559e-03 2.405494e-03 2.429670e-03 2.454088e-03
##  [401] 2.478752e-03 2.503664e-03 2.528826e-03 2.554241e-03 2.579912e-03
##  [406] 2.605841e-03 2.632030e-03 2.658482e-03 2.685200e-03 2.712187e-03
##  [411] 2.739445e-03 2.766977e-03 2.794785e-03 2.822873e-03 2.851244e-03
##  [416] 2.879899e-03 2.908843e-03 2.938077e-03 2.967605e-03 2.997430e-03
##  [421] 3.027555e-03 3.057982e-03 3.088715e-03 3.119758e-03 3.151112e-03
##  [426] 3.182781e-03 3.214768e-03 3.247077e-03 3.279711e-03 3.312673e-03
##  [431] 3.345965e-03 3.379593e-03 3.413558e-03 3.447865e-03 3.482517e-03
##  [436] 3.517517e-03 3.552868e-03 3.588575e-03 3.624641e-03 3.661069e-03
##  [441] 3.697864e-03 3.735028e-03 3.772566e-03 3.810480e-03 3.848776e-03
##  [446] 3.887457e-03 3.926527e-03 3.965989e-03 4.005848e-03 4.046107e-03
##  [451] 4.086771e-03 4.127844e-03 4.169330e-03 4.211232e-03 4.253556e-03
##  [456] 4.296305e-03 4.339483e-03 4.383096e-03 4.427147e-03 4.471640e-03
##  [461] 4.516581e-03 4.561973e-03 4.607822e-03 4.654131e-03 4.700906e-03
##  [466] 4.748151e-03 4.795871e-03 4.844070e-03 4.892754e-03 4.941927e-03
##  [471] 4.991594e-03 5.041760e-03 5.092431e-03 5.143611e-03 5.195305e-03
##  [476] 5.247518e-03 5.300257e-03 5.353525e-03 5.407329e-03 5.461674e-03
##  [481] 5.516564e-03 5.572007e-03 5.628006e-03 5.684569e-03 5.741700e-03
##  [486] 5.799405e-03 5.857690e-03 5.916560e-03 5.976023e-03 6.036083e-03
##  [491] 6.096747e-03 6.158020e-03 6.219909e-03 6.282420e-03 6.345560e-03
##  [496] 6.409333e-03 6.473748e-03 6.538811e-03 6.604527e-03 6.670903e-03
##  [501] 6.737947e-03 6.805664e-03 6.874063e-03 6.943148e-03 7.012928e-03
##  [506] 7.083409e-03 7.154598e-03 7.226503e-03 7.299131e-03 7.372488e-03
##  [511] 7.446583e-03 7.521422e-03 7.597014e-03 7.673365e-03 7.750484e-03
##  [516] 7.828378e-03 7.907054e-03 7.986521e-03 8.066787e-03 8.147860e-03
##  [521] 8.229747e-03 8.312457e-03 8.395999e-03 8.480380e-03 8.565609e-03
##  [526] 8.651695e-03 8.738646e-03 8.826471e-03 8.915179e-03 9.004778e-03
##  [531] 9.095277e-03 9.186686e-03 9.279014e-03 9.372270e-03 9.466462e-03
##  [536] 9.561602e-03 9.657698e-03 9.754759e-03 9.852796e-03 9.951818e-03
##  [541] 1.005184e-02 1.015286e-02 1.025490e-02 1.035796e-02 1.046206e-02
##  [546] 1.056720e-02 1.067341e-02 1.078068e-02 1.088902e-02 1.099846e-02
##  [551] 1.110900e-02 1.122064e-02 1.133341e-02 1.144732e-02 1.156236e-02
##  [556] 1.167857e-02 1.179594e-02 1.191449e-02 1.203423e-02 1.215518e-02
##  [561] 1.227734e-02 1.240073e-02 1.252536e-02 1.265124e-02 1.277839e-02
##  [566] 1.290681e-02 1.303653e-02 1.316755e-02 1.329988e-02 1.343355e-02
##  [571] 1.356856e-02 1.370493e-02 1.384266e-02 1.398178e-02 1.412230e-02
##  [576] 1.426423e-02 1.440759e-02 1.455239e-02 1.469864e-02 1.484637e-02
##  [581] 1.499558e-02 1.514628e-02 1.529851e-02 1.545226e-02 1.560756e-02
##  [586] 1.576442e-02 1.592285e-02 1.608288e-02 1.624451e-02 1.640777e-02
##  [591] 1.657268e-02 1.673923e-02 1.690747e-02 1.707739e-02 1.724902e-02
##  [596] 1.742237e-02 1.759747e-02 1.777433e-02 1.795296e-02 1.813340e-02
##  [601] 1.831564e-02 1.849971e-02 1.868564e-02 1.887343e-02 1.906311e-02
##  [606] 1.925470e-02 1.944821e-02 1.964367e-02 1.984109e-02 2.004050e-02
##  [611] 2.024191e-02 2.044535e-02 2.065083e-02 2.085837e-02 2.106800e-02
##  [616] 2.127974e-02 2.149360e-02 2.170962e-02 2.192780e-02 2.214818e-02
##  [621] 2.237077e-02 2.259560e-02 2.282269e-02 2.305206e-02 2.328374e-02
##  [626] 2.351775e-02 2.375410e-02 2.399284e-02 2.423397e-02 2.447752e-02
##  [631] 2.472353e-02 2.497200e-02 2.522297e-02 2.547647e-02 2.573251e-02
##  [636] 2.599113e-02 2.625234e-02 2.651618e-02 2.678268e-02 2.705185e-02
##  [641] 2.732372e-02 2.759833e-02 2.787570e-02 2.815585e-02 2.843882e-02
##  [646] 2.872464e-02 2.901333e-02 2.930492e-02 2.959944e-02 2.989691e-02
##  [651] 3.019738e-02 3.050087e-02 3.080741e-02 3.111703e-02 3.142976e-02
##  [656] 3.174564e-02 3.206469e-02 3.238694e-02 3.271243e-02 3.304120e-02
##  [661] 3.337327e-02 3.370868e-02 3.404745e-02 3.438964e-02 3.473526e-02
##  [666] 3.508435e-02 3.543696e-02 3.579311e-02 3.615283e-02 3.651617e-02
##  [671] 3.688317e-02 3.725385e-02 3.762826e-02 3.800643e-02 3.838840e-02
##  [676] 3.877421e-02 3.916390e-02 3.955750e-02 3.995506e-02 4.035661e-02
##  [681] 4.076220e-02 4.117187e-02 4.158566e-02 4.200360e-02 4.242574e-02
##  [686] 4.285213e-02 4.328280e-02 4.371780e-02 4.415717e-02 4.460096e-02
##  [691] 4.504920e-02 4.550195e-02 4.595926e-02 4.642115e-02 4.688770e-02
##  [696] 4.735892e-02 4.783489e-02 4.831564e-02 4.880122e-02 4.929168e-02
##  [701] 4.978707e-02 5.028744e-02 5.079283e-02 5.130331e-02 5.181892e-02
##  [706] 5.233971e-02 5.286573e-02 5.339704e-02 5.393369e-02 5.447573e-02
##  [711] 5.502322e-02 5.557621e-02 5.613476e-02 5.669893e-02 5.726876e-02
##  [716] 5.784432e-02 5.842567e-02 5.901285e-02 5.960594e-02 6.020499e-02
##  [721] 6.081006e-02 6.142121e-02 6.203851e-02 6.266200e-02 6.329177e-02
##  [726] 6.392786e-02 6.457035e-02 6.521929e-02 6.587475e-02 6.653681e-02
##  [731] 6.720551e-02 6.788094e-02 6.856315e-02 6.925223e-02 6.994822e-02
##  [736] 7.065121e-02 7.136127e-02 7.207846e-02 7.280286e-02 7.353454e-02
##  [741] 7.427358e-02 7.502004e-02 7.577400e-02 7.653555e-02 7.730474e-02
##  [746] 7.808167e-02 7.886640e-02 7.965902e-02 8.045961e-02 8.126824e-02
##  [751] 8.208500e-02 8.290997e-02 8.374323e-02 8.458486e-02 8.543495e-02
##  [756] 8.629359e-02 8.716085e-02 8.803683e-02 8.892162e-02 8.981529e-02
##  [761] 9.071795e-02 9.162968e-02 9.255058e-02 9.348073e-02 9.442022e-02
##  [766] 9.536916e-02 9.632764e-02 9.729575e-02 9.827359e-02 9.926125e-02
##  [771] 1.002588e-01 1.012665e-01 1.022842e-01 1.033122e-01 1.043505e-01
##  [776] 1.053992e-01 1.064585e-01 1.075284e-01 1.086091e-01 1.097006e-01
##  [781] 1.108032e-01 1.119167e-01 1.130415e-01 1.141776e-01 1.153251e-01
##  [786] 1.164842e-01 1.176548e-01 1.188373e-01 1.200316e-01 1.212380e-01
##  [791] 1.224564e-01 1.236871e-01 1.249302e-01 1.261858e-01 1.274540e-01
##  [796] 1.287349e-01 1.300287e-01 1.313355e-01 1.326555e-01 1.339887e-01
##  [801] 1.353353e-01 1.366954e-01 1.380692e-01 1.394569e-01 1.408584e-01
##  [806] 1.422741e-01 1.437039e-01 1.451482e-01 1.466070e-01 1.480804e-01
##  [811] 1.495686e-01 1.510718e-01 1.525901e-01 1.541237e-01 1.556726e-01
##  [816] 1.572372e-01 1.588174e-01 1.604136e-01 1.620258e-01 1.636541e-01
##  [821] 1.652989e-01 1.669602e-01 1.686381e-01 1.703330e-01 1.720449e-01
##  [826] 1.737739e-01 1.755204e-01 1.772844e-01 1.790661e-01 1.808658e-01
##  [831] 1.826835e-01 1.845195e-01 1.863740e-01 1.882471e-01 1.901390e-01
##  [836] 1.920499e-01 1.939800e-01 1.959296e-01 1.978987e-01 1.998876e-01
##  [841] 2.018965e-01 2.039256e-01 2.059751e-01 2.080452e-01 2.101361e-01
##  [846] 2.122480e-01 2.143811e-01 2.165357e-01 2.187119e-01 2.209100e-01
##  [851] 2.231302e-01 2.253727e-01 2.276377e-01 2.299255e-01 2.322363e-01
##  [856] 2.345703e-01 2.369278e-01 2.393089e-01 2.417140e-01 2.441433e-01
##  [861] 2.465970e-01 2.490753e-01 2.515786e-01 2.541070e-01 2.566608e-01
##  [866] 2.592403e-01 2.618457e-01 2.644773e-01 2.671353e-01 2.698201e-01
##  [871] 2.725318e-01 2.752708e-01 2.780373e-01 2.808316e-01 2.836540e-01
##  [876] 2.865048e-01 2.893842e-01 2.922926e-01 2.952302e-01 2.981973e-01
##  [881] 3.011942e-01 3.042213e-01 3.072787e-01 3.103669e-01 3.134862e-01
##  [886] 3.166368e-01 3.198190e-01 3.230333e-01 3.262798e-01 3.295590e-01
##  [891] 3.328711e-01 3.362165e-01 3.395955e-01 3.430085e-01 3.464558e-01
##  [896] 3.499377e-01 3.534547e-01 3.570070e-01 3.605949e-01 3.642190e-01
##  [901] 3.678794e-01 3.715767e-01 3.753111e-01 3.790830e-01 3.828929e-01
##  [906] 3.867410e-01 3.906278e-01 3.945537e-01 3.985190e-01 4.025242e-01
##  [911] 4.065697e-01 4.106558e-01 4.147829e-01 4.189515e-01 4.231621e-01
##  [916] 4.274149e-01 4.317105e-01 4.360493e-01 4.404317e-01 4.448581e-01
##  [921] 4.493290e-01 4.538448e-01 4.584060e-01 4.630131e-01 4.676664e-01
##  [926] 4.723666e-01 4.771139e-01 4.819090e-01 4.867523e-01 4.916442e-01
##  [931] 4.965853e-01 5.015761e-01 5.066170e-01 5.117086e-01 5.168513e-01
##  [936] 5.220458e-01 5.272924e-01 5.325918e-01 5.379444e-01 5.433509e-01
##  [941] 5.488116e-01 5.543273e-01 5.598984e-01 5.655254e-01 5.712091e-01
##  [946] 5.769498e-01 5.827483e-01 5.886050e-01 5.945205e-01 6.004956e-01
##  [951] 6.065307e-01 6.126264e-01 6.187834e-01 6.250023e-01 6.312836e-01
##  [956] 6.376282e-01 6.440364e-01 6.505091e-01 6.570468e-01 6.636503e-01
##  [961] 6.703200e-01 6.770569e-01 6.838614e-01 6.907343e-01 6.976763e-01
##  [966] 7.046881e-01 7.117703e-01 7.189237e-01 7.261490e-01 7.334470e-01
##  [971] 7.408182e-01 7.482636e-01 7.557837e-01 7.633795e-01 7.710516e-01
##  [976] 7.788008e-01 7.866279e-01 7.945336e-01 8.025188e-01 8.105842e-01
##  [981] 8.187308e-01 8.269591e-01 8.352702e-01 8.436648e-01 8.521438e-01
##  [986] 8.607080e-01 8.693582e-01 8.780954e-01 8.869204e-01 8.958341e-01
##  [991] 9.048374e-01 9.139312e-01 9.231163e-01 9.323938e-01 9.417645e-01
##  [996] 9.512294e-01 9.607894e-01 9.704455e-01 9.801987e-01 9.900498e-01
## [1001] 1.000000e+00 1.010050e+00 1.020201e+00 1.030455e+00 1.040811e+00
## [1006] 1.051271e+00 1.061837e+00 1.072508e+00 1.083287e+00 1.094174e+00
## [1011] 1.105171e+00 1.116278e+00 1.127497e+00 1.138828e+00 1.150274e+00
## [1016] 1.161834e+00 1.173511e+00 1.185305e+00 1.197217e+00 1.209250e+00
## [1021] 1.221403e+00 1.233678e+00 1.246077e+00 1.258600e+00 1.271249e+00
## [1026] 1.284025e+00 1.296930e+00 1.309964e+00 1.323130e+00 1.336427e+00
## [1031] 1.349859e+00 1.363425e+00 1.377128e+00 1.390968e+00 1.404948e+00
## [1036] 1.419068e+00 1.433329e+00 1.447735e+00 1.462285e+00 1.476981e+00
## [1041] 1.491825e+00 1.506818e+00 1.521962e+00 1.537258e+00 1.552707e+00
## [1046] 1.568312e+00 1.584074e+00 1.599994e+00 1.616074e+00 1.632316e+00
## [1051] 1.648721e+00 1.665291e+00 1.682028e+00 1.698932e+00 1.716007e+00
## [1056] 1.733253e+00 1.750673e+00 1.768267e+00 1.786038e+00 1.803988e+00
## [1061] 1.822119e+00 1.840431e+00 1.858928e+00 1.877611e+00 1.896481e+00
## [1066] 1.915541e+00 1.934792e+00 1.954237e+00 1.973878e+00 1.993716e+00
## [1071] 2.013753e+00 2.033991e+00 2.054433e+00 2.075081e+00 2.095936e+00
## [1076] 2.117000e+00 2.138276e+00 2.159766e+00 2.181472e+00 2.203396e+00
## [1081] 2.225541e+00 2.247908e+00 2.270500e+00 2.293319e+00 2.316367e+00
## [1086] 2.339647e+00 2.363161e+00 2.386911e+00 2.410900e+00 2.435130e+00
## [1091] 2.459603e+00 2.484323e+00 2.509290e+00 2.534509e+00 2.559981e+00
## [1096] 2.585710e+00 2.611696e+00 2.637944e+00 2.664456e+00 2.691234e+00
## [1101] 2.718282e+00 2.745601e+00 2.773195e+00 2.801066e+00 2.829217e+00
## [1106] 2.857651e+00 2.886371e+00 2.915379e+00 2.944680e+00 2.974274e+00
## [1111] 3.004166e+00 3.034358e+00 3.064854e+00 3.095657e+00 3.126768e+00
## [1116] 3.158193e+00 3.189933e+00 3.221993e+00 3.254374e+00 3.287081e+00
## [1121] 3.320117e+00 3.353485e+00 3.387188e+00 3.421230e+00 3.455613e+00
## [1126] 3.490343e+00 3.525421e+00 3.560853e+00 3.596640e+00 3.632787e+00
## [1131] 3.669297e+00 3.706174e+00 3.743421e+00 3.781043e+00 3.819044e+00
## [1136] 3.857426e+00 3.896193e+00 3.935351e+00 3.974902e+00 4.014850e+00
## [1141] 4.055200e+00 4.095955e+00 4.137120e+00 4.178699e+00 4.220696e+00
## [1146] 4.263115e+00 4.305960e+00 4.349235e+00 4.392946e+00 4.437096e+00
## [1151] 4.481689e+00 4.526731e+00 4.572225e+00 4.618177e+00 4.664590e+00
## [1156] 4.711470e+00 4.758821e+00 4.806648e+00 4.854956e+00 4.903749e+00
## [1161] 4.953032e+00 5.002811e+00 5.053090e+00 5.103875e+00 5.155170e+00
## [1166] 5.206980e+00 5.259311e+00 5.312168e+00 5.365556e+00 5.419481e+00
## [1171] 5.473947e+00 5.528961e+00 5.584528e+00 5.640654e+00 5.697343e+00
## [1176] 5.754603e+00 5.812437e+00 5.870853e+00 5.929856e+00 5.989452e+00
## [1181] 6.049647e+00 6.110447e+00 6.171858e+00 6.233887e+00 6.296538e+00
## [1186] 6.359820e+00 6.423737e+00 6.488296e+00 6.553505e+00 6.619369e+00
## [1191] 6.685894e+00 6.753089e+00 6.820958e+00 6.889510e+00 6.958751e+00
## [1196] 7.028688e+00 7.099327e+00 7.170676e+00 7.242743e+00 7.315534e+00
## [1201] 7.389056e+00 7.463317e+00 7.538325e+00 7.614086e+00 7.690609e+00
## [1206] 7.767901e+00 7.845970e+00 7.924823e+00 8.004469e+00 8.084915e+00
## [1211] 8.166170e+00 8.248241e+00 8.331137e+00 8.414867e+00 8.499438e+00
## [1216] 8.584858e+00 8.671138e+00 8.758284e+00 8.846306e+00 8.935213e+00
## [1221] 9.025013e+00 9.115716e+00 9.207331e+00 9.299866e+00 9.393331e+00
## [1226] 9.487736e+00 9.583089e+00 9.679401e+00 9.776680e+00 9.874938e+00
## [1231] 9.974182e+00 1.007442e+01 1.017567e+01 1.027794e+01 1.038124e+01
## [1236] 1.048557e+01 1.059095e+01 1.069739e+01 1.080490e+01 1.091349e+01
## [1241] 1.102318e+01 1.113396e+01 1.124586e+01 1.135888e+01 1.147304e+01
## [1246] 1.158835e+01 1.170481e+01 1.182245e+01 1.194126e+01 1.206128e+01
## [1251] 1.218249e+01 1.230493e+01 1.242860e+01 1.255351e+01 1.267967e+01
## [1256] 1.280710e+01 1.293582e+01 1.306582e+01 1.319714e+01 1.332977e+01
## [1261] 1.346374e+01 1.359905e+01 1.373572e+01 1.387377e+01 1.401320e+01
## [1266] 1.415404e+01 1.429629e+01 1.443997e+01 1.458509e+01 1.473168e+01
## [1271] 1.487973e+01 1.502928e+01 1.518032e+01 1.533289e+01 1.548699e+01
## [1276] 1.564263e+01 1.579984e+01 1.595863e+01 1.611902e+01 1.628102e+01
## [1281] 1.644465e+01 1.660992e+01 1.677685e+01 1.694546e+01 1.711577e+01
## [1286] 1.728778e+01 1.746153e+01 1.763702e+01 1.781427e+01 1.799331e+01
## [1291] 1.817415e+01 1.835680e+01 1.854129e+01 1.872763e+01 1.891585e+01
## [1296] 1.910595e+01 1.929797e+01 1.949192e+01 1.968782e+01 1.988568e+01
## [1301] 2.008554e+01 2.028740e+01 2.049129e+01 2.069723e+01 2.090524e+01
## [1306] 2.111534e+01 2.132756e+01 2.154190e+01 2.175840e+01 2.197708e+01
## [1311] 2.219795e+01 2.242104e+01 2.264638e+01 2.287398e+01 2.310387e+01
## [1316] 2.333606e+01 2.357060e+01 2.380748e+01 2.404675e+01 2.428843e+01
## [1321] 2.453253e+01 2.477909e+01 2.502812e+01 2.527966e+01 2.553372e+01
## [1326] 2.579034e+01 2.604954e+01 2.631134e+01 2.657577e+01 2.684286e+01
## [1331] 2.711264e+01 2.738513e+01 2.766035e+01 2.793834e+01 2.821913e+01
## [1336] 2.850273e+01 2.878919e+01 2.907853e+01 2.937077e+01 2.966595e+01
## [1341] 2.996410e+01 3.026524e+01 3.056942e+01 3.087664e+01 3.118696e+01
## [1346] 3.150039e+01 3.181698e+01 3.213674e+01 3.245972e+01 3.278595e+01
## [1351] 3.311545e+01 3.344827e+01 3.378443e+01 3.412397e+01 3.446692e+01
## [1356] 3.481332e+01 3.516320e+01 3.551659e+01 3.587354e+01 3.623408e+01
## [1361] 3.659823e+01 3.696605e+01 3.733757e+01 3.771282e+01 3.809184e+01
## [1366] 3.847467e+01 3.886134e+01 3.925191e+01 3.964639e+01 4.004485e+01
## [1371] 4.044730e+01 4.085381e+01 4.126439e+01 4.167911e+01 4.209799e+01
## [1376] 4.252108e+01 4.294843e+01 4.338006e+01 4.381604e+01 4.425640e+01
## [1381] 4.470118e+01 4.515044e+01 4.560421e+01 4.606254e+01 4.652547e+01
## [1386] 4.699306e+01 4.746535e+01 4.794239e+01 4.842422e+01 4.891089e+01
## [1391] 4.940245e+01 4.989895e+01 5.040044e+01 5.090698e+01 5.141860e+01
## [1396] 5.193537e+01 5.245733e+01 5.298453e+01 5.351703e+01 5.405489e+01
## [1401] 5.459815e+01 5.514687e+01 5.570111e+01 5.626091e+01 5.682634e+01
## [1406] 5.739746e+01 5.797431e+01 5.855696e+01 5.914547e+01 5.973989e+01
## [1411] 6.034029e+01 6.094672e+01 6.155924e+01 6.217792e+01 6.280282e+01
## [1416] 6.343400e+01 6.407152e+01 6.471545e+01 6.536585e+01 6.602279e+01
## [1421] 6.668633e+01 6.735654e+01 6.803348e+01 6.871723e+01 6.940785e+01
## [1426] 7.010541e+01 7.080998e+01 7.152164e+01 7.224044e+01 7.296647e+01
## [1431] 7.369979e+01 7.444049e+01 7.518863e+01 7.594429e+01 7.670754e+01
## [1436] 7.747846e+01 7.825713e+01 7.904363e+01 7.983803e+01 8.064042e+01
## [1441] 8.145087e+01 8.226946e+01 8.309629e+01 8.393142e+01 8.477494e+01
## [1446] 8.562694e+01 8.648751e+01 8.735672e+01 8.823467e+01 8.912145e+01
## [1451] 9.001713e+01 9.092182e+01 9.183560e+01 9.275856e+01 9.369080e+01
## [1456] 9.463241e+01 9.558348e+01 9.654411e+01 9.751439e+01 9.849443e+01
## [1461] 9.948432e+01 1.004841e+02 1.014940e+02 1.025141e+02 1.035443e+02
## [1466] 1.045850e+02 1.056361e+02 1.066977e+02 1.077701e+02 1.088532e+02
## [1471] 1.099472e+02 1.110522e+02 1.121683e+02 1.132956e+02 1.144342e+02
## [1476] 1.155843e+02 1.167459e+02 1.179192e+02 1.191044e+02 1.203014e+02
## [1481] 1.215104e+02 1.227316e+02 1.239651e+02 1.252110e+02 1.264694e+02
## [1486] 1.277404e+02 1.290242e+02 1.303209e+02 1.316307e+02 1.329536e+02
## [1491] 1.342898e+02 1.356394e+02 1.370026e+02 1.383795e+02 1.397702e+02
## [1496] 1.411750e+02 1.425938e+02 1.440269e+02 1.454744e+02 1.469364e+02
## [1501] 1.484132e+02 1.499047e+02 1.514113e+02 1.529330e+02 1.544700e+02
## [1506] 1.560225e+02 1.575905e+02 1.591743e+02 1.607741e+02 1.623899e+02
## [1511] 1.640219e+02 1.656704e+02 1.673354e+02 1.690171e+02 1.707158e+02
## [1516] 1.724315e+02 1.741645e+02 1.759148e+02 1.776828e+02 1.794686e+02
## [1521] 1.812722e+02 1.830941e+02 1.849342e+02 1.867928e+02 1.886701e+02
## [1526] 1.905663e+02 1.924815e+02 1.944160e+02 1.963699e+02 1.983434e+02
## [1531] 2.003368e+02 2.023502e+02 2.043839e+02 2.064380e+02 2.085127e+02
## [1536] 2.106083e+02 2.127249e+02 2.148629e+02 2.170223e+02 2.192034e+02
## [1541] 2.214064e+02 2.236316e+02 2.258791e+02 2.281492e+02 2.304422e+02
## [1546] 2.327582e+02 2.350974e+02 2.374602e+02 2.398467e+02 2.422572e+02
## [1551] 2.446919e+02 2.471511e+02 2.496350e+02 2.521439e+02 2.546780e+02
## [1556] 2.572376e+02 2.598228e+02 2.624341e+02 2.650716e+02 2.677356e+02
## [1561] 2.704264e+02 2.731442e+02 2.758894e+02 2.786621e+02 2.814627e+02
## [1566] 2.842915e+02 2.871486e+02 2.900345e+02 2.929494e+02 2.958936e+02
## [1571] 2.988674e+02 3.018711e+02 3.049049e+02 3.079693e+02 3.110644e+02
## [1576] 3.141907e+02 3.173483e+02 3.205377e+02 3.237592e+02 3.270130e+02
## [1581] 3.302996e+02 3.336191e+02 3.369721e+02 3.403587e+02 3.437793e+02
## [1586] 3.472344e+02 3.507241e+02 3.542490e+02 3.578092e+02 3.614053e+02
## [1591] 3.650375e+02 3.687062e+02 3.724117e+02 3.761545e+02 3.799349e+02
## [1596] 3.837533e+02 3.876101e+02 3.915057e+02 3.954404e+02 3.994146e+02
## [1601] 4.034288e+02 4.074833e+02 4.115786e+02 4.157150e+02 4.198930e+02
## [1606] 4.241130e+02 4.283754e+02 4.326807e+02 4.370292e+02 4.414214e+02
## [1611] 4.458578e+02 4.503387e+02 4.548647e+02 4.594362e+02 4.640536e+02
## [1616] 4.687174e+02 4.734281e+02 4.781861e+02 4.829920e+02 4.878461e+02
## [1621] 4.927490e+02 4.977013e+02 5.027032e+02 5.077555e+02 5.128585e+02
## [1626] 5.180128e+02 5.232189e+02 5.284774e+02 5.337887e+02 5.391533e+02
## [1631] 5.445719e+02 5.500449e+02 5.555730e+02 5.611566e+02 5.667963e+02
## [1636] 5.724927e+02 5.782464e+02 5.840578e+02 5.899277e+02 5.958566e+02
## [1641] 6.018450e+02 6.078937e+02 6.140031e+02 6.201739e+02 6.264068e+02
## [1646] 6.327023e+02 6.390611e+02 6.454837e+02 6.519709e+02 6.585234e+02
## [1651] 6.651416e+02 6.718264e+02 6.785784e+02 6.853982e+02 6.922866e+02
## [1656] 6.992442e+02 7.062717e+02 7.133698e+02 7.205393e+02 7.277809e+02
## [1661] 7.350952e+02 7.424830e+02 7.499451e+02 7.574822e+02 7.650950e+02
## [1666] 7.727843e+02 7.805509e+02 7.883956e+02 7.963191e+02 8.043223e+02
## [1671] 8.124058e+02 8.205706e+02 8.288175e+02 8.371473e+02 8.455607e+02
## [1676] 8.540588e+02 8.626422e+02 8.713119e+02 8.800687e+02 8.889136e+02
## [1681] 8.978473e+02 9.068708e+02 9.159850e+02 9.251908e+02 9.344891e+02
## [1686] 9.438809e+02 9.533671e+02 9.629486e+02 9.726264e+02 9.824014e+02
## [1691] 9.922747e+02 1.002247e+03 1.012320e+03 1.022494e+03 1.032770e+03
## [1696] 1.043150e+03 1.053634e+03 1.064223e+03 1.074918e+03 1.085721e+03
## [1701] 1.096633e+03 1.107655e+03 1.118787e+03 1.130031e+03 1.141388e+03
## [1706] 1.152859e+03 1.164445e+03 1.176148e+03 1.187969e+03 1.199908e+03
## [1711] 1.211967e+03 1.224148e+03 1.236450e+03 1.248877e+03 1.261428e+03
## [1716] 1.274106e+03 1.286911e+03 1.299845e+03 1.312908e+03 1.326103e+03
## [1721] 1.339431e+03 1.352892e+03 1.366489e+03 1.380223e+03 1.394094e+03
## [1726] 1.408105e+03 1.422257e+03 1.436550e+03 1.450988e+03 1.465571e+03
## [1731] 1.480300e+03 1.495177e+03 1.510204e+03 1.525382e+03 1.540712e+03
## [1736] 1.556197e+03 1.571837e+03 1.587634e+03 1.603590e+03 1.619706e+03
## [1741] 1.635984e+03 1.652426e+03 1.669034e+03 1.685808e+03 1.702750e+03
## [1746] 1.719863e+03 1.737148e+03 1.754607e+03 1.772241e+03 1.790052e+03
## [1751] 1.808042e+03 1.826214e+03 1.844567e+03 1.863106e+03 1.881830e+03
## [1756] 1.900743e+03 1.919846e+03 1.939140e+03 1.958629e+03 1.978314e+03
## [1761] 1.998196e+03 2.018278e+03 2.038562e+03 2.059050e+03 2.079744e+03
## [1766] 2.100646e+03 2.121757e+03 2.143081e+03 2.164620e+03 2.186375e+03
## [1771] 2.208348e+03 2.230542e+03 2.252960e+03 2.275602e+03 2.298472e+03
## [1776] 2.321572e+03 2.344905e+03 2.368471e+03 2.392275e+03 2.416318e+03
## [1781] 2.440602e+03 2.465130e+03 2.489905e+03 2.514929e+03 2.540205e+03
## [1786] 2.565734e+03 2.591520e+03 2.617566e+03 2.643873e+03 2.670444e+03
## [1791] 2.697282e+03 2.724390e+03 2.751771e+03 2.779427e+03 2.807361e+03
## [1796] 2.835575e+03 2.864073e+03 2.892857e+03 2.921931e+03 2.951297e+03
## [1801] 2.980958e+03 3.010917e+03 3.041177e+03 3.071742e+03 3.102613e+03
## [1806] 3.133795e+03 3.165290e+03 3.197102e+03 3.229233e+03 3.261688e+03
## [1811] 3.294468e+03 3.327578e+03 3.361021e+03 3.394800e+03 3.428918e+03
## [1816] 3.463379e+03 3.498187e+03 3.533344e+03 3.568855e+03 3.604722e+03
## [1821] 3.640950e+03 3.677542e+03 3.714502e+03 3.751834e+03 3.789540e+03
## [1826] 3.827626e+03 3.866094e+03 3.904949e+03 3.944194e+03 3.983834e+03
## [1831] 4.023872e+03 4.064313e+03 4.105160e+03 4.146418e+03 4.188090e+03
## [1836] 4.230181e+03 4.272695e+03 4.315636e+03 4.359009e+03 4.402818e+03
## [1841] 4.447067e+03 4.491761e+03 4.536903e+03 4.582500e+03 4.628555e+03
## [1846] 4.675073e+03 4.722058e+03 4.769515e+03 4.817450e+03 4.865866e+03
## [1851] 4.914769e+03 4.964163e+03 5.014054e+03 5.064446e+03 5.115344e+03
## [1856] 5.166754e+03 5.218681e+03 5.271130e+03 5.324106e+03 5.377614e+03
## [1861] 5.431660e+03 5.486249e+03 5.541386e+03 5.597078e+03 5.653330e+03
## [1866] 5.710147e+03 5.767535e+03 5.825499e+03 5.884047e+03 5.943182e+03
## [1871] 6.002912e+03 6.063242e+03 6.124179e+03 6.185728e+03 6.247896e+03
## [1876] 6.310688e+03 6.374112e+03 6.438172e+03 6.502877e+03 6.568232e+03
## [1881] 6.634244e+03 6.700919e+03 6.768265e+03 6.836287e+03 6.904993e+03
## [1886] 6.974389e+03 7.044483e+03 7.115281e+03 7.186791e+03 7.259019e+03
## [1891] 7.331974e+03 7.405661e+03 7.480089e+03 7.555265e+03 7.631197e+03
## [1896] 7.707892e+03 7.785357e+03 7.863602e+03 7.942632e+03 8.022457e+03
## [1901] 8.103084e+03 8.184521e+03 8.266777e+03 8.349860e+03 8.433777e+03
## [1906] 8.518538e+03 8.604151e+03 8.690624e+03 8.777966e+03 8.866186e+03
## [1911] 8.955293e+03 9.045295e+03 9.136202e+03 9.228022e+03 9.320765e+03
## [1916] 9.414440e+03 9.509057e+03 9.604625e+03 9.701153e+03 9.798651e+03
## [1921] 9.897129e+03 9.996597e+03 1.009706e+04 1.019854e+04 1.030104e+04
## [1926] 1.040457e+04 1.050913e+04 1.061475e+04 1.072143e+04 1.082918e+04
## [1931] 1.093802e+04 1.104795e+04 1.115898e+04 1.127113e+04 1.138441e+04
## [1936] 1.149882e+04 1.161439e+04 1.173112e+04 1.184901e+04 1.196810e+04
## [1941] 1.208838e+04 1.220987e+04 1.233258e+04 1.245653e+04 1.258172e+04
## [1946] 1.270817e+04 1.283588e+04 1.296489e+04 1.309519e+04 1.322680e+04
## [1951] 1.335973e+04 1.349399e+04 1.362961e+04 1.376659e+04 1.390495e+04
## [1956] 1.404469e+04 1.418585e+04 1.432842e+04 1.447242e+04 1.461787e+04
## [1961] 1.476478e+04 1.491317e+04 1.506305e+04 1.521444e+04 1.536734e+04
## [1966] 1.552179e+04 1.567778e+04 1.583535e+04 1.599450e+04 1.615524e+04
## [1971] 1.631761e+04 1.648160e+04 1.664724e+04 1.681455e+04 1.698354e+04
## [1976] 1.715423e+04 1.732663e+04 1.750077e+04 1.767665e+04 1.785431e+04
## [1981] 1.803374e+04 1.821499e+04 1.839805e+04 1.858295e+04 1.876972e+04
## [1986] 1.895835e+04 1.914889e+04 1.934134e+04 1.953572e+04 1.973206e+04
## [1991] 1.993037e+04 2.013067e+04 2.033299e+04 2.053734e+04 2.074374e+04
## [1996] 2.095222e+04 2.116280e+04 2.137549e+04 2.159031e+04 2.180730e+04
## [2001] 2.202647e+04
## 
## $performance
## $performance$`beta -&gt; lambda: 22026.4657948067`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 21807.2987982302`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 21590.3125497062`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 21375.4853504291`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 21162.7957175002`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 20952.2223817786`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 20743.7442857556`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 20537.3405814475`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 20332.9906283122`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 20130.6739911839`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 19930.3704382303`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 19732.0599389292`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 19535.7226620655`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 19341.3389737478`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 19148.8894354453`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 18958.3548020439`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 18769.7160199212`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 18582.9542250422`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 18398.0507410714`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 18214.9870775064`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 18033.7449278285`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 17854.3061676715`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 17676.65285301`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 17500.7672183642`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 17326.6316750244`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 17154.228809291`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 16983.5413807338`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 16814.5523204676`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 16647.2447294456`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 16481.6018767693`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 16317.6071980154`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 16155.2442935794`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 15994.4969270355`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 15835.3490235131`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 15677.7846680892`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 15521.788104197`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 15367.34373205`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 15214.4361070824`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 15063.0499384043`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 14913.1700872726`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 14764.7815655773`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 14617.8695343426`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 14472.4193022429`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 14328.4163241338`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 14185.8461995975`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 14044.6946715028`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 13904.9476245792`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 13766.5910840055`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 13629.6112140124`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 13493.9943164988`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 13359.7268296619`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 13226.7953266411`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 13095.1865141752`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12964.8872312735`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12835.8844478991`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12708.165263666`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12581.7169065495`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12456.5267316084`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12332.582219721`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12209.8709763327`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 12088.380730217`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11968.099332248`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11849.0147541856`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11731.1150874729`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11614.3885420449`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11498.8234451498`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11384.4082401816`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11271.1314855245`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11158.9818534085`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 11047.9481287771`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10938.0192081652`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10829.1840985891`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10721.4319164473`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10614.7518864317`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10509.1333404504`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10404.5657165607`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10301.0385579133`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10198.5415117058`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 10097.0643281483`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9996.59685943788`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9897.12905874391`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9798.65097920349`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9701.15277292652`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9604.6246900112`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9509.05707756873`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9414.44037875829`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9320.76513183108`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9228.02196918439`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9136.2016164247`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 9045.29489144014`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8955.29270348252`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8866.186052258`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8777.96602702724`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8690.62380571415`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8604.15065402384`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8518.53792456912`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8433.77705600563`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8349.85957217593`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8266.77708126167`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8184.52127494457`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8103.08392757538`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 8022.45689535158`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7942.63211550269`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7863.60160548423`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7785.35746217936`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7707.89186110851`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7631.19705564706`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7555.2653762505`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7480.08922968767`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7405.66109828121`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7331.97353915601`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7259.01918349469`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7186.79073580093`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7115.28097316979`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 7044.48274456536`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6974.38897010583`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6904.9926403553`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6836.286815623`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6768.26462526917`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6700.91926701811`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6634.24400627789`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6568.23217546683`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6502.87717334688`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6438.17246436333`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6374.1115779914`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6310.68810808902`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6247.8957122564`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6185.72811120158`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6124.17908811267`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6063.24248803609`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 6002.91221726102`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5943.18224271013`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5884.04659133616`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5825.49934952474`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5767.53466250285`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5710.14673375352`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5653.32982443602`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5597.07825281208`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5541.3863936777`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5486.2486778005`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5431.65959136299`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5377.61367541099`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5324.10552530792`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5271.12979019412`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5218.68117245197`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5166.754427176`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5115.34436164836`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5064.44583481972`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 5014.05375679492`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4964.16308832421`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4914.76884029913`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4865.86607325376`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4817.44989687059`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4769.51546949167`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4722.05799763432`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4675.07273551178`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4628.55498455872`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4582.50009296124`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4536.90345519183`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4491.76051154869`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4447.06674769987`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4402.8176942317`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4359.00892620198`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4315.63606269742`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4272.69476639549`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4230.1807431308`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4188.08974146558`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4146.4175522646`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4105.16000827419`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4064.31298370559`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 4023.87239382231`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3983.83419453164`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3944.19438198031`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3904.948992154`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3866.09410048106`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3827.62582143991`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3789.54030817061`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3751.83375209008`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3714.50238251129`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3677.5424662662`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3640.95030733235`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3604.72224646338`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3568.854660823`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3533.34396362276`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3498.18660376333`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3463.37906547946`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3428.91786798828`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3394.79956514135`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3361.02074507995`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3327.5780298939`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3294.46807528385`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3261.68757022671`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3229.23323664469`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3197.10182907735`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3165.29013435719`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3133.79497128823`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3102.61319032788`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3071.7416732721`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3041.17733294343`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 3010.91711288239`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2980.95798704173`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2951.29695948392`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2921.93106408148`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2892.85736422039`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2864.07295250646`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2835.57495047451`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2807.3605083006`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2779.42680451698`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2751.77104573003`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2724.39046634078`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2697.28232826851`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2670.44392067681`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2643.87255970255`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2617.5655881875`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2591.52037541257`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2565.7343168348`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2540.20483382683`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2514.92937341909`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2489.90540804446`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2465.13043528557`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2440.6019776245`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2416.31758219502`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2392.27482053738`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2368.47128835535`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2344.9046052759`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2321.57241461106`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2298.47238312234`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2275.60220078732`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2252.95958056872`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2230.54225818566`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2208.34799188721`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2186.37456222824`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2164.61977184748`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2143.08144524776`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2121.75742857847`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2100.64558942018`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2079.74381657137`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2059.05001983734`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2038.56212982119`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 2018.27809771681`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1998.19589510412`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1978.3135137461`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1958.62896538806`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1939.14028155876`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1919.84551337356`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1900.74273133958`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1881.83002516269`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1863.10550355652`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1844.56729405329`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1826.21354281661`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1808.04241445606`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1790.05209184367`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1772.24077593218`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1754.60668557515`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1737.14805734885`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1719.86314537592`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1702.75022115076`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1685.80757336666`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1669.03350774476`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1652.42634686448`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1635.98442999593`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1619.7061129337`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1603.58976783251`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1587.63378304445`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1571.83656295772`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1556.19652783716`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1540.71211366621`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1525.38177199056`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1510.20396976326`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1495.17718919145`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1480.29992758455`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1465.57069720398`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1450.98802511446`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1436.5504530366`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1422.25653720118`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1408.1048482047`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1394.09397086646`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1380.22250408705`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1366.48906070825`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1352.89226737426`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1339.43076439442`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1326.10320560722`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1312.90825824566`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1299.84460280403`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1286.91093290588`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1274.10595517346`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1261.4283890983`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1248.87696691325`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1236.45043346563`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1224.14754609174`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1211.96707449258`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1199.90780061084`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1187.9685185091`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1176.14803424917`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1164.4451657728`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1152.85874278339`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1141.38760662897`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1130.03061018637`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1118.78661774649`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1107.65450490071`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1096.63315842846`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1085.72147618592`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1074.91836699577`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1064.22275053809`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1053.63355724232`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1043.1497281803`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1032.7702149604`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1022.49397962264`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1012.31999453492`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 1002.24724229025`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 992.274715605024`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 982.401417218259`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 972.626359791883`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 962.948565812015`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 953.367067491183`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 943.88090667158`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 934.48913472921`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 925.19081247906`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 915.98501008115`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 906.870806947571`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 897.847291650418`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 888.913561830636`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 880.068724107804`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 871.311893990772`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 862.642195789238`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 854.058762526152`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 845.560735851038`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 837.147265954143`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 828.817511481469`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 820.570639450629`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 812.405825167543`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 804.322252143983`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 796.319112015905`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 788.395604462634`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 780.550937126804`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 772.784325535149`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 765.094993020038`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 757.482170641809`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 749.945097111883`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 742.483018716622`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 735.095189241974`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 727.780869898828`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 720.539329249161`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 713.369843132868`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 706.271694595365`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 699.244173815886`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 692.286578036491`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 685.39821149181`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 678.578385339442`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 671.826417591095`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 665.141633044362`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 658.523363215222`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 651.970946271172`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 645.483726965061`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 639.061056569553`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 632.702292812253`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 626.40679981149`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 620.173948012713`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 614.003114125553`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 607.893681061474`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 601.845037872081`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 595.856579688017`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 589.927707658468`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 584.057828891295`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 578.246356393726`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 572.492709013672`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 566.796311381596`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 561.156593852992`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 555.572992451403`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 550.044948812038`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 544.571910125929`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 539.153329084642`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 533.788663825562`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 528.477377877687`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 523.218940108002`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 518.012824668342`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 512.85851094283`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 507.755483495794`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 502.703232020238`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 497.701251286808`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 492.749041093256`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 487.846106214441`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 482.991956352786`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 478.186106089262`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 473.428074834835`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 468.717386782416`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 464.053570859277`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 459.436160679934`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 454.864694499525`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 450.338715167621`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 445.857770082518`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 441.421411145971`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 437.029194718392`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 432.680681574476`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 428.375436859286`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 424.113030044765`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 419.893034886674`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 415.715029381986`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 411.578595726666`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 407.483320273902`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 403.428793492735`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 399.41460992711`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 395.440368155324`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 391.505670749889`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 387.610124237784`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 383.753339061112`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 379.934929538142`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 376.154513824739`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 372.411713876182`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 368.706155409357`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 365.037467865329`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 361.405284372286`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 357.809241708853`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 354.248980267766`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 350.724144019914`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 347.234380478734`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 343.779340664966`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 340.358679071749`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 336.972053630071`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 333.619125674568`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 330.299559909649`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 327.013024375971`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 323.759190417243`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 320.537732647356`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 317.34832891785`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 314.190660285694`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 311.064410981393`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 307.969268377411`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 304.904922956909`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 301.87106828279`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 298.867400967061`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 295.893620640484`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 292.94942992255`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 290.034534391735`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 287.148642556054`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 284.291465823921`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 281.46271847528`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 278.66211763304`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 275.889383234783`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 273.144238004757`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 270.426407426153`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 267.735619713647`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 265.071605786227`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 262.434099240279`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 259.822836322951`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 257.237555905775`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 254.677999458555`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 252.143911023513`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 249.635037189694`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 247.151127067624`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 244.69193226422`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 242.257206857954`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 239.846707374255`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 237.460192761167`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 235.097424365239`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 232.758165907662`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 230.442183460642`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 228.149245424004`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 225.879122502033`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 223.631587680546`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 221.406416204187`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 219.203385553955`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 217.022275424948`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 214.862867704336`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 212.724946449547`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 210.608297866674`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 208.512710289096`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 206.437974156308`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 204.383881992968`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 202.350228388148`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 200.336809974792`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 198.343425409381`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 196.369875351799`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 194.415962445393`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 192.481491297246`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 190.56626845863`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 188.670102405666`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 186.792803520168`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 184.934184070684`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 183.094058193718`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 181.272241875151`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 179.468552931832`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 177.682810993364`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 175.914837484065`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 174.164455605111`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 172.431490316854`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 170.715768321323`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 169.017118044887`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 167.335369621104`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 165.67035487373`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 164.021907299902`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 162.389862053489`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 160.774055928607`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 159.174327343297`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 157.590516323367`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 156.022464486395`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 154.470015025891`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 152.933012695615`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 151.411303794053`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 149.904736149047`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 148.413159102577`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 146.936423495695`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 145.47438165361`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 144.02688737092`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 142.593795896989`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 141.174963921477`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 139.770249560003`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 138.379512339961`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 137.002613186469`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 135.639414408465`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 134.289779684936`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 132.953574051283`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 131.63066388583`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 130.320916896459`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 129.024202107378`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 127.740389846029`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 126.469351730115`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 125.210960654765`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 123.965090779824`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 122.731617517265`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 121.510417518735`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 120.301368663216`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 119.104350044814`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 117.919241960671`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 116.74592589899`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 115.584284527188`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 114.434201680159`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 113.29556234866`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 112.168252667809`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 111.052159905699`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 109.947172452124`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 108.853179807416`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 107.7700725714`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 106.697742432451`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 105.63608215666`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 104.584985577114`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 103.544347583281`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 102.514064110494`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 101.494032129546`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 100.484149636389`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 99.4843156419338`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 98.4944301619463`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 97.514394207054`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 96.5441097728447`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 95.5834798300663`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 94.6324083149241`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 93.6908001194741`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 92.7585610821118`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 91.8355979781567`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 90.9218185105295`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 90.0171313005218`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 89.1214458786587`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 88.2346726756515`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 87.3567230134411`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 86.4875090963295`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 85.6269440022007`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 84.774941673828`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 83.9314169102688`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 83.0962853583438`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 82.2694635042017`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 81.4508686649681`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 80.6404189804771`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 79.8380334050846`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 79.0436316995645`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 78.2571344230842`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 77.4784629252608`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 76.7075393382956`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 75.9442865691873`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 75.1886282920231`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 74.4404889403455`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 73.6997936995958`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 72.9664684996329`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 72.2404400073254`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 71.5216356192192`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 70.8099834542765`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 70.1054123466879`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 69.4078518387552`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 68.7172321738465`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 68.0334842894197`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 67.3565398101166`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 66.6863310409252`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 66.0227909604099`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 65.3658532140099`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 64.715452107403`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 64.0715225999366`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 63.4340002981233`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 62.8028214492017`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 62.1779229347609`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 61.5592422644286`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 60.9467175696222`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 60.340287597362`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 59.7398917041452`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 59.1454698498823`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 58.5569625918924`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 57.9743110789593`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 57.3974570454462`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 56.8263428054691`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 56.2609112471279`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 55.7011058267956`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 55.1468705634638`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 54.5981500331442`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 54.0548893633266`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 53.5170342274912`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 52.9845308396762`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 52.4573259490991`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 51.9353668348315`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 51.4186013005269`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 50.9069776692014`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 50.4004447780655`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 49.8989519734079`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 49.4024491055302`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 48.9108865237319`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 48.4242150713452`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 47.9423860808193`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 47.4653513688535`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 46.9930632315793`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 46.5254744397892`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 46.0625382342145`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 45.6042083208487`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 45.1504388663187`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 44.7011844933009`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 44.2564002759834`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 43.816041735574`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 43.3800648358516`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 42.948425978763`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 42.5210820000628`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 42.0979901649969`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 41.6791081640293`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 41.2643941086108`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 40.8538065269903`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 40.4473043600674`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 40.0448469572867`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 39.6463940725726`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 39.2519058603045`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 38.8613428713325`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 38.4746660490321`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 38.091836725399`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 37.7128166171818`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 37.3375678220537`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 36.9660528148225`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 36.598234443678`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 36.2340759264765`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 35.8735408470628`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 35.5165931516285`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 35.1631971451066`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 34.813317487602`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 34.4669191908574`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 34.1239676147544`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 33.7844284638495`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 33.4482677839449`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 33.1154519586923`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 32.7859477062319`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 32.4597220758638`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 32.1367424447532`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 31.8169765146677`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 31.500392308748`
## [1] 0.6247788
## 
## $performance$`beta -&gt; lambda: 31.1869581683094`
## [1] 0.6265487
## 
## $performance$`beta -&gt; lambda: 30.876642749677`
## [1] 0.6300885
## 
## $performance$`beta -&gt; lambda: 30.5694150210502`
## [1] 0.6353982
## 
## $performance$`beta -&gt; lambda: 30.2652442594001`
## [1] 0.6389381
## 
## $performance$`beta -&gt; lambda: 29.964100047397`
## [1] 0.6477876
## 
## $performance$`beta -&gt; lambda: 29.6659522703689`
## [1] 0.660177
## 
## $performance$`beta -&gt; lambda: 29.3707711132895`
## [1] 0.6690265
## 
## $performance$`beta -&gt; lambda: 29.0785270577971`
## [1] 0.6743363
## 
## $performance$`beta -&gt; lambda: 28.7891908792427`
## [1] 0.6884956
## 
## $performance$`beta -&gt; lambda: 28.5027336437673`
## [1] 0.699115
## 
## $performance$`beta -&gt; lambda: 28.2191267054086`
## [1] 0.7061947
## 
## $performance$`beta -&gt; lambda: 27.9383417032365`
## [1] 0.7132743
## 
## $performance$`beta -&gt; lambda: 27.6603505585168`
## [1] 0.7238938
## 
## $performance$`beta -&gt; lambda: 27.3851254719032`
## [1] 0.7274336
## 
## $performance$`beta -&gt; lambda: 27.1126389206579`
## [1] 0.7469027
## 
## $performance$`beta -&gt; lambda: 26.8428636558986`
## [1] 0.7646018
## 
## $performance$`beta -&gt; lambda: 26.575772699874`
## [1] 0.7752212
## 
## $performance$`beta -&gt; lambda: 26.3113393432659`
## [1] 0.7787611
## 
## $performance$`beta -&gt; lambda: 26.0495371425183`
## [1] 0.7876106
## 
## $performance$`beta -&gt; lambda: 25.7903399171931`
## [1] 0.7946903
## 
## $performance$`beta -&gt; lambda: 25.5337217473515`
## [1] 0.8070796
## 
## $performance$`beta -&gt; lambda: 25.2796569709629`
## [1] 0.8283186
## 
## $performance$`beta -&gt; lambda: 25.0281201813378`
## [1] 0.8353982
## 
## $performance$`beta -&gt; lambda: 24.7790862245877`
## [1] 0.8371681
## 
## $performance$`beta -&gt; lambda: 24.5325301971094`
## [1] 0.8424779
## 
## $performance$`beta -&gt; lambda: 24.2884274430945`
## [1] 0.8513274
## 
## $performance$`beta -&gt; lambda: 24.0467535520645`
## [1] 0.8637168
## 
## $performance$`beta -&gt; lambda: 23.8074843564287`
## [1] 0.8707965
## 
## $performance$`beta -&gt; lambda: 23.5705959290681`
## [1] 0.8761062
## 
## $performance$`beta -&gt; lambda: 23.3360645809427`
## [1] 0.8761062
## 
## $performance$`beta -&gt; lambda: 23.1038668587222`
## [1] 0.8831858
## 
## $performance$`beta -&gt; lambda: 22.8739795424408`
## [1] 0.8884956
## 
## $performance$`beta -&gt; lambda: 22.6463796431754`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 22.4210444007463`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 22.1979512814416`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 21.9770779757634`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 21.7584023961971`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 21.5419026750024`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 21.3275571620269`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 21.1153444225406`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 20.9052432350928`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 20.6972325893895`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 20.4912916841929`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 20.2873999252409`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 20.0855369231877`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 19.8856824915647`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 19.6878166447624`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 19.4919195960311`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 19.2979717555028`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 19.1059537282317`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 18.915846312255`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 18.7276304966729`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 18.5412874597469`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 18.3567985670179`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 18.1741453694431`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 17.9933096015503`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 17.8142731796122`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 17.6370181998373`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 17.46152693658`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 17.2877818405676`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 17.1157655371459`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 16.945460824541`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 16.7768506721399`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 16.6099182187867`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 16.4446467710971`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 16.2810198017884`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 16.1190209480276`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 15.958634009794`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 15.7998429482604`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 15.6426318841882`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 15.4869850963399`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 15.3328870199072`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 15.1803222449539`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 15.0292755148754`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 14.8797317248728`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 14.7316759204426`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 14.5850932958808`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 14.4399691928029`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 14.2962890986776`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 14.1540386453758`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 14.0132036077336`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 13.8737699021299`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 13.7357235850779`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 13.5990508518309`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 13.4637380350017`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 13.3297716031958`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 13.1971381596584`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 13.0658244409346`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.9358173155431`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.807103782663`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.6796709708339`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.5535061366682`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.4285966635775`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.3049300605104`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.1824939607035`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 12.0612761204447`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.9412644178491`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.8224468516464`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.7048115399809`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.5883467192234`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.4730407427948`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.3588820800015`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.2458593148818`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.1339611450653`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 11.0231763806416`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.913493943042`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.8049028639313`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.6973922841111`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.5909514524338`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.4855697247276`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.3812365627318`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.2779415330434`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.1756743060733`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 10.0744246550136`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.97418245481473`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.87493768117319`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.77668040952892`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.67940081407284`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.58308916676438`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.48773583635853`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.39333128744278`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.29986607948359`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.20733086588226`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.11571639304031`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 9.02501349943413`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.93521311469874`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.84630625872088`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.75828404074083`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.67113765846346`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.5848583971779`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.49943762888613`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.41486681144014`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.3311374876877`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.24824128462666`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.16616991256765`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.08491516430506`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 8.00446891429635`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.92482311784949`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.84596981031845`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.76790110630678`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.69060919887901`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.61408635877998`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.53832493366192`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.46331734731919`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.38905609893065`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.31553376230957`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.24274298516102`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.17067648834662`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.09932706515664`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 7.0286875805893`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.95875097063727`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.88951024158129`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.82095846929075`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.75308879853129`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.68589444227927`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.61936868104308`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.55350486219115`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.48829639928672`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.42373677142913`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.35981952260183`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.29653826102666`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.23388665852472`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.17185844988355`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.11044743223061`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 6.04964746441295`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.98945246638312`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.92985641859114`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.8708533613826`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.81243739440259`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.75460267600573`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.69734342267199`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.64065390842832`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.58452846427606`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.52896147762401`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.47394739172721`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.4194807051312`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.36555597112197`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.31216779718117`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.2593108444469`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.20697982717985`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.15516951223468`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.10387471853673`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.05309031656387`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 5.00281122783359`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.95303242439511`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.90374892832662`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.85495581123743`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.80664819377518`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.75882124513786`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.71147018259074`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.66459027098813`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.61817682229978`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.57222519514216`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.52673079431425`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.48168907033806`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.43709551900367`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.39294568091876`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.34923514106274`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.30595952834521`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.26311451516882`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.22069581699655`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.17869919192325`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.13712044025139`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.09595540407118`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.05519996684468`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 4.0148500529942`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.97490162749475`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.93535069547048`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.89619330179521`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.85742553069697`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.81904350536634`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.78104338756878`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.74342137726086`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.7061737122102`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.66929666761925`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.63278655575281`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.59663972556928`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.56085256235552`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.52542148736538`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.49034295746184`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.45561346476268`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.42122953628968`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.38718773362134`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.35348465254903`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.32011692273655`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.28708120738312`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.25437420288967`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.2219926385285`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.18993327611618`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.15819290968977`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.12676836518616`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.09565650012471`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.06485420329301`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.03435839443567`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 3.00416602394643`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.97427407256306`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.94467955106552`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.915379499977`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.88637098926796`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.85765111806317`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.82921701435156`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.80106583469908`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.7731947639643`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.74560101501692`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.71828182845905`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.69123447234926`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.66445624192942`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.63794445935415`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.61169647342312`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.58570965931585`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.55998141832927`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.53450917761785`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.5092903899363`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.48432253338482`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.45960311115695`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.43512965128988`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.41089970641721`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.38691085352428`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.36316069370579`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.33964685192599`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.31636697678109`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.29331874026418`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.27049983753241`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.24790798667647`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.22554092849247`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.20339642625594`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.1814722654982`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.15976625378491`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.13827622049682`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.11700001661267`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.09593551449437`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.07508060767412`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.05443321064389`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.03399125864675`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 2.01375270747048`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.99371553324308`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.97387773223045`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.95423732063594`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.93479233440203`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.9155408290139`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.89648087930495`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.87761057926434`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.85892804184634`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.84043139878164`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.82211880039051`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.80398841539786`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.78603843075007`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.76826705143374`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.7506725002961`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.7332530178674`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.71600686218486`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.69893230861855`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.68202764969889`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.66529119494589`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.64872127070013`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.63231621995538`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.61607440219289`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.59999419321736`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.58407398499448`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.56831218549017`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.55270721851134`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.53725752354828`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.52196155561863`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.50681778511285`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.49182469764127`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.47698079388264`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.46228458943423`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.44773461466333`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.43332941456034`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.41906754859326`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.40494759056359`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.39096812846378`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.37712776433596`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.36342511413218`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.349858807576`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.33642748802547`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.32312981233744`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.30996445073325`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.29693008666577`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.28402541668774`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.2712491503214`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.25860000992948`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.24607673058738`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.23367805995674`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.22140275816017`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.20924959765725`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.19721736312181`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.18530485132037`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.17351087099181`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.16183424272828`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.15027379885723`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.13882838332462`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.12749685157938`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.11627807045887`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.10517091807565`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.09417428370521`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.08328706767496`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.07250818125422`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.06183654654536`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.05127109637602`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.04081077419239`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.03045453395352`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.02020134002676`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1.01005016708417`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 1`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.990049833749168`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.980198673306756`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.970445533548509`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.960789439152324`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.951229424500715`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.941764533584248`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.932393819905948`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.923116346386636`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.913931185271228`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.90483741803596`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.895834135296529`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.886920436717158`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.878095430920562`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.869358235398805`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.860707976425057`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.852143788966211`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.843664816596384`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.835270211411272`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.826959133943363`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.818730753077982`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.810584245970188`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.802518797962478`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.794533602503334`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.786627861066553`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.778800783071405`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.771051585803566`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.763379494336853`
## [1] 0.899115
## 
## $performance$`beta -&gt; lambda: 0.755783741455726`
## [1] 0.900885
## 
## $performance$`beta -&gt; lambda: 0.748263567578566`
## [1] 0.9026549
## 
## $performance$`beta -&gt; lambda: 0.740818220681719`
## [1] 0.9044248
## 
## $performance$`beta -&gt; lambda: 0.733446956224289`
## [1] 0.9044248
## 
## $performance$`beta -&gt; lambda: 0.726149037073691`
## [1] 0.9044248
## 
## $performance$`beta -&gt; lambda: 0.718923733431926`
## [1] 0.9044248
## 
## $performance$`beta -&gt; lambda: 0.71177032276261`
## [1] 0.9044248
## 
## $performance$`beta -&gt; lambda: 0.704688089718714`
## [1] 0.9044248
## 
## $performance$`beta -&gt; lambda: 0.697676326071031`
## [1] 0.9221239
## 
## $performance$`beta -&gt; lambda: 0.690734330637355`
## [1] 0.9221239
## 
## $performance$`beta -&gt; lambda: 0.683861409212356`
## [1] 0.920354
## 
## $performance$`beta -&gt; lambda: 0.677056874498164`
## [1] 0.9238938
## 
## $performance$`beta -&gt; lambda: 0.670320046035639`
## [1] 0.9221239
## 
## $performance$`beta -&gt; lambda: 0.663650250136319`
## [1] 0.9221239
## 
## $performance$`beta -&gt; lambda: 0.657046819815057`
## [1] 0.9256637
## 
## $performance$`beta -&gt; lambda: 0.650509094723317`
## [1] 0.9238938
## 
## $performance$`beta -&gt; lambda: 0.644036421083142`
## [1] 0.9238938
## 
## $performance$`beta -&gt; lambda: 0.637628151621774`
## [1] 0.9221239
## 
## $performance$`beta -&gt; lambda: 0.631283645506927`
## [1] 0.9221239
## 
## $performance$`beta -&gt; lambda: 0.6250022682827`
## [1] 0.9238938
## 
## $performance$`beta -&gt; lambda: 0.618783391806141`
## [1] 0.9256637
## 
## $performance$`beta -&gt; lambda: 0.612626394184416`
## [1] 0.9256637
## 
## $performance$`beta -&gt; lambda: 0.606530659712633`
## [1] 0.9716814
## 
## $performance$`beta -&gt; lambda: 0.600495578812266`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.594520547970195`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.588604969678356`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.58274825237399`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.576949810380487`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.571209063848815`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.565525438699537`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.559898366565402`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.554327284734507`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.548811636094027`
## [1] 0.9628319
## 
## $performance$`beta -&gt; lambda: 0.5433508690745`
## [1] 0.9628319
## 
## $performance$`beta -&gt; lambda: 0.537944437594675`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.532591801006898`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.527292424043048`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.522045776761016`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.516851334491699`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.511708577786543`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.50661699236559`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.501576069066056`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.49658530379141`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.491644197460966`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.486752255959971`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.481908990090202`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.477113915521034`
## [1] 0.9681416
## 
## $performance$`beta -&gt; lambda: 0.472366552741015`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.467666427009909`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.463013068311228`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.458406011305224`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.453844795282356`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.449328964117222`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.444858066222941`
## [1] 0.9663717
## 
## $performance$`beta -&gt; lambda: 0.440431654505999`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.436049286321536`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.43171052342908`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.427414931948727`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.423162082317749`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.418951549247639`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.414782911681582`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.410655752752345`
## [1] 0.9646018
## 
## $performance$`beta -&gt; lambda: 0.406569659740599`
## [1] 0.9628319
## 
## $performance$`beta -&gt; lambda: 0.402524224033636`
## [1] 0.9628319
## 
## $performance$`beta -&gt; lambda: 0.398519041084514`
## [1] 0.9628319
## 
## $performance$`beta -&gt; lambda: 0.394553710371601`
## [1] 0.9628319
## 
## $performance$`beta -&gt; lambda: 0.390627835358521`
## [1] 0.9628319
## 
## $performance$`beta -&gt; lambda: 0.386741023454502`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.382892885975112`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.379083038103399`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.375311098851399`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.371576691022046`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.367879441171442`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.364218979571523`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.360594940173078`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.357006960569148`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.35345468195878`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.349937749111156`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.346455810330057`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.343008517418707`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.339595525644939`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.336216493706733`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.33287108369808`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.329558961075189`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.32627979462304`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.323033256422253`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.319819021816304`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.316636769379053`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.313486180882605`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.310366941265485`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.307278738601131`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.304221264066704`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.301194211912202`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.298197279429888`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.295230166924014`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.292292577680859`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.289384217939051`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.28650479686019`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.28365402649977`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.28083162177838`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.278037300453194`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.275270783089753`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.272531793034013`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.269820056384687`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.26713530196585`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.264477261299824`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.261845668580326`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.259240260645892`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.256660776953556`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.2541069595528`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.251578553059757`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.249075304631668`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.246596963941606`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.244143283153437`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.241714016897036`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.239308922243755`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.236927758682122`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.234570288093798`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.232236274729759`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.229925485186724`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.227637688383813`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.225372655539439`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.22313016014843`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.220909977959378`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.218711886952215`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.216535667316007`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.214381101426978`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.212247973826743`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.210136071200765`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.20804518235702`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.205975098204883`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.203925611734213`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.201896517994655`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.199887614075145`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.197898699083615`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.19592957412691`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.193980042290892`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.192049908620754`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.19013898010152`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.188247065638747`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.18637397603941`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.184519523992989`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.182683524052735`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.180865792617122`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.179066147911493`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.177284409969878`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.175520400616997`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.173773943450445`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.172044863823051`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.17033298882541`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.168638147268596`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.166960169667041`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.165298888221586`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.163654136802704`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.162025750933881`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.160413567775173`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.158817426106921`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.157237166313628`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.155672630367997`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.154123661815132`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.152590105756884`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.151071808836371`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.149568619222635`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.148080386595462`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.14660696213035`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.145148198483624`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.143703949777703`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.142274071586514`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.140858420921045`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.139456856215051`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.138069237310893`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.136695425445524`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.135335283236613`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.133988674668805`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.132655465080122`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.131335521148493`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.130028710878426`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.128734903587804`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.127453969894821`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.126185781705039`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.124930212198582`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.123687135817455`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.122456428252982`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.121237966433382`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.120031628511457`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.11883729385241`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.117654843021779`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.116484157773497`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.115325121038063`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.114177616910836`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.11304153064045`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.111916748617329`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.110803158362334`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.109700648515511`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.108609108824958`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.107528430135795`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.106458504379253`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.105399224561864`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.104350484754765`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.1033121800831`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.102284206715537`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.101266461853883`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.100258843722804`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.0992612515596457`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.0982735856043615`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.0972957470895328`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.096327638230493`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0953691622155497`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0944202231963024`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0934807262780585`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0925505775103433`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0916296838775049`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0907179532894126`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0898152945722476`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0889216174593863`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0880368325823726`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0871608514619813`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0862935864993705`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0854349509673212`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0845848590015647`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.083743225592196`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0829099665751727`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0820849986238988`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0812682392408917`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0804596067495325`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.079659020285898`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0788663997906749`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0780816660011532`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0773047404432998`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0765355454239115`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0757740040228455`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.075020040085327`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0742735782143339`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0735345437630571`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0728028628274356`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0720784622387661`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0713612695563861`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0706512130604296`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0699482217446554`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.069252225309346`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0685631541542779`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0678809393717615`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0672055127397498`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0665368067150169`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.065874754426403`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0652192896681276`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0645703468931685`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0639278612067076`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0632917683596407`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0626620047421532`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0620385073773583`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0614212139150001`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.060810062625218`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0602049923923736`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0596059427089393`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0590128536694478`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0584256659645008`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0578443208748385`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0572687602654674`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0566989265798469`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0561347628341337`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0555762126114831`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0550232200564073`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0544757298691899`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.053933687300356`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0533970381451971`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0528657287383504`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0523397059484324`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0518189171727258`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0513033103319191`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0507928338648985`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0502874367235919`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0497870683678639`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0492916787604622`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.048801218362013`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0483156381260678`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0478348894941984`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0473589243911409`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0468876952199885`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0464211548574313`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0459592566490442`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0455019544046216`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0450492023935578`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0446009553402746`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0441571684196929`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0437177972527509`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0432827979019659`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0428521268670402`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0424257410805114`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0420035979034456`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0415856551211732`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0411718709390678`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0407622039783662`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0403566132720311`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0399550582606539`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0395574987883987`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0391638950989871`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.038774207831722`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0383883980175521`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0380064270751743`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0376282568071762`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0372538493962158`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.03688316740124`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0365161737537404`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0361528317540464`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0357931050676553`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0354369577215986`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.035084354100845`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0347352589447386`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0343896373434727`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0340474547345993`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0337086768995724`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0333732699603261`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0330412003758869`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0327124349390198`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0323869407729071`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0320646853278608`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0317456363780679`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0314297620183677`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0311170306610609`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0308074110327511`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0305008721712175`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0301973834223185`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0298969144369263`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.029599435167892`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0293049158670407`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0290133270821971`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0287246396542394`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0284388247141845`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0281558536803001`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.027875698255247`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0275983304232493`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0273237224472926`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0270518468663504`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0267826764926382`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0265161844088942`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.026252343965688`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0259911287787554`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0257325127263599`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.025476469946681`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0252229748352272`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0249720020422762`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0247235264703394`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0244775232716527`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0242339678456911`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0239928358367092`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.023754103131305`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0235177458560091`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.023283740374897`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0230520632872256`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.022822691425093`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0225956018511219`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0223707718561656`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0221481789570373`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0219278008942616`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0217096156298486`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0214936013450899`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0212797364383772`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0210679995230414`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0208583694252147`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0206508251817126`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0204453460379377`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0202419114458044`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.020040501061684`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0198410947443703`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0196436725530653`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0194482147453854`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0192547017753869`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0190631142916116`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0188734331351515`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0186856393377328`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0184997141198192`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0183156388887342`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0181333952368011`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0179529649395029`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0177743299536594`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0175974724156234`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0174223746394935`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0172490191153463`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0170773885074848`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0169074656527053`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0167392335585806`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0165726754017613`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0164077745262926`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0162445144419499`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0160828788225884`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0159228515045117`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0157644164848545`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0156075579199828`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0154522601239095`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0152985075667255`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.015146284873047`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0149955768204777`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0148463683380868`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0146986445049018`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0145523905484161`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0144075918431123`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0142642339089993`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.014122302410164`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0139817831533383`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0138426620864795`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0137049252973649`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0135685590122009`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0134335495942453`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0132998835424438`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0131675474900798`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0130365282034377`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0129068125804799`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0127783876495358`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0126512405680053`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0125253586210744`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0124007292204434`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0122773399030684`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0121551783299149`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0120342322847238`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0119144896727896`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0117959385197516`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0116785669703954`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0115623632874685`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0114473158505057`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0113334131546674`
## [1] 0.9610619
## 
## $performance$`beta -&gt; lambda: 0.0112206438095891`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0111089965382423`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0109984601758069`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0108890236685545`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0107806760727431`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0106734065535229`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0105672043838527`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0104620589434268`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0103579597176137`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.010254896296404`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0101528583733698`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0100518357446336`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00995181830784842`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00985279606118726`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0097547591023429`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00965769762753778`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00956160193054351`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00946646240171032`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00937226952700606`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00927901388706474`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00918668615624467`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00909527710169582`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00900477758243656`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00891517854843955`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00882647103972673`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00873864618547329`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00865169520312063`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00856560939749806`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00848038015995327`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00839599896749147`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00831245738192312`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00822974704902003`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00814785969767999`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00806678713909961`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0079865212659555`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00790705405159344`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00782837754922577`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00775048389113669`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00767336528789549`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00759701402757757`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00752142247499327`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00744658307092434`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00737248833136801`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00729913084678858`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00722650328137646`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00715459837231459`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00708340892905212`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00701292783258542`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00694314803474611`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00687406255749626`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00680566449223054`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00673794699908547`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00667090330625527`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00660452670931481`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00653881057054906`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0064737483182894`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00640933344625638`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00634555951290912`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00628242014080112`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00621990901594257`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0061580198871689`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00609674656551564`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00603608292359956`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00597602289500594`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00591656047368186`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00585768971333562`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00579940472684215`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0057416996856542`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0056845688192196`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00562800641440407`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00557200681492`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00551656442076077`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00546167368764078`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00540732912644096`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00535352530265991`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0053002568358704`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00524751839918138`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00519530471870523`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00514361057303038`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00509243079269919`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00504176025969098`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00499159390691022`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00494192671767982`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00489275372523948`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00484407001224897`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00479587071029642`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00474815099941148`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00470090610758328`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00465413131028327`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00460782192999275`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0045619733357351`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00451658094261267`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00447164021134833`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00442714664783151`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00438309580266878`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0043394832707389`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00429630469075234`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00425355574481513`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00421123215799704`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00416932969790412`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00412784417425544`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00408677143846407`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0040461073832222`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00400584794209042`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00396598908909106`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00392652683830562`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00388745724347613`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00384877639761054`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00381048043259204`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00377256551879221`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00373502786468807`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00369786371648293`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00366106935773101`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00362464110896576`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00358857532733195`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00355286840622136`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00351751677491213`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00348251689821166`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00344786527610313`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00341355844339543`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00337959296937672`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00334596545747127`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00331267254489989`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00327971090234357`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00324707723361059`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00321476827530687`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00318278079650967`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00315111159844444`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00311975751416499`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00308871540823677`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00305798217642331`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00302755474537582`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00299743007232583`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00296760514478094`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00293807698022355`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00290884262581258`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00287989915808824`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00285124368267963`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00282287333401534`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00279478527503684`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00276697669691485`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00273944481876837`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00271218688738664`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00268520017695382`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00265848198877637`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0026320296510132`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0026058405184085`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00257991197202718`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.002554241418993`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00252882629222926`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0025036640502021`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00247875217666636`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00245408818041392`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0024296695950246`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00240549397861951`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00238155891361687`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00235786200649023`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00233440088752913`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00231117321060213`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00228817665292217`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00226540891481432`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0022428677194858`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0022205508127983`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00219845596304253`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00217658096071513`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00215492361829761`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00213348177003771`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00211225327173271`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00209123600051511`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00207042785464026`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00204982675327624`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00202943063629574`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00200923746407006`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00198924521726516`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0019694518966397`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00194985552284512`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00193045413622771`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00191124579663264`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00189222858320994`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00187340059422243`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0018547599468555`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00183630477702891`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00181803323921027`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00179994350623059`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00178203376910149`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00176430223683434`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00174674713626112`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00172936671185716`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00171215922556552`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00169512295662325`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00167825620138925`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00166155727317393`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00164502450207057`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00162865623478828`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00161245083448668`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00159640668061225`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00158052216873622`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00156479571039417`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00154922573292716`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00153381067932446`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00151854900806788`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00150343919297757`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00148847972305943`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.001473669102354`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00145900584978686`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00144448849902054`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00143011559830787`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0014158857103468`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00140179741213667`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00138784929483593`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00137403996362121`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00136036803754789`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00134683214941197`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00133343094561336`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0013201630860205`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00130702724383639`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00129402210546585`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00128114637038421`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00126839875100724`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00125577797256237`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00124328277296124`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00123091190267348`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00121866412460175`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00120653821395804`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00119453295814118`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00118264715661557`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00117087962079117`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00115922917390459`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00114769465090143`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00113627489831977`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00112496877417484`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0011137751478448`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0011026928999577`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00109172092227951`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00108085811760332`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0010701033996396`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00105945569290761`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00104891393262779`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00103847706461533`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00102814404517473`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00101791384099544`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00100778542904851`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000997757796484312`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000987829940531229`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000978000868395395`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000968269597161403`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000958635153694021`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000949096574540873`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000939652905836096`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000930303203204949`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000921046531669378`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000911881965554516`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000902808588396114`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000893825492848894`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000884931780595815`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000876126562258242`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000867408957307003`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000858778093974336`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000850233109166719`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000841773148378549`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000833397365606696`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000825104923265905`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000816894992105029`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000808766751124111`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000800719387492281`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000792752096466468`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000784864081310932`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000777054553217582`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000769322731227101`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000761667842150847`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000754089120493534`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00074658580837668`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00073915715546282`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000731802418880473`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000724520863149851`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000717311760109313`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000710174388842549`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000703108035606483`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000696111993759903`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000689185563692794`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000682328052756377`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000675538775193844`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000668817052071782`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000662162211212276`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000655573587125696`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000649050520944141`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000642592360355558`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000636198459538506`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000629868179097574`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000623600885999444`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000617395953509583`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000611252761129572`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000605170694535053`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000599149145514298`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000593187511907387`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000587285197545991`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000581441612193756`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000575656171487276`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00056992829687766`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000564257415572674`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000558642960479461`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000553084370147834`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000547581088714126`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000542132565845609`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000536738256685455`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000531397621798253`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000526110127116064`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000520875243885012`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000515692448612414`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000510561223014422`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.0005054810539642`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000500451433440611`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00049547185847741`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000490541831112951`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000485660858340389`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00048082845205838`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000476044129022269`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000471307410795765`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000466617823703098`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000461974898781651`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000457378171735063`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000452827182886797`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000448321477134178`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000443860603902874`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000439444117101845`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000435071575078732`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000430742540575688`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000426456580685654`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00042221326680907`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000418012174611013`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000413852883978762`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000409734978979787`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000405658047820157`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000401621682803358`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000397625480289526`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000393669040655078`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000389751968252755`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000385873871372051`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000382034362200047`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000378233056782626`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000374469574986078`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000370743540459088`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000367054580595098`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000363402326495048`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000359786412930483`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000356206478307034`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000352662164628256`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000349153117459826`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000345678985894105`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000342239422515039`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000338834083363426`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000335462627902512`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000332124718983941`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.00032882002281404`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000325548208920438`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000322308950119019`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000319101922481203`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000315926805301555`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000312783281065711`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000309671035418626`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000306589757133144`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000303539138078867`
## [1] 0.959292
## 
## $performance$`beta -&gt; lambda: 0.000300518873191348`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000297528660441581`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.0002945682008058`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000291637198235574`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000288735359628203`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000285862394797409`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000283018016444314`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000280201940128712`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000277413884240626`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000274653569972143`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000271920721289535`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000269215064905658`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000266536330252618`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000263884249454718`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000261258557301668`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000258658991222064`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000256085291257132`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00025353720003473`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000251014462743614`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000248516827107952`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000246044043362099`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000243595864225619`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000241172044878559`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000238772342936964`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000236396518428641`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000234044333769158`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00023171555373809`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000229409945455492`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000227127278358615`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000224867324178848`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000222629856918889`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000220414652830147`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000218221490390368`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000216050150281479`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000213900415367661`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000211772070673631`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000209664903363145`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000207578702717718`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000205513260115544`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000203468369010644`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000201443824912203`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000199439425364123`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000197454969924779`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000195490260146975`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000193545099558094`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000191619293640457`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000189712649811868`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000187824977406354`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000185956087655102`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000184105793667579`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000182273910412845`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000180460254701048`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000178664645165106`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000176886902242567`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000175126848157658`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000173384306903506`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00017165910422453`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000169951067599028`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000168260026221911`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000166585810987634`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000164928254473277`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000163287190921808`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000161662456225505`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000160053887909543`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000158461325115751`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000156884608586522`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00015532358064889`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000153778085198759`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000152247967685297`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000150733075095477`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000149233255938777`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000147748360232034`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000146278239484437`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000144822746682688`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000143381736276293`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000141955064163011`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000140542587674442`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000139144165561759`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000137759657981586`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000136388926482011`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000135031833988743`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.0001336882447914`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000132358024529944`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000131041040181239`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000129737160045754`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000128446253734388`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000127168192155434`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00012590284750167`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000124650093237575`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00012340980408668`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000122181856019035`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00012096612623881`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000119762493172015`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000118570836454339`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000117391036919118`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000116222976585415`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000115066538646224`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000113921607456786`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000112788068523029`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000111665808490115`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000110554715131105`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000109454677335737`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000108365585099315`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000107287329511708`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000106219802746459`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000105162898050001`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000104116509730984`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000103080533149705`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000102054864707641`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.000101039401837093`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0.00010003404299093`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.9038687632427e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.80532362252201e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.70775902233471e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.61116520613947e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.51553251447417e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.42085138398996e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.32711234649488e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.23430602800707e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.14242314781733e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 9.05145451756109e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.96139104029952e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.87222370960982e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.78394360868463e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.69654190944029e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.61000987163404e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.52433884198997e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.43952025333374e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.3555456237358e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.27240655566322e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.1900947351399e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.1086019309152e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 8.02791999364078e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.94804085505568e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.86895652717947e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.79065910151345e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.71314074824984e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.63639371548869e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.56041032846277e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.48518298877006e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.4107041736139e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.33696643505071e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.26396239924518e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.1916847657329e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.12012630669027e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 7.04927986621178e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.97913835959434e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.90969477262882e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.84094216089865e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.77287364908539e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.70548243028111e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.63876176530778e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.5727049820433e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.50730547475429e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.44255670343554e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.37845219315595e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.31498553341106e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.25215037748203e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.18994044180088e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.12834950532221e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.06737140890104e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 6.00700005467694e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.94722940546415e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.88805348414794e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.82946637308688e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.77146221352103e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.71403520498611e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.65717960473339e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.60088972715548e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.54515994321769e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.48998467989523e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.43535841961575e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.38127569970771e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.32773111185406e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.27471930155139e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.22223496757448e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.1702728614462e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.11882778691264e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.06789459942348e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 5.01746820561753e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.96754356281337e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.91811567850513e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.86917960986318e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.82073046323988e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.7727633936802e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.72527360443719e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.67825634649237e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.63170691808076e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.58562066422073e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 4.53999297624849e-05`
## [1] 0.9575221
## 
## $performance$`beta -&gt; lambda: 0`
## [1] 0.9575221</code></pre>
<pre class="r"><code># after select the best model by cv, we can use the best lambda here, and retrain a model
# this is an example to train a model and get the beta coefficients, and predicting response variable.

train_model = logit_lasso(x, y, lambda = 0.01, tol = 0.01, include_zero_lambda = FALSE)</code></pre>
<pre><code>## [1] &quot;step =  1  lambda =  0.01  loss:  8.94818107868982&quot;
## [1] &quot;step =  2  lambda =  0.01  loss:  7.54591761593958&quot;
## [1] &quot;step =  3  lambda =  0.01  loss:  7.16696350061689&quot;
## [1] &quot;step =  4  lambda =  0.01  loss:  7.03667494804914&quot;
## [1] &quot;step =  5  lambda =  0.01  loss:  7.01486995777148&quot;
## [1] &quot;step =  6  lambda =  0.01  loss:  7.04206096115444&quot;</code></pre>
<pre class="r"><code># the model consist of beta coefficients and other necessary variables for model.
train_model</code></pre>
<pre><code>## $lambda
## [1] 0.01
## 
## $beta
## $beta$`beta -&gt; lambda: 0.01`
##              [,1]
##  [1,]  -0.4182299
##  [2,]  18.2158710
##  [3,]  19.9678717
##  [4,]  21.5893474
##  [5,]  35.0899285
##  [6,]   2.7140873
##  [7,] -22.1534567
##  [8,]  40.6832661
##  [9,]  37.1832464
## [10,]   4.7942974
## [11,]  -9.3452703
## [12,]  66.7714990
## [13,] -19.2413731
## [14,]   1.9379077
## [15,]  18.1647256
## [16,]  -7.8421518
## [17,] -30.3500339
## [18,]   6.4217328
## [19,]   3.0614744
## [20,]  -2.7809971
## [21,]  -4.2470243
## [22,]  17.2954555
## [23,]  47.6841659
## [24,]  29.4927888
## [25,]   7.3433944
## [26,]  47.9050147
## [27,]  14.1252804
## [28,]  19.8038408
## [29,]  24.1633811
## [30,]  17.3378277
## [31,] -14.0976430
## 
## 
## $colmean
##             radius_mean            texture_mean          perimeter_mean 
##            1.412729e+01            1.928965e+01            9.196903e+01 
##               area_mean         smoothness_mean        compactness_mean 
##            6.548891e+02            9.636028e-02            1.043410e-01 
##          concavity_mean     concave.points_mean           symmetry_mean 
##            8.879932e-02            4.891915e-02            1.811619e-01 
##  fractal_dimension_mean               radius_se              texture_se 
##            6.279761e-02            4.051721e-01            1.216853e+00 
##            perimeter_se                 area_se           smoothness_se 
##            2.866059e+00            4.033708e+01            7.040979e-03 
##          compactness_se            concavity_se       concave.points_se 
##            2.547814e-02            3.189372e-02            1.179614e-02 
##             symmetry_se    fractal_dimension_se            radius_worst 
##            2.054230e-02            3.794904e-03            1.626919e+01 
##           texture_worst         perimeter_worst              area_worst 
##            2.567722e+01            1.072612e+02            8.805831e+02 
##        smoothness_worst       compactness_worst         concavity_worst 
##            1.323686e-01            2.542650e-01            2.721885e-01 
##    concave.points_worst          symmetry_worst fractal_dimension_worst 
##            1.146062e-01            2.900756e-01            8.394582e-02 
## 
## $colscale
##  [1] 8.398778e+01 1.025055e+02 5.791116e+02 8.387082e+03 3.351869e-01
##  [6] 1.258673e+00 1.899942e+00 9.247785e-01 6.533577e-01 1.682679e-01
## [11] 6.609125e+00 1.314730e+01 4.818636e+01 1.084176e+03 7.155826e-02
## [16] 4.268012e-01 7.194168e-01 1.470549e-01 1.970104e-01 6.306315e-02
## [21] 1.151894e+02 1.464822e+02 8.008410e+02 1.356934e+04 5.441596e-01
## [26] 3.749761e+00 4.972090e+00 1.566582e+00 1.474472e+00 4.304497e-01</code></pre>
<pre class="r"><code>predict_result = predict(train_model, x)

predict_result</code></pre>
<pre><code>## $`beta -&gt; lambda: 0.01`
##        [,1]
##   [1,]    1
##   [2,]    1
##   [3,]    1
##   [4,]    1
##   [5,]    1
##   [6,]    1
##   [7,]    1
##   [8,]    1
##   [9,]    1
##  [10,]    1
##  [11,]    1
##  [12,]    1
##  [13,]    1
##  [14,]    1
##  [15,]    1
##  [16,]    1
##  [17,]    1
##  [18,]    1
##  [19,]    1
##  [20,]    0
##  [21,]    0
##  [22,]    0
##  [23,]    1
##  [24,]    1
##  [25,]    1
##  [26,]    1
##  [27,]    1
##  [28,]    1
##  [29,]    1
##  [30,]    1
##  [31,]    1
##  [32,]    1
##  [33,]    1
##  [34,]    1
##  [35,]    1
##  [36,]    1
##  [37,]    1
##  [38,]    0
##  [39,]    1
##  [40,]    1
##  [41,]    0
##  [42,]    1
##  [43,]    1
##  [44,]    1
##  [45,]    1
##  [46,]    1
##  [47,]    0
##  [48,]    1
##  [49,]    0
##  [50,]    0
##  [51,]    0
##  [52,]    0
##  [53,]    0
##  [54,]    1
##  [55,]    1
##  [56,]    0
##  [57,]    1
##  [58,]    1
##  [59,]    0
##  [60,]    0
##  [61,]    0
##  [62,]    0
##  [63,]    1
##  [64,]    0
##  [65,]    1
##  [66,]    1
##  [67,]    0
##  [68,]    0
##  [69,]    0
##  [70,]    0
##  [71,]    1
##  [72,]    0
##  [73,]    1
##  [74,]    0
##  [75,]    0
##  [76,]    1
##  [77,]    0
##  [78,]    1
##  [79,]    1
##  [80,]    0
##  [81,]    0
##  [82,]    0
##  [83,]    1
##  [84,]    1
##  [85,]    0
##  [86,]    1
##  [87,]    1
##  [88,]    1
##  [89,]    0
##  [90,]    0
##  [91,]    0
##  [92,]    1
##  [93,]    0
##  [94,]    0
##  [95,]    1
##  [96,]    1
##  [97,]    0
##  [98,]    0
##  [99,]    0
## [100,]    1
## [101,]    1
## [102,]    0
## [103,]    0
## [104,]    0
## [105,]    0
## [106,]    1
## [107,]    0
## [108,]    0
## [109,]    1
## [110,]    0
## [111,]    0
## [112,]    0
## [113,]    0
## [114,]    0
## [115,]    0
## [116,]    0
## [117,]    0
## [118,]    1
## [119,]    1
## [120,]    1
## [121,]    0
## [122,]    1
## [123,]    1
## [124,]    0
## [125,]    0
## [126,]    0
## [127,]    1
## [128,]    1
## [129,]    0
## [130,]    1
## [131,]    0
## [132,]    1
## [133,]    1
## [134,]    0
## [135,]    1
## [136,]    0
## [137,]    0
## [138,]    0
## [139,]    1
## [140,]    0
## [141,]    0
## [142,]    1
## [143,]    0
## [144,]    0
## [145,]    0
## [146,]    0
## [147,]    1
## [148,]    0
## [149,]    0
## [150,]    0
## [151,]    0
## [152,]    0
## [153,]    0
## [154,]    0
## [155,]    0
## [156,]    0
## [157,]    1
## [158,]    0
## [159,]    0
## [160,]    0
## [161,]    0
## [162,]    1
## [163,]    1
## [164,]    0
## [165,]    1
## [166,]    0
## [167,]    0
## [168,]    1
## [169,]    1
## [170,]    0
## [171,]    0
## [172,]    1
## [173,]    1
## [174,]    0
## [175,]    0
## [176,]    0
## [177,]    0
## [178,]    1
## [179,]    0
## [180,]    0
## [181,]    1
## [182,]    1
## [183,]    1
## [184,]    0
## [185,]    1
## [186,]    0
## [187,]    1
## [188,]    0
## [189,]    0
## [190,]    0
## [191,]    1
## [192,]    0
## [193,]    0
## [194,]    1
## [195,]    1
## [196,]    0
## [197,]    1
## [198,]    1
## [199,]    1
## [200,]    1
## [201,]    0
## [202,]    1
## [203,]    1
## [204,]    1
## [205,]    0
## [206,]    1
## [207,]    0
## [208,]    1
## [209,]    0
## [210,]    0
## [211,]    1
## [212,]    0
## [213,]    1
## [214,]    1
## [215,]    1
## [216,]    0
## [217,]    0
## [218,]    0
## [219,]    1
## [220,]    1
## [221,]    0
## [222,]    0
## [223,]    0
## [224,]    1
## [225,]    0
## [226,]    0
## [227,]    0
## [228,]    0
## [229,]    0
## [230,]    1
## [231,]    1
## [232,]    0
## [233,]    0
## [234,]    1
## [235,]    0
## [236,]    0
## [237,]    1
## [238,]    1
## [239,]    0
## [240,]    1
## [241,]    0
## [242,]    0
## [243,]    0
## [244,]    0
## [245,]    1
## [246,]    0
## [247,]    0
## [248,]    0
## [249,]    0
## [250,]    0
## [251,]    1
## [252,]    0
## [253,]    1
## [254,]    1
## [255,]    1
## [256,]    1
## [257,]    1
## [258,]    1
## [259,]    1
## [260,]    1
## [261,]    1
## [262,]    1
## [263,]    1
## [264,]    0
## [265,]    1
## [266,]    1
## [267,]    0
## [268,]    0
## [269,]    0
## [270,]    0
## [271,]    0
## [272,]    0
## [273,]    1
## [274,]    0
## [275,]    1
## [276,]    0
## [277,]    0
## [278,]    1
## [279,]    0
## [280,]    0
## [281,]    1
## [282,]    0
## [283,]    1
## [284,]    1
## [285,]    0
## [286,]    0
## [287,]    0
## [288,]    0
## [289,]    0
## [290,]    0
## [291,]    0
## [292,]    0
## [293,]    0
## [294,]    0
## [295,]    0
## [296,]    0
## [297,]    0
## [298,]    0
## [299,]    0
## [300,]    0
## [301,]    1
## [302,]    0
## [303,]    1
## [304,]    0
## [305,]    0
## [306,]    0
## [307,]    0
## [308,]    0
## [309,]    0
## [310,]    0
## [311,]    0
## [312,]    0
## [313,]    0
## [314,]    0
## [315,]    0
## [316,]    0
## [317,]    0
## [318,]    1
## [319,]    0
## [320,]    0
## [321,]    0
## [322,]    1
## [323,]    0
## [324,]    1
## [325,]    0
## [326,]    0
## [327,]    0
## [328,]    0
## [329,]    1
## [330,]    1
## [331,]    1
## [332,]    0
## [333,]    0
## [334,]    0
## [335,]    0
## [336,]    1
## [337,]    0
## [338,]    1
## [339,]    0
## [340,]    1
## [341,]    0
## [342,]    0
## [343,]    0
## [344,]    1
## [345,]    0
## [346,]    0
## [347,]    0
## [348,]    0
## [349,]    0
## [350,]    0
## [351,]    0
## [352,]    1
## [353,]    1
## [354,]    1
## [355,]    0
## [356,]    0
## [357,]    0
## [358,]    0
## [359,]    0
## [360,]    0
## [361,]    0
## [362,]    0
## [363,]    0
## [364,]    1
## [365,]    0
## [366,]    1
## [367,]    1
## [368,]    0
## [369,]    1
## [370,]    1
## [371,]    1
## [372,]    0
## [373,]    1
## [374,]    1
## [375,]    0
## [376,]    0
## [377,]    0
## [378,]    0
## [379,]    0
## [380,]    1
## [381,]    0
## [382,]    0
## [383,]    0
## [384,]    0
## [385,]    0
## [386,]    1
## [387,]    0
## [388,]    0
## [389,]    0
## [390,]    1
## [391,]    0
## [392,]    0
## [393,]    1
## [394,]    1
## [395,]    0
## [396,]    0
## [397,]    0
## [398,]    0
## [399,]    0
## [400,]    0
## [401,]    1
## [402,]    0
## [403,]    0
## [404,]    0
## [405,]    0
## [406,]    0
## [407,]    0
## [408,]    0
## [409,]    1
## [410,]    0
## [411,]    0
## [412,]    0
## [413,]    0
## [414,]    0
## [415,]    1
## [416,]    0
## [417,]    0
## [418,]    1
## [419,]    0
## [420,]    0
## [421,]    0
## [422,]    0
## [423,]    0
## [424,]    0
## [425,]    0
## [426,]    0
## [427,]    0
## [428,]    0
## [429,]    0
## [430,]    0
## [431,]    1
## [432,]    0
## [433,]    1
## [434,]    1
## [435,]    0
## [436,]    1
## [437,]    0
## [438,]    0
## [439,]    0
## [440,]    0
## [441,]    0
## [442,]    1
## [443,]    0
## [444,]    0
## [445,]    1
## [446,]    0
## [447,]    1
## [448,]    0
## [449,]    0
## [450,]    1
## [451,]    0
## [452,]    1
## [453,]    0
## [454,]    0
## [455,]    0
## [456,]    0
## [457,]    0
## [458,]    0
## [459,]    0
## [460,]    0
## [461,]    1
## [462,]    1
## [463,]    0
## [464,]    0
## [465,]    0
## [466,]    0
## [467,]    0
## [468,]    0
## [469,]    1
## [470,]    0
## [471,]    0
## [472,]    0
## [473,]    0
## [474,]    0
## [475,]    0
## [476,]    0
## [477,]    0
## [478,]    0
## [479,]    0
## [480,]    1
## [481,]    0
## [482,]    0
## [483,]    0
## [484,]    0
## [485,]    0
## [486,]    0
## [487,]    0
## [488,]    1
## [489,]    0
## [490,]    1
## [491,]    0
## [492,]    0
## [493,]    1
## [494,]    0
## [495,]    0
## [496,]    0
## [497,]    0
## [498,]    0
## [499,]    1
## [500,]    1
## [501,]    0
## [502,]    1
## [503,]    0
## [504,]    1
## [505,]    0
## [506,]    0
## [507,]    0
## [508,]    0
## [509,]    0
## [510,]    1
## [511,]    0
## [512,]    0
## [513,]    1
## [514,]    0
## [515,]    1
## [516,]    0
## [517,]    1
## [518,]    1
## [519,]    0
## [520,]    0
## [521,]    0
## [522,]    1
## [523,]    0
## [524,]    0
## [525,]    0
## [526,]    0
## [527,]    0
## [528,]    0
## [529,]    0
## [530,]    0
## [531,]    0
## [532,]    0
## [533,]    0
## [534,]    1
## [535,]    0
## [536,]    1
## [537,]    1
## [538,]    0
## [539,]    0
## [540,]    0
## [541,]    0
## [542,]    1
## [543,]    0
## [544,]    0
## [545,]    0
## [546,]    0
## [547,]    0
## [548,]    0
## [549,]    0
## [550,]    0
## [551,]    0
## [552,]    0
## [553,]    0
## [554,]    0
## [555,]    0
## [556,]    0
## [557,]    0
## [558,]    0
## [559,]    0
## [560,]    0
## [561,]    0
## [562,]    0
## [563,]    1
## [564,]    1
## [565,]    1
## [566,]    1
## [567,]    1
## [568,]    1
## [569,]    0</code></pre>
</div>
<div id="comparing-logistic-lasso-data-with-actual-data" class="section level1">
<h1>Comparing logistic lasso data with actual data</h1>
<pre class="r"><code>rmse = function(x, y) {
  actual_diagnosis = cancer_data$diagnosis 
  actual_diagnosis = as.data.frame(recode(actual_diagnosis, &quot;B&quot; = 0, &quot;M&quot; = 1))
  
  i = 1
  lambda_vec = exp(seq(-5, 0, 0.05))
  rmse_vec = rep(0, length(lambda_vec))
  while (i &lt;= length(lambda_vec)) {
    lasso_lambda = logit_lasso(x, y, lambda_vec[i], tol = 0.01, include_zero_lambda = FALSE)
    predict_result = predict(lasso_lambda, x)
    rmse = (sum(actual_diagnosis - predict_result))^2 / 569
    rmse_vec[i] = rmse
    i = i + 1
  }
  return(rmse_vec)
}

rmse_vector = rmse(x, y)</code></pre>
<pre><code>## [1] &quot;step =  1  lambda =  0.00673794699908547  loss:  8.04329290848855&quot;
## [1] &quot;step =  2  lambda =  0.00673794699908547  loss:  6.52617354420321&quot;
## [1] &quot;step =  3  lambda =  0.00673794699908547  loss:  6.03544622957151&quot;
## [1] &quot;step =  4  lambda =  0.00673794699908547  loss:  5.8205369962749&quot;
## [1] &quot;step =  5  lambda =  0.00673794699908547  loss:  5.73791489999854&quot;
## [1] &quot;step =  6  lambda =  0.00673794699908547  loss:  5.71727619908277&quot;
## [1] &quot;step =  7  lambda =  0.00673794699908547  loss:  5.72640547087275&quot;
## [1] &quot;step =  1  lambda =  0.00708340892905212  loss:  8.13912880114802&quot;
## [1] &quot;step =  2  lambda =  0.00708340892905212  loss:  6.6341768845136&quot;
## [1] &quot;step =  3  lambda =  0.00708340892905212  loss:  6.15529411018227&quot;
## [1] &quot;step =  4  lambda =  0.00708340892905212  loss:  5.94935447789539&quot;
## [1] &quot;step =  5  lambda =  0.00708340892905212  loss:  5.87317905344978&quot;
## [1] &quot;step =  6  lambda =  0.00708340892905212  loss:  5.85760885396795&quot;
## [1] &quot;step =  7  lambda =  0.00708340892905212  loss:  5.87102679366124&quot;
## [1] &quot;step =  1  lambda =  0.00744658307092434  loss:  8.23987694656695&quot;
## [1] &quot;step =  2  lambda =  0.00744658307092434  loss:  6.7477153355016&quot;
## [1] &quot;step =  3  lambda =  0.00744658307092434  loss:  6.28128249400994&quot;
## [1] &quot;step =  4  lambda =  0.00744658307092434  loss:  6.08477030079679&quot;
## [1] &quot;step =  5  lambda =  0.00744658307092434  loss:  6.01537055677543&quot;
## [1] &quot;step =  6  lambda =  0.00744658307092434  loss:  6.00512791253619&quot;
## [1] &quot;step =  7  lambda =  0.00744658307092434  loss:  6.02305363235666&quot;
## [1] &quot;step =  1  lambda =  0.00782837754922577  loss:  8.34578905853949&quot;
## [1] &quot;step =  2  lambda =  0.00782837754922577  loss:  6.86707244259724&quot;
## [1] &quot;step =  3  lambda =  0.00782837754922577  loss:  6.41372576702043&quot;
## [1] &quot;step =  4  lambda =  0.00782837754922577  loss:  6.22712211006167&quot;
## [1] &quot;step =  5  lambda =  0.00782837754922577  loss:  6.1648437643894&quot;
## [1] &quot;step =  6  lambda =  0.00782837754922577  loss:  6.16020092613822&quot;
## [1] &quot;step =  7  lambda =  0.00782837754922577  loss:  6.18286469172397&quot;
## [1] &quot;step =  1  lambda =  0.00822974704902003  loss:  8.45712974149872&quot;
## [1] &quot;step =  2  lambda =  0.00822974704902003  loss:  6.99254626310585&quot;
## [1] &quot;step =  3  lambda =  0.00822974704902003  loss:  6.55295438742367&quot;
## [1] &quot;step =  4  lambda =  0.00822974704902003  loss:  6.37676479297566&quot;
## [1] &quot;step =  5  lambda =  0.00822974704902003  loss:  6.32197111293999&quot;
## [1] &quot;step =  6  lambda =  0.00822974704902003  loss:  6.32321419612689&quot;
## [1] &quot;step =  1  lambda =  0.00865169520312063  loss:  8.57417714985817&quot;
## [1] &quot;step =  2  lambda =  0.00865169520312063  loss:  7.12445010753944&quot;
## [1] &quot;step =  3  lambda =  0.00865169520312063  loss:  6.69931570481759&quot;
## [1] &quot;step =  4  lambda =  0.00865169520312063  loss:  6.53407135578746&quot;
## [1] &quot;step =  5  lambda =  0.00865169520312063  loss:  6.48714403939839&quot;
## [1] &quot;step =  6  lambda =  0.00865169520312063  loss:  6.49457372526615&quot;
## [1] &quot;step =  1  lambda =  0.00909527710169582  loss:  8.69722368099327&quot;
## [1] &quot;step =  2  lambda =  0.00909527710169582  loss:  7.26311331867305&quot;
## [1] &quot;step =  3  lambda =  0.00909527710169582  loss:  6.85317482081703&quot;
## [1] &quot;step =  4  lambda =  0.00909527710169582  loss:  6.69943384465928&quot;
## [1] &quot;step =  5  lambda =  0.00909527710169582  loss:  6.66077394527561&quot;
## [1] &quot;step =  6  lambda =  0.00909527710169582  loss:  6.6747062168828&quot;
## [1] &quot;step =  1  lambda =  0.0095616019305435  loss:  8.82657670356997&quot;
## [1] &quot;step =  2  lambda =  0.0095616019305435  loss:  7.40888209023065&quot;
## [1] &quot;step =  3  lambda =  0.0095616019305435  loss:  7.01491549324093&quot;
## [1] &quot;step =  4  lambda =  0.0095616019305435  loss:  6.87326431299381&quot;
## [1] &quot;step =  5  lambda =  0.0095616019305435  loss:  6.84329320923448&quot;
## [1] &quot;step =  6  lambda =  0.0095616019305435  loss:  6.86406012410094&quot;
## [1] &quot;step =  1  lambda =  0.0100518357446336  loss:  8.96255932301376&quot;
## [1] &quot;step =  2  lambda =  0.0100518357446336  loss:  7.56212032719957&quot;
## [1] &quot;step =  3  lambda =  0.0100518357446336  loss:  7.18494108603188&quot;
## [1] &quot;step =  4  lambda =  0.0100518357446336  loss:  7.05599583742734&quot;
## [1] &quot;step =  5  lambda =  0.0100518357446336  loss:  7.03515625046929&quot;
## [1] &quot;step =  6  lambda =  0.0100518357446336  loss:  7.06310675160762&quot;
## [1] &quot;step =  1  lambda =  0.0105672043838527  loss:  9.10551118600222&quot;
## [1] &quot;step =  2  lambda =  0.0105672043838527  loss:  7.7232105498716&quot;
## [1] &quot;step =  3  lambda =  0.0105672043838527  loss:  7.36367556718703&quot;
## [1] &quot;step =  4  lambda =  0.0105672043838527  loss:  7.24808358488688&quot;
## [1] &quot;step =  5  lambda =  0.0105672043838527  loss:  7.23684064533603&quot;
## [1] &quot;step =  6  lambda =  0.0105672043838527  loss:  7.27234141251169&quot;
## [1] &quot;step =  1  lambda =  0.0111089965382423  loss:  9.25578932595843&quot;
## [1] &quot;step =  2  lambda =  0.0111089965382423  loss:  7.89255484381279&quot;
## [1] &quot;step =  3  lambda =  0.0111089965382423  loss:  7.55156455708984&quot;
## [1] &quot;step =  4  lambda =  0.0111089965382423  loss:  7.45000593322205&quot;
## [1] &quot;step =  5  lambda =  0.0111089965382423  loss:  7.44884829983091&quot;
## [1] &quot;step =  6  lambda =  0.0111089965382423  loss:  7.49228464297539&quot;
## [1] &quot;step =  1  lambda =  0.0116785669703954  loss:  9.41376905162168&quot;
## [1] &quot;step =  2  lambda =  0.0116785669703954  loss:  8.07057585807246&quot;
## [1] &quot;step =  3  lambda =  0.0116785669703954  loss:  7.74907642974677&quot;
## [1] &quot;step =  4  lambda =  0.0116785669703954  loss:  7.66226564803993&quot;
## [1] &quot;step =  5  lambda =  0.0116785669703954  loss:  7.6717066806344&quot;
## [1] &quot;step =  1  lambda =  0.0122773399030684  loss:  9.57984488087557&quot;
## [1] &quot;step =  2  lambda =  0.0122773399030684  loss:  8.257717854056&quot;
## [1] &quot;step =  3  lambda =  0.0122773399030684  loss:  7.95670346955328&quot;
## [1] &quot;step =  4  lambda =  0.0122773399030684  loss:  7.88539111849333&quot;
## [1] &quot;step =  5  lambda =  0.0122773399030684  loss:  7.90597010756211&quot;
## [1] &quot;step =  1  lambda =  0.0129068125804799  loss:  9.7544315221224&quot;
## [1] &quot;step =  2  lambda =  0.0129068125804799  loss:  8.45444780760527&quot;
## [1] &quot;step =  3  lambda =  0.0129068125804799  loss:  8.17496308633878&quot;
## [1] &quot;step =  4  lambda =  0.0129068125804799  loss:  8.11993765490073&quot;
## [1] &quot;step =  5  lambda =  0.0129068125804799  loss:  8.15222111039288&quot;
## [1] &quot;step =  1  lambda =  0.0135685590122009  loss:  9.93796490560698&quot;
## [1] &quot;step =  2  lambda =  0.0135685590122009  loss:  8.66125656695518&quot;
## [1] &quot;step =  3  lambda =  0.0135685590122009  loss:  8.40439909157101&quot;
## [1] &quot;step =  4  lambda =  0.0135685590122009  loss:  8.36648885120803&quot;
## [1] &quot;step =  5  lambda =  0.0135685590122009  loss:  8.41107185317802&quot;
## [1] &quot;step =  1  lambda =  0.0142642339089993  loss:  10.1309032672123&quot;
## [1] &quot;step =  2  lambda =  0.0142642339089993  loss:  8.87866006936563&quot;
## [1] &quot;step =  3  lambda =  0.0142642339089993  loss:  8.64558303873704&quot;
## [1] &quot;step =  4  lambda =  0.0142642339089993  loss:  8.62565801544059&quot;
## [1] &quot;step =  5  lambda =  0.0142642339089993  loss:  8.68316562927494&quot;
## [1] &quot;step =  1  lambda =  0.0149955768204777  loss:  10.333728287375&quot;
## [1] &quot;step =  2  lambda =  0.0149955768204777  loss:  9.10720061936473&quot;
## [1] &quot;step =  3  lambda =  0.0149955768204777  loss:  8.89911563106009&quot;
## [1] &quot;step =  4  lambda =  0.0149955768204777  loss:  8.89808967143663&quot;
## [1] &quot;step =  5  lambda =  0.0149955768204777  loss:  8.96917843049164&quot;
## [1] &quot;step =  1  lambda =  0.0157644164848545  loss:  10.5469462878998&quot;
## [1] &quot;step =  2  lambda =  0.0157644164848545  loss:  9.34744823168234&quot;
## [1] &quot;step =  3  lambda =  0.0157644164848545  loss:  9.16562819985996&quot;
## [1] &quot;step =  4  lambda =  0.0157644164848545  loss:  9.1844611353019&quot;
## [1] &quot;step =  1  lambda =  0.0165726754017613  loss:  10.7710894895904&quot;
## [1] &quot;step =  2  lambda =  0.0165726754017613  loss:  9.60000204210243&quot;
## [1] &quot;step =  3  lambda =  0.0165726754017613  loss:  9.44578425701964&quot;
## [1] &quot;step =  4  lambda =  0.0165726754017613  loss:  9.48548417017932&quot;
## [1] &quot;step =  1  lambda =  0.0174223746394935  loss:  11.0067173337563&quot;
## [1] &quot;step =  2  lambda =  0.0174223746394935  loss:  9.86549178961902&quot;
## [1] &quot;step =  3  lambda =  0.0174223746394935  loss:  9.740281125181&quot;
## [1] &quot;step =  4  lambda =  0.0174223746394935  loss:  9.80190672308707&quot;
## [1] &quot;step =  1  lambda =  0.0183156388887342  loss:  11.25441787081&quot;
## [1] &quot;step =  2  lambda =  0.0183156388887342  loss:  10.1445793734443&quot;
## [1] &quot;step =  3  lambda =  0.0183156388887342  loss:  10.0498516494608&quot;
## [1] &quot;step =  4  lambda =  0.0183156388887342  loss:  10.1345147477439&quot;
## [1] &quot;step =  1  lambda =  0.0192547017753869  loss:  11.514809219322&quot;
## [1] &quot;step =  2  lambda =  0.0192547017753869  loss:  10.4379604885877&quot;
## [1] &quot;step =  3  lambda =  0.0192547017753869  loss:  10.375265994652&quot;
## [1] &quot;step =  4  lambda =  0.0192547017753869  loss:  10.4841341174712&quot;
## [1] &quot;step =  1  lambda =  0.0202419114458044  loss:  11.7885410990717&quot;
## [1] &quot;step =  2  lambda =  0.0202419114458044  loss:  10.7463663439041&quot;
## [1] &quot;step =  3  lambda =  0.0202419114458044  loss:  10.7173335320557&quot;
## [1] &quot;step =  4  lambda =  0.0202419114458044  loss:  10.8516326324383&quot;
## [1] &quot;step =  1  lambda =  0.0212797364383772  loss:  12.0762964418016&quot;
## [1] &quot;step =  2  lambda =  0.0212797364383772  loss:  11.0705654666945&quot;
## [1] &quot;step =  3  lambda =  0.0212797364383772  loss:  11.0769048202797&quot;
## [1] &quot;step =  1  lambda =  0.0223707718561656  loss:  12.3787930835661&quot;
## [1] &quot;step =  2  lambda =  0.0223707718561656  loss:  11.4113655981376&quot;
## [1] &quot;step =  3  lambda =  0.0223707718561656  loss:  11.4548736845312&quot;
## [1] &quot;step =  1  lambda =  0.0235177458560091  loss:  12.6967855427541&quot;
## [1] &quot;step =  2  lambda =  0.0235177458560091  loss:  11.7696156840341&quot;
## [1] &quot;step =  3  lambda =  0.0235177458560091  loss:  11.8521793991368&quot;
## [1] &quot;step =  1  lambda =  0.0247235264703394  loss:  13.0310668880656&quot;
## [1] &quot;step =  2  lambda =  0.0247235264703394  loss:  12.1462079655559&quot;
## [1] &quot;step =  3  lambda =  0.0247235264703394  loss:  12.2698089782315&quot;
## [1] &quot;step =  1  lambda =  0.0259911287787553  loss:  13.3824707009259&quot;
## [1] &quot;step =  2  lambda =  0.0259911287787553  loss:  12.5420801749147&quot;
## [1] &quot;step =  3  lambda =  0.0259911287787553  loss:  12.7087995797747&quot;
## [1] &quot;step =  1  lambda =  0.0273237224472926  loss:  13.7518731370432&quot;
## [1] &quot;step =  2  lambda =  0.0273237224472926  loss:  12.9582178410927&quot;
## [1] &quot;step =  3  lambda =  0.0273237224472926  loss:  13.1702410282792&quot;
## [1] &quot;step =  1  lambda =  0.0287246396542394  loss:  14.1401950920376&quot;
## [1] &quot;step =  2  lambda =  0.0287246396542394  loss:  13.3956567110186&quot;
## [1] &quot;step =  3  lambda =  0.0287246396542394  loss:  13.655278461866&quot;
## [1] &quot;step =  1  lambda =  0.0301973834223185  loss:  14.5484044763087&quot;
## [1] &quot;step =  2  lambda =  0.0301973834223185  loss:  13.8554852918231&quot;
## [1] &quot;step =  3  lambda =  0.0301973834223185  loss:  14.165115109504&quot;
## [1] &quot;step =  1  lambda =  0.0317456363780679  loss:  14.9775186045564&quot;
## [1] &quot;step =  2  lambda =  0.0317456363780679  loss:  14.3388475200621&quot;
## [1] &quot;step =  3  lambda =  0.0317456363780679  loss:  14.7010152045343&quot;
## [1] &quot;step =  1  lambda =  0.0333732699603261  loss:  15.4286067056265&quot;
## [1] &quot;step =  2  lambda =  0.0333732699603261  loss:  14.8469455640731&quot;
## [1] &quot;step =  3  lambda =  0.0333732699603261  loss:  15.2643070408379&quot;
## [1] &quot;step =  1  lambda =  0.035084354100845  loss:  15.9027925586248&quot;
## [1] &quot;step =  2  lambda =  0.035084354100845  loss:  15.3810427659042&quot;
## [1] &quot;step =  3  lambda =  0.035084354100845  loss:  15.8563861782653&quot;
## [1] &quot;step =  1  lambda =  0.03688316740124  loss:  16.4012572615202&quot;
## [1] &quot;step =  2  lambda =  0.03688316740124  loss:  15.9424667295499&quot;
## [1] &quot;step =  3  lambda =  0.03688316740124  loss:  16.478684900356&quot;
## [1] &quot;step =  1  lambda =  0.038774207831722  loss:  16.9252421387569&quot;
## [1] &quot;step =  2  lambda =  0.038774207831722  loss:  16.5326125625288&quot;
## [1] &quot;step =  3  lambda =  0.038774207831722  loss:  17.1326393962941&quot;
## [1] &quot;step =  1  lambda =  0.0407622039783662  loss:  17.4760517946949&quot;
## [1] &quot;step =  2  lambda =  0.0407622039783662  loss:  17.1529462781517&quot;
## [1] &quot;step =  3  lambda =  0.0407622039783662  loss:  17.8199626747399&quot;
## [1] &quot;step =  1  lambda =  0.0428521268670402  loss:  18.055057320022&quot;
## [1] &quot;step =  2  lambda =  0.0428521268670402  loss:  17.8050083661533&quot;
## [1] &quot;step =  3  lambda =  0.0428521268670402  loss:  18.5423488190692&quot;
## [1] &quot;step =  1  lambda =  0.0450492023935578  loss:  18.6636996586072&quot;
## [1] &quot;step =  2  lambda =  0.0450492023935578  loss:  18.4904175396974&quot;
## [1] &quot;step =  3  lambda =  0.0450492023935578  loss:  19.3015770186289&quot;
## [1] &quot;step =  1  lambda =  0.0473589243911409  loss:  19.3034931426163&quot;
## [1] &quot;step =  2  lambda =  0.0473589243911409  loss:  19.2108746671126&quot;
## [1] &quot;step =  3  lambda =  0.0473589243911409  loss:  20.0995157499822&quot;
## [1] &quot;step =  1  lambda =  0.0497870683678639  loss:  19.9760292040633&quot;
## [1] &quot;step =  2  lambda =  0.0497870683678639  loss:  19.9681668970758&quot;
## [1] &quot;step =  3  lambda =  0.0497870683678639  loss:  20.9381271536854&quot;
## [1] &quot;step =  1  lambda =  0.0523397059484324  loss:  20.6829802713498&quot;
## [1] &quot;step =  2  lambda =  0.0523397059484324  loss:  20.764171986329&quot;
## [1] &quot;step =  1  lambda =  0.0550232200564072  loss:  21.4261038597279&quot;
## [1] &quot;step =  2  lambda =  0.0550232200564072  loss:  21.6008628394033&quot;
## [1] &quot;step =  1  lambda =  0.0578443208748385  loss:  22.20724686503&quot;
## [1] &quot;step =  2  lambda =  0.0578443208748385  loss:  22.480312270211&quot;
## [1] &quot;step =  1  lambda =  0.060810062625218  loss:  23.0283500704197&quot;
## [1] &quot;step =  2  lambda =  0.060810062625218  loss:  23.4046979957774&quot;
## [1] &quot;step =  1  lambda =  0.0639278612067076  loss:  23.891452876358&quot;
## [1] &quot;step =  2  lambda =  0.0639278612067076  loss:  24.3763078727988&quot;
## [1] &quot;step =  1  lambda =  0.0672055127397498  loss:  24.7986982644225&quot;
## [1] &quot;step =  2  lambda =  0.0672055127397498  loss:  25.3975453881391&quot;
## [1] &quot;step =  1  lambda =  0.0706512130604296  loss:  25.7523380060843&quot;
## [1] &quot;step =  2  lambda =  0.0706512130604296  loss:  26.4709354148183&quot;
## [1] &quot;step =  1  lambda =  0.0742735782143339  loss:  26.754738128026&quot;
## [1] &quot;step =  2  lambda =  0.0742735782143339  loss:  27.5991302454891&quot;
## [1] &quot;step =  1  lambda =  0.0780816660011532  loss:  27.8083846460805&quot;
## [1] &quot;step =  2  lambda =  0.0780816660011532  loss:  28.7849159158608&quot;
## [1] &quot;step =  1  lambda =  0.0820849986238988  loss:  28.9158895803826&quot;
## [1] &quot;step =  2  lambda =  0.0820849986238988  loss:  30.0312188309884&quot;
## [1] &quot;step =  1  lambda =  0.0862935864993705  loss:  30.0799972648522&quot;
## [1] &quot;step =  2  lambda =  0.0862935864993705  loss:  31.3411127078236&quot;
## [1] &quot;step =  1  lambda =  0.0907179532894125  loss:  31.3035909646715&quot;
## [1] &quot;step =  2  lambda =  0.0907179532894125  loss:  32.7178258479007&quot;
## [1] &quot;step =  1  lambda =  0.0953691622155497  loss:  32.5896998159766&quot;
## [1] &quot;step =  2  lambda =  0.0953691622155497  loss:  34.1647487545171&quot;
## [1] &quot;step =  1  lambda =  0.100258843722804  loss:  33.9415061025592&quot;
## [1] &quot;step =  2  lambda =  0.100258843722804  loss:  35.6854421092576&quot;
## [1] &quot;step =  1  lambda =  0.105399224561864  loss:  35.36235288496&quot;
## [1] &quot;step =  2  lambda =  0.105399224561864  loss:  37.2836451232056&quot;
## [1] &quot;step =  1  lambda =  0.110803158362334  loss:  36.8557519979396&quot;
## [1] &quot;step =  2  lambda =  0.110803158362334  loss:  38.9632842786742&quot;
## [1] &quot;step =  1  lambda =  0.116484157773497  loss:  38.4253924329284&quot;
## [1] &quot;step =  2  lambda =  0.116484157773497  loss:  40.7284824777897&quot;
## [1] &quot;step =  1  lambda =  0.122456428252982  loss:  40.0751491226834&quot;
## [1] &quot;step =  2  lambda =  0.122456428252982  loss:  42.5835686147443&quot;
## [1] &quot;step =  1  lambda =  0.128734903587804  loss:  41.809092146023&quot;
## [1] &quot;step =  2  lambda =  0.128734903587804  loss:  44.5330875890255&quot;
## [1] &quot;step =  1  lambda =  0.135335283236613  loss:  43.6314963711575&quot;
## [1] &quot;step =  2  lambda =  0.135335283236613  loss:  46.5818107774045&quot;
## [1] &quot;step =  1  lambda =  0.142274071586514  loss:  45.5468515567967&quot;
## [1] &quot;step =  2  lambda =  0.142274071586514  loss:  48.7347469829401&quot;
## [1] &quot;step =  1  lambda =  0.149568619222635  loss:  47.5598729308805&quot;
## [1] &quot;step =  2  lambda =  0.149568619222635  loss:  50.9971538797039&quot;
## [1] &quot;step =  1  lambda =  0.157237166313628  loss:  49.6755122674522&quot;
## [1] &quot;step =  2  lambda =  0.157237166313628  loss:  53.3745499723811&quot;
## [1] &quot;step =  1  lambda =  0.165298888221587  loss:  51.8989694828751&quot;
## [1] &quot;step =  2  lambda =  0.165298888221587  loss:  55.8727270903164&quot;
## [1] &quot;step =  1  lambda =  0.173773943450445  loss:  54.2357047732675&quot;
## [1] &quot;step =  2  lambda =  0.173773943450445  loss:  58.49776343598&quot;
## [1] &quot;step =  1  lambda =  0.182683524052735  loss:  56.6914513157185&quot;
## [1] &quot;step =  2  lambda =  0.182683524052735  loss:  61.2560372081994&quot;
## [1] &quot;step =  1  lambda =  0.192049908620754  loss:  59.2722285565181&quot;
## [1] &quot;step =  2  lambda =  0.192049908620754  loss:  64.1542408208501&quot;
## [1] &quot;step =  1  lambda =  0.201896517994655  loss:  61.9843561103116&quot;
## [1] &quot;step =  2  lambda =  0.201896517994655  loss:  67.1993957380095&quot;
## [1] &quot;step =  1  lambda =  0.212247973826743  loss:  64.8344682947542&quot;
## [1] &quot;step =  2  lambda =  0.212247973826743  loss:  70.3988679468557&quot;
## [1] &quot;step =  1  lambda =  0.22313016014843  loss:  67.8295293258925&quot;
## [1] &quot;step =  2  lambda =  0.22313016014843  loss:  73.7603840898278&quot;
## [1] &quot;step =  1  lambda =  0.234570288093798  loss:  70.9768492001436&quot;
## [1] &quot;step =  2  lambda =  0.234570288093798  loss:  77.2920482777573&quot;
## [1] &quot;step =  1  lambda =  0.246596963941606  loss:  74.2841002893655&quot;
## [1] &quot;step =  2  lambda =  0.246596963941606  loss:  81.0023596058256&quot;
## [1] &quot;step =  1  lambda =  0.259240260645892  loss:  77.7593346761138&quot;
## [1] &quot;step =  2  lambda =  0.259240260645892  loss:  84.9002303943019&quot;
## [1] &quot;step =  1  lambda =  0.272531793034013  loss:  81.4110022567642&quot;
## [1] &quot;step =  2  lambda =  0.272531793034013  loss:  88.9950051760556&quot;
## [1] &quot;step =  1  lambda =  0.28650479686019  loss:  85.2479696407289&quot;
## [1] &quot;step =  2  lambda =  0.28650479686019  loss:  93.2964804528352&quot;
## [1] &quot;step =  1  lambda =  0.301194211912202  loss:  89.2795398745235&quot;
## [1] &quot;step =  2  lambda =  0.301194211912202  loss:  97.814925242239&quot;
## [1] &quot;step =  1  lambda =  0.316636769379053  loss:  93.5154730199283&quot;
## [1] &quot;step =  2  lambda =  0.316636769379053  loss:  102.56110243719&quot;
## [1] &quot;step =  1  lambda =  0.33287108369808  loss:  97.9660076159487&quot;
## [1] &quot;step =  2  lambda =  0.33287108369808  loss:  107.546290999565&quot;
## [1] &quot;step =  1  lambda =  0.349937749111155  loss:  102.641883054696&quot;
## [1] &quot;step =  2  lambda =  0.349937749111155  loss:  112.782309009405&quot;
## [1] &quot;step =  1  lambda =  0.367879441171442  loss:  107.554362901692&quot;
## [1] &quot;step =  2  lambda =  0.367879441171442  loss:  118.281537590894&quot;
## [1] &quot;step =  1  lambda =  0.386741023454501  loss:  112.715259191451&quot;
## [1] &quot;step =  2  lambda =  0.386741023454501  loss:  124.056945735995&quot;
## [1] &quot;step =  1  lambda =  0.406569659740599  loss:  118.136957729486&quot;
## [1] &quot;step =  2  lambda =  0.406569659740599  loss:  130.122116046341&quot;
## [1] &quot;step =  1  lambda =  0.427414931948727  loss:  123.832444432164&quot;
## [1] &quot;step =  2  lambda =  0.427414931948727  loss:  136.49127141364&quot;
## [1] &quot;step =  1  lambda =  0.449328964117222  loss:  129.81533273609&quot;
## [1] &quot;step =  2  lambda =  0.449328964117222  loss:  143.179302658598&quot;
## [1] &quot;step =  1  lambda =  0.472366552741015  loss:  136.099892108865&quot;
## [1] &quot;step =  2  lambda =  0.472366552741015  loss:  150.201797148083&quot;
## [1] &quot;step =  1  lambda =  0.496585303791409  loss:  142.701077693321&quot;
## [1] &quot;step =  2  lambda =  0.496585303791409  loss:  157.575068410063&quot;
## [1] &quot;step =  1  lambda =  0.522045776761016  loss:  149.63456111746&quot;
## [1] &quot;step =  2  lambda =  0.522045776761016  loss:  165.316186765831&quot;
## [1] &quot;step =  1  lambda =  0.548811636094027  loss:  156.916762502564&quot;
## [1] &quot;step =  2  lambda =  0.548811636094027  loss:  173.443010999059&quot;
## [1] &quot;step =  1  lambda =  0.576949810380487  loss:  164.564883702125&quot;
## [1] &quot;step =  2  lambda =  0.576949810380487  loss:  181.974221081588&quot;
## [1] &quot;step =  1  lambda =  0.606530659712633  loss:  172.596942804512&quot;
## [1] &quot;step =  2  lambda =  0.606530659712633  loss:  190.931970825532&quot;
## [1] &quot;step =  1  lambda =  0.637628151621773  loss:  181.031809932611&quot;
## [1] &quot;step =  2  lambda =  0.637628151621773  loss:  200.335652891735&quot;
## [1] &quot;step =  1  lambda =  0.67032004603564  loss:  189.88050448363&quot;
## [1] &quot;step =  2  lambda =  0.67032004603564  loss:  210.214502707777&quot;
## [1] &quot;step =  1  lambda =  0.704688089718714  loss:  199.168489852346&quot;
## [1] &quot;step =  2  lambda =  0.704688089718714  loss:  220.58585984664&quot;
## [1] &quot;step =  1  lambda =  0.740818220681718  loss:  208.919306461682&quot;
## [1] &quot;step =  2  lambda =  0.740818220681718  loss:  231.431923053934&quot;
## [1] &quot;step =  1  lambda =  0.778800783071405  loss:  219.155335137169&quot;
## [1] &quot;step =  2  lambda =  0.778800783071405  loss:  242.78975779555&quot;
## [1] &quot;step =  1  lambda =  0.818730753077982  loss:  229.899971284962&quot;
## [1] &quot;step =  2  lambda =  0.818730753077982  loss:  254.701496593553&quot;
## [1] &quot;step =  1  lambda =  0.860707976425058  loss:  241.177664305522&quot;
## [1] &quot;step =  2  lambda =  0.860707976425058  loss:  267.192767515863&quot;
## [1] &quot;step =  1  lambda =  0.90483741803596  loss:  253.013957905544&quot;
## [1] &quot;step =  2  lambda =  0.90483741803596  loss:  280.290252904607&quot;
## [1] &quot;step =  1  lambda =  0.951229424500714  loss:  265.435531264113&quot;
## [1] &quot;step =  2  lambda =  0.951229424500714  loss:  294.021720542266&quot;
## [1] &quot;step =  1  lambda =  1  loss:  278.470241000535&quot;
## [1] &quot;step =  2  lambda =  1  loss:  308.416054609187&quot;</code></pre>
<pre class="r"><code>lambda_vector = exp(seq(-5, 0, 0.05))

plot(lambda_vector, rmse_vector)</code></pre>
<p><img src="data:image/png;base64,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" width="672" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
